token_context,word_context,seg_context,sent_cotext,label
['on neural constituency parsing  #TAUTHOR_TAG has found multiple'],['on neural constituency parsing  #TAUTHOR_TAG has found multiple'],['work on neural constituency parsing  #TAUTHOR_TAG has found multiple cases where generative scoring models for which inference is'],"['work on neural constituency parsing  #TAUTHOR_TAG has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.', '']",5
"['current constituent.', 'we refer to  #TAUTHOR_TAG for a complete description of these actions, and the constraints on them necessary to ensure valid parse']","['current constituent.', 'we refer to  #TAUTHOR_TAG for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.', '1 the primary difference between the actions in the discriminative and generative models is that, whereas']","['##s the current constituent.', 'we refer to  #TAUTHOR_TAG for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.', '1 the primary difference between the actions in the discriminative and generative models is that, whereas']","['', 'we refer to  #TAUTHOR_TAG for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.', '1 the primary difference between the actions in the discriminative and generative models is that, whereas the discriminative model uses a shift action which is fixed to produce the next word in the sentence, the generative models use gen ( w ) to define a distribution over all possible words w in the lexicon.', ""this stems from the generative model's definition of a joint probability p ( x, y ) over all possible sentences x and parses y. to use a generative model as a parser, we are interested in finding the maximum probability parse for a given sentence."", 'this is made more complicated by not having an explicit representation for p ( y | x ), as we do in the discriminative setting.', 'however, we can start by applying similar approximate search procedures as are used for the discriminative parser, constraining the set of actions such that it is only possible to produce the observed sentence : i. e. only allow a gen ( w ) action when w is the next terminal in the sentence, and prohibit gen actions if all terminals have been produced']",5
"[') models, following  #TAUTHOR_TAG by using the']","['( rg ) models, following  #TAUTHOR_TAG by using the']","[') models, following  #TAUTHOR_TAG by using the same hyperparameter settings, and using pretrained word embeddings']","['the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments.', 'our base experiments are performed on the penn treebank  #AUTHOR_TAG, using sections 2 - 21 for training, section 22 for development, and section 23 for testing.', 'for the lstm generative model ( lm ), we use the pre - trained model released by  #AUTHOR_TAG.', 'we train rnng discriminative ( rd ) and generative ( rg ) models, following  #TAUTHOR_TAG by using the same hyperparameter settings, and using pretrained word embeddings from  #AUTHOR_TAG for the discriminative model.', 'the automaticallypredicted part - of - speech tags we use as input for rd are the same as those used by  #AUTHOR_TAG.', 'in each experiment, we obtain a set of candidate parses for each sentence by performing beam search in one or more parsers.', 'we use actionsynchronous beam search ( section 2. 1 ) with beam size k = 100 for rd and word - synchronous beam ( section 2. 2 ) with k w = 100 and k a = 1000 for the generative models rg and lm.', '']",5
['##s  #TAUTHOR_TAG'],['treebank - based parsers  #TAUTHOR_TAG'],['- based parsers  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
['parser  #TAUTHOR_TAG'],['parser  #TAUTHOR_TAG'],[' #TAUTHOR_TAG should add the aux category to'],"['from accuracy, there are several other ways that automatic and manual annotation differs.', 'for penn - treebank ( ptb ) parsing, for example, most parsers ( not all ) leave out function tags and empty categories.', 'consistency is an important goal for manual annotation for many reasons including : ( 1 ) in the absence of a clear correct answer, consistency helps clarify measures of annotation quality ( inter - annotator agreement scores ) ; and ( 2 ) consistent annotation is better training data for machine learning.', 'thus, annotation specifications use defaults to ensure the consistent handling of spurious ambiguity.', 'for example, given a sentence like i bought three acres of land in california, the pp in california can be attached to either acres or land with no difference in meaning.', 'while annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e. g., the probability that land is modified by a pp ( headed by in ) versus the probability that acres can be so modified.', 'even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs.', ""for example, users of the charniak parser  #TAUTHOR_TAG should add the aux category to the ptb parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens is andn't."", 'similarly, tokenization decisions with respect to hyphens vary among different versions of the penn treebank, as well as different parsers based on these treebanks.', 'thus if a system uses multiple parsers, such differences must be accounted for.', 'differences that are not important for a particular application should be ignored ( e. g., by merging alternative analyses ).', 'for example, in the case of spurious attachment ambiguity, a system may need to either accept both as right answers or derive a common representation for both.', 'of course, many of the particular problems that result from spurious ambiguity can be accounted for in hind sight.', 'nevertheless, it is precisely this lack of a controlled environment which adds elements of spurious ambiguity.', 'using new processors or training on new treebanks can bring new instances of spurious ambiguity']",0
"['##s  #TAUTHOR_TAG, jet']","['knp parsers  #TAUTHOR_TAG, jet']","['knp parsers  #TAUTHOR_TAG,']","['use charniak, umd and knp parsers  #TAUTHOR_TAG, jet named entity tagger  #AUTHOR_TAG and other resources in conjunction with languagespecific glarfers that incorporate hand - written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper.', 'english glarfer rules use comlex  #AUTHOR_TAG a ) and the various nombank lexicons ( http : / / nlp. cs. nyu. edu / meyers / nombank / ) for lexical lookup.', 'the glarf rules implemented vary by language as follows.', 'english : correcting / standardizing phrase boundaries and part of speech ( pos ) ; recognizing multiword expressions ; marking subconstituents ; labeling relations ; incorporating nes ; regularizing infinitival, passives, relatives, vp deletion, predicative and numerous other constructions.', 'chinese : correcting / standardizing phrase boundaries and pos, marking subconstituents, labeling relations ; regularizing copula constructions ; incorporating nes ; recognizing dates and number expressions.', 'japanese : converting to ptb format ; correcting / standardizing phrase boundaries and pos ; labeling relations ; processing nes, double quote constructions, number phrases, common idioms, light verbs and copula constructions']",5
"['4,  #TAUTHOR_TAG 6 ]']","['proved useful in the previous works [ 4,  #TAUTHOR_TAG 6 ]']","['4,  #TAUTHOR_TAG 6 ].', 'how to']","['', 'learning knowledge from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4,  #TAUTHOR_TAG 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts']",0
"['4,  #TAUTHOR_TAG 6 ].', 'how to extract useful information from unann']","['is compulsory and proved useful in the previous works [ 4,  #TAUTHOR_TAG 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be']","['4,  #TAUTHOR_TAG 6 ].', 'how to extract useful information from unann']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4,  #TAUTHOR_TAG 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', '']",0
['2007 task 14  #TAUTHOR_TAG comprises 1000 english news headlines which are annotated'],['2007 task 14  #TAUTHOR_TAG comprises 1000 english news headlines which are annotated'],['##al 2007 task 14  #TAUTHOR_TAG comprises 1000 english news headlines which are annotated according'],"['our study, we selected corpora of small size ( ≤1000 ) where each instance bears numerical ratings regarding multiple emotion variables.', '1 according to these criteria, we came up with the following four data sets covering three typologically diverse languages ( exemplary entries in table 1 ).', 'se07 : the test set of semeval 2007 task 14  #TAUTHOR_TAG comprises 1000 english news headlines which are annotated according to six basic emotions, joy, anger, sadness, fear, disgust, and surprise on a [ 0 ; 100 ] - scale ( be6 annotation format ).', 'anet : the affective norms for english text  #AUTHOR_TAG are an adaptation of the popular lexical database anew  #AUTHOR_TAG to short texts.', 'the corpus comprises 120 situation description which are annotated according to valence, arousal, and dominance on a 9 - point scale ( vad annotation format ).', 'anpst and mas : the affective norms of polish short texts  #AUTHOR_TAG ) and the minho affective sentences  #AUTHOR_TAG can be seen as loose adaptations of anet, very similar in methodology, but different in size and linguistic characteristics ( see table 1 ).', 'both are annotated according to vad.', ""additionally mas is also annotated according the the first five basic emotions ( omitting'surprise') on a 5 - point scale ( be5 )."", 'to increase both performance and reproducibility we employ pre - trained, publicly available word embeddings.', 'we rely mostly on fasttext vectors  #AUTHOR_TAG, yet for se07 we use the word2vec embeddings 2  #AUTHOR_TAG trained on similar data than se07 comprises ( newswire material ).', 'for anet, we rely on the fasttext embeddings trained on common crawl.', 'for anpst and mas, we use the fasttext embeddings by trained on the respective wikipedias.', 'an overview of our corpora and embedding models is given in table 2']",5
"['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is']","['of our gru from the 10×10 - cv. as can be seen, the gru established a new state - of - the - art result and even achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is - broadly speaking - based on the reliability of a single human rater. 5 interestingly, the gru shows particularly large improvements over human performance', 'for categories where the iaa is low ( anger, disgust, and surprise ) which might be an effect of the additional supervision', 'introduced by multi - task learning. training size vs. model performance. in our last analysis', ', again focusing on the se07 corpus, we examine the behavior of our full set of models when varying the amount of training data. for each number n ∈ { 1, 10, 20,..., 100, 200,..., 900 }, we randomly sampled n instances of the entirety of the corpus for training and tested on the held out data. this procedure was repeated', '100 times for each of the training data sizes before averaging the results. each of the models was evaluated with the identical data splits. the outcome', 'of this experiment is depicted in figure 1. as can be seen, recurrent models suffer only a moderate loss of performance down to a third', 'of the original training data ( about 300 observations ). the cnn, ffn and ridge bv model remain stable even longer - their performance only begins to decline rapidly', 'at about 100 instances. astonishingly, the cnn achieves human - performance even with as little 200 training samples', '. in contrast, ridge ngram declines more steadily', 'yet its overall performance on larger training sets is much lower. cnn - lstm on four topologically diverse data sets', 'of sizes ranging between 1000 and only 120 instances. counterintuitively, we found that all dl approaches performed well under every experimental condition. our proposed gru model even established a', 'novel state - of - the - art result on the semeval 2007 test set  #TAUTHOR_TAG outperforming human reliability. moreover, it has been frequently argued that pre - trained word embeddings do not comprise sufficient affective information to be used verbati', '##m in emotion analysis. we here provided evidence that in actuality the opposite holds -', 'high - quality pre - trained word embeddings are instrumental in achieving strong results in low - resource scenarios and largely boost performance independent of model type. hence, this contribution pointed out two obstructive misconceptions thus opening up dl for applications in low - resource scenarios']",5
"['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is']","['of our gru from the 10×10 - cv. as can be seen, the gru established a new state - of - the - art result and even achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is - broadly speaking - based on the reliability of a single human rater. 5 interestingly, the gru shows particularly large improvements over human performance', 'for categories where the iaa is low ( anger, disgust, and surprise ) which might be an effect of the additional supervision', 'introduced by multi - task learning. training size vs. model performance. in our last analysis', ', again focusing on the se07 corpus, we examine the behavior of our full set of models when varying the amount of training data. for each number n ∈ { 1, 10, 20,..., 100, 200,..., 900 }, we randomly sampled n instances of the entirety of the corpus for training and tested on the held out data. this procedure was repeated', '100 times for each of the training data sizes before averaging the results. each of the models was evaluated with the identical data splits. the outcome', 'of this experiment is depicted in figure 1. as can be seen, recurrent models suffer only a moderate loss of performance down to a third', 'of the original training data ( about 300 observations ). the cnn, ffn and ridge bv model remain stable even longer - their performance only begins to decline rapidly', 'at about 100 instances. astonishingly, the cnn achieves human - performance even with as little 200 training samples', '. in contrast, ridge ngram declines more steadily', 'yet its overall performance on larger training sets is much lower. cnn - lstm on four topologically diverse data sets', 'of sizes ranging between 1000 and only 120 instances. counterintuitively, we found that all dl approaches performed well under every experimental condition. our proposed gru model even established a', 'novel state - of - the - art result on the semeval 2007 test set  #TAUTHOR_TAG outperforming human reliability. moreover, it has been frequently argued that pre - trained word embeddings do not comprise sufficient affective information to be used verbati', '##m in emotion analysis. we here provided evidence that in actuality the opposite holds -', 'high - quality pre - trained word embeddings are instrumental in achieving strong results in low - resource scenarios and largely boost performance independent of model type. hence, this contribution pointed out two obstructive misconceptions thus opening up dl for applications in low - resource scenarios']",5
"['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is']","['of our gru from the 10×10 - cv. as can be seen, the gru established a new state - of - the - art result and even achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is - broadly speaking - based on the reliability of a single human rater. 5 interestingly, the gru shows particularly large improvements over human performance', 'for categories where the iaa is low ( anger, disgust, and surprise ) which might be an effect of the additional supervision', 'introduced by multi - task learning. training size vs. model performance. in our last analysis', ', again focusing on the se07 corpus, we examine the behavior of our full set of models when varying the amount of training data. for each number n ∈ { 1, 10, 20,..., 100, 200,..., 900 }, we randomly sampled n instances of the entirety of the corpus for training and tested on the held out data. this procedure was repeated', '100 times for each of the training data sizes before averaging the results. each of the models was evaluated with the identical data splits. the outcome', 'of this experiment is depicted in figure 1. as can be seen, recurrent models suffer only a moderate loss of performance down to a third', 'of the original training data ( about 300 observations ). the cnn, ffn and ridge bv model remain stable even longer - their performance only begins to decline rapidly', 'at about 100 instances. astonishingly, the cnn achieves human - performance even with as little 200 training samples', '. in contrast, ridge ngram declines more steadily', 'yet its overall performance on larger training sets is much lower. cnn - lstm on four topologically diverse data sets', 'of sizes ranging between 1000 and only 120 instances. counterintuitively, we found that all dl approaches performed well under every experimental condition. our proposed gru model even established a', 'novel state - of - the - art result on the semeval 2007 test set  #TAUTHOR_TAG outperforming human reliability. moreover, it has been frequently argued that pre - trained word embeddings do not comprise sufficient affective information to be used verbati', '##m in emotion analysis. we here provided evidence that in actuality the opposite holds -', 'high - quality pre - trained word embeddings are instrumental in achieving strong results in low - resource scenarios and largely boost performance independent of model type. hence, this contribution pointed out two obstructive misconceptions thus opening up dl for applications in low - resource scenarios']",4
"['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance']","['achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is']","['of our gru from the 10×10 - cv. as can be seen, the gru established a new state - of - the - art result and even achieves superhuman', 'performance. this may sound', 'improbable at first glance. however,  #TAUTHOR_TAG employ a rather weak notion of human performance which is - broadly speaking - based on the reliability of a single human rater. 5 interestingly, the gru shows particularly large improvements over human performance', 'for categories where the iaa is low ( anger, disgust, and surprise ) which might be an effect of the additional supervision', 'introduced by multi - task learning. training size vs. model performance. in our last analysis', ', again focusing on the se07 corpus, we examine the behavior of our full set of models when varying the amount of training data. for each number n ∈ { 1, 10, 20,..., 100, 200,..., 900 }, we randomly sampled n instances of the entirety of the corpus for training and tested on the held out data. this procedure was repeated', '100 times for each of the training data sizes before averaging the results. each of the models was evaluated with the identical data splits. the outcome', 'of this experiment is depicted in figure 1. as can be seen, recurrent models suffer only a moderate loss of performance down to a third', 'of the original training data ( about 300 observations ). the cnn, ffn and ridge bv model remain stable even longer - their performance only begins to decline rapidly', 'at about 100 instances. astonishingly, the cnn achieves human - performance even with as little 200 training samples', '. in contrast, ridge ngram declines more steadily', 'yet its overall performance on larger training sets is much lower. cnn - lstm on four topologically diverse data sets', 'of sizes ranging between 1000 and only 120 instances. counterintuitively, we found that all dl approaches performed well under every experimental condition. our proposed gru model even established a', 'novel state - of - the - art result on the semeval 2007 test set  #TAUTHOR_TAG outperforming human reliability. moreover, it has been frequently argued that pre - trained word embeddings do not comprise sufficient affective information to be used verbati', '##m in emotion analysis. we here provided evidence that in actuality the opposite holds -', 'high - quality pre - trained word embeddings are instrumental in achieving strong results in low - resource scenarios and largely boost performance independent of model type. hence, this contribution pointed out two obstructive misconceptions thus opening up dl for applications in low - resource scenarios']",4
"['by various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question']","['by various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question']","['even language modeling was achieved by various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question']","['demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.', 'indeed, its performance of 86. 36 % ( lp / lr f 1 ) is better than that of early lexicalized pcfg models, and surprisingly close to the current state - of - theart.', 'this result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models : an unlexicalized pcfg is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.', 'in the early 1990s, as probabilistic methods swept nlp, parsing work revived the investigation of probabilistic context - free grammars ( pcfgs )  #AUTHOR_TAG.', 'however, early results on the utility of pcfgs for parse disambiguation and language modeling were somewhat disappointing.', 'a conviction arose that lexicalized pcfgs ( where head words annotate phrasal nodes ) were the key tool for high performance pcfg parsing.', 'this approach was congruent with the great success of word n - gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as pp attachments  #AUTHOR_TAG.', 'in the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question how large a role lexicalization plays in such parsers.', ' #AUTHOR_TAG showed that the performance of an unlexicalized pcfg over the penn treebank could be improved enormously simply by annotating each node by its parent category.', 'the penn treebank covering pcfg is a poor tool for parsing because the context - freedom assumptions it embodies are far too strong, and weakening them in this way makes the model much better.', 'more recently,  #AUTHOR_TAG discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0. 5 % for test text from the same domain as the training data, and not at all for test text from a different domain.', 'but it is precisely these bilexical dependencies that backed the intuition that lexicalized pcfgs should be very successful, for example in hind']",0
"['various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question how large a']","['various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question how large a']","['various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question how large a role lexicalization plays in']","['the early 1990s, as probabilistic methods swept nlp, parsing work revived the investigation of probabilistic context - free grammars ( pcfgs )  #AUTHOR_TAG.', 'however, early results on the utility of pcfgs for parse disambiguation and language modeling were somewhat disappointing.', 'a conviction arose that lexicalized pcfgs ( where head words annotate phrasal nodes ) were the key tool for high performance pcfg parsing.', 'this approach was congruent with the great success of word n - gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as pp attachments  #AUTHOR_TAG.', 'in the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models  #TAUTHOR_TAG.', 'however, several results have brought into question how large a role lexicalization plays in such parsers.', ' #AUTHOR_TAG showed that the performance of an unlexicalized pcfg over the penn treebank could be improved enormously simply by annotating each node by its parent category.', 'the penn treebank covering pcfg is a poor tool for parsing because the context - freedom assumptions it embodies are far too strong, and weakening them in this way makes the model much better.', 'more recently,  #AUTHOR_TAG discusses how taking the bilexical probabilities out of a good current lexicalized pcfg parser hurts performance hardly at all : by at most 0. 5 % for test text from the same domain as the training data, and not at all for test text from a different domain']",0
[' #TAUTHOR_TAG uses a range of linguistically motivated'],[' #TAUTHOR_TAG uses a range of linguistically motivated'],[' #TAUTHOR_TAG uses a range of linguistically motivated'],"['best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the categories appearing in the penn treebank.', ' #AUTHOR_TAG shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and  #TAUTHOR_TAG uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg, such as differentiating "" base nps "" from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject np.', 'while he gives incomplete experimental results as to their efficacy, we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization.', 'in this paper, we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible.', 'we describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models.', 'specifically, we construct an unlexicalized pcfg which outperforms the lexicalized pcfgs of  #AUTHOR_TAG and  #AUTHOR_TAG ( though not more recent models, such as  #AUTHOR_TAG or  #TAUTHOR_TAG.', 'one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg.', 'to the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data.', 'secondly, this result affirms the value of linguistic analysis for feature discovery.', 'the result has other uses and advantages : an unlexicalized pcfg is easier to interpret, reason about, and improve than the more complex lexicalized models.', 'the grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities.', 'the parsing algorithms have lower asymptotic complexity 4 and have much smaller grammar egory is divided into several subcategories, for example dividing verb phrases into finite and non - finite verb phrases, rather than in the modern restricted usage where the term refers only to the syntactic argument frames of predicators.', '4 o ( n 3 ) vs. o ( n 5 ) for a naive implementation, or vs. o ( n 4 ) if using the clever approach of']",0
"['second - order model in  #TAUTHOR_TAG, is broadly a smoothed version']","['second - order model in  #TAUTHOR_TAG, is broadly a smoothed version']","['', 'the second - order model in  #TAUTHOR_TAG, is broadly a smoothed version of']","['rule vp → vbz np pp pp, it will be broken into several stages, each a binary or unary rule, which conceptually represent a head - outward generation', 'of the right hand size, as shown in figure 1.', 'the bottom layer will be a unary over the head declaring the goal : vp :', '[ 7 finally, we have another unary to finish the vp. note that while it is convenient to think of this as a head - outward process, these are just pc', '##fg rewrites, and so the actual scores attached to each rule will correspond to a downward generation order.', 'figure 2 presents a grid of horizontal and vertical markovizations of the grammar. the raw treebank grammar', 'corresponds to v = 1, h = ∞ ( the upper right corner ), while the parent annotation in  #AUTHOR_TAG corresponds to v = 2, h = ∞, and', 'the second - order model in  #TAUTHOR_TAG, is broadly a smoothed version of v = 2, h = 2. in addition to exact nth - order models, we tried variable - figure 3 : size and devset performance of the cumulatively annotated models, starting with', 'the markovized baseline. the right two columns show the change in f 1 from the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in', 'isolation. history models similar in intent to those described in  #AUTHOR_TAG figure 2 shows', 'parsing accuracies as well as the number of symbols in each markovization. these symbol counts include all the intermediate states which represent partially completed constituents. the general trend is that, in the absence of further annotation, more vertical annotation is better', '- even exhaustive grandparent annotation. this is not true for horizontal markovization, where the variableorder second - order model was superior.', 'the best entry, v = 3, h ≤ 2, has an f 1 of 79. 74, already a substantial', 'improvement over the baseline. in the remaining sections, we discuss other annotations which increasingly split the', 'symbol space. since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial', ', and not all sets of useful splits are guaranteed to co - exist well. in particular, while v = 3, h ≤ 2 markovization is good on its own, it has a large number of states and does', 'not tolerate further splitting well. therefore, we base all further exploration on the v ≤ 2, h', '≤ 2 grammar. although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories']",0
"['second - order model in  #TAUTHOR_TAG, is broadly a smoothed version']","['second - order model in  #TAUTHOR_TAG, is broadly a smoothed version']","['', 'the second - order model in  #TAUTHOR_TAG, is broadly a smoothed version of']","['rule vp → vbz np pp pp, it will be broken into several stages, each a binary or unary rule, which conceptually represent a head - outward generation', 'of the right hand size, as shown in figure 1.', 'the bottom layer will be a unary over the head declaring the goal : vp :', '[ 7 finally, we have another unary to finish the vp. note that while it is convenient to think of this as a head - outward process, these are just pc', '##fg rewrites, and so the actual scores attached to each rule will correspond to a downward generation order.', 'figure 2 presents a grid of horizontal and vertical markovizations of the grammar. the raw treebank grammar', 'corresponds to v = 1, h = ∞ ( the upper right corner ), while the parent annotation in  #AUTHOR_TAG corresponds to v = 2, h = ∞, and', 'the second - order model in  #TAUTHOR_TAG, is broadly a smoothed version of v = 2, h = 2. in addition to exact nth - order models, we tried variable - figure 3 : size and devset performance of the cumulatively annotated models, starting with', 'the markovized baseline. the right two columns show the change in f 1 from the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in', 'isolation. history models similar in intent to those described in  #AUTHOR_TAG figure 2 shows', 'parsing accuracies as well as the number of symbols in each markovization. these symbol counts include all the intermediate states which represent partially completed constituents. the general trend is that, in the absence of further annotation, more vertical annotation is better', '- even exhaustive grandparent annotation. this is not true for horizontal markovization, where the variableorder second - order model was superior.', 'the best entry, v = 3, h ≤ 2, has an f 1 of 79. 74, already a substantial', 'improvement over the baseline. in the remaining sections, we discuss other annotations which increasingly split the', 'symbol space. since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial', ', and not all sets of useful splits are guaranteed to co - exist well. in particular, while v = 3, h ≤ 2 markovization is good on its own, it has a large number of states and does', 'not tolerate further splitting well. therefore, we base all further exploration on the v ≤ 2, h', '≤ 2 grammar. although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories']",0
"['.', ' #TAUTHOR_TAG captures this notion by introducing the notion of a base']","['base np.', ' #TAUTHOR_TAG captures this notion by introducing the notion of a base np, in']","['', ' #TAUTHOR_TAG captures this notion by introducing the notion of a base np, in']","['', ' #TAUTHOR_TAG']",0
"['.', ' #TAUTHOR_TAG captures this notion by introducing the notion of a base']","['base np.', ' #TAUTHOR_TAG captures this notion by introducing the notion of a base np, in']","['', ' #TAUTHOR_TAG captures this notion by introducing the notion of a base np, in']","['', ' #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG uses a range of linguistically motivated'],[' #TAUTHOR_TAG uses a range of linguistically motivated'],[' #TAUTHOR_TAG uses a range of linguistically motivated'],"['best - performing lexicalized pcfgs have increasingly made use of subcategorization 3 of the categories appearing in the penn treebank.', ' #AUTHOR_TAG shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and  #TAUTHOR_TAG uses a range of linguistically motivated and carefully hand - engineered subcategorizations to break down wrong context - freedom assumptions of the naive penn treebank covering pcfg, such as differentiating "" base nps "" from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject np.', 'while he gives incomplete experimental results as to their efficacy, we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization.', 'in this paper, we show that the parsing performance that can be achieved by an unlexicalized pcfg is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible.', 'we describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state - of - the - art lexicalized models.', 'specifically, we construct an unlexicalized pcfg which outperforms the lexicalized pcfgs of  #AUTHOR_TAG and  #AUTHOR_TAG ( though not more recent models, such as  #AUTHOR_TAG or  #TAUTHOR_TAG.', 'one benefit of this result is a much - strengthened lower bound on the capacity of an unlexicalized pcfg.', 'to the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data.', 'secondly, this result affirms the value of linguistic analysis for feature discovery.', 'the result has other uses and advantages : an unlexicalized pcfg is easier to interpret, reason about, and improve than the more complex lexicalized models.', 'the grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities.', 'the parsing algorithms have lower asymptotic complexity 4 and have much smaller grammar egory is divided into several subcategories, for example dividing verb phrases into finite and non - finite verb phrases, rather than in the modern restricted usage where the term refers only to the syntactic argument frames of predicators.', '4 o ( n 3 ) vs. o ( n 5 ) for a naive implementation, or vs. o ( n 4 ) if using the clever approach of']",4
"['second - order model in  #TAUTHOR_TAG, is broadly a smoothed version']","['second - order model in  #TAUTHOR_TAG, is broadly a smoothed version']","['', 'the second - order model in  #TAUTHOR_TAG, is broadly a smoothed version of']","['rule vp → vbz np pp pp, it will be broken into several stages, each a binary or unary rule, which conceptually represent a head - outward generation', 'of the right hand size, as shown in figure 1.', 'the bottom layer will be a unary over the head declaring the goal : vp :', '[ 7 finally, we have another unary to finish the vp. note that while it is convenient to think of this as a head - outward process, these are just pc', '##fg rewrites, and so the actual scores attached to each rule will correspond to a downward generation order.', 'figure 2 presents a grid of horizontal and vertical markovizations of the grammar. the raw treebank grammar', 'corresponds to v = 1, h = ∞ ( the upper right corner ), while the parent annotation in  #AUTHOR_TAG corresponds to v = 2, h = ∞, and', 'the second - order model in  #TAUTHOR_TAG, is broadly a smoothed version of v = 2, h = 2. in addition to exact nth - order models, we tried variable - figure 3 : size and devset performance of the cumulatively annotated models, starting with', 'the markovized baseline. the right two columns show the change in f 1 from the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in', 'isolation. history models similar in intent to those described in  #AUTHOR_TAG figure 2 shows', 'parsing accuracies as well as the number of symbols in each markovization. these symbol counts include all the intermediate states which represent partially completed constituents. the general trend is that, in the absence of further annotation, more vertical annotation is better', '- even exhaustive grandparent annotation. this is not true for horizontal markovization, where the variableorder second - order model was superior.', 'the best entry, v = 3, h ≤ 2, has an f 1 of 79. 74, already a substantial', 'improvement over the baseline. in the remaining sections, we discuss other annotations which increasingly split the', 'symbol space. since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial', ', and not all sets of useful splits are guaranteed to co - exist well. in particular, while v = 3, h ≤ 2 markovization is good on its own, it has a large number of states and does', 'not tolerate further splitting well. therefore, we base all further exploration on the v ≤ 2, h', '≤ 2 grammar. although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories']",3
"['of information in the original trees is the presence of empty elements.', 'following  #TAUTHOR_TAG, the annotation gapped - s marks s nodes which have an empty subject ( i. e., raising and control constructions ).', 'this brought']","['of information in the original trees is the presence of empty elements.', 'following  #TAUTHOR_TAG, the annotation gapped - s marks s nodes which have an empty subject ( i. e., raising and control constructions ).', 'this brought']","['have different prepositions below in.', 'a second kind of information in the original trees is the presence of empty elements.', 'following  #TAUTHOR_TAG, the annotation gapped - s marks s nodes which have an empty subject ( i. e., raising and control constructions ).', 'this brought f 1 to 82. 28']","['this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar.', 'by and large, the treebank out - of - the package tags, such as pp - loc or advp - tmp, have negative utility.', 'recall that the raw treebank grammar, with no annotation or markovization, had an f 1 of 72. 62 % on our development set.', 'with the functional annotation left in, this drops to 71. 49 %.', 'the h ≤ 2, v ≤ 1 markovization baseline of 77. 77 % dropped even further, all the way to 72. 87 %, when these annotations were included.', 'nonetheless, some distinctions present in the raw treebank trees were valuable.', 'for example, an np with an s parent could be either a temporal np or a subject.', 'for the annotation tmp - np, we retained the original - tmp tags on nps, and, furthermore, propagated the tag down to the tag of the head of the np.', 'this is illustrated in figure 6, which also shows an example of its utility, clarifying that cnn last night is not a plausible compound and facilitating the otherwise unusual high attachment of the smaller np.', 'tmp - np brought the cumulative f 1 to 82. 25 %.', 'note that this technique of pushing the functional tags down to preterminals might be useful more generally ; for example, locative pps expand roughly the same way as all other pps ( usually as in np ), but they do tend to have different prepositions below in.', 'a second kind of information in the original trees is the presence of empty elements.', 'following  #TAUTHOR_TAG, the annotation gapped - s marks s nodes which have an empty subject ( i. e., raising and control constructions ).', 'this brought f 1 to 82. 28 %']",5
"['google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG']","['google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG']","['by google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG']","['##co and refcoco + datasets in our work along with another recently collected referring expression dataset, released by google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG. the most', 'relevant work to ours is mao et al  #TAUTHOR_TAG which introduced the first deep learning approach to reg. in this model, the authors use a convolutional neural network ( cnn ) [ 36 ] model pre - trained on imagenet [ 34 ]', 'to extract visual features from a bounding box around the target object and from the entire image. they use these features plus 5 features encoding the target object location and size as', '']",5
"[' #TAUTHOR_TAG.', 'we use these as our baselines ( sec 3. 1 )']","[' #TAUTHOR_TAG.', 'we use these as our baselines ( sec 3. 1 ).', 'next, we investigate incorporating better visual context features into']","['al  #TAUTHOR_TAG.', 'we use these as our baselines ( sec 3. 1 )']","['implement several model variations for referring expression generation and comprehension.', 'the first set of models are recent state of the art deep learning approaches from mao et al  #TAUTHOR_TAG.', 'we use these as our baselines ( sec 3. 1 ).', 'next, we investigate incorporating better visual context features into the models ( sec 3. 2 ).', 'finally, we explore methods to jointly produce an entire set of referring expressions for all depicted objects of the same category ( sec 3. 3 )']",5
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['al  #TAUTHOR_TAG.', 'both models utilize a pre - trained cnn network to model the target object and']","['comparison, we implement both the baseline and strong model of mao et al  #TAUTHOR_TAG.', 'both models utilize a pre - trained cnn network to model the target object and its context within the image, and then use a lstm for generation.', 'in particular, object and context are modeled as features from a cnn trained to recognize 1, 000 object categories [ 36 ] from imagenet [ 34 ].', '']",5
"[', refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other']","['[ 24 ].', 'one dataset, refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other']","['the microsoft coco image collection [ 24 ].', 'one dataset, refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other two datasets, refcoco']","['make use of 3 referring expression datasets in our work, all collected on top of the microsoft coco image collection [ 24 ].', 'one dataset, refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other two datasets, refcoco and refcoco +, are collected interactively in a two - player game [ 19 ].', 'in the following, we describe each dataset and provide some analysis of their similarities and differences, and then discuss splits of the datasets used in our experiments']",5
['approach  #TAUTHOR_TAG. the second type is people -'],['- of - the - art approach  #TAUTHOR_TAG. the second type is people -'],"['approach  #TAUTHOR_TAG. the second type is people - vs - objects splits. one thing we observe from analyzing the datasets is that about half of the referred objects are people. therefore, we create a split for refcoco and refcoco', '+ datasets that evaluates images containing multiple people ( test']","['', '##coco + suggests that incorporating visual comparisons to same - type objecs will be useful. dataset splits :', 'there are two types of splits of the data into train / test sets :', 'a per - object split and a people - vs - objects split. the', 'first type is per - object split. in this split, the dataset is divided by randomly', 'partitioning objects into training and testing sets. this means that each object will only appear either in training or testing set, but that one object from an image may appear in the training set while another object from the same image may appear in the test set. we use this split for', 'refcocog since same division was used in the previous state - of - the - art approach  #TAUTHOR_TAG. the second type is people - vs - objects splits. one thing we observe from analyzing the datasets is that about half of the referred objects are people. therefore, we create a split for refcoco and refcoco', '+ datasets that evaluates images containing multiple people ( testa ) vs images containing multiple instances of all other objects ( testb ). in this split all objects from an image will appear either in the training or testing sets, but not', 'both. this split creates a more meaningfully separated division between training and testing, allowing us to evaluate the usefulness of context more fairly']",5
"[' #TAUTHOR_TAG, namely']","[' #TAUTHOR_TAG, namely']","['al  #TAUTHOR_TAG, namely']",[' #TAUTHOR_TAG'],5
"['works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented']","['works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented']","['representation as previously discussed, we suggest that the approaches proposed in recent referring expression works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented both the baseline']","['representation as previously discussed, we suggest that the approaches proposed in recent referring expression works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented both the baseline and strong mmi models from mao et al  #TAUTHOR_TAG, and compare the results for referring expression comprehension task with and without global context on refcoco and refcoco + in table 1.', 'surprisingly we find that the global context does not improve the performance of the model.', 'in fact, adding context even decreases performance slightly.', 'this may be due to the fact that the global context for each object in an image would be the same, introducing some ambiguity into the referring expression comprehension task.', 'given these findings, we implemented a simple modification to the global context, computing the same visual representation, but on a somewhat scaled window centered around the target object.', 'we found this to improve performance, suggesting room for improving the visual context feature.', 'this motivate our development of a better context feature.', 'visual comparison for our visual comparison model, there could be several choices regarding which objects from the image should be compared to the target object.', 'we experiment with three sets of reference objects on refcoco and refcoco + datasets : a ) objects of the same - category in the image, b ) objects of different - category in the image, and c ) all objects appeared in the image.', 'we use our "" visdif "" model for this experiment.', 'the results are shown in figure 3.', 'it is clear to see the visual comparisons to the same - category objects are most useful for referring expression comprehension task.', 'this is more like mimicing how human refer object - we tend to point out the difference between the target object with the other same - category objects within the same image.', 'fig. 3.', 'comprehension accuracies on refcoco and refcoco + datasets.', 'we compare the performance of "" visdif "" model without visual comparison, and visual comparison between different - category objects, between all objects, and between same - type objects']",5
"['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","[') and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on""]","['evaluate performance on the referring expression comprehension task on refcoco, refcoco + and refcocog datasets.', 'for refcoco and refcoco +, we evaluate on the two subsets of people ( testa ) and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper - parameters on refcoco."", 'table 2 shows the comprehension accuracies.', '']",5
"['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","[') and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on""]","['evaluate performance on the referring expression comprehension task on refcoco, refcoco + and refcocog datasets.', 'for refcoco and refcoco +, we evaluate on the two subsets of people ( testa ) and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper - parameters on refcoco."", 'table 2 shows the comprehension accuracies.', '']",5
"['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","[') and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on""]","['evaluate performance on the referring expression comprehension task on refcoco, refcoco + and refcocog datasets.', 'for refcoco and refcoco +, we evaluate on the two subsets of people ( testa ) and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper - parameters on refcoco."", 'table 2 shows the comprehension accuracies.', '']",5
"['google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG']","['google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG']","['by google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG']","['##co and refcoco + datasets in our work along with another recently collected referring expression dataset, released by google, denoted in our', 'paper as refcocog  #TAUTHOR_TAG. the most', 'relevant work to ours is mao et al  #TAUTHOR_TAG which introduced the first deep learning approach to reg. in this model, the authors use a convolutional neural network ( cnn ) [ 36 ] model pre - trained on imagenet [ 34 ]', 'to extract visual features from a bounding box around the target object and from the entire image. they use these features plus 5 features encoding the target object location and size as', '']",0
['generation  #TAUTHOR_TAG and comprehension'],"['generation  #TAUTHOR_TAG and comprehension [ 14, 33 ] also take a deep learning approach.', 'however, we add visual']","['referring expression generation  #TAUTHOR_TAG and comprehension [ 14, 33 ] also take a deep learning approach.', 'however, we add visual object comparisons']","['expressions are closely related to the more general problem of modeling the connection between images and descriptive language.', 'in recent years, this has been studied in the image captioning task [ 6, 37, 13, 31, 23 ].', 'there, the aim is to condition the generation of language on the visual information from an image.', 'the wide range of aspects of an image that could be described, and the variety of words that could be chosen for a particular description complicate studying image captioning.', 'our study of referring expressions is partially motivated by focusing on description for a specific, and more easily evaluated, communication goal.', 'although our task is somewhat different, we borrow machinery from state of the art caption generation [ 3, 39, 27, 5, 18, 21, 41 ] using lstm to generate captions based on cnn features computed on an input image.', 'three recent approaches for referring expression generation  #TAUTHOR_TAG and comprehension [ 14, 33 ] also take a deep learning approach.', 'however, we add visual object comparisons and tie together language generation for multiple objects.', 'referring expression generation has been studied for many years [ 40, 22, 30 ] in linguistics and natural language processing.', 'these works were limited by data collection and insufficient computer vision algorithms.', 'together amazon mechanical turk and cnns have somewhat mitigated these limitations, allowing us to revisit these ideas on large - scale datasets.', 'we still use such work to motivate the architecture of our pipeline.', 'for instance, mitchell and jordan et al [ 30, 16 ] show the importance of using attributes, funakoshi et al [ 8 ] show the importance of relative relations between objects in the same perceptual group, and kelleher et al [ 20 ] show the importance of spatial relationships.', 'these provide motivation for our modeling choices : when considering a referring expression for an object, the model takes into account the relative spatial location of other objects of the same type and visual comparisons to objects in the same perceptual group.', 'the reg datasets of the past were sometimes limited to using computer generated images [ 38 ], or relatively small collections of natural objects [ 29, 28, 7 ].', 'recently, a large - scale referring expression dataset was collected by kazemzadeh et al [ 19 ] featuring natural objects in the real world.', 'since then, another three reg datasets based on the object labels in mscoco have been collected [ 19,  #TAUTHOR_TAG.', 'the availability of large - scale referring expression datasets allows us to train deep learning models.', 'additionally, our analysis of these datasets motivates our incorporation of visual comparisons between']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['al  #TAUTHOR_TAG.', 'both models utilize a pre - trained cnn network to model the target object and']","['comparison, we implement both the baseline and strong model of mao et al  #TAUTHOR_TAG.', 'both models utilize a pre - trained cnn network to model the target object and its context within the image, and then use a lstm for generation.', 'in particular, object and context are modeled as features from a cnn trained to recognize 1, 000 object categories [ 36 ] from imagenet [ 34 ].', '']",0
"[', refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other']","['[ 24 ].', 'one dataset, refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other']","['the microsoft coco image collection [ 24 ].', 'one dataset, refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other two datasets, refcoco']","['make use of 3 referring expression datasets in our work, all collected on top of the microsoft coco image collection [ 24 ].', 'one dataset, refcocog  #TAUTHOR_TAG is collected in a non - interactive setting, while the other two datasets, refcoco and refcoco +, are collected interactively in a two - player game [ 19 ].', 'in the following, we describe each dataset and provide some analysis of their similarities and differences, and then discuss splits of the datasets used in our experiments']",0
"['works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented']","['works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented']","['representation as previously discussed, we suggest that the approaches proposed in recent referring expression works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented both the baseline']","['representation as previously discussed, we suggest that the approaches proposed in recent referring expression works  #TAUTHOR_TAG 14 ] make use of relatively weak contextual information, by only considering a single global image context for all objects.', 'to verify this intuition, we implemented both the baseline and strong mmi models from mao et al  #TAUTHOR_TAG, and compare the results for referring expression comprehension task with and without global context on refcoco and refcoco + in table 1.', 'surprisingly we find that the global context does not improve the performance of the model.', 'in fact, adding context even decreases performance slightly.', 'this may be due to the fact that the global context for each object in an image would be the same, introducing some ambiguity into the referring expression comprehension task.', 'given these findings, we implemented a simple modification to the global context, computing the same visual representation, but on a somewhat scaled window centered around the target object.', 'we found this to improve performance, suggesting room for improving the visual context feature.', 'this motivate our development of a better context feature.', 'visual comparison for our visual comparison model, there could be several choices regarding which objects from the image should be compared to the target object.', 'we experiment with three sets of reference objects on refcoco and refcoco + datasets : a ) objects of the same - category in the image, b ) objects of different - category in the image, and c ) all objects appeared in the image.', 'we use our "" visdif "" model for this experiment.', 'the results are shown in figure 3.', 'it is clear to see the visual comparisons to the same - category objects are most useful for referring expression comprehension task.', 'this is more like mimicing how human refer object - we tend to point out the difference between the target object with the other same - category objects within the same image.', 'fig. 3.', 'comprehension accuracies on refcoco and refcoco + datasets.', 'we compare the performance of "" visdif "" model without visual comparison, and visual comparison between different - category objects, between all objects, and between same - type objects']",0
['generation  #TAUTHOR_TAG and comprehension'],"['generation  #TAUTHOR_TAG and comprehension [ 14, 33 ] also take a deep learning approach.', 'however, we add visual']","['referring expression generation  #TAUTHOR_TAG and comprehension [ 14, 33 ] also take a deep learning approach.', 'however, we add visual object comparisons']","['expressions are closely related to the more general problem of modeling the connection between images and descriptive language.', 'in recent years, this has been studied in the image captioning task [ 6, 37, 13, 31, 23 ].', 'there, the aim is to condition the generation of language on the visual information from an image.', 'the wide range of aspects of an image that could be described, and the variety of words that could be chosen for a particular description complicate studying image captioning.', 'our study of referring expressions is partially motivated by focusing on description for a specific, and more easily evaluated, communication goal.', 'although our task is somewhat different, we borrow machinery from state of the art caption generation [ 3, 39, 27, 5, 18, 21, 41 ] using lstm to generate captions based on cnn features computed on an input image.', 'three recent approaches for referring expression generation  #TAUTHOR_TAG and comprehension [ 14, 33 ] also take a deep learning approach.', 'however, we add visual object comparisons and tie together language generation for multiple objects.', 'referring expression generation has been studied for many years [ 40, 22, 30 ] in linguistics and natural language processing.', 'these works were limited by data collection and insufficient computer vision algorithms.', 'together amazon mechanical turk and cnns have somewhat mitigated these limitations, allowing us to revisit these ideas on large - scale datasets.', 'we still use such work to motivate the architecture of our pipeline.', 'for instance, mitchell and jordan et al [ 30, 16 ] show the importance of using attributes, funakoshi et al [ 8 ] show the importance of relative relations between objects in the same perceptual group, and kelleher et al [ 20 ] show the importance of spatial relationships.', 'these provide motivation for our modeling choices : when considering a referring expression for an object, the model takes into account the relative spatial location of other objects of the same type and visual comparisons to objects in the same perceptual group.', 'the reg datasets of the past were sometimes limited to using computer generated images [ 38 ], or relatively small collections of natural objects [ 29, 28, 7 ].', 'recently, a large - scale referring expression dataset was collected by kazemzadeh et al [ 19 ] featuring natural objects in the real world.', 'since then, another three reg datasets based on the object labels in mscoco have been collected [ 19,  #TAUTHOR_TAG.', 'the availability of large - scale referring expression datasets allows us to train deep learning models.', 'additionally, our analysis of these datasets motivates our incorporation of visual comparisons between']",4
"['separately [ 15 ]  #TAUTHOR_TAG, we consider tying the generation process together into a single task to jointly generate']","['separately [ 15 ]  #TAUTHOR_TAG, we consider tying the generation process together into a single task to jointly generate']","['task, rather than generating sentences for each object in an image separately [ 15 ]  #TAUTHOR_TAG, we consider tying the generation process together into a single task to jointly generate']","['the referring expression generation task, rather than generating sentences for each object in an image separately [ 15 ]  #TAUTHOR_TAG, we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image.', 'this makes sense intuitively - when a person attempts to generate a referring expression for an object in an image they inherently compose that expression while keeping in mind expressions for the other objects in the picture.', 'this can be observed in the fact that the expressions people generate for objects in an image tend to share similar patterns of expression.', '']",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['al  #TAUTHOR_TAG.', 'both models utilize a pre - trained cnn network to model the target object and']","['comparison, we implement both the baseline and strong model of mao et al  #TAUTHOR_TAG.', 'both models utilize a pre - trained cnn network to model the target object and its context within the image, and then use a lstm for generation.', 'in particular, object and context are modeled as features from a cnn trained to recognize 1, 000 object categories [ 36 ] from imagenet [ 34 ].', '']",1
['approach  #TAUTHOR_TAG. the second type is people -'],['- of - the - art approach  #TAUTHOR_TAG. the second type is people -'],"['approach  #TAUTHOR_TAG. the second type is people - vs - objects splits. one thing we observe from analyzing the datasets is that about half of the referred objects are people. therefore, we create a split for refcoco and refcoco', '+ datasets that evaluates images containing multiple people ( test']","['', '##coco + suggests that incorporating visual comparisons to same - type objecs will be useful. dataset splits :', 'there are two types of splits of the data into train / test sets :', 'a per - object split and a people - vs - objects split. the', 'first type is per - object split. in this split, the dataset is divided by randomly', 'partitioning objects into training and testing sets. this means that each object will only appear either in training or testing set, but that one object from an image may appear in the training set while another object from the same image may appear in the test set. we use this split for', 'refcocog since same division was used in the previous state - of - the - art approach  #TAUTHOR_TAG. the second type is people - vs - objects splits. one thing we observe from analyzing the datasets is that about half of the referred objects are people. therefore, we create a split for refcoco and refcoco', '+ datasets that evaluates images containing multiple people ( testa ) vs images containing multiple instances of all other objects ( testb ). in this split all objects from an image will appear either in the training or testing sets, but not', 'both. this split creates a more meaningfully separated division between training and testing, allowing us to evaluate the usefulness of context more fairly']",3
"[' #TAUTHOR_TAG, namely']","[' #TAUTHOR_TAG, namely']","['al  #TAUTHOR_TAG, namely']",[' #TAUTHOR_TAG'],3
"['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","[') and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on""]","['evaluate performance on the referring expression comprehension task on refcoco, refcoco + and refcocog datasets.', 'for refcoco and refcoco +, we evaluate on the two subsets of people ( testa ) and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper - parameters on refcoco."", 'table 2 shows the comprehension accuracies.', '']",3
"['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","['per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the""]","[') and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on""]","['evaluate performance on the referring expression comprehension task on refcoco, refcoco + and refcocog datasets.', 'for refcoco and refcoco +, we evaluate on the two subsets of people ( testa ) and all other objects ( testb ).', 'for refcocog, we evaluate on the per - object split as previous work  #TAUTHOR_TAG.', ""since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper - parameters on refcoco."", 'table 2 shows the comprehension accuracies.', '']",3
"[' #TAUTHOR_TAG, namely']","[' #TAUTHOR_TAG, namely']","['al  #TAUTHOR_TAG, namely']",[' #TAUTHOR_TAG'],7
"['by  #AUTHOR_TAG.', ' #TAUTHOR_TAG is currently the']","['by  #AUTHOR_TAG.', ' #TAUTHOR_TAG is currently the']","['on probabilistic ranking inferred from a dataset of 25000 split points.', 'using the same dataset,  #AUTHOR_TAG proposed a sandhi splitter for sanskrit.', 'the method is an extension of bayesian word segmentation approach by  #AUTHOR_TAG.', ' #TAUTHOR_TAG is currently the state of the art in sanskrit word segmentation.', 'the system treats the problem as an iterative query expansion problem.', 'using a shallow parser for sanskr']","['number of methods have been proposed for word segmentation in sanskrit.', ' #AUTHOR_TAG treats the problem as a character level rnn sequence labelling task.', 'the author, in addition to reporting sandhi splits to upto 155 cases, additionally categorises the rules to 5 different types.', 'since, the results reported by the author are not at word - level, as is the standard with word segmentation systems in general, a direct comparison with the other systems is not meaningful.', ' #AUTHOR_TAG proposed a method based on finite state transducers by incorporating rules of sandhi.', 'the system generates all possible splits and then provides a ranking of various splits, based on probabilistic ranking inferred from a dataset of 25000 split points.', 'using the same dataset,  #AUTHOR_TAG proposed a sandhi splitter for sanskrit.', 'the method is an extension of bayesian word segmentation approach by  #AUTHOR_TAG.', ' #TAUTHOR_TAG is currently the state of the art in sanskrit word segmentation.', 'the system treats the problem as an iterative query expansion problem.', 'using a shallow parser for sanskrit  #AUTHOR_TAG, an input sentence is first converted to a graph of possible candidates and desirable nodes are iteratively selected using path constrained random walks  #AUTHOR_TAG.', 'to further catalyse the research in word segmentation for  #AUTHOR_TAG has released a dataset for the word segmentation task.', 'the work releases a dataset of 119, 000 sentences in sanskrit along with the lexical and morphological analysis from a shallow parser.', 'the work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word.', 'the additional information will be beneficial in further processing of sanskrit texts, such as dependency parsing or summarisation  #AUTHOR_TAG. so far, no system successfully predicts the morphological information of the words in addition to the final word form.', 'though  #TAUTHOR_TAG currently only predicts the final word - form']",0
[' #TAUTHOR_TAG. the'],"[' #TAUTHOR_TAG. the proportion of sentences with more than 10 words', 'in our']",['words. it needs to be noted that the average length of a string in the digital corpus of sanskrit is 6. 7  #TAUTHOR_TAG. the'],"['', 'word order nature of sentences in sanskrit. since there are multiple permutations', 'of words in a sentence which are valid syntactically and', 'convey the same semantic meaning, the entire input context is required to understand the meaning of a sentence for any distributional semantic model. figure 2 shows the results of the competing systems on strings of', 'different lengths in terms of words in the sentence.', ""this should not be confused with sequence length. here, we mean the'word'as per the original vocabulary and is common for all the competing systems. for all the strings"", ""with up to 10 words, our system'attnsegseq2seq'consistently outperforms all the systems in terms of both precision and recall. the current"", 'state of the art performs slightly better than our system, for sentences with more than 10 words. it needs to be noted that the average length of a string in the digital corpus of sanskrit is 6. 7  #TAUTHOR_TAG. the proportion of sentences with more than 10 words', 'in our dataset is less than 1 %. the test dataset has slightly more than 4 % sentences with', ""10 or more words. the'segseq2seq'model performs better than the state of the art for both precision and recall for strings with less than or equal to 6 words. figure 2a shows"", 'the proportion of sentences in the test data based on the frequency of words in it. figure 2b shows the', 'proportion of strings in the test dataset based on', 'the number of words in the strings. our systems attnsegseq2seq takes overall 11 hours 40 minutes', ""and for 80 epochs in a'titan x'12gb gpu memory, 3584 gpu cores, 62gb ram and intel xeon cpu e5 - 26"", '##20 2. 40ghz system. for segseq2seq it takes 7 hours for the same setting']",0
['existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic'],['existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic'],"['the existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic information about the sentences.', 'this limits the scalability']","['purpose of our proposed model is purely to identify the word splits and correctness of the inflected word forms from a sandhied string.', 'the word - level indexing in retrieval systems is often affected by phonetic transformations in words due to sandhi.', ""for example, the term'paramesvarah.'is split as'parama'( ultimate ) and'isvarah."", ""' ( god )."", ""now, a search for instances of the word'isvarah.'might lead to missing search results without proper indexing."", 'string matching approaches often result in low precision results.', 'using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible.', 'for paramesvarah.', ""', it can be split as'parama'( ultimate ),'sva'( dog ) and'rah.'( to facilitate )."", 'though this is not semantically meaningful it is lexically valid.', 'such tools are put to use by some of the existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic information about the sentences.', 'this limits the scalability of  #TAUTHOR_TAG, as they cannot handle out of vocabulary words.', 'scalability of  #TAUTHOR_TAG is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.', 'the systems by  #TAUTHOR_TAG and  #AUTHOR_TAG assume that the parser by  #AUTHOR_TAG, identifies all the possible candidate chunks.', 'our proposed model is built with precisely one purpose in mind, which is to predict the final word - forms in a given sequence.', ' #AUTHOR_TAG states that it is desirable to predict the morphological information of a word from along with the final word - form as the information will be helpful in further processing of sanskrit.', 'the segmentation task is seen as a means and not an end itself.', 'here, we overlook this aspect and see the segmentation task as an end in itself.', 'so we achieve scalability at the cost of missing out on providing valuable linguistic information.', 'models that use linguistic resources are at an advantage here.', 'those systems such as  #TAUTHOR_TAG currently store the morphological information of predicted candidates, but do not use them for evaluation as of now.', 'currently, no system exists that performs the prediction of wordform and morphological information jointly for sanskrit.', 'in our case, since we learn a new vocabulary altogether, the real word boundaries are opaque to the system.', 'the decoder predicts from its own vocabulary.', 'but predicting morphological information requires the knowledge of exact word boundaries.', 'this should be seen as a multitask learning set up.', ""one possible solution is to learn'gibberishvo""]",0
['existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic'],['existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic'],"['the existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic information about the sentences.', 'this limits the scalability']","['purpose of our proposed model is purely to identify the word splits and correctness of the inflected word forms from a sandhied string.', 'the word - level indexing in retrieval systems is often affected by phonetic transformations in words due to sandhi.', ""for example, the term'paramesvarah.'is split as'parama'( ultimate ) and'isvarah."", ""' ( god )."", ""now, a search for instances of the word'isvarah.'might lead to missing search results without proper indexing."", 'string matching approaches often result in low precision results.', 'using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible.', 'for paramesvarah.', ""', it can be split as'parama'( ultimate ),'sva'( dog ) and'rah.'( to facilitate )."", 'though this is not semantically meaningful it is lexically valid.', 'such tools are put to use by some of the existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic information about the sentences.', 'this limits the scalability of  #TAUTHOR_TAG, as they cannot handle out of vocabulary words.', 'scalability of  #TAUTHOR_TAG is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.', 'the systems by  #TAUTHOR_TAG and  #AUTHOR_TAG assume that the parser by  #AUTHOR_TAG, identifies all the possible candidate chunks.', 'our proposed model is built with precisely one purpose in mind, which is to predict the final word - forms in a given sequence.', ' #AUTHOR_TAG states that it is desirable to predict the morphological information of a word from along with the final word - form as the information will be helpful in further processing of sanskrit.', 'the segmentation task is seen as a means and not an end itself.', 'here, we overlook this aspect and see the segmentation task as an end in itself.', 'so we achieve scalability at the cost of missing out on providing valuable linguistic information.', 'models that use linguistic resources are at an advantage here.', 'those systems such as  #TAUTHOR_TAG currently store the morphological information of predicted candidates, but do not use them for evaluation as of now.', 'currently, no system exists that performs the prediction of wordform and morphological information jointly for sanskrit.', 'in our case, since we learn a new vocabulary altogether, the real word boundaries are opaque to the system.', 'the decoder predicts from its own vocabulary.', 'but predicting morphological information requires the knowledge of exact word boundaries.', 'this should be seen as a multitask learning set up.', ""one possible solution is to learn'gibberishvo""]",0
['existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic'],['existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic'],"['the existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic information about the sentences.', 'this limits the scalability']","['purpose of our proposed model is purely to identify the word splits and correctness of the inflected word forms from a sandhied string.', 'the word - level indexing in retrieval systems is often affected by phonetic transformations in words due to sandhi.', ""for example, the term'paramesvarah.'is split as'parama'( ultimate ) and'isvarah."", ""' ( god )."", ""now, a search for instances of the word'isvarah.'might lead to missing search results without proper indexing."", 'string matching approaches often result in low precision results.', 'using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible.', 'for paramesvarah.', ""', it can be split as'parama'( ultimate ),'sva'( dog ) and'rah.'( to facilitate )."", 'though this is not semantically meaningful it is lexically valid.', 'such tools are put to use by some of the existing systems  #TAUTHOR_TAG to obtain additional morphological or syntactic information about the sentences.', 'this limits the scalability of  #TAUTHOR_TAG, as they cannot handle out of vocabulary words.', 'scalability of  #TAUTHOR_TAG is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.', 'the systems by  #TAUTHOR_TAG and  #AUTHOR_TAG assume that the parser by  #AUTHOR_TAG, identifies all the possible candidate chunks.', 'our proposed model is built with precisely one purpose in mind, which is to predict the final word - forms in a given sequence.', ' #AUTHOR_TAG states that it is desirable to predict the morphological information of a word from along with the final word - form as the information will be helpful in further processing of sanskrit.', 'the segmentation task is seen as a means and not an end itself.', 'here, we overlook this aspect and see the segmentation task as an end in itself.', 'so we achieve scalability at the cost of missing out on providing valuable linguistic information.', 'models that use linguistic resources are at an advantage here.', 'those systems such as  #TAUTHOR_TAG currently store the morphological information of predicted candidates, but do not use them for evaluation as of now.', 'currently, no system exists that performs the prediction of wordform and morphological information jointly for sanskrit.', 'in our case, since we learn a new vocabulary altogether, the real word boundaries are opaque to the system.', 'the decoder predicts from its own vocabulary.', 'but predicting morphological information requires the knowledge of exact word boundaries.', 'this should be seen as a multitask learning set up.', ""one possible solution is to learn'gibberishvo""]",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', 'since, we tackle the problem with a non - linguistic approach,']","['of the art  #TAUTHOR_TAG.', '']","['this work we presented a model for word segmentation in sanskrit using a purely engineering based appraoch.', 'our model with attention outperforms the current state of the art  #TAUTHOR_TAG.', 'since, we tackle the problem with a non - linguistic approach, we hope to extend the work to other indic languages as well where sandhi is prevalent such as hindi, marathi, malayalam, telugu etc.', 'since we find that the inclusion of attention is highly beneficial in improving the performance of the system, we intend to experiment with recent advances in the encoder - decoder architectures, such as  #AUTHOR_TAG and  #AUTHOR_TAG, where different novel approaches in using attention are experimented with.', 'our experiments in line with the measures reported in  #TAUTHOR_TAG show that our system performs robustly across strings of varying word size']",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', 'since, we tackle the problem with a non - linguistic approach,']","['of the art  #TAUTHOR_TAG.', '']","['this work we presented a model for word segmentation in sanskrit using a purely engineering based appraoch.', 'our model with attention outperforms the current state of the art  #TAUTHOR_TAG.', 'since, we tackle the problem with a non - linguistic approach, we hope to extend the work to other indic languages as well where sandhi is prevalent such as hindi, marathi, malayalam, telugu etc.', 'since we find that the inclusion of attention is highly beneficial in improving the performance of the system, we intend to experiment with recent advances in the encoder - decoder architectures, such as  #AUTHOR_TAG and  #AUTHOR_TAG, where different novel approaches in using attention are experimented with.', 'our experiments in line with the measures reported in  #TAUTHOR_TAG show that our system performs robustly across strings of varying word size']",5
"['sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts']","['sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts']","['word, sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts']","['', 'machines ( svm ) and conditional random fields ( crf )  #AUTHOR_TAG. these models are shallow and limited in terms of modeling capacity. furthermore, most of these classifiers are trained to extract pio', 'elements one by one which is suboptimal since this approach does not', 'allow the use of shared structure among the individual classifiers. deep neural network models have increased in popularity in the field of nlp. they have pushed the state of the art of', 'text representation and information retrieval. more specifically, these techniques enhanced nlp algorithms', 'through the use of contextualized text embeddings at word, sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts. in the present paper, we build a dataset of pio elements by improving the methodology found in  #AUTHOR_TAG. furthermore, we', 'built a multi - label pio classifier, along with a boosting framework, based on the state of the', 'art text embedding, bert. this embedding model has been proven to offer a better contextualization compared to a bidirectional lstm model  #TAUTHOR_TAG']",0
"['layers using a transformer  #TAUTHOR_TAG.', '']","['layers using a transformer  #TAUTHOR_TAG.', '']","['from transformers ) is a deep bidirectional attention text embedding model.', 'the idea behind this model is to pre - train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer  #TAUTHOR_TAG.', '']","['( bidirectional encoder representations from transformers ) is a deep bidirectional attention text embedding model.', 'the idea behind this model is to pre - train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer  #TAUTHOR_TAG.', 'like any other language model, bert can be pre - trained on different contexts.', 'a contextualized representation is generally optimized for downstream nlp tasks.', 'since its release, bert has been pre - trained on a multitude of corpora.', 'in the following, we describe different bert embedding versions used for our classification problem.', 'the first version is based on the original bert release  #TAUTHOR_TAG.', 'this model is pre - trained on the bookscorpus ( 800m words )  #AUTHOR_TAG and english wikipedia ( 2, 500m words ).', 'for wikipedia, text passages were extracted while lists were ignored.', 'the second version is biobert  #AUTHOR_TAG, which was trained on biomedical corpora : pubmed ( 4. 5b words ) and pmc ( 13. 5b words )']",0
"['sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts']","['sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts']","['word, sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts']","['', 'machines ( svm ) and conditional random fields ( crf )  #AUTHOR_TAG. these models are shallow and limited in terms of modeling capacity. furthermore, most of these classifiers are trained to extract pio', 'elements one by one which is suboptimal since this approach does not', 'allow the use of shared structure among the individual classifiers. deep neural network models have increased in popularity in the field of nlp. they have pushed the state of the art of', 'text representation and information retrieval. more specifically, these techniques enhanced nlp algorithms', 'through the use of contextualized text embeddings at word, sentence, and paragraph', 'levels  #TAUTHOR_TAG. more recently,  #AUTHOR_TAG components from pubmed abstracts. to our knowledge, that study was the first in which a deep learning framework', 'was used to extract pio elements from pubmed', 'abstracts. in the present paper, we build a dataset of pio elements by improving the methodology found in  #AUTHOR_TAG. furthermore, we', 'built a multi - label pio classifier, along with a boosting framework, based on the state of the', 'art text embedding, bert. this embedding model has been proven to offer a better contextualization compared to a bidirectional lstm model  #TAUTHOR_TAG']",4
"['layers using a transformer  #TAUTHOR_TAG.', '']","['layers using a transformer  #TAUTHOR_TAG.', '']","['from transformers ) is a deep bidirectional attention text embedding model.', 'the idea behind this model is to pre - train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer  #TAUTHOR_TAG.', '']","['( bidirectional encoder representations from transformers ) is a deep bidirectional attention text embedding model.', 'the idea behind this model is to pre - train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer  #TAUTHOR_TAG.', 'like any other language model, bert can be pre - trained on different contexts.', 'a contextualized representation is generally optimized for downstream nlp tasks.', 'since its release, bert has been pre - trained on a multitude of corpora.', 'in the following, we describe different bert embedding versions used for our classification problem.', 'the first version is based on the original bert release  #TAUTHOR_TAG.', 'this model is pre - trained on the bookscorpus ( 800m words )  #AUTHOR_TAG and english wikipedia ( 2, 500m words ).', 'for wikipedia, text passages were extracted while lists were ignored.', 'the second version is biobert  #AUTHOR_TAG, which was trained on biomedical corpora : pubmed ( 4. 5b words ) and pmc ( 13. 5b words )']",1
"['layers using a transformer  #TAUTHOR_TAG.', '']","['layers using a transformer  #TAUTHOR_TAG.', '']","['from transformers ) is a deep bidirectional attention text embedding model.', 'the idea behind this model is to pre - train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer  #TAUTHOR_TAG.', '']","['( bidirectional encoder representations from transformers ) is a deep bidirectional attention text embedding model.', 'the idea behind this model is to pre - train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer  #TAUTHOR_TAG.', 'like any other language model, bert can be pre - trained on different contexts.', 'a contextualized representation is generally optimized for downstream nlp tasks.', 'since its release, bert has been pre - trained on a multitude of corpora.', 'in the following, we describe different bert embedding versions used for our classification problem.', 'the first version is based on the original bert release  #TAUTHOR_TAG.', 'this model is pre - trained on the bookscorpus ( 800m words )  #AUTHOR_TAG and english wikipedia ( 2, 500m words ).', 'for wikipedia, text passages were extracted while lists were ignored.', 'the second version is biobert  #AUTHOR_TAG, which was trained on biomedical corpora : pubmed ( 4. 5b words ) and pmc ( 13. 5b words )']",5
"['to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in']","['to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in']","['to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in figure 1.', 'using this framework, we trained the model using the two pretrained embedding models described in the previous section.', 'it is']","['', 'for the original bert model, we have chosen the smallest uncased model, bert - base.', 'the model has 12 attention layers and all texts are converted to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in figure 1.', 'using this framework, we trained the model using the two pretrained embedding models described in the previous section.', 'it is worth to mention that the embedding is contextualized during the training phase.', 'for both models, the pretrained embedding layer is frozen during the first epoch ( the embedding vectors are not updated ).', 'after the first epoch, the embedding layer is unfrozen and the vectors are fine - tuned for the classification task during training.', 'the advantage of this approach is that few parameters need to be learned from scratch  #TAUTHOR_TAG']",5
"['to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in']","['to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in']","['to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in figure 1.', 'using this framework, we trained the model using the two pretrained embedding models described in the previous section.', 'it is']","['', 'for the original bert model, we have chosen the smallest uncased model, bert - base.', 'the model has 12 attention layers and all texts are converted to lowercase by the tokenizer  #TAUTHOR_TAG.', 'the architecture of the model is illustrated in figure 1.', 'using this framework, we trained the model using the two pretrained embedding models described in the previous section.', 'it is worth to mention that the embedding is contextualized during the training phase.', 'for both models, the pretrained embedding layer is frozen during the first epoch ( the embedding vectors are not updated ).', 'after the first epoch, the embedding layer is unfrozen and the vectors are fine - tuned for the classification task during training.', 'the advantage of this approach is that few parameters need to be learned from scratch  #TAUTHOR_TAG']",7
"['political, social or technical topic.', 'classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence  #TAUTHOR_TAG.', 'our work is inspired by']","['political, social or technical topic.', 'classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence  #TAUTHOR_TAG.', 'our work is inspired by']","['social or technical topic.', 'classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence  #TAUTHOR_TAG.', 'our work is inspired by']","['work has highlighted the challenges of identifying the stance that a speaker holds towards a particular political, social or technical topic.', 'classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence  #TAUTHOR_TAG.', 'our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion websites vary a great deal.', 'one important contextual variable, discussed at length below, is the percentage of posts that are rebuttals to previous posts, which varies in our data from 34 % to 80 %.', 'the ability to explicitly rebut a previous post gives these debates both monologic and dialogic properties  #AUTHOR_TAG ; compare figure 1 to figure 2.', 'we believe that discussions containing many rebuttal links require a different type of analysis than other types of debates or discussions']",0
"['as persuasion and disagreement  #TAUTHOR_TAG. as a first step, in this paper', 'we aim to automatically']","['as persuasion and disagreement  #TAUTHOR_TAG. as a first step, in this paper', 'we aim to automatically']","['as persuasion and disagreement  #TAUTHOR_TAG. as a first step, in this paper', 'we aim to automatically']","['', ""criminals are more afraid to live - then die. i'd like to see life sentences without parole in lieu of capital punishment with hard labor and no amen"", '##ities for hard core', ""repeat offenders, the hell with pc and prisoner's rights - they lose priveledges for their behaviour"", '. figure 2 : posts on the topic capital punishment without explicit link structure. the discussion topic was "" death penalty "", and the argument was framed as yes we should keep it vs. no we should not. our long term goal is to understand the discourse and dialogic structure of such conversations. this could', 'be useful for : ( 1 ) creating automatic summaries of each position on an issue ( sparck  #AUTHOR_TAG ; ( 2 ) gaining a deeper understanding of what makes an argument persuasive  #AUTHOR_TAG ; and ( 3 ) identifying the linguistic reflexes of perlocutionary', 'acts such as persuasion and disagreement  #TAUTHOR_TAG. as a first step, in this paper', ""we aim to automatically identify rebuttals, and identify the speaker's stance towards a particular topic""]",0
"['sentiment features alone  #TAUTHOR_TAG.', 'this']","['sentiment features alone  #TAUTHOR_TAG.', 'this work, along with others, indicates that']","['sentiment features alone  #TAUTHOR_TAG.', 'this']",[' #TAUTHOR_TAG'],0
,,,,0
"['types of debates  #TAUTHOR_TAG.', 'thus']","['types of debates  #TAUTHOR_TAG.', 'thus']","['difficult to beat for certain types of debates  #TAUTHOR_TAG.', 'thus we derived both unigrams']","['', 'we used two classifiers with different properties : naivebayes and jrip.', 'jrip is a rule based classifier which produces a compact model suitable for human consumption and quick application.', 'table 3 provides a summary of the features we extract for each post.', 'we describe and motivate these feature sets below.', 'counts, unigrams, bigrams.', 'previous work suggests that the unigram baseline can be difficult to beat for certain types of debates  #TAUTHOR_TAG.', 'thus we derived both unigrams and bigrams as features.', 'we also include basic counts such as post length.', 'cue words.', 'we represent each posts initial unigram, bigram and trigram sequences to capture the useage of cue words to mark responses of particular type, such as oh really, so, and well ; these features were based on both previous work and our examination of the corpus  #AUTHOR_TAG.', 'repeated punctuation.', 'our informal analyses suggested that repeated sequential use of particular types of punctuation such as!! and?? did not mean the same thing as simple counts or frequencies of punctuation across a whole post.', 'thus we developed distinct features for a subset of these repetitions.', 'liwc.', 'we also derived features using the linguistics inquiry word count tool ( liwc - 2001 )  #AUTHOR_TAG.', 'liwc provides metalevel conceptual categories for words to use in word counts.', 'some liwc features that we expect to be important are words per sentence ( wps ), pronominal forms ( pro ), and positive and negative emotion words ( pose ) and ( nege ).', '']",0
"['types of debates  #TAUTHOR_TAG.', 'thus']","['types of debates  #TAUTHOR_TAG.', 'thus']","['difficult to beat for certain types of debates  #TAUTHOR_TAG.', 'thus we derived both unigrams']","['', 'we used two classifiers with different properties : naivebayes and jrip.', 'jrip is a rule based classifier which produces a compact model suitable for human consumption and quick application.', 'table 3 provides a summary of the features we extract for each post.', 'we describe and motivate these feature sets below.', 'counts, unigrams, bigrams.', 'previous work suggests that the unigram baseline can be difficult to beat for certain types of debates  #TAUTHOR_TAG.', 'thus we derived both unigrams and bigrams as features.', 'we also include basic counts such as post length.', 'cue words.', 'we represent each posts initial unigram, bigram and trigram sequences to capture the useage of cue words to mark responses of particular type, such as oh really, so, and well ; these features were based on both previous work and our examination of the corpus  #AUTHOR_TAG.', 'repeated punctuation.', 'our informal analyses suggested that repeated sequential use of particular types of punctuation such as!! and?? did not mean the same thing as simple counts or frequencies of punctuation across a whole post.', 'thus we developed distinct features for a subset of these repetitions.', 'liwc.', 'we also derived features using the linguistics inquiry word count tool ( liwc - 2001 )  #AUTHOR_TAG.', 'liwc provides metalevel conceptual categories for words to use in word counts.', 'some liwc features that we expect to be important are words per sentence ( wps ), pronominal forms ( pro ), and positive and negative emotion words ( pose ) and ( nege ).', '']",0
"[';  #TAUTHOR_TAG. while', 'our method']","[';  #TAUTHOR_TAG. while', 'our method']","['similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose,']","['', 'for either the pos generalized dependency features ( gdepp ) or the opinion generalized dependency features ( gdep0 ) is surprising given that they improve', 'accuracy for other similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose, 2009 ), our', 'method for extracting gdepo is an approximation of the method of  #TAUTHOR_TAG, that does not rely on selecting particular patterns indicating the topics of arguing by using a', 'development set. the liwc feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show', 'improvement over a good range of topics. we believe that further analysis is needed ; we do not', 'want to handpick topics for which particular feature sets perform well. our results also showed that context', 'did not seem to help uniformly over all topics. the mean performance over', '']",0
"[';  #TAUTHOR_TAG. while', 'our method']","[';  #TAUTHOR_TAG. while', 'our method']","['similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose,']","['', 'for either the pos generalized dependency features ( gdepp ) or the opinion generalized dependency features ( gdep0 ) is surprising given that they improve', 'accuracy for other similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose, 2009 ), our', 'method for extracting gdepo is an approximation of the method of  #TAUTHOR_TAG, that does not rely on selecting particular patterns indicating the topics of arguing by using a', 'development set. the liwc feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show', 'improvement over a good range of topics. we believe that further analysis is needed ; we do not', 'want to handpick topics for which particular feature sets perform well. our results also showed that context', 'did not seem to help uniformly over all topics. the mean performance over', '']",0
,,,,3
"[';  #TAUTHOR_TAG. while', 'our method']","[';  #TAUTHOR_TAG. while', 'our method']","['similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose,']","['', 'for either the pos generalized dependency features ( gdepp ) or the opinion generalized dependency features ( gdep0 ) is surprising given that they improve', 'accuracy for other similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose, 2009 ), our', 'method for extracting gdepo is an approximation of the method of  #TAUTHOR_TAG, that does not rely on selecting particular patterns indicating the topics of arguing by using a', 'development set. the liwc feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show', 'improvement over a good range of topics. we believe that further analysis is needed ; we do not', 'want to handpick topics for which particular feature sets perform well. our results also showed that context', 'did not seem to help uniformly over all topics. the mean performance over', '']",4
"[';  #TAUTHOR_TAG. while', 'our method']","[';  #TAUTHOR_TAG. while', 'our method']","['similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose,']","['', 'for either the pos generalized dependency features ( gdepp ) or the opinion generalized dependency features ( gdep0 ) is surprising given that they improve', 'accuracy for other similar tasks ( joshi and penstein - rose, 2009 ;  #TAUTHOR_TAG. while', 'our method of extracting the gdepp features is identical to ( joshi and penstein - rose, 2009 ), our', 'method for extracting gdepo is an approximation of the method of  #TAUTHOR_TAG, that does not rely on selecting particular patterns indicating the topics of arguing by using a', 'development set. the liwc feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show', 'improvement over a good range of topics. we believe that further analysis is needed ; we do not', 'want to handpick topics for which particular feature sets perform well. our results also showed that context', 'did not seem to help uniformly over all topics. the mean performance over', '']",6
"['- annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentangle']","['- annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentanglement datasets.  #AUTHOR_TAG studied disentanglement and', 'topic identification, but did not release']","['linux irc channel  #AUTHOR_TAG 2011', '). until now, their dataset was the only publicly available set of messages with annotated conversations ( partially re - annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentanglement datasets.  #AUTHOR_TAG studied disentanglement and', 'topic identification, but did not release their data.  #AUTHOR_TAG annotated conversations and discourse relations']","['##ntanglement is a line of papers developing data and models for the # linux irc channel  #AUTHOR_TAG 2011', '). until now, their dataset was the only publicly available set of messages with annotated conversations ( partially re - annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentanglement datasets.  #AUTHOR_TAG studied disentanglement and', 'topic identification, but did not release their data.  #AUTHOR_TAG annotated conversations and discourse relations in the # ubuntu - fr', 'channel ( french ubuntu support ).  #AUTHOR_TAG lowe et al.', '(, 2017 heuristically extracted conversations from the # ubuntu channel. 2 their work opened up a new research opportunity by providing 930, 000', 'disentangled conversations, and has already been the basis of many papers ( 315 citations ), particularly on', 'developing dialogue agents. this is far beyond', 'the size of resources previously collected, even with crowdsourcing  #AUTHOR_TAG. using', 'our data we provide the first empirical evaluation of their method. other disentangle', '##ment data : irc is not the only form of synchronous group conversation online. other platforms with', 'similar communication formats have been studied in settings such as classes  #AUTHOR_TAG', ', support communities  #AUTHOR_TAG, and customer service  #AUTHOR_TAG. unfortunately, only one of these resources  #AUTHOR_TAG is available, possibly due to privacy concerns. another stream of research', 'has used userprovided structure to get conversation labels  #AUTHOR_TAG and replyto relations ( wang and rose, 2010 ;  #AUTHOR_TAG a ;  #AUTHOR_TAG balali et al.,, 2014  #AUTHOR_TAG a ). by removing these', 'labels and mixing conversations they create a disentanglement problem. while convenient, this risks introducing a bias, as people', 'write differently when explicit structure is defined, and only a few papers have', 'released data  #AUTHOR_TAG. models :  #AUTHOR_TAG explored various message - pair feature sets and linear classifiers', ', combined with local and global inference methods. their system is the only publicly released statistical model for disentanglement', 'of chat conversation, but most of the other work cited above applied similar models. we evaluate their model on', 'both our data and our re - annotated version of their data. recent work has applied neural networks  #TAUTHOR_TAG ;  #AUTHOR_TAG 1, 500 1 48 hr 5 n / a 2 table 1 : annotated disentangle', '##ment dataset comparison. our data is much larger than prior work, one of the only released', 'sets, and the only one with context and adjudication.', ""' + a'indicates there was an adjudication step to resolve disagreements. '? '"", 'indicates the value is not in the paper and the authors no longer have access to the data']",0
"['- annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentangle']","['- annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentanglement datasets.  #AUTHOR_TAG studied disentanglement and', 'topic identification, but did not release']","['linux irc channel  #AUTHOR_TAG 2011', '). until now, their dataset was the only publicly available set of messages with annotated conversations ( partially re - annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentanglement datasets.  #AUTHOR_TAG studied disentanglement and', 'topic identification, but did not release their data.  #AUTHOR_TAG annotated conversations and discourse relations']","['##ntanglement is a line of papers developing data and models for the # linux irc channel  #AUTHOR_TAG 2011', '). until now, their dataset was the only publicly available set of messages with annotated conversations ( partially re - annotated by', ' #TAUTHOR_TAG. we are aware of three', 'other irc disentanglement datasets.  #AUTHOR_TAG studied disentanglement and', 'topic identification, but did not release their data.  #AUTHOR_TAG annotated conversations and discourse relations in the # ubuntu - fr', 'channel ( french ubuntu support ).  #AUTHOR_TAG lowe et al.', '(, 2017 heuristically extracted conversations from the # ubuntu channel. 2 their work opened up a new research opportunity by providing 930, 000', 'disentangled conversations, and has already been the basis of many papers ( 315 citations ), particularly on', 'developing dialogue agents. this is far beyond', 'the size of resources previously collected, even with crowdsourcing  #AUTHOR_TAG. using', 'our data we provide the first empirical evaluation of their method. other disentangle', '##ment data : irc is not the only form of synchronous group conversation online. other platforms with', 'similar communication formats have been studied in settings such as classes  #AUTHOR_TAG', ', support communities  #AUTHOR_TAG, and customer service  #AUTHOR_TAG. unfortunately, only one of these resources  #AUTHOR_TAG is available, possibly due to privacy concerns. another stream of research', 'has used userprovided structure to get conversation labels  #AUTHOR_TAG and replyto relations ( wang and rose, 2010 ;  #AUTHOR_TAG a ;  #AUTHOR_TAG balali et al.,, 2014  #AUTHOR_TAG a ). by removing these', 'labels and mixing conversations they create a disentanglement problem. while convenient, this risks introducing a bias, as people', 'write differently when explicit structure is defined, and only a few papers have', 'released data  #AUTHOR_TAG. models :  #AUTHOR_TAG explored various message - pair feature sets and linear classifiers', ', combined with local and global inference methods. their system is the only publicly released statistical model for disentanglement', 'of chat conversation, but most of the other work cited above applied similar models. we evaluate their model on', 'both our data and our re - annotated version of their data. recent work has applied neural networks  #TAUTHOR_TAG ;  #AUTHOR_TAG 1, 500 1 48 hr 5 n / a 2 table 1 : annotated disentangle', '##ment dataset comparison. our data is much larger than prior work, one of the only released', 'sets, and the only one with context and adjudication.', ""' + a'indicates there was an adjudication step to resolve disagreements. '? '"", 'indicates the value is not in the paper and the authors no longer have access to the data']",0
"['problem we explore.', 'studies that do consider graphs for disentanglement have used small datasets  #TAUTHOR_TAG that are not always released  #AUTHOR_TAG']","['with annotated graph structures has been for threaded web forums  #AUTHOR_TAG b ), which do not exhibit the disentanglement problem we explore.', 'studies that do consider graphs for disentanglement have used small datasets  #TAUTHOR_TAG that are not always released  #AUTHOR_TAG']","['with annotated graph structures has been for threaded web forums  #AUTHOR_TAG b ), which do not exhibit the disentanglement problem we explore.', 'studies that do consider graphs for disentanglement have used small datasets  #TAUTHOR_TAG that are not always released  #AUTHOR_TAG']","['structure : within a conversation, we define a graph of reply - to relations.', 'almost all prior work with annotated graph structures has been for threaded web forums  #AUTHOR_TAG b ), which do not exhibit the disentanglement problem we explore.', 'studies that do consider graphs for disentanglement have used small datasets  #TAUTHOR_TAG that are not always released  #AUTHOR_TAG']",0
[' #TAUTHOR_TAG'],"["" #TAUTHOR_TAG's annotations."", 'results are not shown for  #AUTHOR_TAG']",[' #TAUTHOR_TAG'],"['annotations define two levels of structure : ( 1 ) links between pairs of messages, and ( 2 ) sets of messages, where each set is one conversation.', 'annotators label ( 1 ), from which ( 2 ) can be inferred.', 'table 2 presents inter - annotator agreement measures for both cases.', 'these are measured in the standard manner, by comparing the labels from different annotators on the same data.', 'we also include measurements for annotations in prior work.', 'figure 2 shows ambiguous examples from our data to provide some intuition for the source of disagreements.', 'in both examples the disagreement involves one link, but the conversation structure in the second case is substantially changed.', 'some disagreements in our data are mistakes, where one annotation is clearly incorrect, and some are ambiguous cases, such as these.', ""in channel two, we also see mistakes and ambiguous cases, including a particularly long discussion about a user's financial difficulties that could be divided in multiple ways ( also noted by  #AUTHOR_TAG )."", ""graphs : we measure agreement on the graph structure annotation using  #AUTHOR_TAG's κ."", 'this measure of inter - rater reliability corrects for chance agreement, accounting for the class imbalance between linked and not - linked pairs.', ""values are in the good agreement range proposed by  #AUTHOR_TAG, and slightly higher than for  #TAUTHOR_TAG's annotations."", 'results are not shown for  #AUTHOR_TAG because they did not annotate graphs']",4
"['optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['that larger values are better.', '( 2 ) one - to - one overlap ( 1 - 1,  #AUTHOR_TAG.', 'percentage overlap when conversations from two annotations are optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['consider three metrics : 6 ( 1 ) variation of information ( vi,  #AUTHOR_TAG.', 'a measure of information gained or lost when going from one clustering to another.', 'it is the sum of conditional entropies h ( y | x ) + h ( x | y ), where x and y are clusterings of the same set of items.', 'we consider a scaled version, using the bound for n items that vi ( x ; y ) ≤ log ( n ), and present 1−vi so that larger values are better.', '( 2 ) one - to - one overlap ( 1 - 1,  #AUTHOR_TAG.', 'percentage overlap when conversations from two annotations are optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG and keep system messages.', '( 3 ) exact match f 1.', 'calculated using the number of perfectly matching conversations, excluding conversations with only one message ( mostly system messages ).', 'this is an extremely challenging metric.', 'we include it because it is easy to understand and it directly measures a desired value ( perfectly extracted conversations ).', 'our scores are higher in 4 cases and lower in 5.', 'interestingly, while κ was higher for us than  #TAUTHOR_TAG, our scores for conversations are lower.', 'this is possible because a single link can merge two conversations, meaning a single disagreement in links can cause a major difference in conversations.', 'this may reflect the fact that our annotation guide was developed for the ubuntu channel, which differs in conversation style from the channel two data.', 'manually comparing the annotations, there was no clear differences in the types of disagreements.', 'agreement is lower on the channel two data, particularly on its test set.', 'from this we conclude that there is substantial variation in the difficulty of conversation disentanglement across datasets.', '']",4
"['apart consecutive messages in a conversation are :  #AUTHOR_TAG and  #TAUTHOR_TAG use a limit of 129 seconds,  #AUTHOR_TAG limit']","['apart consecutive messages in a conversation are :  #AUTHOR_TAG and  #TAUTHOR_TAG use a limit of 129 seconds,  #AUTHOR_TAG limit']","['.', 'this demonstrates the importance of evaluating on data from more than one point in time to get a robust estimate of performance.', 'how far apart consecutive messages in a conversation are :  #AUTHOR_TAG and  #TAUTHOR_TAG use a limit of 129 seconds,  #AUTHOR_TAG limit']","['', 'this demonstrates the importance of evaluating on data from more than one point in time to get a robust estimate of performance.', 'how far apart consecutive messages in a conversation are :  #AUTHOR_TAG and  #TAUTHOR_TAG use a limit of 129 seconds,  #AUTHOR_TAG limit to within 1 hour,  #AUTHOR_TAG limit to within 8 messages, and we limit to within 100 messages.', 'figure 4 shows the distribution of time differences in our conversations.', '']",4
"['optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['that larger values are better.', '( 2 ) one - to - one overlap ( 1 - 1,  #AUTHOR_TAG.', 'percentage overlap when conversations from two annotations are optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['consider three metrics : 6 ( 1 ) variation of information ( vi,  #AUTHOR_TAG.', 'a measure of information gained or lost when going from one clustering to another.', 'it is the sum of conditional entropies h ( y | x ) + h ( x | y ), where x and y are clusterings of the same set of items.', 'we consider a scaled version, using the bound for n items that vi ( x ; y ) ≤ log ( n ), and present 1−vi so that larger values are better.', '( 2 ) one - to - one overlap ( 1 - 1,  #AUTHOR_TAG.', 'percentage overlap when conversations from two annotations are optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG and keep system messages.', '( 3 ) exact match f 1.', 'calculated using the number of perfectly matching conversations, excluding conversations with only one message ( mostly system messages ).', 'this is an extremely challenging metric.', 'we include it because it is easy to understand and it directly measures a desired value ( perfectly extracted conversations ).', 'our scores are higher in 4 cases and lower in 5.', 'interestingly, while κ was higher for us than  #TAUTHOR_TAG, our scores for conversations are lower.', 'this is possible because a single link can merge two conversations, meaning a single disagreement in links can cause a major difference in conversations.', 'this may reflect the fact that our annotation guide was developed for the ubuntu channel, which differs in conversation style from the channel two data.', 'manually comparing the annotations, there was no clear differences in the types of disagreements.', 'agreement is lower on the channel two data, particularly on its test set.', 'from this we conclude that there is substantial variation in the difficulty of conversation disentanglement across datasets.', '']",3
"['other messages as singleton conversations.', 'for channel two we also compare to  #AUTHOR_TAG and  #TAUTHOR_TAG, but their code was unavailable, preventing evaluation on our data.', 'we']","['other messages as singleton conversations.', 'for channel two we also compare to  #AUTHOR_TAG and  #TAUTHOR_TAG, but their code was unavailable, preventing evaluation on our data.', 'we']","['other messages as singleton conversations.', 'for channel two we also compare to  #AUTHOR_TAG and  #TAUTHOR_TAG, but their code was unavailable, preventing evaluation on our data.', 'we exclude  #AUTHOR_TAG']","['', 'link messages with no agreed antecedent to themselves.', 'intersect : conversations that 10 ff models agree on, and other messages as singleton conversations.', 'for channel two we also compare to  #AUTHOR_TAG and  #TAUTHOR_TAG, but their code was unavailable, preventing evaluation on our data.', 'we exclude  #AUTHOR_TAG as they substantially modified the dataset.', 'for details of models, including hyperparameters tuned on the development set, see the supplementary material.', 'table 5 : performance with different training conditions on the ubuntu test set.', 'for graph - f, * indicates a significant difference at the 0. 01 level compared to standard.', 'results are averages over 10 runs, varying the data and random seeds.', 'the standard deviation is shown in parentheses']",3
"['optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['that larger values are better.', '( 2 ) one - to - one overlap ( 1 - 1,  #AUTHOR_TAG.', 'percentage overlap when conversations from two annotations are optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG']","['consider three metrics : 6 ( 1 ) variation of information ( vi,  #AUTHOR_TAG.', 'a measure of information gained or lost when going from one clustering to another.', 'it is the sum of conditional entropies h ( y | x ) + h ( x | y ), where x and y are clusterings of the same set of items.', 'we consider a scaled version, using the bound for n items that vi ( x ; y ) ≤ log ( n ), and present 1−vi so that larger values are better.', '( 2 ) one - to - one overlap ( 1 - 1,  #AUTHOR_TAG.', 'percentage overlap when conversations from two annotations are optimally paired up using the max - flow algorithm.', 'we follow  #TAUTHOR_TAG and keep system messages.', '( 3 ) exact match f 1.', 'calculated using the number of perfectly matching conversations, excluding conversations with only one message ( mostly system messages ).', 'this is an extremely challenging metric.', 'we include it because it is easy to understand and it directly measures a desired value ( perfectly extracted conversations ).', 'our scores are higher in 4 cases and lower in 5.', 'interestingly, while κ was higher for us than  #TAUTHOR_TAG, our scores for conversations are lower.', 'this is possible because a single link can merge two conversations, meaning a single disagreement in links can cause a major difference in conversations.', 'this may reflect the fact that our annotation guide was developed for the ubuntu channel, which differs in conversation style from the channel two data.', 'manually comparing the annotations, there was no clear differences in the types of disagreements.', 'agreement is lower on the channel two data, particularly on its test set.', 'from this we conclude that there is substantial variation in the difficulty of conversation disentanglement across datasets.', '']",5
"['##s and conjunctions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG presented evidence that gaze features']","['and longer on open syntactic categories ( verbs, nouns, adjectives ) than on closed class items like prepositions and conjunctions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG presented evidence that gaze features']","['##s and conjunctions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG presented evidence that gaze features can be used to discriminate between most pairs of parts of speech ( pos ).', 'their study uses all the coarse - grained pos labels proposed']","['fixate more and longer on open syntactic categories ( verbs, nouns, adjectives ) than on closed class items like prepositions and conjunctions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG presented evidence that gaze features can be used to discriminate between most pairs of parts of speech ( pos ).', 'their study uses all the coarse - grained pos labels proposed by  #AUTHOR_TAG.', 'this paper investigates to what extent gaze data can also be used to predict grammatical functions such as subjects and objects.', 'we first show that a simple logistic regression classifier trained on a very small seed of data using gaze features discriminates between some pairs of grammatical functions.', 'we show that the same kind of classifier distinguishes well between the four main grammatical functions of nouns, pobj, dobj, nn and nsubj.', 'in § 3, we also show how gaze features can be used to improve dependency parsing.', 'many gaze features correlate with word length and word figure 1 : a dependency structure with average fixation duration per word frequency  #AUTHOR_TAG and these could be as good as gaze features, while being easier to obtain.', 'we use frequencies from the unlabelled portions of the english web treebank and word length as baseline in all types of experiments and find that gaze features to be better predictors for the noun experiment as well as for improving parsers.', 'this work is of psycholinguistic interest, but we show that gaze features may have practical relevance, by demonstrating that they can be used to improve a dependency parser.', 'eye - tracking data becomes more readily available with the emergence of eye trackers in mainstream consumer products  #AUTHOR_TAG.', 'with the development of robust eye - tracking in laptops, it is easy to imagine digital text providers storing gaze data, which could then be used as partial annotation of their publications.', 'contributions we demonstrate that we can discriminate between some grammatical functions using gaze features and which features are fit for the task.', 'we show a practical use for data reflecting human cognitive processing.', 'finally, we use gaze features to improve a transition - based dependency parser, comparing also to dependency parsers augmented with word embeddings']",0
"['data comes from  #TAUTHOR_TAG and is publicly available 1.', 'in this experiment 10 native english speakers read 250 syntactically annotated sentences in english ( min.', '3 tokens, max.', '120 characters ).', 'the sentences were randomly sampled from']","['data comes from  #TAUTHOR_TAG and is publicly available 1.', 'in this experiment 10 native english speakers read 250 syntactically annotated sentences in english ( min.', '3 tokens, max.', '120 characters ).', 'the sentences were randomly sampled from']","['data comes from  #TAUTHOR_TAG and is publicly available 1.', 'in this experiment 10 native english speakers read 250 syntactically annotated sentences in english ( min.', '3 tokens, max.', '120 characters ).', 'the sentences were randomly sampled from']","['data comes from  #TAUTHOR_TAG and is publicly available 1.', 'in this experiment 10 native english speakers read 250 syntactically annotated sentences in english ( min.', '3 tokens, max.', '120 characters ).', 'the sentences were randomly sampled from one of five different, manually annotated corpora from different domains : wall street journal articles ( wsj ), wall street journal headlines ( hdl ), emails ( mai ), weblogs ( wbl ), and twitter ( twi ) 2.', 'see figure 1 for an example.', 'features it is not yet established which eye movement reading features are fit for the task of distinguishing grammatical functions of the words.', 'to explore this, we extracted a broad selection of word - and sentence - based features.', 'the features are inspired by salojarvi et al. ( 2003 ) who used a similar exploratory approach.', 'for a full list of features, see appendix']",5
"['to be predictive of pos  #TAUTHOR_TAG, however, the probability that']","['to be predictive of pos  #TAUTHOR_TAG, however, the probability that']","['to the features that were found to be predictive of pos  #TAUTHOR_TAG, however, the probability that']","['features to investigate which gaze features were more predictive of grammatical function, we used stability selection ( meinshausen and buhlmann, 2010 ) with logistic regression classification on binary dependency relation classifications on the most frequent dependency relations.', 'for each pair of dependencies, we perform a five - fold cross validation and record the informative features from each run.', 'table 1 shows the 15 most used features in ranked order with their proportion of all votes.', 'the features predictive of grammatical functions are similar to the features that were found to be predictive of pos  #TAUTHOR_TAG, however, the probability that a word gets first and second fixation were not important features for pos classification, whereas they are contributing to dependency classification.', 'this could suggest that words with certain grammatical functions are consistently more likely or less likely to get first and second fixation, but could also be due to a frequent syntactic order in the sample.', 'binary discrimination error reduction over the baseline can be seen in figure 2.', 'the mean accuracy using logistic regression on all binary classification problems between grammatical functions is 0. 722.', 'the frequency - position - word length baseline is 0. 706.', 'in other words, using gaze features leads to a 5. 6 % error reduction over the baseline.', '']",3
"['to  #TAUTHOR_TAG, our']","['to  #TAUTHOR_TAG, our']","['to  #TAUTHOR_TAG, our']","['addition to  #TAUTHOR_TAG, our work relates to matthies and søgaard ( 2013 ), who study the robustness of a fixation prediction model across readers, not domains, but our work also relates in spirit to research on using weak supervision in nlp, e. g., work on using html markup to improve dependency parsers  #AUTHOR_TAG or using click - through data to improve pos taggers  #AUTHOR_TAG.', 'there have been few studies correlating reading behavior and general dependency syntax in the literature.', ' #AUTHOR_TAG, having parsed the dundee corpus using minipar, show that dependency integration cost, roughly the distance between a word and its head, is pre - dictive of reading times for nouns.', 'our finding could be a side - effect of this, since nsubj, nn and dobj / pobj typically have very different dependency integration costs, while dobj and pobj have about the same.', 'their study thus seems to support our finding that gaze features can be used to discriminate between the grammatical functions of nouns.', 'most other work of this kind focus on specific phenomena, e. g.,  #AUTHOR_TAG, who show that subjects find it harder to process object relative clauses than subject relative clauses.', 'this paper is related to such work, but our interest is a broader model of syntactic influences on reading patterns']",7
"['following  #TAUTHOR_TAG,']","['following  #TAUTHOR_TAG,']","['following  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG,']","['each phrase x i : j by l i, j = h i : j · h.', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', '[CLS] corresponding to the start word ( and dropping the superscript']","['', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', ""[CLS] corresponding to the start word ( and dropping the superscript'start') ; the second term can be computed in the same way."", 'denote the question side query, key, and n - gram feature matrices, respectively.', 'we can efficiently compute it if we precompute ff ∈ r n ×m.', '']",5
"['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG,']","['each phrase x i : j by l i, j = h i : j · h.', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', '[CLS] corresponding to the start word ( and dropping the superscript']","['', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', ""[CLS] corresponding to the start word ( and dropping the superscript'start') ; the second term can be computed in the same way."", 'denote the question side query, key, and n - gram feature matrices, respectively.', 'we can efficiently compute it if we precompute ff ∈ r n ×m.', '']",5
"['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG,']","['each phrase x i : j by l i, j = h i : j · h.', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', '[CLS] corresponding to the start word ( and dropping the superscript']","['', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', ""[CLS] corresponding to the start word ( and dropping the superscript'start') ; the second term can be computed in the same way."", 'denote the question side query, key, and n - gram feature matrices, respectively.', 'we can efficiently compute it if we precompute ff ∈ r n ×m.', '']",5
"['storage reduction and search techniques by  #TAUTHOR_TAG.', 'for storage, the total size of the index is 1']","['storage reduction and search techniques by  #TAUTHOR_TAG.', 'for storage, the total size of the index is 1. 3 tb including unigram and bigram sparse representations.', 'for search, we']","['encoded phrase representations of all documents in wikipedia ( more than 5 million documents ).', 'it takes 600 gpu hours to index all phrases in wikipedia.', 'each phrase representation has 2d se + 2f + 1 dimensions.', 'we use the same storage reduction and search techniques by  #TAUTHOR_TAG.', 'for storage, the total size of the index is 1']","['use and finetune bert large for our encoders.', 'we use bert vocabulary which has 30522 unique tokens based on byte pair encodings.', 'as a result, we have f = 30522 when using unigram feature for f, and f ≈ 1b when using both uni / bigram features.', 'we do not finetune the word embedding during training.', 'we pre - compute and store all encoded phrase representations of all documents in wikipedia ( more than 5 million documents ).', 'it takes 600 gpu hours to index all phrases in wikipedia.', 'each phrase representation has 2d se + 2f + 1 dimensions.', 'we use the same storage reduction and search techniques by  #TAUTHOR_TAG.', 'for storage, the total size of the index is 1. 3 tb including unigram and bigram sparse representations.', 'for search, we either perform dense search first and then rerank with sparse scores ( dfs ) or perform sparse search first and rerank with dense scores ( sfs ), and also consider a combination of both ( hybrid )']",5
"['', 'table 4 shows the outputs of three openqa models : drqa  #AUTHOR_TAG, denspi  #TAUTHOR_TAG, and our denspi + cospr']","['', 'table 4 shows the outputs of three openqa models : drqa  #AUTHOR_TAG, denspi  #TAUTHOR_TAG, and our denspi + cospr.', 'denspi + cospr is able to']","['', 'table 4 shows the outputs of three openqa models : drqa  #AUTHOR_TAG, denspi  #TAUTHOR_TAG, and our denspi + cospr']","['##ability of sparse representations sparse representations often have better interpretability than dense representations as each dimension of a sparse vector corresponds to a specific word.', 'we compare tf - idf vectors and cospr ( uni / bigram ) by showing top weighted n - grams in each representation.', 'note that the scale of weights in tf - idf vectors is normalized in open - domain setups to match the scale between tf - idf vectors and dense vectors.', 'we observe that tf - idf vectors usually assign high weights on infrequent ( often meaningless ) n - grams, while cospr focuses on contextually important entities such as 1991 for 415, 000 or california state, state university for 12.', 'our sparse question representation also learns meaningful n - gram weights compared to tf - idf vectors.', 'table 4 shows the outputs of three openqa models : drqa  #AUTHOR_TAG, denspi  #TAUTHOR_TAG, and our denspi + cospr.', 'denspi + cospr is able to retrieve various correct answers from different documents, and it often correctly answers questions with specific dates or numbers compared to denspi showing the effectiveness of learned sparse representations']",5
"['following  #TAUTHOR_TAG,']","['following  #TAUTHOR_TAG,']","['following  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],4
"['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG,']","['each phrase x i : j by l i, j = h i : j · h.', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', '[CLS] corresponding to the start word ( and dropping the superscript']","['', ""unlike  #TAUTHOR_TAG, each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."", 'we define the sparse logit for phrase x i : j as l sparse', '. for brevity, we describe how we compute the first term s start i · s start', ""[CLS] corresponding to the start word ( and dropping the superscript'start') ; the second term can be computed in the same way."", 'denote the question side query, key, and n - gram feature matrices, respectively.', 'we can efficiently compute it if we precompute ff ∈ r n ×m.', '']",4
"[',  #TAUTHOR_TAG propose to']","['this problem,  #TAUTHOR_TAG propose to']","['this problem,  #TAUTHOR_TAG propose to']",[' #TAUTHOR_TAG'],0
['computing coherency of the phrase. refer to  #TAUTHOR_TAG for details ; we'],['computing coherency of the phrase. refer to  #TAUTHOR_TAG for details ; we'],['computing coherency of the phrase. refer to  #TAUTHOR_TAG for details ; we mostly reuse its architecture'],"['', 'domain qa ( e. g. 5 million for english wikipedia ), which makes the task computationally challenging. pipeline - based methods typically leverage a document retriever to reduce the number of documents to read', ', but they suffer from error propagation when wrong documents are retrieved and can be slow if the reader model is computationally cumbersome. open - domain qa with phrase encoding and retrieval as an alternative, phrase - retrieval approaches  #AUTHOR_TAG', 'mitigate this problem by directly accessing all phrases in k documents by decomposing f into two functions, a = arg max where · denotes inner product operation. unlike running a complex reading comprehension model in pipeline - based', 'approaches, h x query - agn', '##ostically encode ( all possible phrases of ) each document just once, so that we just need to compute h q ( which is very fast ) and perform similarity search on the phrase encoding ( which is also fast ).  #AUTHOR_TAG have shown that encoding each phrase with a concatenation of dense and sparse representations is effective,', 'where the dense part is computed from bert  #AUTHOR_TAG and the sparse part is obtained from the tf - idf vector of the document and the paragraph of the phrase. we briefly describe how the dense part is obtained below. dense representation assuming that the document x', 'has n words as x 1,...,', 'x n, seo et al. ( 2019 ) use bert  #AUTHOR_TAG to compute contextualized representation of each word as h 1', ',..., h n = bert ( x 1,..., x n ). based on the contextualized embeddings, we obtain dense phrase representations as follows : we split each', 'and d c are chosen to make d = 2d se + 2d c ).', 'then each phrase x i : j is densely represented as follows : where · denotes inner product', 'operation. h 1 i and h 2 j are start / end', 'representations of a phrase, and the', 'inner product of h 3 i and h 4 j is used for computing coherency of the phrase. refer to  #TAUTHOR_TAG for details ; we mostly reuse its architecture']",0
"[',  #TAUTHOR_TAG propose to']","['this problem,  #TAUTHOR_TAG propose to']","['this problem,  #TAUTHOR_TAG propose to']",[' #TAUTHOR_TAG'],1
['denspi  #TAUTHOR_TAG with contextualized sparse'],['of cospr by augmenting denspi  #TAUTHOR_TAG with contextualized sparse'],['evaluate the effectiveness of cospr by augmenting denspi  #TAUTHOR_TAG with contextualized sparse representations ( denspi + cospr )'],"['evaluate the effectiveness of cospr by augmenting denspi  #TAUTHOR_TAG with contextualized sparse representations ( denspi + cospr ).', 'we extensively compare the model with the original denspi and previous pipeline - based qa models.', 'model squadopen curatedtrec em f1 exact match s / q drqa  #AUTHOR_TAG 29. 8 * * - 25. 4 * 35 r 3  #AUTHOR_TAG a ) 29. 1 37. 5 28. 4 * - paragraph ranker  #AUTHOR_TAG 30. 2 - 35. 4 * - multi - step - reasoner  #AUTHOR_TAG 31. 9 39. 2 - - bertserini  #AUTHOR_TAG 38. 6 46. 1 - 115 orqa 20. 2 - 30. 1 - multi - passage bert † †  #AUTHOR_TAG 53. 0 60. 9 - - denspi  #AUTHOR_TAG 36  #AUTHOR_TAG while being almost two orders of magnitude faster.', 'we expect much bigger speed gaps between ours and other pipeline methods as most of them put additional complex components to the original pipelined methods.', 'on curatedtrec, which is constructed from real user queries, our model also achieves the stateof - the - art performance.', 'even though our model is only trained on squad ( i. e. zero - shot ), it outperforms all other models which are either distant - or semi - supervised with at least 29x faster inference.', 'note that our method is orthogonal to end - to - end training or weak supervision  #AUTHOR_TAG methods and future work can potentially benefit from these']",6
"['resource poor ones.', 'transfer learning  #TAUTHOR_TAG is a simple approach in which we']","['resource poor ones.', 'transfer learning  #TAUTHOR_TAG is a simple approach in which we']","['resource poor ones.', 'transfer learning  #TAUTHOR_TAG is a simple approach in which we']","['machine translation ( nmt ) is known to outperform phrase based statistical machine translation ( pbsmt ) for resource rich language pairs but not for resource poor ones.', 'transfer learning  #TAUTHOR_TAG is a simple approach in which we can simply initialize an nmt model ( child model ) for a resource poor language pair using a previously trained model ( parent model ) for a resource rich language pair where the target languages are the same.', 'this paper explores how different choices of parent models affect the performance of child models.', 'we empirically show that using a parent model with the source language falling in the same or linguistically similar language family as the source language of the child model is the best']",1
"['by two factors :', '• a. we wanted to replicate the basic transfer learning results  #TAUTHOR_TAG and']","['by two factors :', '• a. we wanted to replicate the basic transfer learning results  #TAUTHOR_TAG and']","['from the open web they represent a realistic scenario and hence it should be evident that the child language pairs are truly resource poor.', 'our choice of languages was influenced by two factors :', '• a. we wanted to replicate the basic transfer learning results  #TAUTHOR_TAG and']","['set of parent languages ( and abbreviations ) we considered is : hindi ( hi ), indonesian ( id ), turkish ( tr ), russian ( ru ), german ( de ) and french ( fr ).', 'the set of child languages ( and abbreviations ) consists of : luxembourgish ( lb ), hausa ( ha ), somali ( so ), malayalam ( ml ), punjabi ( pa ), marathi ( mr ), uzbek ( uz ), javanese ( jw ), kazakah ( kk ) and sundanese ( su ).', 'table 1 groups the languages into language families.', 'for each child model we try around 3 to 4 parent models out of which one is mostly learned from a linguistically close parent language pair.', 'the source languages vary but the target language is always english.', 'since there are no standard training sets for many of these language pairs, we use parallel data automatically mined from the web using an in - house crawler.', 'for evaluation, we use a set of 9k english sentences collected from the web and translated by humans into each of the source languages mentioned above.', 'each sentence has one reference translation.', 'we use 5k sentences for evaluation and the rest form the development set.', 'to give a rough idea of the corpora sizes consider the wmt14 dataset for german - english which contains around 5m lines of parallel corpora for training.', 'the child language pair corpora sizes vary from being one decimal order of magnitude smaller to one decimal order of magnitude larger than the wmt14 german - english corpus.', 'however the parent language pair corpora are two to three decimal orders of magnitude larger than the aforementioned dataset.', 'from left to right, the languages above are ordered according to the size of their corpora with the leftmost being the one with the smallest dataset.', 'since these datasets are mined from the open web they represent a realistic scenario and hence it should be evident that the child language pairs are truly resource poor.', 'our choice of languages was influenced by two factors :', '• a. we wanted to replicate the basic transfer learning results  #TAUTHOR_TAG and hence chose french, german for hausa and uzbek.', '• b. we wanted to compare the effects of using parent languages belonging to the same']",1
"['javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the']","['javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the french - english corpus since it is the largest one amongst all our pairs.', 'we deliberately chose this']","['javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the french - english corpus since it is the largest one amongst all our pairs.', 'we deliberately chose this']","['', '• opportunistic experimentation on 4 child languages ( kazakh, javanese, sundanese and luxembourgish ) by using 3 parent languages out of which one is from the same language family and the other two are from another language family.', 'turkish being the related language for kazakh, german for luxembourgish and indonesian for javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the french - english corpus since it is the largest one amongst all our pairs.', 'we deliberately chose this since we wished to maintain the same target side vocabulary for all our experiments ( both baseline and transfer ) for fair comparison.', 'the parent source vocabulary ( and hence embeddings ) is randomly mapped to child source vocabulary since it was shown that nmt is less sensitive to it']",1
['as described in  #TAUTHOR_TAG where we'],['as described in  #TAUTHOR_TAG where we'],['as described in  #TAUTHOR_TAG where we learn a model ( parent model ) for a resource rich language pair ( hindi - english ) and use'],"['to figure 1 for an overview of the method.', 'it is essentially the same as described in  #TAUTHOR_TAG where we learn a model ( parent model ) for a resource rich language pair ( hindi - english ) and use it to initialize the model ( child model ) for the resource poor pair ( marathi - english ).', 'henceforth the source languages of the parent model and 282 child models will be known as parent and child languages respectively and the corresponding language pairs will be known as the parent and child language pairs respectively.', 'the target language vocabulary ( english ) should be the same for both the parent and the child models.', 'following the originally proposed method we focused on freezing 1 ( by setting gradients to zero ) the decoder embeddings and softmax layers when learning child models since they represent the majority of the decoder parameter space.', 'this method can easily be applied in cases where we wish to use the x - y pair to help the z - y pair where y is usually english']",3
"['javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the']","['javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the french - english corpus since it is the largest one amongst all our pairs.', 'we deliberately chose this']","['javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the french - english corpus since it is the largest one amongst all our pairs.', 'we deliberately chose this']","['', '• opportunistic experimentation on 4 child languages ( kazakh, javanese, sundanese and luxembourgish ) by using 3 parent languages out of which one is from the same language family and the other two are from another language family.', 'turkish being the related language for kazakh, german for luxembourgish and indonesian for javanese and sundanese.', 'the model and training details are the same as that in  #TAUTHOR_TAG note that the target language ( english ) vocabulary is same for all settings and the wpm is learned on the english side of the french - english corpus since it is the largest one amongst all our pairs.', 'we deliberately chose this since we wished to maintain the same target side vocabulary for all our experiments ( both baseline and transfer ) for fair comparison.', 'the parent source vocabulary ( and hence embeddings ) is randomly mapped to child source vocabulary since it was shown that nmt is less sensitive to it']",3
['as described in  #TAUTHOR_TAG where we'],['as described in  #TAUTHOR_TAG where we'],['as described in  #TAUTHOR_TAG where we learn a model ( parent model ) for a resource rich language pair ( hindi - english ) and use'],"['to figure 1 for an overview of the method.', 'it is essentially the same as described in  #TAUTHOR_TAG where we learn a model ( parent model ) for a resource rich language pair ( hindi - english ) and use it to initialize the model ( child model ) for the resource poor pair ( marathi - english ).', 'henceforth the source languages of the parent model and 282 child models will be known as parent and child languages respectively and the corresponding language pairs will be known as the parent and child language pairs respectively.', 'the target language vocabulary ( english ) should be the same for both the parent and the child models.', 'following the originally proposed method we focused on freezing 1 ( by setting gradients to zero ) the decoder embeddings and softmax layers when learning child models since they represent the majority of the decoder parameter space.', 'this method can easily be applied in cases where we wish to use the x - y pair to help the z - y pair where y is usually english']",5
"['nmt model design as in  #TAUTHOR_TAG.', 'in']","['nmt model design as in  #TAUTHOR_TAG.', 'in']","['of our experiments were performed using an encoder - decoder nmt system with attention for the various baselines and transfer learning experiments.', 'we used an in house nmt system developed using the tensorflow  #AUTHOR_TAG framework so as to exploit multiple gpus to speed up training.', 'to ensure replicability we use the same nmt model design as in  #TAUTHOR_TAG.', 'in']","['of our experiments were performed using an encoder - decoder nmt system with attention for the various baselines and transfer learning experiments.', 'we used an in house nmt system developed using the tensorflow  #AUTHOR_TAG framework so as to exploit multiple gpus to speed up training.', 'to ensure replicability we use the same nmt model design as in  #TAUTHOR_TAG.', 'in order to enable infinite vocabulary we use the word piece model ( wpm )  #AUTHOR_TAG as a segmentation model which is closely related to the byte pair encoding ( bpe ) based segmentation approach  #AUTHOR_TAG.', 'we evaluate our models using the standard bleu  #AUTHOR_TAG metric 2 on the detokenized translations of the test set.', 'however we report the only the difference between the bleu scores of the transferred and the baseline models since our focus is not on the bleu scores themselves but rather the improvement by using transfer learning and on observing the language relatedness phenomenon.', 'baseline models are simply ones trained from scratch by initializing the model parameters with random values']",5
"['atop these representations [ 1,  #TAUTHOR_TAG.', 'there has been increasing']","['atop these representations [ 1,  #TAUTHOR_TAG.', 'there has been increasing']","['for signal reconstruction.', ""deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [ 1,  #TAUTHOR_TAG."", 'there has been increasing']","['speech waveforms are densely sampled in time, and thus require downsampling to make many analysis techniques computationally tractable.', 'for speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information.', 'representations based on the fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction.', ""deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [ 1,  #TAUTHOR_TAG."", 'there has been increasing focus on extending this end - toend learning approach down to the level of the raw waveform.', 'a popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [ 3, 4, 5, 6, 7, 8 ].', 'while some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand - engineered features on large vocabulary speech recognition tasks [ 4 ].', 'features based on the fourier transform are computationally efficient, but exhibit an intrinsic tradeoff of temporal and frequency resolution.', 'convolutional filters decouple time and frequency resolution as the number of filters and stride are chosen independently.', 'despite this, a filter bank is constrained by its window size to a single scale.', 'herein, we explore jointly learning filter banks at multiple different scales on raw wave - figure 1 : diagram of multiscale convolutions for learning directly from waveforms.', 'sampled at 16khz, the waveform is originally 1 / 16ms per frame.', 'three convolutions with different window sizes and strides are applied, leading to feature maps with high ( 1ms / frame ), mid ( 5ms / frame ), and low ( 10ms / frame ) temporal resolution.', 'next, maxpooling and concatenation ensure a consistent sampling frequency ( 20ms / frame ) for the rest of the network.', 'the diagram shows real feature maps that are extracted by applying learned features ( also shown ) to a recording of one of the authors saying "" i like cats "" in mandarin.', 'forms.', 'multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [ 9 ], scene labeling [ 10 ], and gesture detection [ 11 ].', 'these successful applications exploit structure at different scales, which encourages us to explore multiscale representations for waveforms as well.', 'further, multiscale convolutions enable us to split the spectrum into different filter banks with independent choice of stride, window size, and number of filters.', 'to learn high']",0
"['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['in figure 2. ( superpositions of youtube clips ) added at signal - to - noise ratios ranging from 0db', 'to 15db [ 12 ]. all input data ( either spectrogram or waveform ) is sampled at 16khz, and normalized', 'so that each input feature has zero mean and', 'unit variance. for models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1 / 16ms frame. for models that learn from spectrograms', ', linear fft features are extracted with a hop size of 10ms, and window size of 20ms', '. the network inputs are thus spectral magnitude maps ranging from 0 - 8khz with 161 features per 10ms', 'frame. we train using stochastic gradient descent with nesterov momentum and a batch size of 128. hyperparameters are tuned for each model by optimizing a hold -', 'out set. typical values are a learning rate of 3e - 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the first epoch by utterance length ( sortagrad ),', 'to promote stability of training on long utterances. while the ctc - trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement', 'it with a kneser - ney', 'smoothed 5 - gram model that is trained using the kenlm toolkit [ 15 ] on cleaned text from the common crawl repository. decoding', 'is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.', 'test set word error rates ( wer ) are reported on a difficult in - house test set of 2048 utterances, diversely composed of', 'noisy, conversational, voice - command, and accented speech. the test set is collected internally and from industry partners and is not represented', 'in the training data. as previously observed  #TAUTHOR_TAG, deep neural networks trained on sufficient data perform better as the model', 'size grows. in order to make fair comparisons, the number of parameters of the models used', 'in our experiments is held constant at 35m. we are aware that the', 'results are not directly comparable to literature due to the use of proprietary datasets. however, we attempt to tightly control our experiments rather', 'than focus on overall performance, with the aim that the conclusions will generalize to other architectures and datasets as well. if optimizing for performance', ', it is worth noting that the wer drops by ∼50 % when training on all 12, 000 hours of data with 7 bidirectional layers ( 70m parameters ) in the backend']",5
"['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['in figure 2. ( superpositions of youtube clips ) added at signal - to - noise ratios ranging from 0db', 'to 15db [ 12 ]. all input data ( either spectrogram or waveform ) is sampled at 16khz, and normalized', 'so that each input feature has zero mean and', 'unit variance. for models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1 / 16ms frame. for models that learn from spectrograms', ', linear fft features are extracted with a hop size of 10ms, and window size of 20ms', '. the network inputs are thus spectral magnitude maps ranging from 0 - 8khz with 161 features per 10ms', 'frame. we train using stochastic gradient descent with nesterov momentum and a batch size of 128. hyperparameters are tuned for each model by optimizing a hold -', 'out set. typical values are a learning rate of 3e - 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the first epoch by utterance length ( sortagrad ),', 'to promote stability of training on long utterances. while the ctc - trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement', 'it with a kneser - ney', 'smoothed 5 - gram model that is trained using the kenlm toolkit [ 15 ] on cleaned text from the common crawl repository. decoding', 'is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.', 'test set word error rates ( wer ) are reported on a difficult in - house test set of 2048 utterances, diversely composed of', 'noisy, conversational, voice - command, and accented speech. the test set is collected internally and from industry partners and is not represented', 'in the training data. as previously observed  #TAUTHOR_TAG, deep neural networks trained on sufficient data perform better as the model', 'size grows. in order to make fair comparisons, the number of parameters of the models used', 'in our experiments is held constant at 35m. we are aware that the', 'results are not directly comparable to literature due to the use of proprietary datasets. however, we attempt to tightly control our experiments rather', 'than focus on overall performance, with the aim that the conclusions will generalize to other architectures and datasets as well. if optimizing for performance', ', it is worth noting that the wer drops by ∼50 % when training on all 12, 000 hours of data with 7 bidirectional layers ( 70m parameters ) in the backend']",5
"['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['in figure 2. ( superpositions of youtube clips ) added at signal - to - noise ratios ranging from 0db', 'to 15db [ 12 ]. all input data ( either spectrogram or waveform ) is sampled at 16khz, and normalized', 'so that each input feature has zero mean and', 'unit variance. for models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1 / 16ms frame. for models that learn from spectrograms', ', linear fft features are extracted with a hop size of 10ms, and window size of 20ms', '. the network inputs are thus spectral magnitude maps ranging from 0 - 8khz with 161 features per 10ms', 'frame. we train using stochastic gradient descent with nesterov momentum and a batch size of 128. hyperparameters are tuned for each model by optimizing a hold -', 'out set. typical values are a learning rate of 3e - 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the first epoch by utterance length ( sortagrad ),', 'to promote stability of training on long utterances. while the ctc - trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement', 'it with a kneser - ney', 'smoothed 5 - gram model that is trained using the kenlm toolkit [ 15 ] on cleaned text from the common crawl repository. decoding', 'is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.', 'test set word error rates ( wer ) are reported on a difficult in - house test set of 2048 utterances, diversely composed of', 'noisy, conversational, voice - command, and accented speech. the test set is collected internally and from industry partners and is not represented', 'in the training data. as previously observed  #TAUTHOR_TAG, deep neural networks trained on sufficient data perform better as the model', 'size grows. in order to make fair comparisons, the number of parameters of the models used', 'in our experiments is held constant at 35m. we are aware that the', 'results are not directly comparable to literature due to the use of proprietary datasets. however, we attempt to tightly control our experiments rather', 'than focus on overall performance, with the aim that the conclusions will generalize to other architectures and datasets as well. if optimizing for performance', ', it is worth noting that the wer drops by ∼50 % when training on all 12, 000 hours of data with 7 bidirectional layers ( 70m parameters ) in the backend']",5
"['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['in figure 2. ( superpositions of youtube clips ) added at signal - to - noise ratios ranging from 0db', 'to 15db [ 12 ]. all input data ( either spectrogram or waveform ) is sampled at 16khz, and normalized', 'so that each input feature has zero mean and', 'unit variance. for models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1 / 16ms frame. for models that learn from spectrograms', ', linear fft features are extracted with a hop size of 10ms, and window size of 20ms', '. the network inputs are thus spectral magnitude maps ranging from 0 - 8khz with 161 features per 10ms', 'frame. we train using stochastic gradient descent with nesterov momentum and a batch size of 128. hyperparameters are tuned for each model by optimizing a hold -', 'out set. typical values are a learning rate of 3e - 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the first epoch by utterance length ( sortagrad ),', 'to promote stability of training on long utterances. while the ctc - trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement', 'it with a kneser - ney', 'smoothed 5 - gram model that is trained using the kenlm toolkit [ 15 ] on cleaned text from the common crawl repository. decoding', 'is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.', 'test set word error rates ( wer ) are reported on a difficult in - house test set of 2048 utterances, diversely composed of', 'noisy, conversational, voice - command, and accented speech. the test set is collected internally and from industry partners and is not represented', 'in the training data. as previously observed  #TAUTHOR_TAG, deep neural networks trained on sufficient data perform better as the model', 'size grows. in order to make fair comparisons, the number of parameters of the models used', 'in our experiments is held constant at 35m. we are aware that the', 'results are not directly comparable to literature due to the use of proprietary datasets. however, we attempt to tightly control our experiments rather', 'than focus on overall performance, with the aim that the conclusions will generalize to other architectures and datasets as well. if optimizing for performance', ', it is worth noting that the wer drops by ∼50 % when training on all 12, 000 hours of data with 7 bidirectional layers ( 70m parameters ) in the backend']",5
"['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['in figure 2. ( superpositions of youtube clips ) added at signal - to - noise ratios ranging from 0db', 'to 15db [ 12 ]. all input data ( either spectrogram or waveform ) is sampled at 16khz, and normalized', 'so that each input feature has zero mean and', 'unit variance. for models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1 / 16ms frame. for models that learn from spectrograms', ', linear fft features are extracted with a hop size of 10ms, and window size of 20ms', '. the network inputs are thus spectral magnitude maps ranging from 0 - 8khz with 161 features per 10ms', 'frame. we train using stochastic gradient descent with nesterov momentum and a batch size of 128. hyperparameters are tuned for each model by optimizing a hold -', 'out set. typical values are a learning rate of 3e - 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the first epoch by utterance length ( sortagrad ),', 'to promote stability of training on long utterances. while the ctc - trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement', 'it with a kneser - ney', 'smoothed 5 - gram model that is trained using the kenlm toolkit [ 15 ] on cleaned text from the common crawl repository. decoding', 'is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.', 'test set word error rates ( wer ) are reported on a difficult in - house test set of 2048 utterances, diversely composed of', 'noisy, conversational, voice - command, and accented speech. the test set is collected internally and from industry partners and is not represented', 'in the training data. as previously observed  #TAUTHOR_TAG, deep neural networks trained on sufficient data perform better as the model', 'size grows. in order to make fair comparisons, the number of parameters of the models used', 'in our experiments is held constant at 35m. we are aware that the', 'results are not directly comparable to literature due to the use of proprietary datasets. however, we attempt to tightly control our experiments rather', 'than focus on overall performance, with the aim that the conclusions will generalize to other architectures and datasets as well. if optimizing for performance', ', it is worth noting that the wer drops by ∼50 % when training on all 12, 000 hours of data with 7 bidirectional layers ( 70m parameters ) in the backend']",5
"['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the']","['- 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort']","['in figure 2. ( superpositions of youtube clips ) added at signal - to - noise ratios ranging from 0db', 'to 15db [ 12 ]. all input data ( either spectrogram or waveform ) is sampled at 16khz, and normalized', 'so that each input feature has zero mean and', 'unit variance. for models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1 / 16ms frame. for models that learn from spectrograms', ', linear fft features are extracted with a hop size of 10ms, and window size of 20ms', '. the network inputs are thus spectral magnitude maps ranging from 0 - 8khz with 161 features per 10ms', 'frame. we train using stochastic gradient descent with nesterov momentum and a batch size of 128. hyperparameters are tuned for each model by optimizing a hold -', 'out set. typical values are a learning rate of 3e - 4 and momentum of 0. 99', ', and training converges after 20 epochs. following  #TAUTHOR_TAG, we sort the first epoch by utterance length ( sortagrad ),', 'to promote stability of training on long utterances. while the ctc - trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement', 'it with a kneser - ney', 'smoothed 5 - gram model that is trained using the kenlm toolkit [ 15 ] on cleaned text from the common crawl repository. decoding', 'is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.', 'test set word error rates ( wer ) are reported on a difficult in - house test set of 2048 utterances, diversely composed of', 'noisy, conversational, voice - command, and accented speech. the test set is collected internally and from industry partners and is not represented', 'in the training data. as previously observed  #TAUTHOR_TAG, deep neural networks trained on sufficient data perform better as the model', 'size grows. in order to make fair comparisons, the number of parameters of the models used', 'in our experiments is held constant at 35m. we are aware that the', 'results are not directly comparable to literature due to the use of proprietary datasets. however, we attempt to tightly control our experiments rather', 'than focus on overall performance, with the aim that the conclusions will generalize to other architectures and datasets as well. if optimizing for performance', ', it is worth noting that the wer drops by ∼50 % when training on all 12, 000 hours of data with 7 bidirectional layers ( 70m parameters ) in the backend']",3
"[' #TAUTHOR_TAG 16 ].', 'our']","[' #TAUTHOR_TAG 16 ].', 'our']","[' #TAUTHOR_TAG 16 ].', 'our learned features under']",[' #TAUTHOR_TAG'],3
"[' #TAUTHOR_TAG 16 ].', 'our']","[' #TAUTHOR_TAG 16 ].', 'our']","[' #TAUTHOR_TAG 16 ].', 'our learned features under']",[' #TAUTHOR_TAG'],4
['published result on the hotel review data  #TAUTHOR_TAG reaching 91'],['published result on the hotel review data  #TAUTHOR_TAG reaching 91. 2 %'],['published result on the hotel review data  #TAUTHOR_TAG reaching 91'],"['previous studies in computerized deception detection have relied only on shallow lexico - syntactic patterns.', 'this paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature.', 'over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from context free grammar ( cfg ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features.', 'our results improve the best published result on the hotel review data  #TAUTHOR_TAG reaching 91. 2 % accuracy with 14 % error reduction']",4
"['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', 'deep syntactic features, encoded asr * slightly improves this performance,']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching']","['first discuss the results for the tripadvisorgold dataset shown in table 2.', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', '']",4
"['- speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between']","['as n - grams and part - of - speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between']","['- speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between certain lexical items']","['studies in computerized deception detection have relied only on shallow lexicosyntactic cues.', 'most are based on dictionarybased word counting using liwc  #AUTHOR_TAG ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ), while some recent ones explored the use of machine learning techniques using simple lexico - syntactic patterns, such as n - grams and part - of - speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges.', 'for instance, the work of  #TAUTHOR_TAG in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns ( e. g., "" i "", "" my "" ) more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions.', 'in parallel to these shallow lexical patterns, might there be deep syntactic structures that are lurking in deceptive writing?', 'this paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature.', 'over four different datasets spanning from the product review domain to the essay domain, we find that features driven from context free grammar ( cfg ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features.', 'our results improve the best published result on the hotel review data of  #TAUTHOR_TAG reaching 91. 2 % accuracy with 14 % error reduction.', 'we also achieve substantial improvement over the essay data of  #AUTHOR_TAG, obtaining upto 85. 0 % accuracy']",0
"['- speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between']","['as n - grams and part - of - speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between']","['- speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between certain lexical items']","['studies in computerized deception detection have relied only on shallow lexicosyntactic cues.', 'most are based on dictionarybased word counting using liwc  #AUTHOR_TAG ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ), while some recent ones explored the use of machine learning techniques using simple lexico - syntactic patterns, such as n - grams and part - of - speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges.', 'for instance, the work of  #TAUTHOR_TAG in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns ( e. g., "" i "", "" my "" ) more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions.', 'in parallel to these shallow lexical patterns, might there be deep syntactic structures that are lurking in deceptive writing?', 'this paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature.', 'over four different datasets spanning from the product review domain to the essay domain, we find that features driven from context free grammar ( cfg ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features.', 'our results improve the best published result on the hotel review data of  #TAUTHOR_TAG reaching 91. 2 % accuracy with 14 % error reduction.', 'we also achieve substantial improvement over the essay data of  #AUTHOR_TAG, obtaining upto 85. 0 % accuracy']",0
"[' #TAUTHOR_TAG.', 'we consider uni']","['domain - specific deception  #TAUTHOR_TAG.', 'we consider unigram, bigram, and the union of the two as features.', 'shallow syntax as has']","['in detecting domain - specific deception  #TAUTHOR_TAG.', 'we consider uni']","['previous work has shown that bag - ofwords are effective in detecting domain - specific deception  #TAUTHOR_TAG.', 'we consider unigram, bigram, and the union of the two as features.', 'shallow syntax as has been used in many previous studies in stylometry ( e. g., argamon  #AUTHOR_TAG,  #AUTHOR_TAG ), we utilize part - of - speech ( pos ) tags to encode shallow syntactic information.', 'note that  #TAUTHOR_TAG found that even though pos tags are effective in detecting fake product reviews, they are not as effective as words.', 'therefore, we strengthen pos features with unigram features.', 'deep syntax we experiment with four different encodings of production rules based on the probabilistic context free grammar ( pcfg ) parse trees as follows :', '• r : unlexicalized production rules ( i. e., all production rules except for those with terminal nodes ), e. g., np 2 → np 3 sbar.', '• r * : lexicalized production rules ( i. e., all production rules ), e. g., prp → "" you "".', '• r : unlexicalized production rules combined with the grandparent node, e. g., np 2ˆv p table 2 : deception detection accuracy ( % ).', '• r * : lexicalized production rules ( i. e., all production rules ) combined with the grandparent node, e. g., prpˆnp 4 → "" you ""']",0
"[' #TAUTHOR_TAG.', 'we consider uni']","['domain - specific deception  #TAUTHOR_TAG.', 'we consider unigram, bigram, and the union of the two as features.', 'shallow syntax as has']","['in detecting domain - specific deception  #TAUTHOR_TAG.', 'we consider uni']","['previous work has shown that bag - ofwords are effective in detecting domain - specific deception  #TAUTHOR_TAG.', 'we consider unigram, bigram, and the union of the two as features.', 'shallow syntax as has been used in many previous studies in stylometry ( e. g., argamon  #AUTHOR_TAG,  #AUTHOR_TAG ), we utilize part - of - speech ( pos ) tags to encode shallow syntactic information.', 'note that  #TAUTHOR_TAG found that even though pos tags are effective in detecting fake product reviews, they are not as effective as words.', 'therefore, we strengthen pos features with unigram features.', 'deep syntax we experiment with four different encodings of production rules based on the probabilistic context free grammar ( pcfg ) parse trees as follows :', '• r : unlexicalized production rules ( i. e., all production rules except for those with terminal nodes ), e. g., np 2 → np 3 sbar.', '• r * : lexicalized production rules ( i. e., all production rules ), e. g., prp → "" you "".', '• r : unlexicalized production rules combined with the grandparent node, e. g., np 2ˆv p table 2 : deception detection accuracy ( % ).', '• r * : lexicalized production rules ( i. e., all production rules ) combined with the grandparent node, e. g., prpˆnp 4 → "" you ""']",0
"['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', 'deep syntactic features, encoded asr * slightly improves this performance,']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching']","['first discuss the results for the tripadvisorgold dataset shown in table 2.', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', '']",0
"['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', 'deep syntactic features, encoded asr * slightly improves this performance,']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching']","['first discuss the results for the tripadvisorgold dataset shown in table 2.', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', '']",0
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this']","[' #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this study with']","['of the previous work for detecting deceptive product reviews focused on related, but slightly different problems, e. g., detecting duplicate reviews or review spams ( e. g.,  #AUTHOR_TAG,,  #AUTHOR_TAG due to notable difficulty in obtaining gold standard labels.', '4 the yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v. s. truthful reviews.', 'two previous work obtained more precise gold standard labels by hiring amazon turkers to write deceptive articles ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this study with respect to their syntactic characteristics.', 'although we are not aware of any prior work that dealt with syntactic cues in deceptive writing directly, prior work on hedge detection ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ) relates to our findings']",0
"['- speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between']","['as n - grams and part - of - speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between']","['- speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between certain lexical items']","['studies in computerized deception detection have relied only on shallow lexicosyntactic cues.', 'most are based on dictionarybased word counting using liwc  #AUTHOR_TAG ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ), while some recent ones explored the use of machine learning techniques using simple lexico - syntactic patterns, such as n - grams and part - of - speech ( pos ) tags (  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'these previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges.', 'for instance, the work of  #TAUTHOR_TAG in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns ( e. g., "" i "", "" my "" ) more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions.', 'in parallel to these shallow lexical patterns, might there be deep syntactic structures that are lurking in deceptive writing?', 'this paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature.', 'over four different datasets spanning from the product review domain to the essay domain, we find that features driven from context free grammar ( cfg ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features.', 'our results improve the best published result on the hotel review data of  #TAUTHOR_TAG reaching 91. 2 % accuracy with 14 % error reduction.', 'we also achieve substantial improvement over the essay data of  #AUTHOR_TAG, obtaining upto 85. 0 % accuracy']",5
"['- gold : introduced in  #TAUTHOR_TAG, this dataset contains 400']","['product review to the essay domain :', 'i. tripadvisor - gold : introduced in  #TAUTHOR_TAG, this dataset contains 400 truthful reviews']","['explore different types of deceptive writing, we consider the following four datasets spanning from the product review to the essay domain :', 'i. tripadvisor - gold : introduced in  #TAUTHOR_TAG, this dataset contains 400 truthful reviews']","['explore different types of deceptive writing, we consider the following four datasets spanning from the product review to the essay domain :', 'i. tripadvisor - gold : introduced in  #TAUTHOR_TAG, this dataset contains 400 truthful reviews obtained from www. tripadviser. com and 400 deceptive reviews gathered using amazon mechanical turk, evenly distributed across 20 chicago hotels.', 'ii. tripadvisor - heuristic : this dataset contains 400 truthful and 400 deceptive reviews harvested from www. tripadviser. com, based on fake review detection heuristics introduced in  #AUTHOR_TAG.', '']",5
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this']","[' #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this study with']","['of the previous work for detecting deceptive product reviews focused on related, but slightly different problems, e. g., detecting duplicate reviews or review spams ( e. g.,  #AUTHOR_TAG,,  #AUTHOR_TAG due to notable difficulty in obtaining gold standard labels.', '4 the yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v. s. truthful reviews.', 'two previous work obtained more precise gold standard labels by hiring amazon turkers to write deceptive articles ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG, both of which have been examined in this study with respect to their syntactic characteristics.', 'although we are not aware of any prior work that dealt with syntactic cues in deceptive writing directly, prior work on hedge detection ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ) relates to our findings']",5
"[' #TAUTHOR_TAG.', 'we consider uni']","['domain - specific deception  #TAUTHOR_TAG.', 'we consider unigram, bigram, and the union of the two as features.', 'shallow syntax as has']","['in detecting domain - specific deception  #TAUTHOR_TAG.', 'we consider uni']","['previous work has shown that bag - ofwords are effective in detecting domain - specific deception  #TAUTHOR_TAG.', 'we consider unigram, bigram, and the union of the two as features.', 'shallow syntax as has been used in many previous studies in stylometry ( e. g., argamon  #AUTHOR_TAG,  #AUTHOR_TAG ), we utilize part - of - speech ( pos ) tags to encode shallow syntactic information.', 'note that  #TAUTHOR_TAG found that even though pos tags are effective in detecting fake product reviews, they are not as effective as words.', 'therefore, we strengthen pos features with unigram features.', 'deep syntax we experiment with four different encodings of production rules based on the probabilistic context free grammar ( pcfg ) parse trees as follows :', '• r : unlexicalized production rules ( i. e., all production rules except for those with terminal nodes ), e. g., np 2 → np 3 sbar.', '• r * : lexicalized production rules ( i. e., all production rules ), e. g., prp → "" you "".', '• r : unlexicalized production rules combined with the grandparent node, e. g., np 2ˆv p table 2 : deception detection accuracy ( % ).', '• r * : lexicalized production rules ( i. e., all production rules ) combined with the grandparent node, e. g., prpˆnp 4 → "" you ""']",1
"['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', 'deep syntactic features, encoded asr * slightly improves this performance,']","['', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching']","['first discuss the results for the tripadvisorgold dataset shown in table 2.', 'as reported in  #TAUTHOR_TAG, bag - of - words features achieve surprisingly high performance, reaching upto 89. 6 % accuracy.', '']",3
"['gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact']","['gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact']","['twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to youtube, may be used to reliably find gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact']","['gangs are defined as "" a coalition of peers, united by mutual interests, with identifiable leadership and internal organization, who act collectively to conduct illegal activity and to control a territory, facility, or enterprise "" [ mil92 ].', 'they promote criminal activities such as drug trafficking, assault, robbery, and threatening or intimidating a neighborhood [ 20113 ].', 'today, over 1. 4 million people, belonging to more than 33, 000 gangs, are active in the united states [ 20111 ], of which 88 % identify themselves as being members of a street gang 1.', ""they are also active users of social media [ 20111 ] ; according to 2007 national assessment center's survey of gang members, 25 % of individuals in gangs use the internet for at least 4 hours a week [ 20007 ]."", 'more recent studies report approximately 45 % of gang members participate in online offending activities such as threatening, harassing individuals, posting violent videos or attacking someone on the street for something they said online [ dp11, pdj15 ].', 'they confirm that gang members use social media to express themselves in ways similar to their offline behavior on the streets [ peb13 ].', 'despite its public nature, gang members post on social media without fear of consequences because there are only few tools law enforcement can presently use to surveil social media [ wdsd15 ].', 'for example, the new york city police department employs over 300 detectives to combat teen violence triggered by insults, dares, and threats exchanged on social media, and the toronto police department teaches officers about the use of social media in investigations [ pol13 ].', 'from offline clues, the officers monitor just a selected set of social media accounts which are manually discovered and related to a specific investigation.', 'thus, developing tools to identify gang member profiles on social media is an important step in the direction of using machine intelligence to fight crime.', 'to help agencies monitor gang activity on social media, our past work investigated how features from twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to youtube, may be used to reliably find gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results.', 'in this paper, we report our experience in integrating deep learning into our gang member profile classifier.', 'specifically, we investigate the effect of translating the features into a vector space using']",0
"[' #TAUTHOR_TAG, we cur']","[' #TAUTHOR_TAG, we curated']","[' #TAUTHOR_TAG, we curated']","[""have begun investigating the gang members'use of social media and have noticed the importance of identifying gang members'twitter profiles a priori [ peb13, wdsd15 ]."", 'before analyzing any textual context retrieved from their social media posts, knowing that a post has originated from a gang member could help systems to better understand the message conveyed by that post.', 'wijeratne et al. developed a framework to analyze what gang members post on social media [ wdsd15 ].', ""their framework could only extract social media posts from self identified gang members by searching for pre - identified gang names in a user's twitter profile description."", 'patton et al. developed a method to collect tweets from a group of gang members operating in detroit, mi [ pat15 ].', ""however, their approach required the gang members'twitter profile names to be known beforehand, and data collection was localized to a single city in the country."", 'these studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings.', 'in our previous work  #TAUTHOR_TAG, we curated what may be the largest set of gang member profiles to study how gang member twitter profiles can be automatically identified based on the content they share online.', 'a data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on twitter.', 'our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to youtube music videos, can help a classifier distinguish between gang and non - gang member profiles.', 'while a very promising f 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed ( e. g. unigrams of tweet text ) could be amenable to an improved representation for classification.', 'we thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings.', '']",0
"['by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly']","['carry acronyms which can only be deciphered by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly']","['tweets / profile text may carry acronyms which can only be deciphered by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly']","['member tweets and profile descriptions tend to have few textual indicators that demonstrate their gang affiliations or their tweets / profile text may carry acronyms which can only be deciphered by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly when they form new gangs.', 'consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible.', 'instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to youtube videos reflecting their music preferences and affinity.', 'in this section, we briefly discuss the feature types and their broad differences in gang and non - gang member profiles.', 'an in - depth explanation of these feature selection can be found in  #TAUTHOR_TAG']",0
"['by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly']","['carry acronyms which can only be deciphered by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly']","['tweets / profile text may carry acronyms which can only be deciphered by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly']","['member tweets and profile descriptions tend to have few textual indicators that demonstrate their gang affiliations or their tweets / profile text may carry acronyms which can only be deciphered by others involved in gang culture  #TAUTHOR_TAG.', 'these gang - related terms are often local to gangs operating in neighborhoods and change rapidly when they form new gangs.', 'consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible.', 'instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to youtube videos reflecting their music preferences and affinity.', 'in this section, we briefly discuss the feature types and their broad differences in gang and non - gang member profiles.', 'an in - depth explanation of these feature selection can be found in  #TAUTHOR_TAG']",0
"['times more than the average curse words use on twitter  #TAUTHOR_TAG.', 'further, we noticed that gang members mainly use twitter to discuss drugs and money using terms']","['times more than the average curse words use on twitter  #TAUTHOR_TAG.', 'further, we noticed that gang members mainly use twitter to discuss drugs and money using terms']","['our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on twitter  #TAUTHOR_TAG.', 'further, we noticed that gang members mainly use twitter to discuss drugs and money using terms']","['our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on twitter  #TAUTHOR_TAG.', 'further, we noticed that gang members mainly use twitter to discuss drugs and money using terms such as smoke, high, hit, money, got, and need while non - gang members mainly discuss their feelings using terms such as new, like, love, know, want, and look']",0
"['to hip - hop music, gangster rap, and the culture that surrounds this music genre  #TAUTHOR_TAG.', 'moreover, we']","['to hip - hop music, gangster rap, and the culture that surrounds this music genre  #TAUTHOR_TAG.', 'moreover, we']","['to hip - hop music, gangster rap, and the culture that surrounds this music genre  #TAUTHOR_TAG.', 'moreover, we found that eight youtube links are shared on']","['found that 51. 25 % of the gang members in our dataset have a tweet that links to a youtube video.', 'further, we found that 76. 58 % of the shared links are related to hip - hop music, gangster rap, and the culture that surrounds this music genre  #TAUTHOR_TAG.', 'moreover, we found that eight youtube links are shared on average by a gang member.', ""the top 5 terms used in youtube videos shared by gang members were shit, like, nigga, fuck, and lil while like, love, peopl, song, and get were the top 5 terms in nongang members'video data."", 'figure 1 gives an overview of the steps to learn word embeddings and to integrate them into a classifier.', 'we first convert any non - textual features such as emoji and profile images into textual features.', 'we use emoji for python 3 and clarifai services, respectively, to convert emoji and images into text.', 'prior to training the word embeddings, we remove all the seed words used to find gang member profiles and stopwords, and perform stemming across all tweets and profile descriptions.', 'we then feed all the training data ( word w t in figure 1 ) we collected from our twitter dataset to word2vec tool and train it using a skip - gram model with negative sampling.', 'when training the skip - gram model, we set the negative sampling rate to 10 sample words, which seems to work well with medium - sized datasets [ msc + 13 ].', 'we set the context word window to be 5, so that it will consider 5 words to left and right of the target word ( words w t−5 to w t + 5 in figure 1 ).', 'this setting is suitable for sentences where average sentence length is less than 11 words, as is the case in tweets [ htk13 ].', 'we ignore the words that occur less than 5 times in our training corpus']",0
"[' #TAUTHOR_TAG.', 'it was']","[' #TAUTHOR_TAG.', 'it was']","[""##ated gang and non - gang members'twitter profiles collected from our previous work  #TAUTHOR_TAG."", 'it was developed by']","[""consider a dataset of curated gang and non - gang members'twitter profiles collected from our previous work  #TAUTHOR_TAG."", 'it was developed by querying the followerwonk web service api 4 with location - neutral seed words known to be used by gang members across the u. s. in their twitter profiles.', 'the dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words.', 'specific details about our data curation procedure are discussed in  #TAUTHOR_TAG.', 'ultimately, this dataset consists of 400 gang member profiles and 2, 865 non - gang member profiles.', 'for each user profile, we collected up to most recent 3, 200 tweets from their twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every youtube video shared by them.', 'table 1 provides statistics about the number of words found in each type of feature in the dataset.', 'it includes a total of 821, 412 tweets from gang members and 7, 238, 758 tweets from non - gang members.', '']",0
"['gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact']","['gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact']","['twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to youtube, may be used to reliably find gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact']","['gangs are defined as "" a coalition of peers, united by mutual interests, with identifiable leadership and internal organization, who act collectively to conduct illegal activity and to control a territory, facility, or enterprise "" [ mil92 ].', 'they promote criminal activities such as drug trafficking, assault, robbery, and threatening or intimidating a neighborhood [ 20113 ].', 'today, over 1. 4 million people, belonging to more than 33, 000 gangs, are active in the united states [ 20111 ], of which 88 % identify themselves as being members of a street gang 1.', ""they are also active users of social media [ 20111 ] ; according to 2007 national assessment center's survey of gang members, 25 % of individuals in gangs use the internet for at least 4 hours a week [ 20007 ]."", 'more recent studies report approximately 45 % of gang members participate in online offending activities such as threatening, harassing individuals, posting violent videos or attacking someone on the street for something they said online [ dp11, pdj15 ].', 'they confirm that gang members use social media to express themselves in ways similar to their offline behavior on the streets [ peb13 ].', 'despite its public nature, gang members post on social media without fear of consequences because there are only few tools law enforcement can presently use to surveil social media [ wdsd15 ].', 'for example, the new york city police department employs over 300 detectives to combat teen violence triggered by insults, dares, and threats exchanged on social media, and the toronto police department teaches officers about the use of social media in investigations [ pol13 ].', 'from offline clues, the officers monitor just a selected set of social media accounts which are manually discovered and related to a specific investigation.', 'thus, developing tools to identify gang member profiles on social media is an important step in the direction of using machine intelligence to fight crime.', 'to help agencies monitor gang activity on social media, our past work investigated how features from twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to youtube, may be used to reliably find gang member profiles  #TAUTHOR_TAG.', 'the diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results.', 'in this paper, we report our experience in integrating deep learning into our gang member profile classifier.', 'specifically, we investigate the effect of translating the features into a vector space using']",6
"[' #TAUTHOR_TAG, we cur']","[' #TAUTHOR_TAG, we curated']","[' #TAUTHOR_TAG, we curated']","[""have begun investigating the gang members'use of social media and have noticed the importance of identifying gang members'twitter profiles a priori [ peb13, wdsd15 ]."", 'before analyzing any textual context retrieved from their social media posts, knowing that a post has originated from a gang member could help systems to better understand the message conveyed by that post.', 'wijeratne et al. developed a framework to analyze what gang members post on social media [ wdsd15 ].', ""their framework could only extract social media posts from self identified gang members by searching for pre - identified gang names in a user's twitter profile description."", 'patton et al. developed a method to collect tweets from a group of gang members operating in detroit, mi [ pat15 ].', ""however, their approach required the gang members'twitter profile names to be known beforehand, and data collection was localized to a single city in the country."", 'these studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings.', 'in our previous work  #TAUTHOR_TAG, we curated what may be the largest set of gang member profiles to study how gang member twitter profiles can be automatically identified based on the content they share online.', 'a data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on twitter.', 'our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to youtube music videos, can help a classifier distinguish between gang and non - gang member profiles.', 'while a very promising f 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed ( e. g. unigrams of tweet text ) could be amenable to an improved representation for classification.', 'we thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings.', '']",6
"[' #TAUTHOR_TAG.', 'it was']","[' #TAUTHOR_TAG.', 'it was']","[""##ated gang and non - gang members'twitter profiles collected from our previous work  #TAUTHOR_TAG."", 'it was developed by']","[""consider a dataset of curated gang and non - gang members'twitter profiles collected from our previous work  #TAUTHOR_TAG."", 'it was developed by querying the followerwonk web service api 4 with location - neutral seed words known to be used by gang members across the u. s. in their twitter profiles.', 'the dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words.', 'specific details about our data curation procedure are discussed in  #TAUTHOR_TAG.', 'ultimately, this dataset consists of 400 gang member profiles and 2, 865 non - gang member profiles.', 'for each user profile, we collected up to most recent 3, 200 tweets from their twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every youtube video shared by them.', 'table 1 provides statistics about the number of words found in each type of feature in the dataset.', 'it includes a total of 821, 412 tweets from gang members and 7, 238, 758 tweets from non - gang members.', '']",5
"[' #TAUTHOR_TAG.', 'it was']","[' #TAUTHOR_TAG.', 'it was']","[""##ated gang and non - gang members'twitter profiles collected from our previous work  #TAUTHOR_TAG."", 'it was developed by']","[""consider a dataset of curated gang and non - gang members'twitter profiles collected from our previous work  #TAUTHOR_TAG."", 'it was developed by querying the followerwonk web service api 4 with location - neutral seed words known to be used by gang members across the u. s. in their twitter profiles.', 'the dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words.', 'specific details about our data curation procedure are discussed in  #TAUTHOR_TAG.', 'ultimately, this dataset consists of 400 gang member profiles and 2, 865 non - gang member profiles.', 'for each user profile, we collected up to most recent 3, 200 tweets from their twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every youtube video shared by them.', 'table 1 provides statistics about the number of words found in each type of feature in the dataset.', 'it includes a total of 821, 412 tweets from gang members and 7, 238, 758 tweets from non - gang members.', '']",5
"[')  #TAUTHOR_TAG.', 'the grammar formalism']","['( smt )  #TAUTHOR_TAG.', 'the grammar formalism']","[')  #TAUTHOR_TAG.', 'the grammar formalism']","['use of various synchronous grammar based formalisms has been a trend for statistical machine translation ( smt )  #TAUTHOR_TAG.', 'the grammar formalism determines the intrinsic capacities and computational efficiency of the smt systems.', 'to evaluate the capacity of a grammar formalism, two factors, i. e. generative power and expressive power are usually considered  #AUTHOR_TAG.', 'the generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities.', '']",0
"[')  #TAUTHOR_TAG.', 'the grammar formalism']","['( smt )  #TAUTHOR_TAG.', 'the grammar formalism']","[')  #TAUTHOR_TAG.', 'the grammar formalism']","['use of various synchronous grammar based formalisms has been a trend for statistical machine translation ( smt )  #TAUTHOR_TAG.', 'the grammar formalism determines the intrinsic capacities and computational efficiency of the smt systems.', 'to evaluate the capacity of a grammar formalism, two factors, i. e. generative power and expressive power are usually considered  #AUTHOR_TAG.', 'the generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities.', '']",0
"['', 'for example,  #TAUTHOR_TAG']","['', 'for example,  #TAUTHOR_TAG']","['', 'for example,  #TAUTHOR_TAG']","[""efficiency, our model approximately search for the single'best'derivation using beam search as"", 'the major challenge for such a ssg - based decoder is how to apply the heterogeneous rules in a derivation.', 'for example,  #TAUTHOR_TAG adopts a cky style span - based decoding while  #AUTHOR_TAG applies a linguistically syntax node based bottom - up decoding, which are difficult to integrate.', 'fortunately, our current ssg syncretizes fscfg and lstssg. and the conventional decodings of both fscfg and lstssg are spanbased expansion.', 'thus, it would be a natural way for our ssg - based decoder to conduct a spanbased beam search.', 'the search procedure is given by the pseudocode in figure 3.', 'a hypotheses stack h [ i, j ] ( similar to the "" chart cell "" in cky parsing ) is arranged for each span [ i, j ] for storing the translation hypotheses.', '']",0
"[')  #TAUTHOR_TAG.', 'the grammar formalism']","['( smt )  #TAUTHOR_TAG.', 'the grammar formalism']","[')  #TAUTHOR_TAG.', 'the grammar formalism']","['use of various synchronous grammar based formalisms has been a trend for statistical machine translation ( smt )  #TAUTHOR_TAG.', 'the grammar formalism determines the intrinsic capacities and computational efficiency of the smt systems.', 'to evaluate the capacity of a grammar formalism, two factors, i. e. generative power and expressive power are usually considered  #AUTHOR_TAG.', 'the generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities.', '']",5
"['in current implementation can be considered as a combination of the ones in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'given the sentence pair in']","['in current implementation can be considered as a combination of the ones in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'given the sentence pair in']","['in current implementation can be considered as a combination of the ones in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'given the sentence pair in figure 1, some ssg rules']","[', the proposed synthetic synchronous grammar ( ssg ) is a tuple', 'where σ s ( σ t ) is the alphabet set of source ( target ) terminals, namely the vocabulary ; n s ( n t ) is the alphabet set of source ( target ) non - terminals, such as the pos tags and the syntax labels ; x represents the special nonterminal label in fscfg ; and p is the grammar rule set which is the core part of a grammar.', 'every rule r in p is as :', 'where α ∈ [ { x }, n s, σ s ] + is a sequence of one or more source words in σ s and nonterminals symbols in [ { x }, n s ] ; γ ∈ [ { x }, n t, σ t ] + is a sequence of one or more target words in σ t and nonterminals symbols in [ { x }, n t ] ; a t is a many - tomany corresponding set which includes the alignments between the terminal leaf nodes from source and target side, and a n t is a one - to - one corresponding set which includes the synchronizing relations between the non - terminal leaf nodes from source and target side ; ω contains feature values associated with each rule.', 'through this formalization, we can see that fscfg rules and lstssg rules are both included.', 'however, we should point out that the rules with mixture of x non - terminals and syntactic non - terminals are not included in our current implementation despite that they are legal under the proposed formalism.', 'the rule extraction in current implementation can be considered as a combination of the ones in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'given the sentence pair in figure 1, some ssg rules can be extracted as illustrated in figure 2']",5
"['', 'fscfg an in - house implementation of purely formally scfg based model similar to  #TAUTHOR_TAG.', 'mb']","['', 'fscfg an in - house implementation of purely formally scfg based model similar to  #TAUTHOR_TAG.', 'mbr']","['linguistically motivated stssg based model similar to  #AUTHOR_TAG.', 'fscfg an in - house implementation of purely formally scfg based model similar to  #TAUTHOR_TAG.', 'mb']","['system, named hitree, is implemented in standard c + + and stl.', 'in this section we report on experiments with chinese - to - english translation base on it.', 'we used fbis chinese - to - english parallel corpora ( 7. 2m + 9. 2m words ) as the training data.', 'we also used sri language modeling toolkit to train a 4 - gram language model on the xinhua portion of the english gigaword corpus ( 181m words ).', 'nist mt2002 test set is used as the development set.', 'the nist mt2005 test set is used as the test set.', 'the evaluation metric is case - sensitive bleu4.', ""for significant test, we used zhang's implementation  #AUTHOR_TAG ( confidence level of 95 % )."", 'for comparisons, we used the following three baseline systems : lstssg an in - house implementation of linguistically motivated stssg based model similar to  #AUTHOR_TAG.', 'fscfg an in - house implementation of purely formally scfg based model similar to  #TAUTHOR_TAG.', 'mbr we use an in - house combination system which is an implementation of a classic sentence level combination method based on the minimum bayes risk ( mbr ) decoding  #AUTHOR_TAG']",3
"['in  #TAUTHOR_TAG.', ' #AUTHOR_TAG, introduce two new bayesian models that treat unsupervised']","['in  #TAUTHOR_TAG.', ' #AUTHOR_TAG, introduce two new bayesian models that treat unsupervised']","['in  #TAUTHOR_TAG.', ' #AUTHOR_TAG, introduce two new bayesian models that treat unsupervised role induction as the clustering of syntactic argument signatures, with clusters corresponding to semantic roles, and achieve the best state - of - the - art results.', 'in this paper, we propose a novel unsupervised']","['', 'this annotated data is not always available, very expensive to create and often domain specific  #AUTHOR_TAG.', 'there is in particular no such data available for french.', 'to bypass this shortcoming, "" annotation - by - projection "" approaches have been proposed  #AUTHOR_TAG which in essence, ( i ) project the semantic annotations available in one language ( usually english ), to text in another language ( in this case french ) ; and ( ii ) use the resulting annotations to train a semantic role labeller.', ' #AUTHOR_TAG show that the projection - based annotation framework permits bootstrapping a semantic role labeller for framenet which reaches an f - measure of 63 % ; and van der  #AUTHOR_TAG show that training a joint syntactic - semantic parser based on the projection approach permits reaching an f - measure for the labeled attachment score on propbank annotation of 65 %.', 'although they minimize the manual effort involved, these approaches still require both an annotated source corpus and an aligned target corpus.', 'moreover, they assume a specific role labeling ( e. g., propbank, framenet or verbnet roles ) and are not generally portable from one framework to another.', 'these drawbacks with supervised approaches motivated the need for unsupervised methods capable of exploiting large amounts of unannotated data.', 'in this context several approaches have been proposed.', ' #AUTHOR_TAG were the first to introduce unsupervised srl in an approach that used the verbnet lexicon to guide unsupervised learning.', ' #AUTHOR_TAG proposed a directed graphical model for role induction that exploits linguistic priors for syntactic and semantic inference.', 'following this work,  #AUTHOR_TAG formulated role induction as the problem of detecting alternations and mapping non - standard linkings to cannonical ones, and later as a graph partitioning problem in  #AUTHOR_TAG b ).', 'they also proposed an algorithm that uses successive splits and merges of semantic roles clusters in order to improve their quality in  #TAUTHOR_TAG.', ' #AUTHOR_TAG, introduce two new bayesian models that treat unsupervised role induction as the clustering of syntactic argument signatures, with clusters corresponding to semantic roles, and achieve the best state - of - the - art results.', 'in this paper, we propose a novel unsupervised']",0
"['induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be']","['induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be']","['- tasks : argument identification and role induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be associated with a semantic role.', 'the model starts by generating a frame assignment to']","['mentioned in the introduction, semantic role labeling comprises two sub - tasks : argument identification and role induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be associated with a semantic role.', 'the model starts by generating a frame assignment to each verb instance where a frame is a clustering of verbs and associated roles.', '']",3
"['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in']","[""model's parameters have been tuned with a few rounds of trial - and - error on the english development corpus : for the hyper - parameters, we set α f = 0. 5, α r = 1. e −3, α v = 1. e −7, α v o = 1. e −3, α d = 1. e −8 and α w = 0. 5."", 'for the evaluation on french, we only changed the α f and α w parameters.', 'in order to reflect the rather uniform distribution of verb instances across verb classes we set α f to 1.', 'moreover, we set α w to 0. 001 because of the smaller number of words and roles in the french corpus.', 'the number of roles and frames were chosen based on the properties of each corpus.', 'we set number of roles to 40 and 10, and the number of frames to 300 and 60 for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in the j th gold class, and n is the number of argument instances.', ""in a similar way, the collocation of roles'clusters is computed as follows :"", 'then, each score is averaged over all verbs.', 'in the same way as  #TAUTHOR_TAG, we use the micro - average obtained by weighting the scores for individual verbs proportionally to the number of argument instances for that verb.', 'finally the f1 measure is the harmonic mean of the aggregated values of purity and collocation']",3
"['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in']","[""model's parameters have been tuned with a few rounds of trial - and - error on the english development corpus : for the hyper - parameters, we set α f = 0. 5, α r = 1. e −3, α v = 1. e −7, α v o = 1. e −3, α d = 1. e −8 and α w = 0. 5."", 'for the evaluation on french, we only changed the α f and α w parameters.', 'in order to reflect the rather uniform distribution of verb instances across verb classes we set α f to 1.', 'moreover, we set α w to 0. 001 because of the smaller number of words and roles in the french corpus.', 'the number of roles and frames were chosen based on the properties of each corpus.', 'we set number of roles to 40 and 10, and the number of frames to 300 and 60 for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in the j th gold class, and n is the number of argument instances.', ""in a similar way, the collocation of roles'clusters is computed as follows :"", 'then, each score is averaged over all verbs.', 'in the same way as  #TAUTHOR_TAG, we use the micro - average obtained by weighting the scores for individual verbs proportionally to the number of argument instances for that verb.', 'finally the f1 measure is the harmonic mean of the aggregated values of purity and collocation']",3
"['used for instance in  #TAUTHOR_TAG, which simply clusters predicate arguments according to the dependency']","['to verbnet roles.', 'this constitutes our gold evaluation corpus.', 'the baseline model is the "" syntactic function "" used for instance in  #TAUTHOR_TAG, which simply clusters predicate arguments according to the dependency']","['##net roles.', 'this constitutes our gold evaluation corpus.', 'the baseline model is the "" syntactic function "" used for instance in  #TAUTHOR_TAG, which simply clusters predicate arguments according to the dependency relation to']","['', 'this constitutes our gold evaluation corpus.', 'the baseline model is the "" syntactic function "" used for instance in  #TAUTHOR_TAG, which simply clusters predicate arguments according to the dependency relation to their head.', 'this is a standard baseline for unsupervised srl, which, although simple, has been shown difficult to outperform.', 'as done in previous work, it is implemented by allocating a different cluster to each of the 10 most frequent syntactic relations, and one extra cluster for all the other relations.', 'evaluation results are shown in table 2.', 'the proposed model significantly outperforms the deterministic baseline, which validates the unsupervised learning process']",3
"['work  #TAUTHOR_TAG, in']","['work  #TAUTHOR_TAG, in']","['previous work  #TAUTHOR_TAG, in']","['made our best to follow the setup used in previous work  #TAUTHOR_TAG, in order to compare with the current state of the art.', 'the data used is the standard conll 2008 shared task  #AUTHOR_TAG version of penn treebank wsj and propbank.', 'our model is evaluated on gold generated parses, using the gold propbank annotations.', 'in propbank, predicates are associated with a set of roles, where roles a2 - a5 or aa are verb specific, while adjuncts roles ( am ) are consistent across verbs.', 'besides, roles a0 and a1 attempt to capture proto - agent and proto - patient roles  #AUTHOR_TAG, and thus are more valid across verbs and verb instances than a2 - a5 roles.', '']",3
"['work  #TAUTHOR_TAG, in']","['work  #TAUTHOR_TAG, in']","['previous work  #TAUTHOR_TAG, in']","['made our best to follow the setup used in previous work  #TAUTHOR_TAG, in order to compare with the current state of the art.', 'the data used is the standard conll 2008 shared task  #AUTHOR_TAG version of penn treebank wsj and propbank.', 'our model is evaluated on gold generated parses, using the gold propbank annotations.', 'in propbank, predicates are associated with a set of roles, where roles a2 - a5 or aa are verb specific, while adjuncts roles ( am ) are consistent across verbs.', 'besides, roles a0 and a1 attempt to capture proto - agent and proto - patient roles  #AUTHOR_TAG, and thus are more valid across verbs and verb instances than a2 - a5 roles.', '']",3
"['induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be']","['induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be']","['- tasks : argument identification and role induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be associated with a semantic role.', 'the model starts by generating a frame assignment to']","['mentioned in the introduction, semantic role labeling comprises two sub - tasks : argument identification and role induction.', 'following common practice  #TAUTHOR_TAG, we assume oracle argument identification and focus on argument labeling.', 'the approach we propose is an unsupervised generative bayesian model that clusters arguments into classes each of which can be associated with a semantic role.', 'the model starts by generating a frame assignment to each verb instance where a frame is a clustering of verbs and associated roles.', '']",5
"['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in']","[""model's parameters have been tuned with a few rounds of trial - and - error on the english development corpus : for the hyper - parameters, we set α f = 0. 5, α r = 1. e −3, α v = 1. e −7, α v o = 1. e −3, α d = 1. e −8 and α w = 0. 5."", 'for the evaluation on french, we only changed the α f and α w parameters.', 'in order to reflect the rather uniform distribution of verb instances across verb classes we set α f to 1.', 'moreover, we set α w to 0. 001 because of the smaller number of words and roles in the french corpus.', 'the number of roles and frames were chosen based on the properties of each corpus.', 'we set number of roles to 40 and 10, and the number of frames to 300 and 60 for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in the j th gold class, and n is the number of argument instances.', ""in a similar way, the collocation of roles'clusters is computed as follows :"", 'then, each score is averaged over all verbs.', 'in the same way as  #TAUTHOR_TAG, we use the micro - average obtained by weighting the scores for individual verbs proportionally to the number of argument instances for that verb.', 'finally the f1 measure is the harmonic mean of the aggregated values of purity and collocation']",5
"['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', 'for']","['for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in']","[""model's parameters have been tuned with a few rounds of trial - and - error on the english development corpus : for the hyper - parameters, we set α f = 0. 5, α r = 1. e −3, α v = 1. e −7, α v o = 1. e −3, α d = 1. e −8 and α w = 0. 5."", 'for the evaluation on french, we only changed the α f and α w parameters.', 'in order to reflect the rather uniform distribution of verb instances across verb classes we set α f to 1.', 'moreover, we set α w to 0. 001 because of the smaller number of words and roles in the french corpus.', 'the number of roles and frames were chosen based on the properties of each corpus.', 'we set number of roles to 40 and 10, and the number of frames to 300 and 60 for english and french respectively.', 'as done in  #TAUTHOR_TAG and  #AUTHOR_TAG, we use purity and collocation measures to assess the quality of our role induction process.', ""for each verb, the purity of roles'clusters is computed as follows :"", 'where c i is the set of arguments in the i th cluster found, g j is the set of arguments in the j th gold class, and n is the number of argument instances.', ""in a similar way, the collocation of roles'clusters is computed as follows :"", 'then, each score is averaged over all verbs.', 'in the same way as  #TAUTHOR_TAG, we use the micro - average obtained by weighting the scores for individual verbs proportionally to the number of argument instances for that verb.', 'finally the f1 measure is the harmonic mean of the aggregated values of purity and collocation']",5
"['work  #TAUTHOR_TAG, in']","['work  #TAUTHOR_TAG, in']","['previous work  #TAUTHOR_TAG, in']","['made our best to follow the setup used in previous work  #TAUTHOR_TAG, in order to compare with the current state of the art.', 'the data used is the standard conll 2008 shared task  #AUTHOR_TAG version of penn treebank wsj and propbank.', 'our model is evaluated on gold generated parses, using the gold propbank annotations.', 'in propbank, predicates are associated with a set of roles, where roles a2 - a5 or aa are verb specific, while adjuncts roles ( am ) are consistent across verbs.', 'besides, roles a0 and a1 attempt to capture proto - agent and proto - patient roles  #AUTHOR_TAG, and thus are more valid across verbs and verb instances than a2 - a5 roles.', '']",5
"['work  #TAUTHOR_TAG, in']","['work  #TAUTHOR_TAG, in']","['previous work  #TAUTHOR_TAG, in']","['made our best to follow the setup used in previous work  #TAUTHOR_TAG, in order to compare with the current state of the art.', 'the data used is the standard conll 2008 shared task  #AUTHOR_TAG version of penn treebank wsj and propbank.', 'our model is evaluated on gold generated parses, using the gold propbank annotations.', 'in propbank, predicates are associated with a set of roles, where roles a2 - a5 or aa are verb specific, while adjuncts roles ( am ) are consistent across verbs.', 'besides, roles a0 and a1 attempt to capture proto - agent and proto - patient roles  #AUTHOR_TAG, and thus are more valid across verbs and verb instances than a2 - a5 roles.', '']",4
"['tasks without sacrificing annotation quality  #TAUTHOR_TAG a ).', 'once we']","['many nlp tasks without sacrificing annotation quality  #TAUTHOR_TAG a ).', 'once we']","['many nlp tasks without sacrificing annotation quality  #TAUTHOR_TAG a ).', 'once we decide to use al']","['', 'while for the general language english newspaper domain syntactic  #AUTHOR_TAG, semantic  #AUTHOR_TAG, and even discourse  #AUTHOR_TAG annotations are increasingly made available, any language, domain, or genre shift pushes the severe burden on developers of nlp systems to supply comparably sized high - quality annotations.', 'even inner - domain shifts, such as, e. g., moving from hematology  #AUTHOR_TAG to the genetics of cancer  #AUTHOR_TAG within the field of molecular biology may have drastic consequences in the sense that entirely new meta data sets have to produced by annotation teams.', 'thus, reducing the human efforts for the creation of adequate training material is a major challenge.', 'active learning ( al ) copes with this problem as it intelligently selects the data to be labeled.', 'it is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process.', 'al has been successfully applied to speed up the annotation process for many nlp tasks without sacrificing annotation quality  #TAUTHOR_TAG a ).', '']",0
['members several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly'],['members several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly'],['several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly'],"['the idea that from the learning curve one can read the trade - off between annotation effort and classifier performance gain, we here propose an approach to approximate the progression of the learning curve which comes at no extra annotation costs.', 'this approach is designed for use in committee - based al  #AUTHOR_TAG.', 'a committee consists of k classifiers of the same type trained on different subsets of the already labeled ( training ) data.', 'each committee member then makes its predictions on the pool of unlabeled examples, and those examples on which the committee members express the highest disagreement are considered most informative for learning and are thus selected for manual annotation.', 'to calculate the disagreement among the committee members several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly the most well - known one.', 'our approach to approximating the learning curve is based on the disagreement within a committee.', 'however, it is independent of the actual metric used to calculate the disagreement.', 'although in our experiments we considered the nlp task of named entity recognition ( ner ) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well.', ' #AUTHOR_TAG a ) we introduced the selection agreement ( sa ) curve - the average agreement amongst the selected examples plotted over time.', '']",0
['members several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly'],['members several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly'],['several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly'],"['the idea that from the learning curve one can read the trade - off between annotation effort and classifier performance gain, we here propose an approach to approximate the progression of the learning curve which comes at no extra annotation costs.', 'this approach is designed for use in committee - based al  #AUTHOR_TAG.', 'a committee consists of k classifiers of the same type trained on different subsets of the already labeled ( training ) data.', 'each committee member then makes its predictions on the pool of unlabeled examples, and those examples on which the committee members express the highest disagreement are considered most informative for learning and are thus selected for manual annotation.', 'to calculate the disagreement among the committee members several metrics have been proposed including the vote entropy  #TAUTHOR_TAG as possibly the most well - known one.', 'our approach to approximating the learning curve is based on the disagreement within a committee.', 'however, it is independent of the actual metric used to calculate the disagreement.', 'although in our experiments we considered the nlp task of named entity recognition ( ner ) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well.', ' #AUTHOR_TAG a ) we introduced the selection agreement ( sa ) curve - the average agreement amongst the selected examples plotted over time.', '']",3
"['.', 'disagreement is measured by vote entropy  #TAUTHOR_TAG.', 'in our ner scenario, complete sentences are selected by al.', 'while']","['of all examples seen so far.', 'disagreement is measured by vote entropy  #TAUTHOR_TAG.', 'in our ner scenario, complete sentences are selected by al.', 'while']","['.', 'disagreement is measured by vote entropy  #TAUTHOR_TAG.', 'in our ner scenario, complete sentences are selected by al.', 'while we made use of me classifiers']","['', ', l being the set of all examples seen so far.', 'disagreement is measured by vote entropy  #TAUTHOR_TAG.', 'in our ner scenario, complete sentences are selected by al.', 'while we made use of me classifiers during the selection, we employed a ne tagger based on conditional random fields ( crfs )  #AUTHOR_TAG during evaluation time to determine the learning curves.', 'we have already shown that in this scenario, me classifiers perform equally well for al - driven selection as crfs when using the same features.', 'this effect is truly beneficial, especially for real - world annotation projects, due to much lower training times and, by this, shorter annotator idle times  #AUTHOR_TAG a ).', 'for the al simulation, we employed two simulation corpora : the conll corpus, based on the english data set of the conll - 2003 shared task  #AUTHOR_TAG, which consists of newspaper articles annotated with respect to person, location, and organisation entities.', '']",5
"['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain']","['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain']","['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain performance of kernel - based relation extract']","['', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain performance of kernel - based relation extractors can be improved by embedding semantic', 'similarity information generated from word clustering and latent semantic analysis ( lsa ) into syntactic tree kernels. although this', 'idea is interesting, it suffers from two major limitations : + it does not incorporate word cluster information at different levels of granularity. in fact,  #TAUTHOR_TAG only use the 10 - bit cluster prefix in their study. we will demonstrate', 'later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + it is unclear if this approach can encode realvalued features of words ( such as word embeddings  #AUTHOR_TAG ) effectively. as the real - valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially', '']",0
"['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain']","['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain']","['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain performance of kernel - based relation extract']","['', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain performance of kernel - based relation extractors can be improved by embedding semantic', 'similarity information generated from word clustering and latent semantic analysis ( lsa ) into syntactic tree kernels. although this', 'idea is interesting, it suffers from two major limitations : + it does not incorporate word cluster information at different levels of granularity. in fact,  #TAUTHOR_TAG only use the 10 - bit cluster prefix in their study. we will demonstrate', 'later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + it is unclear if this approach can encode realvalued features of words ( such as word embeddings  #AUTHOR_TAG ) effectively. as the real - valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially', '']",0
"['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain']","['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain']","['the only study explicitly', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain performance of kernel - based relation extract']","['', 'targeting this problem so far is by  #TAUTHOR_TAG who find that the out - of - domain performance of kernel - based relation extractors can be improved by embedding semantic', 'similarity information generated from word clustering and latent semantic analysis ( lsa ) into syntactic tree kernels. although this', 'idea is interesting, it suffers from two major limitations : + it does not incorporate word cluster information at different levels of granularity. in fact,  #TAUTHOR_TAG only use the 10 - bit cluster prefix in their study. we will demonstrate', 'later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + it is unclear if this approach can encode realvalued features of words ( such as word embeddings  #AUTHOR_TAG ) effectively. as the real - valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially', '']",0
"['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire (']","['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'we follow the standard']","['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire (']","['', 'in order to answer these questions, following  #AUTHOR_TAG, we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various featurebased re systems.', 'after that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance.', 'for each of these group combinations, we assess the system performance with different numbers of dimensions for both c & w and hlbl word embeddings.', 'let m1 and m2 be the first and second mentions in the relation.', 'to facilitate system comparison later.', 'we evaluate c & w word embeddings with 25, 50 and 100 dimensions as well as hlbl word embeddings with 50 and 100 dimensions that are introduced in  #AUTHOR_TAG and can be downloaded here 4.', 'the fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single - system da.', 'we use the ace 2005 corpus for da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'we follow the standard practices on ace  #TAUTHOR_TAG and use news ( the union of bn and nw ) as the source domain and bc, cts and wl as our target domains.', 'we take half of bc as the only target development set, and use the remaining data and domains for testing purposes ( as they are small already ).', 'as noted in  #TAUTHOR_TAG, the distributions of relations as well as the vocabularies of the domains are quite different']",0
"['', 'following  #TAUTHOR_TAG, we']","['be considered naturally as a simple yet general technique to cope with da problems.', 'following  #TAUTHOR_TAG, we']","['on new domains.', 'eventually, regularization methods can be considered naturally as a simple yet general technique to cope with da problems.', 'following  #TAUTHOR_TAG, we assume that we']","['', 'this is often obtained via regularization methods to penalize complexity of classifiers.', 'exploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for re, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain 1 so that it could generalize well on new domains.', 'eventually, regularization methods can be considered naturally as a simple yet general technique to cope with da problems.', 'following  #TAUTHOR_TAG, we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data.', 'moreover, we consider the singlesystem da setting where we construct a single system able to work robustly with different but related domains ( multiple target domains ).', '']",5
"['system.', 'unfortunately, this feature set includes the human - annotated ( gold - standard ) information on entity and mention types which is often missing or noisy in reality  #TAUTHOR_TAG.', 'this issue becomes more serious in our setting of single - system da where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available.', 'therefore, following the settings of  #TAUTHOR_TAG, we will only']","['system.', 'unfortunately, this feature set includes the human - annotated ( gold - standard ) information on entity and mention types which is often missing or noisy in reality  #TAUTHOR_TAG.', 'this issue becomes more serious in our setting of single - system da where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available.', 'therefore, following the settings of  #TAUTHOR_TAG, we will only']","['the state - of - the - art feature - based re system.', 'unfortunately, this feature set includes the human - annotated ( gold - standard ) information on entity and mention types which is often missing or noisy in reality  #TAUTHOR_TAG.', 'this issue becomes more serious in our setting of single - system da where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available.', 'therefore, following the settings of  #TAUTHOR_TAG, we will only']","['consider two types of word representations and use them as additional features in our da system, namely brown word clustering  #AUTHOR_TAG and word embeddings  #AUTHOR_TAG.', 'while word clusters can be recognized as an one - hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real - valued vectors ( distributed representations ).', 'each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities  #AUTHOR_TAG.', 'we investigate word embeddings induced by two typical language models :  #AUTHOR_TAG embeddings ( c & w )  #AUTHOR_TAG and hierarchical log - bilinear embeddings ( hlbl )  #AUTHOR_TAG.', '5 feature set 5. 1  #AUTHOR_TAG utilize the full feature set from  #AUTHOR_TAG plus some additional features and achieve the state - of - the - art feature - based re system.', 'unfortunately, this feature set includes the human - annotated ( gold - standard ) information on entity and mention types which is often missing or noisy in reality  #TAUTHOR_TAG.', 'this issue becomes more serious in our setting of single - system da where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available.', 'therefore, following the settings of  #TAUTHOR_TAG, we will only assume entity boundaries and not rely on the gold standard information in the experiments.', 'we apply the same feature set as  #AUTHOR_TAG but remove the entity and mention type information 2']",5
"['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire (']","['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'we follow the standard']","['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire (']","['', 'in order to answer these questions, following  #AUTHOR_TAG, we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various featurebased re systems.', 'after that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance.', 'for each of these group combinations, we assess the system performance with different numbers of dimensions for both c & w and hlbl word embeddings.', 'let m1 and m2 be the first and second mentions in the relation.', 'to facilitate system comparison later.', 'we evaluate c & w word embeddings with 25, 50 and 100 dimensions as well as hlbl word embeddings with 50 and 100 dimensions that are introduced in  #AUTHOR_TAG and can be downloaded here 4.', 'the fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single - system da.', 'we use the ace 2005 corpus for da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'we follow the standard practices on ace  #TAUTHOR_TAG and use news ( the union of bn and nw ) as the source domain and bc, cts and wl as our target domains.', 'we take half of bc as the only target development set, and use the remaining data and domains for testing purposes ( as they are small already ).', 'as noted in  #TAUTHOR_TAG, the distributions of relations as well as the vocabularies of the domains are quite different']",5
"['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire (']","['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'we follow the standard']","['da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire (']","['', 'in order to answer these questions, following  #AUTHOR_TAG, we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various featurebased re systems.', 'after that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance.', 'for each of these group combinations, we assess the system performance with different numbers of dimensions for both c & w and hlbl word embeddings.', 'let m1 and m2 be the first and second mentions in the relation.', 'to facilitate system comparison later.', 'we evaluate c & w word embeddings with 25, 50 and 100 dimensions as well as hlbl word embeddings with 50 and 100 dimensions that are introduced in  #AUTHOR_TAG and can be downloaded here 4.', 'the fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single - system da.', 'we use the ace 2005 corpus for da experiments ( as in  #TAUTHOR_TAG.', 'it involves 6 relation types and 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'we follow the standard practices on ace  #TAUTHOR_TAG and use news ( the union of bn and nw ) as the source domain and bc, cts and wl as our target domains.', 'we take half of bc as the only target development set, and use the remaining data and domains for testing purposes ( as they are small already ).', 'as noted in  #TAUTHOR_TAG, the distributions of relations as well as the vocabularies of the domains are quite different']",5
"['.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of']","['baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of 4, 6, 8, 10 together with']","['by ed ) features by adding them individually as well as simultaneously into the baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of']","['', 'we evaluate word cluster and embedding ( denoted by ed ) features by adding them individually as well as simultaneously into the baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string ( denoted by wc ).', '']",5
"['.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of']","['baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of 4, 6, 8, 10 together with']","['by ed ) features by adding them individually as well as simultaneously into the baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of']","['', 'we evaluate word cluster and embedding ( denoted by ed ) features by adding them individually as well as simultaneously into the baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string ( denoted by wc ).', '']",4
"['.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of']","['baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of 4, 6, 8, 10 together with']","['by ed ) features by adding them individually as well as simultaneously into the baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of']","['', 'we evaluate word cluster and embedding ( denoted by ed ) features by adding them individually as well as simultaneously into the baseline feature set.', 'for word clusters, we experiment with two possibilities : ( i ) only using a single prefix length of 10 ( as  #TAUTHOR_TAG did ) ( denoted by wc10 ) and ( ii ) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string ( denoted by wc ).', '']",4
"['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words,']","['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words,']","['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words,']","['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like wordnet  #AUTHOR_TAG are unavailable or lack coverage.', '']",0
"['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words,']","['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words,']","['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words,']","['- scale distributional thesauri created automatically from corpora  #TAUTHOR_TAG are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like wordnet  #AUTHOR_TAG are unavailable or lack coverage.', '']",0
"['the target appears  #TAUTHOR_TAG.', 'the informativeness of']","['the target appears  #TAUTHOR_TAG.', 'the informativeness of']","['the target appears  #TAUTHOR_TAG.', 'the informativeness of each context is calculated using measures']","['a nutshell, the standard approach to build a distributional thesaurus consists of : ( i ) the extraction of contexts for the target words from corpora, ( ii ) the application of an informativeness measure to represent these contexts and ( iii ) the application of a similarity measure to compare sets of contexts.', 'the contexts in which a target word appears can be extracted in terms of a window of cooccurring ( content ) words surrounding the target  #AUTHOR_TAG or in terms of the syntactic dependencies in which the target appears  #TAUTHOR_TAG.', 'the informativeness of each context is calculated using measures like pmi, and t - test while the similarity between contexts is calculated using measures like  #TAUTHOR_TAG, cosine, jensen - shannon divergence, dice or jaccard.', 'evaluation of the quality of distributional thesauri is a well know problem in the area  #TAUTHOR_TAG.', 'for instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri  #TAUTHOR_TAG, and at the overlap and rank agreement between the thesauri for target words like nouns  #AUTHOR_TAG.', 'although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.', 'for instance,  #AUTHOR_TAG found that filtering a subset of contexts based on lmi increased the similarity of a thesaurus with wordnet.', 'in this work, we compare the impact of using different types of filters in terms of thesaurus agreement with wordnet, focusing on a distributional thesaurus of english verbs.', 'we also propose a frequency - based saliency measure to rank and filter contexts and compare it with pmi and lmi.', 'extrinsic evaluation of distributional thesauri has been carried out for tasks such as english lexical substitution ( mc  #AUTHOR_TAG, phrasal verb compositionality detection ( mc  #AUTHOR_TAG and the wordnet - based synonymy test ( wbst )  #AUTHOR_TAG.', 'for comparative purposes in this work we adopt the latter']",0
"['the target appears  #TAUTHOR_TAG.', 'the informativeness of']","['the target appears  #TAUTHOR_TAG.', 'the informativeness of']","['the target appears  #TAUTHOR_TAG.', 'the informativeness of each context is calculated using measures']","['a nutshell, the standard approach to build a distributional thesaurus consists of : ( i ) the extraction of contexts for the target words from corpora, ( ii ) the application of an informativeness measure to represent these contexts and ( iii ) the application of a similarity measure to compare sets of contexts.', 'the contexts in which a target word appears can be extracted in terms of a window of cooccurring ( content ) words surrounding the target  #AUTHOR_TAG or in terms of the syntactic dependencies in which the target appears  #TAUTHOR_TAG.', 'the informativeness of each context is calculated using measures like pmi, and t - test while the similarity between contexts is calculated using measures like  #TAUTHOR_TAG, cosine, jensen - shannon divergence, dice or jaccard.', 'evaluation of the quality of distributional thesauri is a well know problem in the area  #TAUTHOR_TAG.', 'for instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri  #TAUTHOR_TAG, and at the overlap and rank agreement between the thesauri for target words like nouns  #AUTHOR_TAG.', 'although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.', 'for instance,  #AUTHOR_TAG found that filtering a subset of contexts based on lmi increased the similarity of a thesaurus with wordnet.', 'in this work, we compare the impact of using different types of filters in terms of thesaurus agreement with wordnet, focusing on a distributional thesaurus of english verbs.', 'we also propose a frequency - based saliency measure to rank and filter contexts and compare it with pmi and lmi.', 'extrinsic evaluation of distributional thesauri has been carried out for tasks such as english lexical substitution ( mc  #AUTHOR_TAG, phrasal verb compositionality detection ( mc  #AUTHOR_TAG and the wordnet - based synonymy test ( wbst )  #AUTHOR_TAG.', 'for comparative purposes in this work we adopt the latter']",0
"['the target appears  #TAUTHOR_TAG.', 'the informativeness of']","['the target appears  #TAUTHOR_TAG.', 'the informativeness of']","['the target appears  #TAUTHOR_TAG.', 'the informativeness of each context is calculated using measures']","['a nutshell, the standard approach to build a distributional thesaurus consists of : ( i ) the extraction of contexts for the target words from corpora, ( ii ) the application of an informativeness measure to represent these contexts and ( iii ) the application of a similarity measure to compare sets of contexts.', 'the contexts in which a target word appears can be extracted in terms of a window of cooccurring ( content ) words surrounding the target  #AUTHOR_TAG or in terms of the syntactic dependencies in which the target appears  #TAUTHOR_TAG.', 'the informativeness of each context is calculated using measures like pmi, and t - test while the similarity between contexts is calculated using measures like  #TAUTHOR_TAG, cosine, jensen - shannon divergence, dice or jaccard.', 'evaluation of the quality of distributional thesauri is a well know problem in the area  #TAUTHOR_TAG.', 'for instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri  #TAUTHOR_TAG, and at the overlap and rank agreement between the thesauri for target words like nouns  #AUTHOR_TAG.', 'although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.', 'for instance,  #AUTHOR_TAG found that filtering a subset of contexts based on lmi increased the similarity of a thesaurus with wordnet.', 'in this work, we compare the impact of using different types of filters in terms of thesaurus agreement with wordnet, focusing on a distributional thesaurus of english verbs.', 'we also propose a frequency - based saliency measure to rank and filter contexts and compare it with pmi and lmi.', 'extrinsic evaluation of distributional thesauri has been carried out for tasks such as english lexical substitution ( mc  #AUTHOR_TAG, phrasal verb compositionality detection ( mc  #AUTHOR_TAG and the wordnet - based synonymy test ( wbst )  #AUTHOR_TAG.', 'for comparative purposes in this work we adopt the latter']",0
"[', subject ).', 'the thesauri were constructed using  #TAUTHOR_TAG method.', ""lin's version of the distributional hypothesis states that""]","['( object, subject ).', 'the thesauri were constructed using  #TAUTHOR_TAG method.', ""lin's version of the distributional hypothesis states that""]","[', subject ).', 'the thesauri were constructed using  #TAUTHOR_TAG method.', ""lin's version of the distributional hypothesis states that two words ( verbs""]","['focus on thesauri of english verbs constructed from the bnc  #AUTHOR_TAG 1.', 'contexts are extracted from syntactic dependencies generated by rasp  #AUTHOR_TAG, using nouns ( heads of nps ) which have subject and direct object relations with the target verb.', 'thus, each target verb is represented by a set of triples containing ( i ) the verb itself, ( ii ) a context noun and ( iii ) a syntactic relation ( object, subject ).', 'the thesauri were constructed using  #TAUTHOR_TAG method.', ""lin's version of the distributional hypothesis states that two words ( verbs v 1 and v 2 in our case ) are similar if they share a large proportion of contexts weighted by their information content, assessed with pmi  #AUTHOR_TAG."", 'in the literature, little attention is paid to context filters.', ""to investigate their impact, we compare two kinds of filters, and before calculating similarity using lin's measure, we apply them to remove potentially noisy triples :"", '• threshold ( th ) : we remove triples that occur less than a threshold th.', 'threshold values vary from 1 to 50 counts per triple.', '• relevance ( p ) : we keep only the top p most relevant contexts for each verb, were relevance is defined according to the following measures : ( a ) frequency, ( b ) pmi, and ( c ) lmi  #AUTHOR_TAG.', 'values of p vary between 10 and 1000.', '']",5
"['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oo']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['to the word - based ctc. the first one is the oov issue. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oov words cannot be modeled by the lstm - rnn and cannot be recognized during evaluation. the second issue of the wordbased ctc is that it', 'cannot handle hot - words which emerge and become popular after the network has been built. it is impossible to get satisfactory performance by directly adding output nodes in the network with the specified hot - words without retraining the network. inspired by', 'the open vocabulary neural machine translation work [ 31 ], we propose an acoustic - to - word model without oov by first building a word - based ctc', 'in which the output vocabulary contains the frequent words in the training set together with an oov token which all the infrequent words are mapped to. then we train a characterbased', 'ctc by sharing most hidden layers of the word - based ctc. during recognition, the word - based ctc generates a word sequence, and the character - based ctc is only consult', '##ed at the oov segments. evaluated on a microsoft internal cortana voice assistant task, the proposed method can reduce the', '']",0
"['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oo']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['to the word - based ctc. the first one is the oov issue. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oov words cannot be modeled by the lstm - rnn and cannot be recognized during evaluation. the second issue of the wordbased ctc is that it', 'cannot handle hot - words which emerge and become popular after the network has been built. it is impossible to get satisfactory performance by directly adding output nodes in the network with the specified hot - words without retraining the network. inspired by', 'the open vocabulary neural machine translation work [ 31 ], we propose an acoustic - to - word model without oov by first building a word - based ctc', 'in which the output vocabulary contains the frequent words in the training set together with an oov token which all the infrequent words are mapped to. then we train a characterbased', 'ctc by sharing most hidden layers of the word - based ctc. during recognition, the word - based ctc generates a word sequence, and the character - based ctc is only consult', '##ed at the oov segments. evaluated on a microsoft internal cortana voice assistant task, the proposed method can reduce the', '']",0
"['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oo']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['to the word - based ctc. the first one is the oov issue. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oov words cannot be modeled by the lstm - rnn and cannot be recognized during evaluation. the second issue of the wordbased ctc is that it', 'cannot handle hot - words which emerge and become popular after the network has been built. it is impossible to get satisfactory performance by directly adding output nodes in the network with the specified hot - words without retraining the network. inspired by', 'the open vocabulary neural machine translation work [ 31 ], we propose an acoustic - to - word model without oov by first building a word - based ctc', 'in which the output vocabulary contains the frequent words in the training set together with an oov token which all the infrequent words are mapped to. then we train a characterbased', 'ctc by sharing most hidden layers of the word - based ctc. during recognition, the word - based ctc generates a word sequence, and the character - based ctc is only consult', '##ed at the oov segments. evaluated on a microsoft internal cortana voice assistant task, the proposed method can reduce the', '']",0
"['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oo']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['to the word - based ctc. the first one is the oov issue. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oov words cannot be modeled by the lstm - rnn and cannot be recognized during evaluation. the second issue of the wordbased ctc is that it', 'cannot handle hot - words which emerge and become popular after the network has been built. it is impossible to get satisfactory performance by directly adding output nodes in the network with the specified hot - words without retraining the network. inspired by', 'the open vocabulary neural machine translation work [ 31 ], we propose an acoustic - to - word model without oov by first building a word - based ctc', 'in which the output vocabulary contains the frequent words in the training set together with an oov token which all the infrequent words are mapped to. then we train a characterbased', 'ctc by sharing most hidden layers of the word - based ctc. during recognition, the word - based ctc generates a word sequence, and the character - based ctc is only consult', '##ed at the oov segments. evaluated on a microsoft internal cortana voice assistant task, the proposed method can reduce the', '']",0
"['16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map']","['label sequence [ 16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map']","['an output label sequence [ 16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map the label sequence into a ctc path, which forces the output and input sequences to have the same length.', 'denote']","['ctc criterion [ 15 ] was introduced to map the speech input frames into an output label sequence [ 16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map the label sequence into a ctc path, which forces the output and input sequences to have the same length.', 'denote as the speech input sequence, as the original label sequence, and b −1 ( ) represents all the ctc paths mapped from.', 'the ctc loss function is defined as the sum of negative log probabilities of all the ctc paths mapped from the correct label sequence as', 'where is one ctc path.', 'with the conditional independent assumption, ( | ) can be decomposed into a product of posterior from each frame as', 'the calculation of ( | ) is done via the forward - backward process in [ 15 ].', 'the ctc output labels can be phonemes [ 16 ]  #TAUTHOR_TAG [ 23 ] [ 24 ].', 'as the goal of asr is to generate a word sequence from the speech waveform, word unit is the most natural output unit for network modeling.', 'the recently proposed acoustic - toword models [ 23 ] [ 24 ], a. k. a.', 'word - based ctc models, build multiple layer lstm networks and use words as the network output units, optimized with the ctc training criterion.', 'it is very simple to generate the word sequence with this wordbased ctc model : pick the words corresponding to posterior spikes to form the output word sequence.', 'there is neither language model nor complex decoding process involved.', 'however, when training the word - based ctc model, only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.', 'all these oov words cannot be modeled by the network and cannot be recognized during evaluation.', 'in next section, we proposed a hybrid ctc model to address the oov issue and the hotwords issue discussed in the introduction']",0
"['16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map']","['label sequence [ 16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map']","['an output label sequence [ 16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map the label sequence into a ctc path, which forces the output and input sequences to have the same length.', 'denote']","['ctc criterion [ 15 ] was introduced to map the speech input frames into an output label sequence [ 16 ]  #TAUTHOR_TAG [ 18 ].', 'to deal with the issue that the number of output labels is smaller than that of input speech frames, ctc introduces a special blank label and allows the repetition of labels to map the label sequence into a ctc path, which forces the output and input sequences to have the same length.', 'denote as the speech input sequence, as the original label sequence, and b −1 ( ) represents all the ctc paths mapped from.', 'the ctc loss function is defined as the sum of negative log probabilities of all the ctc paths mapped from the correct label sequence as', 'where is one ctc path.', 'with the conditional independent assumption, ( | ) can be decomposed into a product of posterior from each frame as', 'the calculation of ( | ) is done via the forward - backward process in [ 15 ].', 'the ctc output labels can be phonemes [ 16 ]  #TAUTHOR_TAG [ 23 ] [ 24 ].', 'as the goal of asr is to generate a word sequence from the speech waveform, word unit is the most natural output unit for network modeling.', 'the recently proposed acoustic - toword models [ 23 ] [ 24 ], a. k. a.', 'word - based ctc models, build multiple layer lstm networks and use words as the network output units, optimized with the ctc training criterion.', 'it is very simple to generate the word sequence with this wordbased ctc model : pick the words corresponding to posterior spikes to form the output word sequence.', 'there is neither language model nor complex decoding process involved.', 'however, when training the word - based ctc model, only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.', 'all these oov words cannot be modeled by the network and cannot be recognized during evaluation.', 'in next section, we proposed a hybrid ctc model to address the oov issue and the hotwords issue discussed in the introduction']",0
"['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oo']","['. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov.']","['to the word - based ctc. the first one is the oov issue. in  #TAUTHOR_TAG [ 23 ] [ 24 ], only frequent words in the training set are used as the targets and the remaining words are just tagged as the oov. all these oov words cannot be modeled by the lstm - rnn and cannot be recognized during evaluation. the second issue of the wordbased ctc is that it', 'cannot handle hot - words which emerge and become popular after the network has been built. it is impossible to get satisfactory performance by directly adding output nodes in the network with the specified hot - words without retraining the network. inspired by', 'the open vocabulary neural machine translation work [ 31 ], we propose an acoustic - to - word model without oov by first building a word - based ctc', 'in which the output vocabulary contains the frequent words in the training set together with an oov token which all the infrequent words are mapped to. then we train a characterbased', 'ctc by sharing most hidden layers of the word - based ctc. during recognition, the word - based ctc generates a word sequence, and the character - based ctc is only consult', '##ed at the oov segments. evaluated on a microsoft internal cortana voice assistant task, the proposed method can reduce the', '']",1
,,,,5
,,,,3
[' #AUTHOR_TAG b ;  #TAUTHOR_TAG'],[' #AUTHOR_TAG b ;  #TAUTHOR_TAG'],"[' #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'the goal of this tutorial is']","['', 'we will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently - released interactive 3d simulation environments designed for this purpose  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', '']",0
[' #AUTHOR_TAG b ;  #TAUTHOR_TAG'],[' #AUTHOR_TAG b ;  #TAUTHOR_TAG'],"[' #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'the goal of this tutorial is']","['', 'we will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently - released interactive 3d simulation environments designed for this purpose  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', '']",0
"['models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', 'for example,']","['models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', 'for example,']","['semantic parsers are neural encoder - decoder models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', 'for example, figure 1 shows']","['', 'in this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i. e., mapping intents in nl into general purpose source code  #AUTHOR_TAG.', 'state - of - the - art semantic parsers are neural encoder - decoder models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', '']",0
"['directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence -']","['directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence - tosequence model with attention and a copy']","['directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence - tosequence model with attention and a copy mechanism']","['encoder - decoder models have proved effective in mapping nl to logical forms  #AUTHOR_TAG and also for directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence - tosequence model with attention and a copy mechanism to generate source code.', 'instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar - aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree ( ast ) of the code  #AUTHOR_TAG.', 'iy  #TAUTHOR_TAG use a similar decoding approach but use a specialized context encoder for the task of context - dependent code generation.', 'we augment these neural encoder - decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time.', 'another different but related method to produce source code is with the help of sketches, which are code snippets containing slots in the place of low - level information such as variable names and arguments.', ' #AUTHOR_TAG generate sketches as intermediate representations to convert nl to logical forms ;  #AUTHOR_TAG retrieve sketches from a large training corpus and later modify them for the current input ;  #AUTHOR_TAG use a combination of neural learning and type - guided combinatorial search to convert existing sketches into executable programs, whereas  #AUTHOR_TAG additionally also generate the sketches before synthesising programs.', ""while we don't explicitly generate sketches, we find that our idiom - based decoder learns to generate commonly used programming sketches with slots, and fills them in during subsequent decoding timesteps."", 'more closely related to the idioms that we use for decoding is  #AUTHOR_TAG, who develop a system ( haggis ) to automatically mine idioms from large code bases.', 'they focused on finding idioms that are interesting and explainable, e. g., those that can be included as preset code templates in programming ides.', 'instead, we learn idiomatic structures that are frequently used and can be easily associated with natural language phrases in our dataset.', 'the production of large subtrees in a single step directly translates to a large speedup in training and inference']",0
"['directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence -']","['directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence - tosequence model with attention and a copy']","['directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence - tosequence model with attention and a copy mechanism']","['encoder - decoder models have proved effective in mapping nl to logical forms  #AUTHOR_TAG and also for directly producing general purpose programs  #AUTHOR_TAG  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a sequence - tosequence model with attention and a copy mechanism to generate source code.', 'instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar - aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree ( ast ) of the code  #AUTHOR_TAG.', 'iy  #TAUTHOR_TAG use a similar decoding approach but use a specialized context encoder for the task of context - dependent code generation.', 'we augment these neural encoder - decoder models with the ability to decode in terms of frequently occurring higher level idiomatic structures to achieve gains in accuracy and training time.', 'another different but related method to produce source code is with the help of sketches, which are code snippets containing slots in the place of low - level information such as variable names and arguments.', ' #AUTHOR_TAG generate sketches as intermediate representations to convert nl to logical forms ;  #AUTHOR_TAG retrieve sketches from a large training corpus and later modify them for the current input ;  #AUTHOR_TAG use a combination of neural learning and type - guided combinatorial search to convert existing sketches into executable programs, whereas  #AUTHOR_TAG additionally also generate the sketches before synthesising programs.', ""while we don't explicitly generate sketches, we find that our idiom - based decoder learns to generate commonly used programming sketches with slots, and fills them in during subsequent decoding timesteps."", 'more closely related to the idioms that we use for decoding is  #AUTHOR_TAG, who develop a system ( haggis ) to automatically mine idioms from large code bases.', 'they focused on finding idioms that are interesting and explainable, e. g., those that can be included as preset code templates in programming ides.', 'instead, we learn idiomatic structures that are frequently used and can be easily associated with natural language phrases in our dataset.', 'the production of large subtrees in a single step directly translates to a large speedup in training and inference']",0
"['models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', 'for example,']","['models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', 'for example,']","['semantic parsers are neural encoder - decoder models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', 'for example, figure 1 shows']","['', 'in this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i. e., mapping intents in nl into general purpose source code  #AUTHOR_TAG.', 'state - of - the - art semantic parsers are neural encoder - decoder models, where decoding is guided by the grammar of the target programming language  #TAUTHOR_TAG to ensure syntactically valid programs.', 'for general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.', '']",5
"['decoder model of  #TAUTHOR_TAG on the concode dataset, and compare']","['apply our approach to the context dependent encoder - decoder model of  #TAUTHOR_TAG on the concode dataset, and compare']","['apply our approach to the context dependent encoder - decoder model of  #TAUTHOR_TAG on the concode dataset, and compare performance to a better tuned instance of their best model']","['apply our approach to the context dependent encoder - decoder model of  #TAUTHOR_TAG on the concode dataset, and compare performance to a better tuned instance of their best model']",5
[' #TAUTHOR_TAG with'],[' #TAUTHOR_TAG with'],"[' #TAUTHOR_TAG with three major modifications in their encoder, which yields improvements in speed and']","['follow the approach of  #TAUTHOR_TAG with three major modifications in their encoder, which yields improvements in speed and accuracy ( iyersimp ).', 'first, in addition to camel - case splitting of identifier tokens, we further use byte - pair encoding ( bpe )  #AUTHOR_TAG on all nl tokens, identifier names and types and embed all these bpe tokens using a single embedding matrix.', 'next, we replace their rnn that contextualizes the subtokens of identifiers and types with an average of the subtoken embeddings instead.', 'finally, we consolidate their three separate rnns for contextualizing nl, variable names with types, and method names with types, into a single shared rnn, which greatly reduces the number of model parameters.', 'formally, let { q i } represent the set of bpe tokens of the nl, and { t ij }, { v ij }, { r ij } and { m ij } represent the jth bpe token of the ith variable type, variable name, method return type, and method name respectively.', 'first, all these elements are embedded using a bpe token embedding matrix b to give us q i, t ij, v ij, r ij and m ij.', 'using bi - lstm f, the encoder then computes :', 'then, h 1,..., h z, andt i, v i, r i, m i are passed on to the attention mechanism in the decoder, exactly as in  #TAUTHOR_TAG.', 'the decoder of  #TAUTHOR_TAG is left unchanged.', 'this forms our baseline model ( iyer - simp )']",5
[' #TAUTHOR_TAG with'],[' #TAUTHOR_TAG with'],"[' #TAUTHOR_TAG with three major modifications in their encoder, which yields improvements in speed and']","['follow the approach of  #TAUTHOR_TAG with three major modifications in their encoder, which yields improvements in speed and accuracy ( iyersimp ).', 'first, in addition to camel - case splitting of identifier tokens, we further use byte - pair encoding ( bpe )  #AUTHOR_TAG on all nl tokens, identifier names and types and embed all these bpe tokens using a single embedding matrix.', 'next, we replace their rnn that contextualizes the subtokens of identifiers and types with an average of the subtoken embeddings instead.', 'finally, we consolidate their three separate rnns for contextualizing nl, variable names with types, and method names with types, into a single shared rnn, which greatly reduces the number of model parameters.', 'formally, let { q i } represent the set of bpe tokens of the nl, and { t ij }, { v ij }, { r ij } and { m ij } represent the jth bpe token of the ith variable type, variable name, method return type, and method name respectively.', 'first, all these elements are embedded using a bpe token embedding matrix b to give us q i, t ij, v ij, r ij and m ij.', 'using bi - lstm f, the encoder then computes :', 'then, h 1,..., h z, andt i, v i, r i, m i are passed on to the attention mechanism in the decoder, exactly as in  #TAUTHOR_TAG.', 'the decoder of  #TAUTHOR_TAG is left unchanged.', 'this forms our baseline model ( iyer - simp )']",5
"[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400']","[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score ( which serves as a measure of partial credit )  #AUTHOR_TAG, and training time for all these configurations.', ' #TAUTHOR_TAG.', 'significant improvements in training speed after incorporating idioms makes training on large amounts of data possible']","[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score (']","['', 'we use a bpe vocabulary of 10k tokens for embedding matrix b and get the best validation set results using the original hyperparameters used by  #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score ( which serves as a measure of partial credit )  #AUTHOR_TAG, and training time for all these configurations.', ' #TAUTHOR_TAG.', 'significant improvements in training speed after incorporating idioms makes training on large amounts of data possible']",5
"[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400']","[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score ( which serves as a measure of partial credit )  #AUTHOR_TAG, and training time for all these configurations.', ' #TAUTHOR_TAG.', 'significant improvements in training speed after incorporating idioms makes training on large amounts of data possible']","[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score (']","['', 'we use a bpe vocabulary of 10k tokens for embedding matrix b and get the best validation set results using the original hyperparameters used by  #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score ( which serves as a measure of partial credit )  #AUTHOR_TAG, and training time for all these configurations.', ' #TAUTHOR_TAG.', 'significant improvements in training speed after incorporating idioms makes training on large amounts of data possible']",5
"['.', 'the decoder is then trained similar to previous approaches  #TAUTHOR_TAG using the compressed set of rules.', 'in later experiments, we find that this results in a']","['tree can be represented using | r i | = 2 rules instead of the original | p i | = 5 rules.', 'the decoder is then trained similar to previous approaches  #TAUTHOR_TAG using the compressed set of rules.', 'in later experiments, we find that this results in a']","['r i | = 2 rules instead of the original | p i | = 5 rules.', 'the decoder is then trained similar to previous approaches  #TAUTHOR_TAG using the compressed set of rules.', 'in later experiments, we find that this results in a rule set compression of more than 50 % ( see section 7 )']","['', 'once t i cannot be further collapsed, we translate t i into production rules r i based on the collapsed tree, with | r i | ≤ | p i | ( step 7 ).', 'this process is illustrated in figure 3 where we perform two applications of the first idiom from figure 2 ( b ), followed by one application of the second idiom from figure 2 ( d ), after which, the tree cannot be further compressed using those two idioms.', 'the final tree can be represented using | r i | = 2 rules instead of the original | p i | = 5 rules.', 'the decoder is then trained similar to previous approaches  #TAUTHOR_TAG using the compressed set of rules.', 'in later experiments, we find that this results in a rule set compression of more than 50 % ( see section 7 )']",3
"[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400']","[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score ( which serves as a measure of partial credit )  #AUTHOR_TAG, and training time for all these configurations.', ' #TAUTHOR_TAG.', 'significant improvements in training speed after incorporating idioms makes training on large amounts of data possible']","[' #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score (']","['', 'we use a bpe vocabulary of 10k tokens for embedding matrix b and get the best validation set results using the original hyperparameters used by  #TAUTHOR_TAG.', 'since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400k training examples that  #TAUTHOR_TAG released as part of concode.', 'we report exact match accuracy, corpus level bleu score ( which serves as a measure of partial credit )  #AUTHOR_TAG, and training time for all these configurations.', ' #TAUTHOR_TAG.', 'significant improvements in training speed after incorporating idioms makes training on large amounts of data possible']",3
[' #TAUTHOR_TAG with'],[' #TAUTHOR_TAG with'],"[' #TAUTHOR_TAG with three major modifications in their encoder, which yields improvements in speed and']","['follow the approach of  #TAUTHOR_TAG with three major modifications in their encoder, which yields improvements in speed and accuracy ( iyersimp ).', 'first, in addition to camel - case splitting of identifier tokens, we further use byte - pair encoding ( bpe )  #AUTHOR_TAG on all nl tokens, identifier names and types and embed all these bpe tokens using a single embedding matrix.', 'next, we replace their rnn that contextualizes the subtokens of identifiers and types with an average of the subtoken embeddings instead.', 'finally, we consolidate their three separate rnns for contextualizing nl, variable names with types, and method names with types, into a single shared rnn, which greatly reduces the number of model parameters.', 'formally, let { q i } represent the set of bpe tokens of the nl, and { t ij }, { v ij }, { r ij } and { m ij } represent the jth bpe token of the ith variable type, variable name, method return type, and method name respectively.', 'first, all these elements are embedded using a bpe token embedding matrix b to give us q i, t ij, v ij, r ij and m ij.', 'using bi - lstm f, the encoder then computes :', 'then, h 1,..., h z, andt i, v i, r i, m i are passed on to the attention mechanism in the decoder, exactly as in  #TAUTHOR_TAG.', 'the decoder of  #TAUTHOR_TAG is left unchanged.', 'this forms our baseline model ( iyer - simp )']",6
"['the top - 600 idioms are incorporated.', 'compared to the model of  #TAUTHOR_TAG, our significantly']","['the top - 600 idioms are incorporated.', 'compared to the model of  #TAUTHOR_TAG, our significantly']","['the top - 600 idioms are incorporated.', 'compared to the model of  #TAUTHOR_TAG, our significantly']","['##ning comparable em accuracy.', 'using the top - 200 idioms results in a target ast compression of more than 50 %, which results in fewer decoder rnn steps being performed.', 'this reduces training time further by more than 50 %, from 27 hours to 13 hours.', 'in table 2, we illustrate the changes in em, bleu and training time as we vary the number of idioms.', 'we find that 200 idioms performs best overall in terms of balancing accuracy and training time.', 'adding more idioms continues to reduce training time, but accuracy also suffers.', 'since we permit idioms to contain identifier names in order to capture frequently used library methods in idioms, having too many idioms hurts generalization, especially since the test set is built using repositories disjoint from the training set.', 'finally, the amount of compression, and therefore the training time, plateaus after the top - 600 idioms are incorporated.', 'compared to the model of  #TAUTHOR_TAG, our significantly reduced training time enables us to train on their extended training set.', 'we run iyersimp using 400 idioms ( taking advantage of even lower training time ) on up to 5 times the amount of data, making sure that we do not include in training any nl from the validation or the test sets.', 'since the original set of idioms learned from the original training set are quite general, we directly use them rather than relearn the idioms from scratch.', 'we report em and bleu scores for different amounts of training data on the same validation and test sets as concode in table 3.', '']",4
[')  #TAUTHOR_TAG task'],[')  #TAUTHOR_TAG task'],[' #TAUTHOR_TAG task'],"[', we develop a continuous setting that enables these types of studies and take a first step towards integrating vln agents with control via low - level actions. vision - and - language navigation in continuous environments. in this work, we focus', 'in on the vision - and - language navigation ( vln )  #TAUTHOR_TAG task and lift these implicit assumptions by instantiating it in continuous 3d environments rendered in a high', '- throughput simulator [ 19 ]. consequently, we call this task vision - and - language navigation in', 'continuous environments ( vln - ce ). agents in our task are free to navigate to any unobstructed point through a set of low - level actions ( e. g.', 'move forward 0. 25m, turn - left 15 degrees ) rather than teleporting between fixed nodes. this', 'setting introduces many challenges ignored in prior work. agents in vln - ce face significantly longer time horizons ; the average number of actions along a path in vln - ce', 'is ∼55 compared to the 4 - 6 node hops in vln ( as illustrated in fig. 1 ). moreover, the views the agent receives along the way are not well - posed by careful human operators as in the panoramas, but', ""rather a consequence of the agent's actions. agents must also learn to avoid getting stuck on obstacles, something that is structurally impossible in vln's navigability defined nav - graph. further, agents are not provided their location or heading while navigating. we develop agent"", 'architectures for this task and explore how popular mechanisms for vln transfer to the vln - ce setting. specifically, we develop a simple sequence - to - sequence baseline architecture as well as', 'a cross - modal attentionbased model. we perform a number of input - modality ablations to assess the biases and baselines in this new setting ( including models without perception or instructions as', 'suggested in [ 27 ] ). unlike in vl', '##n where depth is rarely used, our analysis reveals depth to be an integral signal for learning embodied navigation - echoing similar findings in point - goal navigation tasks [ 19, 31 ]', '. we also apply existing training augmentations [ 17, 24, 26 ] popular in vln to our setting, finding mixed results. overall, our best', 'performing agent successfully navigates to the goal in approximately a third of episodes in unseen environments - taking an average of 88 actions in this long - horizon task. table 1', '. comparison of language - guided visual navigation tasks. ours is the only to provide unconstrained navigation in real environments for crowdsourced instructions']",0
[')  #TAUTHOR_TAG task'],[')  #TAUTHOR_TAG task'],[' #TAUTHOR_TAG task'],"[', we develop a continuous setting that enables these types of studies and take a first step towards integrating vln agents with control via low - level actions. vision - and - language navigation in continuous environments. in this work, we focus', 'in on the vision - and - language navigation ( vln )  #TAUTHOR_TAG task and lift these implicit assumptions by instantiating it in continuous 3d environments rendered in a high', '- throughput simulator [ 19 ]. consequently, we call this task vision - and - language navigation in', 'continuous environments ( vln - ce ). agents in our task are free to navigate to any unobstructed point through a set of low - level actions ( e. g.', 'move forward 0. 25m, turn - left 15 degrees ) rather than teleporting between fixed nodes. this', 'setting introduces many challenges ignored in prior work. agents in vln - ce face significantly longer time horizons ; the average number of actions along a path in vln - ce', 'is ∼55 compared to the 4 - 6 node hops in vln ( as illustrated in fig. 1 ). moreover, the views the agent receives along the way are not well - posed by careful human operators as in the panoramas, but', ""rather a consequence of the agent's actions. agents must also learn to avoid getting stuck on obstacles, something that is structurally impossible in vln's navigability defined nav - graph. further, agents are not provided their location or heading while navigating. we develop agent"", 'architectures for this task and explore how popular mechanisms for vln transfer to the vln - ce setting. specifically, we develop a simple sequence - to - sequence baseline architecture as well as', 'a cross - modal attentionbased model. we perform a number of input - modality ablations to assess the biases and baselines in this new setting ( including models without perception or instructions as', 'suggested in [ 27 ] ). unlike in vl', '##n where depth is rarely used, our analysis reveals depth to be an integral signal for learning embodied navigation - echoing similar findings in point - goal navigation tasks [ 19, 31 ]', '. we also apply existing training augmentations [ 17, 24, 26 ] popular in vln to our setting, finding mixed results. overall, our best', 'performing agent successfully navigates to the goal in approximately a third of episodes in unseen environments - taking an average of 88 actions in this long - horizon task. table 1', '. comparison of language - guided visual navigation tasks. ours is the only to provide unconstrained navigation in real environments for crowdsourced instructions']",0
"['', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ]']","['- guided visual navigation tasks.', 'language - guided visual navigation tasks require agents to follow navigation directions in simulated environments.', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ]']","['- guided visual navigation tasks.', 'language - guided visual navigation tasks require agents to follow navigation directions in simulated environments.', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ].', 'chen']","['- guided visual navigation tasks.', 'language - guided visual navigation tasks require agents to follow navigation directions in simulated environments.', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ].', 'chen et al. [ 6 ] introduce the touchdown task which studies outdoor language - guided navigation in google street view panoramas.', 'hermann et al. [ 13 ] investigates the same setting ; however, the instructions are automatically generated from google map directions rather than being crowdsourced from human annotators.', 'both adopt a nav - graph setting due to the source data being panoramic images - constraining agent navigation to fixed points.', 'misra et al. [ 20 ] introduce a simulated environment with unconstrained navigation and a dataset of crowdsourced instructions ; however, the environments are unrealistic, synthetic scenes.', '']",0
"['', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ]']","['- guided visual navigation tasks.', 'language - guided visual navigation tasks require agents to follow navigation directions in simulated environments.', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ]']","['- guided visual navigation tasks.', 'language - guided visual navigation tasks require agents to follow navigation directions in simulated environments.', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ].', 'chen']","['- guided visual navigation tasks.', 'language - guided visual navigation tasks require agents to follow navigation directions in simulated environments.', 'there have been a number of recent tasks proposed in this space  #TAUTHOR_TAG 6, 13, 20 ].', 'chen et al. [ 6 ] introduce the touchdown task which studies outdoor language - guided navigation in google street view panoramas.', 'hermann et al. [ 13 ] investigates the same setting ; however, the instructions are automatically generated from google map directions rather than being crowdsourced from human annotators.', 'both adopt a nav - graph setting due to the source data being panoramic images - constraining agent navigation to fixed points.', 'misra et al. [ 20 ] introduce a simulated environment with unconstrained navigation and a dataset of crowdsourced instructions ; however, the environments are unrealistic, synthetic scenes.', '']",0
['.  #TAUTHOR_TAG developed the matterport'],"['et al.  #TAUTHOR_TAG developed the matterport3d simulator.', 'environments in this simulator are defined as nav - graphs']","['.  #TAUTHOR_TAG developed the matterport3d simulator.', 'environments in this simulator are defined as nav -']","['than collecting a new dataset of trajectories and instructions, we instead transfer those from the nav - graph - based room - to - room dataset to our continuous setting.', 'doing so enables us to compare existing nav - graph - based techniques with our methods that operate in continuous environments on the same instructions.', 'matterport3d simulator and the room - to - room dataset.', 'the original vln task is based on panoramas from matterport3d ( mp3d ) [ 5 ].', 'to enable agent interaction with these panoramas, anderson et al.  #TAUTHOR_TAG developed the matterport3d simulator.', 'environments in this simulator are defined as nav - graphs e = { v, e }. each node v ∈ v corresponds to a panoramic image i captured by a matterport camera at location x, y, z - i. e.', 'v = { i, x, y, z }. edges in the graph correspond to navigability between nodes.', 'navigability was defined by ray - tracing between node locations at varying heights to check for obstacles in the reconstructed mp3d scene and then manually inspected.', 'edges were manually added or removed based on judgement whether an agent could navigate between nodes - including by avoiding minor obstacles 4.', 'agents act by teleporting between adjacent nodes in this graph.', 'based on this simulator, anderson et al.  #TAUTHOR_TAG collect the roomto - room ( r2r ) dataset containing 7189 trajectories each with three humangenerated instructions on average.', 'these trajectories consist of a sequence of nodes τ = [ v 1,..., v t ] with length t averaging between 4 and 6 nodes.', 'converting room - to - room trajectories to habitat.', 'given a mapping between the coordinate frames of matterport3d simulator and mp3d in habitat, it is seemingly simple to transfer the room - to - room trajectories - after all, each node has a corresponding xyz location.', 'however, node locations often do not correspond to reachable locations for a ground - based agent - existing at variable height depending on tripod configuration or placed on top of flat furniture like tables.', 'further, the reconstructions and panoramas may differ if objects or doors are moved between camera captures.', 'fig. 2 shows an overview of this process and common errors when directly transferring node locations.', 'for each node, v = { i, x, y, z }, we would like to identify the nearest, navigable point on the reconstructed mesh - i. e.', 'the closest point that can be occupied by a ground - based agent']",0
['.  #TAUTHOR_TAG developed the matterport'],"['et al.  #TAUTHOR_TAG developed the matterport3d simulator.', 'environments in this simulator are defined as nav - graphs']","['.  #TAUTHOR_TAG developed the matterport3d simulator.', 'environments in this simulator are defined as nav -']","['than collecting a new dataset of trajectories and instructions, we instead transfer those from the nav - graph - based room - to - room dataset to our continuous setting.', 'doing so enables us to compare existing nav - graph - based techniques with our methods that operate in continuous environments on the same instructions.', 'matterport3d simulator and the room - to - room dataset.', 'the original vln task is based on panoramas from matterport3d ( mp3d ) [ 5 ].', 'to enable agent interaction with these panoramas, anderson et al.  #TAUTHOR_TAG developed the matterport3d simulator.', 'environments in this simulator are defined as nav - graphs e = { v, e }. each node v ∈ v corresponds to a panoramic image i captured by a matterport camera at location x, y, z - i. e.', 'v = { i, x, y, z }. edges in the graph correspond to navigability between nodes.', 'navigability was defined by ray - tracing between node locations at varying heights to check for obstacles in the reconstructed mp3d scene and then manually inspected.', 'edges were manually added or removed based on judgement whether an agent could navigate between nodes - including by avoiding minor obstacles 4.', 'agents act by teleporting between adjacent nodes in this graph.', 'based on this simulator, anderson et al.  #TAUTHOR_TAG collect the roomto - room ( r2r ) dataset containing 7189 trajectories each with three humangenerated instructions on average.', 'these trajectories consist of a sequence of nodes τ = [ v 1,..., v t ] with length t averaging between 4 and 6 nodes.', 'converting room - to - room trajectories to habitat.', 'given a mapping between the coordinate frames of matterport3d simulator and mp3d in habitat, it is seemingly simple to transfer the room - to - room trajectories - after all, each node has a corresponding xyz location.', 'however, node locations often do not correspond to reachable locations for a ground - based agent - existing at variable height depending on tripod configuration or placed on top of flat furniture like tables.', 'further, the reconstructions and panoramas may differ if objects or doors are moved between camera captures.', 'fig. 2 shows an overview of this process and common errors when directly transferring node locations.', 'for each node, v = { i, x, y, z }, we would like to identify the nearest, navigable point on the reconstructed mesh - i. e.', 'the closest point that can be occupied by a ground - based agent']",0
"['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity']","['from modeling details, much of the remaining progress in vln has come from adjusting the training regime - adding auxiliary losses / rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity by incorporating synthetically generated data augmentation [ 9, 26 ].', 'we explore some of these directions for vln - ce, but note that this is not an exhaustive accounting of impactful techniques.', 'particularly, we suspect that methods addressing exposure bias and data sparsity in vln will help in the vln - ce setting where these problems may be amplified by lengthy action sequences.', 'we report ablations with and without these techniques in sec. 5.', 'imitation learning.', 'a natural starting point for training is simply to maximize the likelihood of the ground truth trajectories.', 'to do so, we perform teacherforcing training with inflection weighting.', 'as described in [ 30 ], inflection weighting places emphasis on time - steps where actions change ( i. e. a t−1 = a t ), adjusting loss weight proportionally to the rarity of such events.', 'this was found to be helpful for problems like navigation with long sequences of repeated actions ( e. g. going forward down a hall ).', 'we observe a similar effect in early experiments and apply inflection weighting in all our experiments.', 'coping with exposure bias.', '']",0
"['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity']","['from modeling details, much of the remaining progress in vln has come from adjusting the training regime - adding auxiliary losses / rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity by incorporating synthetically generated data augmentation [ 9, 26 ].', 'we explore some of these directions for vln - ce, but note that this is not an exhaustive accounting of impactful techniques.', 'particularly, we suspect that methods addressing exposure bias and data sparsity in vln will help in the vln - ce setting where these problems may be amplified by lengthy action sequences.', 'we report ablations with and without these techniques in sec. 5.', 'imitation learning.', 'a natural starting point for training is simply to maximize the likelihood of the ground truth trajectories.', 'to do so, we perform teacherforcing training with inflection weighting.', 'as described in [ 30 ], inflection weighting places emphasis on time - steps where actions change ( i. e. a t−1 = a t ), adjusting loss weight proportionally to the rarity of such events.', 'this was found to be helpful for problems like navigation with long sequences of repeated actions ( e. g. going forward down a hall ).', 'we observe a similar effect in early experiments and apply inflection weighting in all our experiments.', 'coping with exposure bias.', '']",0
[')  #TAUTHOR_TAG task'],[')  #TAUTHOR_TAG task'],[' #TAUTHOR_TAG task'],"[', we develop a continuous setting that enables these types of studies and take a first step towards integrating vln agents with control via low - level actions. vision - and - language navigation in continuous environments. in this work, we focus', 'in on the vision - and - language navigation ( vln )  #TAUTHOR_TAG task and lift these implicit assumptions by instantiating it in continuous 3d environments rendered in a high', '- throughput simulator [ 19 ]. consequently, we call this task vision - and - language navigation in', 'continuous environments ( vln - ce ). agents in our task are free to navigate to any unobstructed point through a set of low - level actions ( e. g.', 'move forward 0. 25m, turn - left 15 degrees ) rather than teleporting between fixed nodes. this', 'setting introduces many challenges ignored in prior work. agents in vln - ce face significantly longer time horizons ; the average number of actions along a path in vln - ce', 'is ∼55 compared to the 4 - 6 node hops in vln ( as illustrated in fig. 1 ). moreover, the views the agent receives along the way are not well - posed by careful human operators as in the panoramas, but', ""rather a consequence of the agent's actions. agents must also learn to avoid getting stuck on obstacles, something that is structurally impossible in vln's navigability defined nav - graph. further, agents are not provided their location or heading while navigating. we develop agent"", 'architectures for this task and explore how popular mechanisms for vln transfer to the vln - ce setting. specifically, we develop a simple sequence - to - sequence baseline architecture as well as', 'a cross - modal attentionbased model. we perform a number of input - modality ablations to assess the biases and baselines in this new setting ( including models without perception or instructions as', 'suggested in [ 27 ] ). unlike in vl', '##n where depth is rarely used, our analysis reveals depth to be an integral signal for learning embodied navigation - echoing similar findings in point - goal navigation tasks [ 19, 31 ]', '. we also apply existing training augmentations [ 17, 24, 26 ] popular in vln to our setting, finding mixed results. overall, our best', 'performing agent successfully navigates to the goal in approximately a third of episodes in unseen environments - taking an average of 88 actions in this long - horizon task. table 1', '. comparison of language - guided visual navigation tasks. ours is the only to provide unconstrained navigation in real environments for crowdsourced instructions']",5
['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3'],"['( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of']","['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3']","['consider a continuous setting for the vision - and - language navigation task which we refer to as vision - and - language navigation in continuous environments ( vln - ce ).', 'given a natural language navigation instruction, an agent must navigate from a start position to the described goal in a continuous 3d environment by executing a sequence of low - level actions based on egocentric perception alone.', 'in overview, we develop this setting by transferring nav - graph - based room - to - room ( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3d ) [ 5 ] dataset, a collection of 90 environments captured through over 10, 800 high - definition rgb - d panoramas.', 'in addition to the panoramic images, mp3d also provides corresponding mesh - based 3d environment reconstructions.', 'to enable agent interaction with these meshes, we develop the vln - ce task on top of the habitat simulator [ 19 ], a high - throughput simulator that supports basic movement and collision checking for 3d environments including mp3d.', 'in contrast to the simulator used in vln  #TAUTHOR_TAG, habitat allows agents to navigate freely in the continuous environments.', 'observations and actions.', 'we select observation and action spaces to emulate a ground - based, zero - turning radius robot with a single, forward - mounted rgbd camera, similar to a locobot [ 1 ].', 'agents perceive the world through egocentric rgbd images from the simulator with a resolution of 256×256 and a horizontal field - of - view of 90 degrees.', 'note that this is similar to the egocentric rgb perception in the original vln task  #TAUTHOR_TAG but differs from the panoramic observation space adopted by nearly all follow - up work [ 9, 17, 26, 29 ].', 'while the simulator is quite flexible in terms of agent actions, we consider four simple, low - level actions for agents in vln - ce - move forward 0. 25m, turn - left or turn - right 15 degrees, or stop to declare that the goal position has been reached.', 'these actions can easily be implemented on robotic agents with standard motion controllers.', 'in contrast, actions to move between panoramas in  #TAUTHOR_TAG traverse 2. 25m on average and can include avoiding obstacles']",5
"['2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation']","['early stopping based on val - unseen performance.', 'we report standard metrics for visual navigation tasks defined in [ 2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation']","['2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation error in']","['and metrics.', 'we train and evaluate our models in vln - ce.', 'as is common practice, we perform early stopping based on val - unseen performance.', 'we report standard metrics for visual navigation tasks defined in [ 2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation error in meters from goal at termination ( ne ), oracle success rate ( os ), success rate ( sr ), success weighted by inverse path length ( spl ), and normalized dynamic - time warping ( ndtw ).', 'for our discussion, we will examine success rate and spl as the primary metrics for performance and use ndtw to describe how paths differ in shape from ground truth trajectories.', 'for full details on these metrics, see [ 2,  #TAUTHOR_TAG 18 ].', 'implementation details.', 'we utilize the adam optimizer [ 15 ] with a learning rate of 2. 5 × 10 −4 and a batch size of 5 full trajectories.', 'we set the inflection weighting coefficient [ 30 ] to 3. 2 ( inverse frequency of inflections in our groundtruth paths ).', 'we train on all ground - truth paths until convergence on val - unseen ( at most 30 epochs ).', 'for dagger [ 24 ], we collect the nth set by taking the oracle action with probability β = 0. 75 n and the current policy action otherwise.', 'we collect 5, 000 trajectories at each stage and then perform 4 epochs of imitation learning ( with inflection weighting ) over all collected trajectories.', 'once again, we train to convergence on val - unseen ( 6 to 10 dataset collections, depending on the model ).', 'we implement our agents in pytorch [ 22 ] and on top of habitat [ 19 ]']",5
"['2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation']","['early stopping based on val - unseen performance.', 'we report standard metrics for visual navigation tasks defined in [ 2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation']","['2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation error in']","['and metrics.', 'we train and evaluate our models in vln - ce.', 'as is common practice, we perform early stopping based on val - unseen performance.', 'we report standard metrics for visual navigation tasks defined in [ 2,  #TAUTHOR_TAG 18 ] - trajectory length in meters ( tl ), navigation error in meters from goal at termination ( ne ), oracle success rate ( os ), success rate ( sr ), success weighted by inverse path length ( spl ), and normalized dynamic - time warping ( ndtw ).', 'for our discussion, we will examine success rate and spl as the primary metrics for performance and use ndtw to describe how paths differ in shape from ground truth trajectories.', 'for full details on these metrics, see [ 2,  #TAUTHOR_TAG 18 ].', 'implementation details.', 'we utilize the adam optimizer [ 15 ] with a learning rate of 2. 5 × 10 −4 and a batch size of 5 full trajectories.', 'we set the inflection weighting coefficient [ 30 ] to 3. 2 ( inverse frequency of inflections in our groundtruth paths ).', 'we train on all ground - truth paths until convergence on val - unseen ( at most 30 epochs ).', 'for dagger [ 24 ], we collect the nth set by taking the oracle action with probability β = 0. 75 n and the current policy action otherwise.', 'we collect 5, 000 trajectories at each stage and then perform 4 epochs of imitation learning ( with inflection weighting ) over all collected trajectories.', 'once again, we train to convergence on val - unseen ( 6 to 10 dataset collections, depending on the model ).', 'we implement our agents in pytorch [ 22 ] and on top of habitat [ 19 ]']",5
['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3'],"['( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of']","['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3']","['consider a continuous setting for the vision - and - language navigation task which we refer to as vision - and - language navigation in continuous environments ( vln - ce ).', 'given a natural language navigation instruction, an agent must navigate from a start position to the described goal in a continuous 3d environment by executing a sequence of low - level actions based on egocentric perception alone.', 'in overview, we develop this setting by transferring nav - graph - based room - to - room ( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3d ) [ 5 ] dataset, a collection of 90 environments captured through over 10, 800 high - definition rgb - d panoramas.', 'in addition to the panoramic images, mp3d also provides corresponding mesh - based 3d environment reconstructions.', 'to enable agent interaction with these meshes, we develop the vln - ce task on top of the habitat simulator [ 19 ], a high - throughput simulator that supports basic movement and collision checking for 3d environments including mp3d.', 'in contrast to the simulator used in vln  #TAUTHOR_TAG, habitat allows agents to navigate freely in the continuous environments.', 'observations and actions.', 'we select observation and action spaces to emulate a ground - based, zero - turning radius robot with a single, forward - mounted rgbd camera, similar to a locobot [ 1 ].', 'agents perceive the world through egocentric rgbd images from the simulator with a resolution of 256×256 and a horizontal field - of - view of 90 degrees.', 'note that this is similar to the egocentric rgb perception in the original vln task  #TAUTHOR_TAG but differs from the panoramic observation space adopted by nearly all follow - up work [ 9, 17, 26, 29 ].', 'while the simulator is quite flexible in terms of agent actions, we consider four simple, low - level actions for agents in vln - ce - move forward 0. 25m, turn - left or turn - right 15 degrees, or stop to declare that the goal position has been reached.', 'these actions can easily be implemented on robotic agents with standard motion controllers.', 'in contrast, actions to move between panoramas in  #TAUTHOR_TAG traverse 2. 25m on average and can include avoiding obstacles']",4
['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3'],"['( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of']","['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3']","['consider a continuous setting for the vision - and - language navigation task which we refer to as vision - and - language navigation in continuous environments ( vln - ce ).', 'given a natural language navigation instruction, an agent must navigate from a start position to the described goal in a continuous 3d environment by executing a sequence of low - level actions based on egocentric perception alone.', 'in overview, we develop this setting by transferring nav - graph - based room - to - room ( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3d ) [ 5 ] dataset, a collection of 90 environments captured through over 10, 800 high - definition rgb - d panoramas.', 'in addition to the panoramic images, mp3d also provides corresponding mesh - based 3d environment reconstructions.', 'to enable agent interaction with these meshes, we develop the vln - ce task on top of the habitat simulator [ 19 ], a high - throughput simulator that supports basic movement and collision checking for 3d environments including mp3d.', 'in contrast to the simulator used in vln  #TAUTHOR_TAG, habitat allows agents to navigate freely in the continuous environments.', 'observations and actions.', 'we select observation and action spaces to emulate a ground - based, zero - turning radius robot with a single, forward - mounted rgbd camera, similar to a locobot [ 1 ].', 'agents perceive the world through egocentric rgbd images from the simulator with a resolution of 256×256 and a horizontal field - of - view of 90 degrees.', 'note that this is similar to the egocentric rgb perception in the original vln task  #TAUTHOR_TAG but differs from the panoramic observation space adopted by nearly all follow - up work [ 9, 17, 26, 29 ].', 'while the simulator is quite flexible in terms of agent actions, we consider four simple, low - level actions for agents in vln - ce - move forward 0. 25m, turn - left or turn - right 15 degrees, or stop to declare that the goal position has been reached.', 'these actions can easily be implemented on robotic agents with standard motion controllers.', 'in contrast, actions to move between panoramas in  #TAUTHOR_TAG traverse 2. 25m on average and can include avoiding obstacles']",4
['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3'],"['( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of']","['##2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3']","['consider a continuous setting for the vision - and - language navigation task which we refer to as vision - and - language navigation in continuous environments ( vln - ce ).', 'given a natural language navigation instruction, an agent must navigate from a start position to the described goal in a continuous 3d environment by executing a sequence of low - level actions based on egocentric perception alone.', 'in overview, we develop this setting by transferring nav - graph - based room - to - room ( r2r )  #TAUTHOR_TAG trajectories to reconstructed continuous matterport3d environments in the habitat simulator [ 19 ].', 'we discuss the task specification and the details of this transfer process in this section.', 'continuous matterport3d environments in habitat.', 'we set our problem in the matterport3d ( mp3d ) [ 5 ] dataset, a collection of 90 environments captured through over 10, 800 high - definition rgb - d panoramas.', 'in addition to the panoramic images, mp3d also provides corresponding mesh - based 3d environment reconstructions.', 'to enable agent interaction with these meshes, we develop the vln - ce task on top of the habitat simulator [ 19 ], a high - throughput simulator that supports basic movement and collision checking for 3d environments including mp3d.', 'in contrast to the simulator used in vln  #TAUTHOR_TAG, habitat allows agents to navigate freely in the continuous environments.', 'observations and actions.', 'we select observation and action spaces to emulate a ground - based, zero - turning radius robot with a single, forward - mounted rgbd camera, similar to a locobot [ 1 ].', 'agents perceive the world through egocentric rgbd images from the simulator with a resolution of 256×256 and a horizontal field - of - view of 90 degrees.', 'note that this is similar to the egocentric rgb perception in the original vln task  #TAUTHOR_TAG but differs from the panoramic observation space adopted by nearly all follow - up work [ 9, 17, 26, 29 ].', 'while the simulator is quite flexible in terms of agent actions, we consider four simple, low - level actions for agents in vln - ce - move forward 0. 25m, turn - left or turn - right 15 degrees, or stop to declare that the goal position has been reached.', 'these actions can easily be implemented on robotic agents with standard motion controllers.', 'in contrast, actions to move between panoramas in  #TAUTHOR_TAG traverse 2. 25m on average and can include avoiding obstacles']",3
['early  #TAUTHOR_TAG and more recent [ 29 ] work in the nav - graph based vln task'],"['early  #TAUTHOR_TAG and more recent [ 29 ] work in the nav - graph based vln task.', 'exploring these gives']","['early  #TAUTHOR_TAG and more recent [ 29 ] work in the nav - graph based vln task.', 'exploring these gives insight into the difficulty of this setting in isolation and by comparison relative']","['develop two models for vln - ce.', 'a simple sequence - to - sequence baseline and a more powerful cross - modal attentional model.', 'while there are many differences in the details, these models are conceptually similar to early  #TAUTHOR_TAG and more recent [ 29 ] work in the nav - graph based vln task.', 'exploring these gives insight into the difficulty of this setting in isolation and by comparison relative to vln.', 'further, these models allow us to test whether improvements from early to later architectures carry over to a more realistic setting.', 'both of our models make use of the same observation and instruction encodings described below.', 'instruction representation.', 'we convert tokenized instructions to corresponding glove [ 23 ] embeddings which are processed by recurrent encoders for each model.', '']",3
"['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity']","['from modeling details, much of the remaining progress in vln has come from adjusting the training regime - adding auxiliary losses / rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity by incorporating synthetically generated data augmentation [ 9, 26 ].', 'we explore some of these directions for vln - ce, but note that this is not an exhaustive accounting of impactful techniques.', 'particularly, we suspect that methods addressing exposure bias and data sparsity in vln will help in the vln - ce setting where these problems may be amplified by lengthy action sequences.', 'we report ablations with and without these techniques in sec. 5.', 'imitation learning.', 'a natural starting point for training is simply to maximize the likelihood of the ground truth trajectories.', 'to do so, we perform teacherforcing training with inflection weighting.', 'as described in [ 30 ], inflection weighting places emphasis on time - steps where actions change ( i. e. a t−1 = a t ), adjusting loss weight proportionally to the rarity of such events.', 'this was found to be helpful for problems like navigation with long sequences of repeated actions ( e. g. going forward down a hall ).', 'we observe a similar effect in early experiments and apply inflection weighting in all our experiments.', 'coping with exposure bias.', '']",3
"['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or']","['rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity']","['from modeling details, much of the remaining progress in vln has come from adjusting the training regime - adding auxiliary losses / rewards [ 17, 29 ], mitigating exposure bias during training  #TAUTHOR_TAG 29 ], or reducing data sparsity by incorporating synthetically generated data augmentation [ 9, 26 ].', 'we explore some of these directions for vln - ce, but note that this is not an exhaustive accounting of impactful techniques.', 'particularly, we suspect that methods addressing exposure bias and data sparsity in vln will help in the vln - ce setting where these problems may be amplified by lengthy action sequences.', 'we report ablations with and without these techniques in sec. 5.', 'imitation learning.', 'a natural starting point for training is simply to maximize the likelihood of the ground truth trajectories.', 'to do so, we perform teacherforcing training with inflection weighting.', 'as described in [ 30 ], inflection weighting places emphasis on time - steps where actions change ( i. e. a t−1 = a t ), adjusting loss weight proportionally to the rarity of such events.', 'this was found to be helpful for problems like navigation with long sequences of repeated actions ( e. g. going forward down a hall ).', 'we observe a similar effect in early experiments and apply inflection weighting in all our experiments.', 'coping with exposure bias.', '']",1
"['rate  #TAUTHOR_TAG.', 'though not directly comparable, this gap']","['rate  #TAUTHOR_TAG.', 'though not directly comparable, this gap']","['##n yields a 16. 3 % success rate  #TAUTHOR_TAG.', 'though not directly comparable, this gap']","['vision instr.', 'history agent selects actions according to the train set action distribution ( 68 % forward, 15 % turn - left, 15 % turn - right, and 2 % stop ).', 'the hand - crafted agent picks a random heading and takes 37 forward actions ( average trajectory length ) before calling stop.', 'despite having no learned components nor processing any input, both these agents achieve approximately 3 % success rates in val - unseen.', 'in contrast, a similar hand - crafted random - heading - and - forward model in vln yields a 16. 3 % success rate  #TAUTHOR_TAG.', 'though not directly comparable, this gap illustrates the strong structural prior provided by the nav - graph in vln.', 'seq2seq and single - modality ablations.', 'tab. 2 also shows performance for the baseline seq2seq model along with input ablations.', 'all models are trained with imitation learning without data augmentation or any auxiliary losses.', 'our baseline seq2seq model significantly outperforms the random and hand - crafted baselines, successfully reaching the goal in 20 % of val - unseen episodes.', 'as illustrated in [ 27 ], models examining only single modalities can be very strong baselines in embodied tasks.', 'we train models without access to the instruction ( no instruction ) and with ablated visual input ( no vision / depth / image ).', 'all of these ablations under - perform the seq2seq baseline.', 'we find that depth is a very strong signal for learning, with models lacking it ( no depth and no vision ) failing to outperform chance ( ≤1 % success rates ).', 'we believe that depth enable agents to quickly begin traversing environments effectively ( e. g. without collisions ) and without this it is very difficult to bootstrap to instruction following.', 'with a success rate of 17 %, the no instruction model performs similarly to a hand - crafted agent in vln, suggesting shared trajectory regularities between vln and vln - ce.', 'while these regularities can be manually exploited in vln via the nav - graph, they are implicit in vln - ce as evidenced by the significantly lower performance of our random and hand crafted agents which collide with and get stuck on obstacles.', 'the no image model also achieves 17 % success, similarly failing to reason about instructions.', 'this hints at the importance of grounding visual referents ( through rgb ) for navigation']",7
"['19, 20, 21,  #TAUTHOR_TAG 5 ]']","['19, 20, 21,  #TAUTHOR_TAG 5 ]']","['has explored learnable alternatives to speech features that rely on a similar computation to spectral representations', '[ 19, 20, 21,  #TAUTHOR_TAG 5 ]']",[' #TAUTHOR_TAG'],0
"['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural network layers that take the raw waveform as input.', 'they can be initialized to replicate mel - filterbanks, and then learnt for the task at hand.', 'the standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters.', 'more formally, the n th melfilterbank of a signal in t is : where', 'is the waveform windowed with an hanning function φ centered in t, ( ψ n ) n = 1... n the n melfilters andf denotes the fourier transform of f.', '[ 26 ] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform :', 'where ( [UNK] n ) n = 1... n are gabor wavelets defined in  #TAUTHOR_TAG shows that this computation can be implemented as neural network layers, referred as timedomain filterbanks ( td - filterbanks ).', 'the waveform goes through a complex - valued convolution, a modulus operator and the a convolution with a lowpass - filter ( the squared hanning window ) that performs the decimation.', 'when not combined with pcen, a log - compression is added on top of td - filterbanks after adding 1 to their absolute value to avoid numerical issues.', 'table 1 shows the detailed layers.', 'following  #TAUTHOR_TAG, the first 1d convolution filters are initialized with gabor wavelets, to replicate mel - filterbanks, and are then learnt at the same time as the rest of the model.', 'the second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering']",0
"['19, 20, 21,  #TAUTHOR_TAG 5 ]']","['19, 20, 21,  #TAUTHOR_TAG 5 ]']","['has explored learnable alternatives to speech features that rely on a similar computation to spectral representations', '[ 19, 20, 21,  #TAUTHOR_TAG 5 ]']",[' #TAUTHOR_TAG'],5
"['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural network layers that take the raw waveform as input.', 'they can be initialized to replicate mel - filterbanks, and then learnt for the task at hand.', 'the standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters.', 'more formally, the n th melfilterbank of a signal in t is : where', 'is the waveform windowed with an hanning function φ centered in t, ( ψ n ) n = 1... n the n melfilters andf denotes the fourier transform of f.', '[ 26 ] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform :', 'where ( [UNK] n ) n = 1... n are gabor wavelets defined in  #TAUTHOR_TAG shows that this computation can be implemented as neural network layers, referred as timedomain filterbanks ( td - filterbanks ).', 'the waveform goes through a complex - valued convolution, a modulus operator and the a convolution with a lowpass - filter ( the squared hanning window ) that performs the decimation.', 'when not combined with pcen, a log - compression is added on top of td - filterbanks after adding 1 to their absolute value to avoid numerical issues.', 'table 1 shows the detailed layers.', 'following  #TAUTHOR_TAG, the first 1d convolution filters are initialized with gabor wavelets, to replicate mel - filterbanks, and are then learnt at the same time as the rest of the model.', 'the second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering']",5
"['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural network layers that take the raw waveform as input.', 'they can be initialized to replicate mel - filterbanks, and then learnt for the task at hand.', 'the standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters.', 'more formally, the n th melfilterbank of a signal in t is : where', 'is the waveform windowed with an hanning function φ centered in t, ( ψ n ) n = 1... n the n melfilters andf denotes the fourier transform of f.', '[ 26 ] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform :', 'where ( [UNK] n ) n = 1... n are gabor wavelets defined in  #TAUTHOR_TAG shows that this computation can be implemented as neural network layers, referred as timedomain filterbanks ( td - filterbanks ).', 'the waveform goes through a complex - valued convolution, a modulus operator and the a convolution with a lowpass - filter ( the squared hanning window ) that performs the decimation.', 'when not combined with pcen, a log - compression is added on top of td - filterbanks after adding 1 to their absolute value to avoid numerical issues.', 'table 1 shows the detailed layers.', 'following  #TAUTHOR_TAG, the first 1d convolution filters are initialized with gabor wavelets, to replicate mel - filterbanks, and are then learnt at the same time as the rest of the model.', 'the second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering']",5
"['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time -']","['the first step of our computational pipeline, we use timedomain filterbanks from  #TAUTHOR_TAG.', 'time - domain filterbanks are neural network layers that take the raw waveform as input.', 'they can be initialized to replicate mel - filterbanks, and then learnt for the task at hand.', 'the standard computation of melfilterbanks relies on passing a spectrogram through a bank of frequency domain filters.', 'more formally, the n th melfilterbank of a signal in t is : where', 'is the waveform windowed with an hanning function φ centered in t, ( ψ n ) n = 1... n the n melfilters andf denotes the fourier transform of f.', '[ 26 ] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform :', 'where ( [UNK] n ) n = 1... n are gabor wavelets defined in  #TAUTHOR_TAG shows that this computation can be implemented as neural network layers, referred as timedomain filterbanks ( td - filterbanks ).', 'the waveform goes through a complex - valued convolution, a modulus operator and the a convolution with a lowpass - filter ( the squared hanning window ) that performs the decimation.', 'when not combined with pcen, a log - compression is added on top of td - filterbanks after adding 1 to their absolute value to avoid numerical issues.', 'table 1 shows the detailed layers.', 'following  #TAUTHOR_TAG, the first 1d convolution filters are initialized with gabor wavelets, to replicate mel - filterbanks, and are then learnt at the same time as the rest of the model.', 'the second convolution layer is kept fixed as a squared hanning window to perform lowpass filtering']",5
['originally introduced by  #TAUTHOR_TAG'],['originally introduced by  #TAUTHOR_TAG'],['##ia structures have been originally introduced by  #TAUTHOR_TAG'],"['##ia structures have been originally introduced by  #TAUTHOR_TAG as well as for bridging reference resolution  #AUTHOR_TAG.', 'further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval  #AUTHOR_TAG.', 'one major bottleneck however is that currently qualia structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as wordnet  #AUTHOR_TAG or framenet 1 1 http : / / framenet. icsi. berkeley. edu / as source of lexical / world knowledge.', 'the work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the web.', '']",0
"['purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristot']","[""purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristotle's basic factors""]","['purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristot']","[""to aristotle, there are four basic factors or causes by which the nature of an object can be described ( cf.  #AUTHOR_TAG ) : the material cause, i. e. the material an object is made of the agentive cause, i. e. the source of movement, creation or change the formal cause, i. e. its form or type the final cause, i. e. its purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristotle's basic factors for the description of the meaning of lexical elements."", 'in fact he introduced so called qualia structures by which the meaning of a lexical element is described in terms of four roles :', 'constitutive : describing physical properties of an object, i. e. its weight, material as well as parts and components', 'agentive : describing factors involved in the bringing about of an object, i. e. its creator or the causal chain leading to its creation formal : describing that properties which distinguish an object in a larger domain, i. e. orientation, magnitude, shape and dimensionality telic : describing the purpose or function of an object most of the qualia structures used in  #TAUTHOR_TAG however seem to have a more restricted interpretation.', '']",0
"['purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristot']","[""purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristotle's basic factors""]","['purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristot']","[""to aristotle, there are four basic factors or causes by which the nature of an object can be described ( cf.  #AUTHOR_TAG ) : the material cause, i. e. the material an object is made of the agentive cause, i. e. the source of movement, creation or change the formal cause, i. e. its form or type the final cause, i. e. its purpose, intention or aim in his generative lexicon ( gl ) framework  #TAUTHOR_TAG reused aristotle's basic factors for the description of the meaning of lexical elements."", 'in fact he introduced so called qualia structures by which the meaning of a lexical element is described in terms of four roles :', 'constitutive : describing physical properties of an object, i. e. its weight, material as well as parts and components', 'agentive : describing factors involved in the bringing about of an object, i. e. its creator or the causal chain leading to its creation formal : describing that properties which distinguish an object in a larger domain, i. e. orientation, magnitude, shape and dimensionality telic : describing the purpose or function of an object most of the qualia structures used in  #TAUTHOR_TAG however seem to have a more restricted interpretation.', '']",0
"['.', ' #AUTHOR_TAG or  #TAUTHOR_TAG, as well as computer, an abstract noun, i. e. conversation, as well as two very specific multi - term words, i. e. natural language processing and data mining.', 'we give the automatically']","['filler of the agentive qualia role.', ' #AUTHOR_TAG or  #TAUTHOR_TAG, as well as computer, an abstract noun, i. e. conversation, as well as two very specific multi - term words, i. e. natural language processing and data mining.', 'we give the automatically']","['of the agentive qualia role.', ' #AUTHOR_TAG or  #TAUTHOR_TAG, as well as computer, an abstract noun, i. e. conversation, as well as two very specific multi - term words, i. e. natural language processing and data mining.', 'we give the automatically']","['mentioned in  #AUTHOR_TAG, it is not always as straightforward to find lexico - syntactic patterns reliably conveying a certain relation.', 'in fact, we did not find any patterns reliably identifying qualia elements for the agentive role.', 'certainly, it would have been possible to find the source of the creation by using patterns such as x is made by y or x is produced by y. however, we found that these patterns do not reliably convey a verb describing how an object is brought into existence.', ""the fact that it is far from straightforward to find patterns indicating an agentive role is further corroborated by the research in  #AUTHOR_TAG, in which only one pattern indicating a qualia relation is used, namely'nn be v [ + en ]'in order to match passive constructions such as the book was written."", 'on the other hand it is clear that constructing a reliable clue for this pattern is not straightforward given the current state - of - the - art concerning search engine queries.', 'nevertheless, in order to also get results for the agentive role, we apply a different method here.', 'instead of issuing a query which is used to search for possible candidates for the role, we take advantage of the fact that the verbs which describe how something comes into being, particularly artificial things, are often quite general phrases like "" make, produce, write, build... "".', 'so instead of generating clues as above, we calculate the value', 'for the nominal we want to acquire a qualia structure for as well as the following verbs : build, produce, make, write, plant, elect, create, cook, construct and design.', 'if this value is over a threshold ( 0. 0005 in our case ), we assume that it is a valid filler of the agentive qualia role.', ' #AUTHOR_TAG or  #TAUTHOR_TAG, as well as computer, an abstract noun, i. e. conversation, as well as two very specific multi - term words, i. e. natural language processing and data mining.', 'we give the automatically learned weighted qualia structures for these entries in figures 3, 4, 5 and 6.', 'the evaluation of our approach consists on the one hand of a discussion of the weighted qualia structures, in particular comparing them to the ideal structures form the literature.', ""on the other hand, we also asked a student at our institute to assign credits to each of the qualia elements from 0 ( incorrect ) to 3 ( totally correct ) whereby 1 credit meaning'not totally wrong'and 2 meaning'still acceptable '""]",0
"['. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']","['we are not able to detect and separate multiple meanings of words, i. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']","['to mention that by this approach we are not able to detect and separate multiple meanings of words, i. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']","['', 'the patterns in our pattern library are actually tuples', 'where is a regular expression defined over part - of - speech tags and is a function returning the plural form of x. we implemented this function as a lookup in a lexicon in which plural nouns are mapped to their base form.', 'with the use of such clues, we thus download a number of google - abstracts in which a corresponding pattern will probably be matched thus restricting the linguistic analysis to a few promising pages.', 'the downloaded abstracts are then part - of - speech tagged using qtag  #AUTHOR_TAG', 'the result is then a weighted qualia structure ( wqs ) in which for each role the qualia elements are weighted according to this jaccard coefficient.', 'in what follows we describe in detail the procedure for acquiring qualia elements for each qualia role.', 'in particular, we describe in detail the clues and lexico - syntactic patterns used.', 'in general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy.', 'in general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']",4
"[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['##th )', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing at the 3rd position', 'is certainly too general. furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly', 'reasonable results. very interesting are the results concoction and libation for the formal role of beer, which unfortunately were rated low', 'by our evaluator ( compare figure 3 ). overall, the discussion has', 'shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. in general, our method produces in', '']",4
"['. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']","['we are not able to detect and separate multiple meanings of words, i. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']","['to mention that by this approach we are not able to detect and separate multiple meanings of words, i. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']","['', 'the patterns in our pattern library are actually tuples', 'where is a regular expression defined over part - of - speech tags and is a function returning the plural form of x. we implemented this function as a lookup in a lexicon in which plural nouns are mapped to their base form.', 'with the use of such clues, we thus download a number of google - abstracts in which a corresponding pattern will probably be matched thus restricting the linguistic analysis to a few promising pages.', 'the downloaded abstracts are then part - of - speech tagged using qtag  #AUTHOR_TAG', 'the result is then a weighted qualia structure ( wqs ) in which for each role the qualia elements are weighted according to this jaccard coefficient.', 'in what follows we describe in detail the procedure for acquiring qualia elements for each qualia role.', 'in particular, we describe in detail the clues and lexico - syntactic patterns used.', 'in general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy.', 'in general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i. e. to handle polysemy, which is appropriately accounted for in the framework of the generative lexicon  #TAUTHOR_TAG']",6
"[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['##th )', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing at the 3rd position', 'is certainly too general. furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly', 'reasonable results. very interesting are the results concoction and libation for the formal role of beer, which unfortunately were rated low', 'by our evaluator ( compare figure 3 ). overall, the discussion has', 'shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. in general, our method produces in', '']",6
"[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['##th )', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing at the 3rd position', 'is certainly too general. furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly', 'reasonable results. very interesting are the results concoction and libation for the formal role of beer, which unfortunately were rated low', 'by our evaluator ( compare figure 3 ). overall, the discussion has', 'shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. in general, our method produces in', '']",3
"[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['##th )', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing at the 3rd position', 'is certainly too general. furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly', 'reasonable results. very interesting are the results concoction and libation for the formal role of beer, which unfortunately were rated low', 'by our evaluator ( compare figure 3 ). overall, the discussion has', 'shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. in general, our method produces in', '']",5
"[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['##th )', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing at the 3rd position', 'is certainly too general. furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly', 'reasonable results. very interesting are the results concoction and libation for the formal role of beer, which unfortunately were rated low', 'by our evaluator ( compare figure 3 ). overall, the discussion has', 'shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. in general, our method produces in', '']",1
"[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['##th )', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","[')', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing']","['', 'are much more specific than liquid as given in  #TAUTHOR_TAG, while thing at the 3rd position', 'is certainly too general. furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly', 'reasonable results. very interesting are the results concoction and libation for the formal role of beer, which unfortunately were rated low', 'by our evaluator ( compare figure 3 ). overall, the discussion has', 'shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. in general, our method produces in', '']",1
['models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank'],"['models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank - order correlation.', 'we']",['- of - the - art attention - based vqa models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank'],"['', 'these human attention maps can be used both for evaluating machine - generated attention maps and for explicitly training attention - based models.', 'contributions.', 'first, we design and test multiple game - inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large - scale vqa dataset  #AUTHOR_TAG ; this vqa - hat ( human attention ) dataset will be released publicly.', 'second, we perform qualitative and quantitative comparison of the maps generated by state - of - the - art attention - based vqa models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank - order correlation.', 'we find that machine - generated attention maps from the most accurate vqa model have a mean rank - correlation of 0. 26 with human attention maps, which is worse than task - independent saliency maps that have a mean rank - correlation of 0. 49.', ""it is well understood that task - independent saliency maps have a'center bias' #AUTHOR_TAG."", 'after we control for this center bias in our human attention maps, we find that the correlation']",0
"['.', 'hierarchical co - attention network  #TAUTHOR_TAG generates multiple levels of']","['image.', 'hierarchical co - attention network  #TAUTHOR_TAG generates multiple levels of']","['', 'hierarchical co - attention network  #TAUTHOR_TAG generates multiple levels of image attention based on words, phrases and complete questions, and is']","['', 'hierarchical co - attention network  #TAUTHOR_TAG generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the vqa challenge 1 as of the time of this submission.', 'another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub - tasks addressed by these modules  #AUTHOR_TAG.', 'note that all these works are unsupervised attention models, where "" attention "" is simply an intermediate variable ( a spatial distribution ) that is produced by the model to optimize downstream loss ( vqa cross - entropy ).', ""the fact that some ( it's unclear how many ) of these spatial distributions end up being interpretable is simply fortuitous."", '']",0
['models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank'],"['models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank - order correlation.', 'we']",['- of - the - art attention - based vqa models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank'],"['', 'these human attention maps can be used both for evaluating machine - generated attention maps and for explicitly training attention - based models.', 'contributions.', 'first, we design and test multiple game - inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large - scale vqa dataset  #AUTHOR_TAG ; this vqa - hat ( human attention ) dataset will be released publicly.', 'second, we perform qualitative and quantitative comparison of the maps generated by state - of - the - art attention - based vqa models  #TAUTHOR_TAG and a task - independent saliency baseline  #AUTHOR_TAG against our human attention maps through visualizations and rank - order correlation.', 'we find that machine - generated attention maps from the most accurate vqa model have a mean rank - correlation of 0. 26 with human attention maps, which is worse than task - independent saliency maps that have a mean rank - correlation of 0. 49.', ""it is well understood that task - independent saliency maps have a'center bias' #AUTHOR_TAG."", 'after we control for this center bias in our human attention maps, we find that the correlation']",1
[')  #TAUTHOR_TAG with'],[' #TAUTHOR_TAG with'],[')  #TAUTHOR_TAG with'],[' #TAUTHOR_TAG'],1
,,,,0
,,,,0
,,,,0
,,,,0
,,,,0
['##r  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach'],"['relation.', 'multir  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach']",['##r  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach'],"['overcome the noise in distantly - labeled examples,  #AUTHOR_TAG introduced an "" at least one "" heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation.', 'multir  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach to support multi - ple relations expressed by different sentences in a bag.', 'unlike these approaches, diejob improves the quality of training data with a bootstrapping step before feeding the noisy examples into a learner, by using the confident examples from a structured corpus as seeds.', 'the benefit of this step is twofold.', 'first, it distills the distantly - labeled examples by propagating labels from good seed examples, and downweights the noisy ones.', 'second, the propagation will walk to more relation examples in the concept mention set that cannot be distantly labeled with triples from knowledge bases.', 'document structure was previously explored by  #TAUTHOR_TAG, which used the structure to enrich an lp graph by adding coupling edges between mentions in the same section of particular documents.', 'in this work, we explore the semantic association between section titles and relation arguments.', 'furthermore, we perform a joint bootstrapping on relation and type mentions to collect training examples with better quality.', ""technically, the propagation graphs used are different : diejob's graph has carefully produced mention nodes ( from those four sets ) and their feature nodes, while diebolds's graph has triple nodes ( i. e., subject - np pairs ) and all singleton and coordinate lists of noun phrases of the corpora."", 'accordingly, their propagation seeds are different : diejob uses confident examples as seeds ( labeled from particular sections of a structured cor - pus ) to propagate labels to more examples via feature similarity, while diebolds directly uses freebase triples as seeds and propagates labels through edges built from coordinate lists and sections.', 'in the classic bootstrap learning scheme  #TAUTHOR_TAG, a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances.', 'then in an iterative fashion, new instances are used to extract more patterns.', 'diejob departs from earlier bootstrapping methods in combining label propagation with a standard classification learner, and it can improve the quality of distant examples and collect new examples simultaneously']",0
['##r  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach'],"['relation.', 'multir  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach']",['##r  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach'],"['overcome the noise in distantly - labeled examples,  #AUTHOR_TAG introduced an "" at least one "" heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation.', 'multir  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach to support multi - ple relations expressed by different sentences in a bag.', 'unlike these approaches, diejob improves the quality of training data with a bootstrapping step before feeding the noisy examples into a learner, by using the confident examples from a structured corpus as seeds.', 'the benefit of this step is twofold.', 'first, it distills the distantly - labeled examples by propagating labels from good seed examples, and downweights the noisy ones.', 'second, the propagation will walk to more relation examples in the concept mention set that cannot be distantly labeled with triples from knowledge bases.', 'document structure was previously explored by  #TAUTHOR_TAG, which used the structure to enrich an lp graph by adding coupling edges between mentions in the same section of particular documents.', 'in this work, we explore the semantic association between section titles and relation arguments.', 'furthermore, we perform a joint bootstrapping on relation and type mentions to collect training examples with better quality.', ""technically, the propagation graphs used are different : diejob's graph has carefully produced mention nodes ( from those four sets ) and their feature nodes, while diebolds's graph has triple nodes ( i. e., subject - np pairs ) and all singleton and coordinate lists of noun phrases of the corpora."", 'accordingly, their propagation seeds are different : diejob uses confident examples as seeds ( labeled from particular sections of a structured cor - pus ) to propagate labels to more examples via feature similarity, while diebolds directly uses freebase triples as seeds and propagates labels through edges built from coordinate lists and sections.', 'in the classic bootstrap learning scheme  #TAUTHOR_TAG, a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances.', 'then in an iterative fashion, new instances are used to extract more patterns.', 'diejob departs from earlier bootstrapping methods in combining label propagation with a standard classification learner, and it can improve the quality of distant examples and collect new examples simultaneously']",0
['##r  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach'],"['relation.', 'multir  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach']",['##r  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach'],"['overcome the noise in distantly - labeled examples,  #AUTHOR_TAG introduced an "" at least one "" heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation.', 'multir  #TAUTHOR_TAG miml - re  #AUTHOR_TAG extend this approach to support multi - ple relations expressed by different sentences in a bag.', 'unlike these approaches, diejob improves the quality of training data with a bootstrapping step before feeding the noisy examples into a learner, by using the confident examples from a structured corpus as seeds.', 'the benefit of this step is twofold.', 'first, it distills the distantly - labeled examples by propagating labels from good seed examples, and downweights the noisy ones.', 'second, the propagation will walk to more relation examples in the concept mention set that cannot be distantly labeled with triples from knowledge bases.', 'document structure was previously explored by  #TAUTHOR_TAG, which used the structure to enrich an lp graph by adding coupling edges between mentions in the same section of particular documents.', 'in this work, we explore the semantic association between section titles and relation arguments.', 'furthermore, we perform a joint bootstrapping on relation and type mentions to collect training examples with better quality.', ""technically, the propagation graphs used are different : diejob's graph has carefully produced mention nodes ( from those four sets ) and their feature nodes, while diebolds's graph has triple nodes ( i. e., subject - np pairs ) and all singleton and coordinate lists of noun phrases of the corpora."", 'accordingly, their propagation seeds are different : diejob uses confident examples as seeds ( labeled from particular sections of a structured cor - pus ) to propagate labels to more examples via feature similarity, while diebolds directly uses freebase triples as seeds and propagates labels through edges built from coordinate lists and sections.', 'in the classic bootstrap learning scheme  #TAUTHOR_TAG, a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances.', 'then in an iterative fashion, new instances are used to extract more patterns.', 'diejob departs from earlier bootstrapping methods in combining label propagation with a standard classification learner, and it can improve the quality of distant examples and collect new examples simultaneously']",0
"[')  #TAUTHOR_TAG, which is a graph - based ssl method related to personalized pagerank ( ppr )  #AUTHOR_TAG ( aka random walk with restart  #AUTHOR_TAG ).', 'mr']","['right - hand side.', 'we use an existing multi - class label propagation method, namely, multirankwalk ( mrw )  #TAUTHOR_TAG, which is a graph - based ssl method related to personalized pagerank ( ppr )  #AUTHOR_TAG ( aka random walk with restart  #AUTHOR_TAG ).', 'mrw can be viewed simply as computing a personalized pagerank vector for']","[')  #TAUTHOR_TAG, which is a graph - based ssl method related to personalized pagerank ( ppr )  #AUTHOR_TAG ( aka random walk with restart  #AUTHOR_TAG ).', 'mr']","['the relation mentions and the concept mentions lying in the range of the corresponding relation, we are able to distill a cleaner set of training relation examples to learn extractors.', 'r s contains more confident relation examples because of constraints by document structure, but it is limited in size.', 'in contrast, the number of r t mentions is larger, but they are noisier.', 'in general, the degree to which r t mentions will be useful may be domain - and corpus - specific.', 'c s and c t are generated with respect to the type of the men - tions, but not their relationship with the title entity : e. g., a mention in c t corresponding to the np "" dizziness "" would not be associated with the triple sideeffect ( meloxican, dizziness ) ; and indeed, dizziness might be a condition treated by, not caused by, the title entity "" meloxican "".', 'therefore, c t itself cannot be directly used as relation examples, however, it can serve as a resource to distill relation examples.', 'in our experiments, r s mentions are always used as seed relation examples in lp, but we build bipartite propagation graphs with different combinations of the four sets of mentions and study their performance.', 'in total, we have 7 bipartite graphs, each with a different set of mentions from the following combinations :', 'in a bipartite graph, one set of nodes are mentions, and the other set of nodes are features of mentions.', 'an edge is added between each feature and each mention containing that feature.', 'the edges are tfidf - weighted ( treating the features as words and the mentions as documents ).', 'figure 3 shows such a bipartite graph ( edge weights are omitted ), which has four mentions on the left - hand side, and eight features on the right - hand side.', 'we use an existing multi - class label propagation method, namely, multirankwalk ( mrw )  #TAUTHOR_TAG, which is a graph - based ssl method related to personalized pagerank ( ppr )  #AUTHOR_TAG ( aka random walk with restart  #AUTHOR_TAG ).', 'mrw can be viewed simply as computing a personalized pagerank vector for each class, each of which is computed using a personalization vector that is initially uniform over the seeds, and finally assigning to each node the class associ - ated with its highest - scoring vector.', ""mrw's final scores depend on the centrality of nodes, as well as their proximity to seeds."", 'the mrw']",5
"['', 'we use svms ( chang and  #TAUTHOR_TAG and discard singleton features,']","['of the list.', 'we use svms ( chang and  #TAUTHOR_TAG and discard singleton features,']","['of the list.', 'we use svms ( chang and  #TAUTHOR_TAG and discard singleton features,']","['', 'for lists, the dependency features are computed relative to the head of the list.', 'we use svms ( chang and  #TAUTHOR_TAG and discard singleton features, as well as the most frequent 5 % of features ( as a stop - wording variant ).', 'specifically, binary classifiers are trained with examples of one relation as the positives, and examples of the other classes as negatives.', 'we also add n general negative examples, randomly picked from those that are not distantly labeled by any relation.', 'a linear kernel and default values for all other parameters are used 2.', 'a threshold 0. 5 is used to cut positive and negative predictions.', 'if a new list or mention is not classified as positive by any classifier, it is predicted as "" other ""']",5
['in  #TAUTHOR_TAG'],['in  #TAUTHOR_TAG'],"['in  #TAUTHOR_TAG.', 'the annotated text fragments are manually']","['evaluation dataset contains 20 manually labeled pages, 10 pages each from the disease corpus wikidisease and the drug corpus dailymed.', 'this data was originally generated in  #TAUTHOR_TAG.', '']",5
[' #TAUTHOR_TAG'],"['latent variable learners.', 'the first is multir  #TAUTHOR_TAG']","['with the same method.', 'we also compare against two latent variable learners.', 'the first is multir  #TAUTHOR_TAG']","['', 'we also compare against two latent variable learners.', 'the first is multir  #TAUTHOR_TAG which models each relation mention separately and aggregates their labels using a deterministic or.', '']",5
[' #TAUTHOR_TAG'],"['latent variable learners.', 'the first is multir  #TAUTHOR_TAG']","['with the same method.', 'we also compare against two latent variable learners.', 'the first is multir  #TAUTHOR_TAG']","['', 'we also compare against two latent variable learners.', 'the first is multir  #TAUTHOR_TAG which models each relation mention separately and aggregates their labels using a deterministic or.', '']",5
"['1.', 'the results for diebolds are from  #TAUTHOR_TAG.', 'the systems with "" * "" are directly tuned on the evaluation']","['1.', 'the results for diebolds are from  #TAUTHOR_TAG.', 'the systems with "" * "" are directly tuned on the evaluation']","['##1 measure are given in table 1.', 'the results for diebolds are from  #TAUTHOR_TAG.', 'the systems with "" * "" are directly tuned on the evaluation data and should be considered as upper bounds on true per - ( note that for']","['', 'the results for diebolds are from  #TAUTHOR_TAG.', 'the systems with "" * "" are directly tuned on the evaluation data and should be considered as upper bounds on true per - ( note that for the disease domain, diejob both and diejob both * get the same results, because they use the same parameters, although they are tuned with different data. )', '']",5
"['', '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to']","['( e', '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to']","['', '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to the wider machine learning community']","['a ), or increased performance, e. g., using word association values instead of raw', 'co - occurrence counts  #AUTHOR_TAG. word embedding algorithms commonly downsample contexts to lessen the impact', ""of highfrequency words ( termed'subsampling'in  #AUTHOR_TAG ) or increase the relative importance of words closer to the center of a context window ( called'dynamic context window '"", 'in  #AUTHOR_TAG ). the effect of using such down - sampling strategies on accuracy in word similarity and analogy tasks was explored in several papers ( e. g.,  #AUTHOR_TAG ). however, down - sampling and details of its implementation also have major effects on the stability of word', ""embeddings ( also known as'reliability'), i. e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. this problem has lately raised severe concerns in the word embedding community ( e"", '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to the wider machine learning community due to the influence of probabilistic - and', 'thus unstablemethods on experimental results  #AUTHOR_TAG, as well as replicability and reproducibility  #AUTHOR_TAG pp. 63 : 3 - 4 ). stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e. g., tracking lexical change  #AUTHOR_TAG. unstable', 'word embeddings can lead to serious problems in such applications, as interpretations will depend on the luck of the draw. this might also affect high - stake fields like medical informatics where patients could be harmed as a consequence of misleading results  #AUTHOR_TAG. in the light of these concerns, we here evaluate down - sampling strategies by modifying', 'the svd ppmi ( singular value', 'decomposition of a positive pointwise mutual information matrix ;  #AUTHOR_TAG ) algorithm and comparing its results with those of two other embedding algorithms, namely, glove', ' #AUTHOR_TAG and sgns  #AUTHOR_TAG a, c ). our analysis is based on three corpora of different sizes and investigates effects on', 'both accuracy and stability. the inclusion of accuracy measurements and the larger size of our training corpora exceed prior work. we show how', 'the choice of down - sampling strategies, a seemingly minor detail, leads to major differences in the characterization of svd ppmi in recent studies  #TAUTHOR_TAG. we also present svd wppmi, a simple modification of svd ppmi that replaces probabilistic down - sampling with weighting. what, at first sight, appears to', 'be a small change leads, nevertheless, to an unrivaled combination of stability and accuracy', ', making it particularly well - suited for the above - mentioned corpus linguistic applications']",1
"['', '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to']","['( e', '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to']","['', '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to the wider machine learning community']","['a ), or increased performance, e. g., using word association values instead of raw', 'co - occurrence counts  #AUTHOR_TAG. word embedding algorithms commonly downsample contexts to lessen the impact', ""of highfrequency words ( termed'subsampling'in  #AUTHOR_TAG ) or increase the relative importance of words closer to the center of a context window ( called'dynamic context window '"", 'in  #AUTHOR_TAG ). the effect of using such down - sampling strategies on accuracy in word similarity and analogy tasks was explored in several papers ( e. g.,  #AUTHOR_TAG ). however, down - sampling and details of its implementation also have major effects on the stability of word', ""embeddings ( also known as'reliability'), i. e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. this problem has lately raised severe concerns in the word embedding community ( e"", '. g.,  #AUTHOR_TAG b ) ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ) and is also of interest to the wider machine learning community due to the influence of probabilistic - and', 'thus unstablemethods on experimental results  #AUTHOR_TAG, as well as replicability and reproducibility  #AUTHOR_TAG pp. 63 : 3 - 4 ). stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e. g., tracking lexical change  #AUTHOR_TAG. unstable', 'word embeddings can lead to serious problems in such applications, as interpretations will depend on the luck of the draw. this might also affect high - stake fields like medical informatics where patients could be harmed as a consequence of misleading results  #AUTHOR_TAG. in the light of these concerns, we here evaluate down - sampling strategies by modifying', 'the svd ppmi ( singular value', 'decomposition of a positive pointwise mutual information matrix ;  #AUTHOR_TAG ) algorithm and comparing its results with those of two other embedding algorithms, namely, glove', ' #AUTHOR_TAG and sgns  #AUTHOR_TAG a, c ). our analysis is based on three corpora of different sizes and investigates effects on', 'both accuracy and stability. the inclusion of accuracy measurements and the larger size of our training corpora exceed prior work. we show how', 'the choice of down - sampling strategies, a seemingly minor detail, leads to major differences in the characterization of svd ppmi in recent studies  #TAUTHOR_TAG. we also present svd wppmi, a simple modification of svd ppmi that replaces probabilistic down - sampling with weighting. what, at first sight, appears to', 'be a small change leads, nevertheless, to an unrivaled combination of stability and accuracy', ', making it particularly well - suited for the above - mentioned corpus linguistic applications']",1
"[', b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former']","['algorithm on one corpus  #AUTHOR_TAG a, b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former approach,']","[', b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former approach,']","['word embedding stability can be linked to older research comparing distributional thesauri  #AUTHOR_TAG by the most similar words they contain for particular anchor words  #AUTHOR_TAG padro et al., 2014 ).', 'most stability experiments focused on repeatedly training the same algorithm on one corpus  #AUTHOR_TAG a, b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former approach, since we deem it more relevant for ensuring that study results can be replicated or reproduced.', 'stability can be quantified by calculating the overlap between sets of words considered most similar in relation to pre - selected anchor words.', 'reasonable metrical choices are, e. g., the jaccard coefficient  #AUTHOR_TAG between these sets  #TAUTHOR_TAG, or a percentage based coefficient  #AUTHOR_TAG a, b ;  #AUTHOR_TAG.', 'we here use j @ n, i. e., the jaccard coefficient for the n most similar words.', ""it depends on a set m of word embedding models, m, for which the n most similar words ( by cosine ) from a set a of anchor words, a, as provided by the'most similar words'function msw ( a, n, m ), are compared""]",0
"[', b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former']","['algorithm on one corpus  #AUTHOR_TAG a, b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former approach,']","[', b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former approach,']","['word embedding stability can be linked to older research comparing distributional thesauri  #AUTHOR_TAG by the most similar words they contain for particular anchor words  #AUTHOR_TAG padro et al., 2014 ).', 'most stability experiments focused on repeatedly training the same algorithm on one corpus  #AUTHOR_TAG a, b, 2017 ;  #TAUTHOR_TAG, whereas  #AUTHOR_TAG quantified stability by comparing word similarity for models trained with different algorithms.', 'we follow the former approach, since we deem it more relevant for ensuring that study results can be replicated or reproduced.', 'stability can be quantified by calculating the overlap between sets of words considered most similar in relation to pre - selected anchor words.', 'reasonable metrical choices are, e. g., the jaccard coefficient  #AUTHOR_TAG between these sets  #TAUTHOR_TAG, or a percentage based coefficient  #AUTHOR_TAG a, b ;  #AUTHOR_TAG.', 'we here use j @ n, i. e., the jaccard coefficient for the n most similar words.', ""it depends on a set m of word embedding models, m, for which the n most similar words ( by cosine ) from a set a of anchor words, a, as provided by the'most similar words'function msw ( a, n, m ), are compared""]",0
"['stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['corpora used in most stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['corpora used in most stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus from  #AUTHOR_TAG each contain about 60m tokens.', ' #AUTHOR_TAG used three corpora of about 100m words each.', 'two exceptions are  #AUTHOR_TAG a, b ) using relatively large google books ngram corpus subsets  #AUTHOR_TAG with 135m to 4. 7g n - grams, as well as  #AUTHOR_TAG who investigated the influence of embedding dimensionality on stability based on three corpora with only 1. 2 - 2. 6m tokens.', '3 we used three different english corpora as training material : the 2000s decade of the corpus of historical american english ( coha ;  #AUTHOR_TAG ), the english news crawl corpus ( news ) collected for the 2018 wmt shared task 4 and a wikipedia corpus ( wiki ).', '5 coha contains 14k texts and 28m tokens, news 27m texts and 550m tokens, and wiki 4. 5m texts and 1. 7g tokens, respectively.', 'coha was selected as it is commonly used in corpus linguistic studies, whereas news and wiki serve to gauge the performance of all algorithms in general applica - tions.', 'the latter two corpora are far larger than common in stability studies, making our study the largest - scale evaluation of embedding stability we are aware of.', 'all three corpora were tokenized, transformed to lower case and cleaned from punctuation.', 'we used both the corpora as - is, as well as independently drawn random subsamples ( see also  #AUTHOR_TAG a ) ;  #TAUTHOR_TAG to simulate the arbitrary content selection in most corpora - texts could be removed or replaced with similar ones without changing the overall nature of a corpus, e. g., wikipedia articles are continuously edited.', 'subsampling allows us to quantify the effect of this arbitrariness on the stability of embeddings, i. e., how consistently word embeddings are trained on variations of a corpus.', 'subsampling was performed on the level of the constituent texts of each corpus, e. g., individual news articles.', 'for a corpus with n texts we drew n samples with replacement.', 'texts could be drawn multiple times, but only one copy was kept, reducing corpora to 1 − 1 / e ≈ 2 / 3 of their original size']",0
"['##7 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', '']","['( corrected p = 0. 027 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', '']","['−6 ) or probabilistic down - sampling ( corrected p = 0. 015 )', ', as well as glove ( corrected p = 0. 027 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', '']","['', 'algorithms ( p = 1. 3 · 10 −7 ). we then used a pairwise wilcoxon rank - sum test with holm - sidak correction ( see demsar ( 2006 ) ) in order to compare other algorithms with svd wp', '##pmi. 13 we found it to be not significantly', 'different in accuracy from sgns ( p = 0. 101 ), but significantly better than svd ppmi without downsampling ( corrected p = 5.', '4 · 10 −6 ) or probabilistic down - sampling ( corrected p = 0. 015 )', ', as well as glove ( corrected p = 0. 027 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', ""thus be explained by their difference in down - sampling options, i. e., no down - sampling or probabilistic down - sampling. glove's high stability"", 'in other studies  #TAUTHOR_TAG seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora']",0
"['##7 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', '']","['( corrected p = 0. 027 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', '']","['−6 ) or probabilistic down - sampling ( corrected p = 0. 015 )', ', as well as glove ( corrected p = 0. 027 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', '']","['', 'algorithms ( p = 1. 3 · 10 −7 ). we then used a pairwise wilcoxon rank - sum test with holm - sidak correction ( see demsar ( 2006 ) ) in order to compare other algorithms with svd wp', '##pmi. 13 we found it to be not significantly', 'different in accuracy from sgns ( p = 0. 101 ), but significantly better than svd ppmi without downsampling ( corrected p = 5.', '4 · 10 −6 ) or probabilistic down - sampling ( corrected p = 0. 015 )', ', as well as glove ( corrected p = 0. 027 ). our results show svd wppmi', 'to be both highly reliable and accurate, especially on coha, which has a size common in both stability studies and corpus linguistic applications. diverging reports on svd ppmi stability - described as perfectly reliable in  #AUTHOR_TAG, yet not in  #TAUTHOR_TAG - can', ""thus be explained by their difference in down - sampling options, i. e., no down - sampling or probabilistic down - sampling. glove's high stability"", 'in other studies  #TAUTHOR_TAG seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora']",0
"['stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['corpora used in most stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['corpora used in most stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus from  #AUTHOR_TAG each contain about 60m tokens.', ' #AUTHOR_TAG used three corpora of about 100m words each.', 'two exceptions are  #AUTHOR_TAG a, b ) using relatively large google books ngram corpus subsets  #AUTHOR_TAG with 135m to 4. 7g n - grams, as well as  #AUTHOR_TAG who investigated the influence of embedding dimensionality on stability based on three corpora with only 1. 2 - 2. 6m tokens.', '3 we used three different english corpora as training material : the 2000s decade of the corpus of historical american english ( coha ;  #AUTHOR_TAG ), the english news crawl corpus ( news ) collected for the 2018 wmt shared task 4 and a wikipedia corpus ( wiki ).', '5 coha contains 14k texts and 28m tokens, news 27m texts and 550m tokens, and wiki 4. 5m texts and 1. 7g tokens, respectively.', 'coha was selected as it is commonly used in corpus linguistic studies, whereas news and wiki serve to gauge the performance of all algorithms in general applica - tions.', 'the latter two corpora are far larger than common in stability studies, making our study the largest - scale evaluation of embedding stability we are aware of.', 'all three corpora were tokenized, transformed to lower case and cleaned from punctuation.', 'we used both the corpora as - is, as well as independently drawn random subsamples ( see also  #AUTHOR_TAG a ) ;  #TAUTHOR_TAG to simulate the arbitrary content selection in most corpora - texts could be removed or replaced with similar ones without changing the overall nature of a corpus, e. g., wikipedia articles are continuously edited.', 'subsampling allows us to quantify the effect of this arbitrariness on the stability of embeddings, i. e., how consistently word embeddings are trained on variations of a corpus.', 'subsampling was performed on the level of the constituent texts of each corpus, e. g., individual news articles.', 'for a corpus with n texts we drew n samples with replacement.', 'texts could be drawn multiple times, but only one copy was kept, reducing corpora to 1 − 1 / e ≈ 2 / 3 of their original size']",3
"['stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['corpora used in most stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus']","['corpora used in most stability studies are relatively small.', 'for instance, the largest corpus in  #TAUTHOR_TAG contains 15m tokens, whereas the corpus used by  #AUTHOR_TAG and the largest corpus from  #AUTHOR_TAG each contain about 60m tokens.', ' #AUTHOR_TAG used three corpora of about 100m words each.', 'two exceptions are  #AUTHOR_TAG a, b ) using relatively large google books ngram corpus subsets  #AUTHOR_TAG with 135m to 4. 7g n - grams, as well as  #AUTHOR_TAG who investigated the influence of embedding dimensionality on stability based on three corpora with only 1. 2 - 2. 6m tokens.', '3 we used three different english corpora as training material : the 2000s decade of the corpus of historical american english ( coha ;  #AUTHOR_TAG ), the english news crawl corpus ( news ) collected for the 2018 wmt shared task 4 and a wikipedia corpus ( wiki ).', '5 coha contains 14k texts and 28m tokens, news 27m texts and 550m tokens, and wiki 4. 5m texts and 1. 7g tokens, respectively.', 'coha was selected as it is commonly used in corpus linguistic studies, whereas news and wiki serve to gauge the performance of all algorithms in general applica - tions.', 'the latter two corpora are far larger than common in stability studies, making our study the largest - scale evaluation of embedding stability we are aware of.', 'all three corpora were tokenized, transformed to lower case and cleaned from punctuation.', 'we used both the corpora as - is, as well as independently drawn random subsamples ( see also  #AUTHOR_TAG a ) ;  #TAUTHOR_TAG to simulate the arbitrary content selection in most corpora - texts could be removed or replaced with similar ones without changing the overall nature of a corpus, e. g., wikipedia articles are continuously edited.', 'subsampling allows us to quantify the effect of this arbitrariness on the stability of embeddings, i. e., how consistently word embeddings are trained on variations of a corpus.', 'subsampling was performed on the level of the constituent texts of each corpus, e. g., individual news articles.', 'for a corpus with n texts we drew n samples with replacement.', 'texts could be drawn multiple times, but only one copy was kept, reducing corpora to 1 − 1 / e ≈ 2 / 3 of their original size']",5
"['weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation']","['weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation']","['glove 7 implementations directly, we had to modify svd ppmi 8 to support the weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation']","['compared five algorithm variants : glove, sgns, svd ppmi without down - sampling, svd ppmi with probabilistic down - sampling, and svd wppmi.', 'while we could use sgns 6 and glove 7 implementations directly, we had to modify svd ppmi 8 to support the weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation to use random numbers generated with a non - fixed seed for probabilistic down - sampling.', 'a fixed seed would benefit reliability, but also act as a bias during all analyses - seed choice has been shown to cause significant differences in experimental results  #AUTHOR_TAG.', 'down - sampling strategies for df and ff can be chosen independently of each other, e. g., using probabilistic down - sampling for df together with weighted down - sampling for ff.', 'however, we decided to use the same down - sampling strategies, e. g., weighting, for both factors, taking into ac - count computational limitations as well as results from pre - tests that revealed little benefit of mixed strategies.', '9 we trained ten models for each algorithm variant and corpus.', '10 in the case of subsampling, each model was trained on one of the independently drawn samples.', 'stability was evaluated by selecting the 1k most frequent words in each non - bootstrap subsampled corpus as anchor words and calculating j @ 10 ( see equation 1 ).', '11  #AUTHOR_TAG a, b ), we did not only investigate stability, but also the accuracy of our models to gauge potential tradeoffs.', 'we measured the spearman rank correlation between cosine - based word similarity judgments and human ones with four psycholinguistic test sets, i. e., the two crowdsourced test sets men  #AUTHOR_TAG and mturk  #AUTHOR_TAG, the especially strict simlex - 999  #AUTHOR_TAG and the widely used wordsim - 353 ( ws - 353 ;  #AUTHOR_TAG ).', 'we also measured the percentage of correctly solved analogies ( using the multiplicative formula from  #AUTHOR_TAG b ) ) with two test sets developed at google  #AUTHOR_TAG a ) and microsoft research ( msr ;  #AUTHOR_TAG b ) ).', 'table 1 shows the accuracy and stability for all tested combinations of algorithm and corpus variants.', 'accuracy differences between test sets are in line with prior observations and general 9 the strongest counterexample is a combination of probabilistic down - sampling for df and weighting for ff which lead to small, yet significant improvements in the men ( 0. 703 ± 0. 001 ) and mturk ( 0. 568 ± 0. 015 ) similarity tasks']",4
"['weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation']","['weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation']","['glove 7 implementations directly, we had to modify svd ppmi 8 to support the weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation']","['compared five algorithm variants : glove, sgns, svd ppmi without down - sampling, svd ppmi with probabilistic down - sampling, and svd wppmi.', 'while we could use sgns 6 and glove 7 implementations directly, we had to modify svd ppmi 8 to support the weighted sampling used in svd wppmi.', 'as proposed by  #TAUTHOR_TAG, we further modified our svd ppmi implementation to use random numbers generated with a non - fixed seed for probabilistic down - sampling.', 'a fixed seed would benefit reliability, but also act as a bias during all analyses - seed choice has been shown to cause significant differences in experimental results  #AUTHOR_TAG.', 'down - sampling strategies for df and ff can be chosen independently of each other, e. g., using probabilistic down - sampling for df together with weighted down - sampling for ff.', 'however, we decided to use the same down - sampling strategies, e. g., weighting, for both factors, taking into ac - count computational limitations as well as results from pre - tests that revealed little benefit of mixed strategies.', '9 we trained ten models for each algorithm variant and corpus.', '10 in the case of subsampling, each model was trained on one of the independently drawn samples.', 'stability was evaluated by selecting the 1k most frequent words in each non - bootstrap subsampled corpus as anchor words and calculating j @ 10 ( see equation 1 ).', '11  #AUTHOR_TAG a, b ), we did not only investigate stability, but also the accuracy of our models to gauge potential tradeoffs.', 'we measured the spearman rank correlation between cosine - based word similarity judgments and human ones with four psycholinguistic test sets, i. e., the two crowdsourced test sets men  #AUTHOR_TAG and mturk  #AUTHOR_TAG, the especially strict simlex - 999  #AUTHOR_TAG and the widely used wordsim - 353 ( ws - 353 ;  #AUTHOR_TAG ).', 'we also measured the percentage of correctly solved analogies ( using the multiplicative formula from  #AUTHOR_TAG b ) ) with two test sets developed at google  #AUTHOR_TAG a ) and microsoft research ( msr ;  #AUTHOR_TAG b ) ).', 'table 1 shows the accuracy and stability for all tested combinations of algorithm and corpus variants.', 'accuracy differences between test sets are in line with prior observations and general 9 the strongest counterexample is a combination of probabilistic down - sampling for df and weighting for ff which lead to small, yet significant improvements in the men ( 0. 703 ± 0. 001 ) and mturk ( 0. 568 ± 0. 015 ) similarity tasks']",6
"['scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare']","['scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare']","['of a set of classes.', 'it is clear that a formal evaluation scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes']","['need', 'although there has been a lot of work done in extracting semantic classes of a given domain, relatively little attention has been paid to the task of evaluating the generated classes.', 'in the absence of an evaluation scheme, the only way to decide if the semantic classes produced by a system are "" reasonable "" or not is by having an expert analyze them by inspection.', 'such informal evaluations make it very difficult to compare one set of classes against another and are also not very reliable estimates of the quality of a set of classes.', 'it is clear that a formal evaluation scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert.', 'their evaluation scheme bases the comparison between two classes on the presence or absence of pairs of words in them.', '']",0
"['scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare']","['scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare']","['of a set of classes.', 'it is clear that a formal evaluation scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes']","['need', 'although there has been a lot of work done in extracting semantic classes of a given domain, relatively little attention has been paid to the task of evaluating the generated classes.', 'in the absence of an evaluation scheme, the only way to decide if the semantic classes produced by a system are "" reasonable "" or not is by having an expert analyze them by inspection.', 'such informal evaluations make it very difficult to compare one set of classes against another and are also not very reliable estimates of the quality of a set of classes.', 'it is clear that a formal evaluation scheme would be of great help.', ' #TAUTHOR_TAG duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert.', 'their evaluation scheme bases the comparison between two classes on the presence or absence of pairs of words in them.', '']",1
"['- measure  #TAUTHOR_TAG.', 'in our computation of the fmeasur']","['f - measure  #TAUTHOR_TAG.', 'in our computation of the fmeasure, we construct a']","['.', 'we have adopted the f - measure  #TAUTHOR_TAG.', 'in our computation of the fmeasure, we construct']","['', 'as mentioned above, we intend to be able to compare a clustering generated by a system against one provided by an expert.', 'since a word can occur in more than one class, it is important to find some kind of mapping between the classes generated by the system and the classes given by the expert.', ""such a mapping tells us which class in the system's clustering maps to which one in the expert's clustering, and an overall comparison of the clusterings is based on the comparison of the mutually mapping classes."", 'before we delve deeper into the evaluation process, we must decide on some measure of "" closeness "" between a pair of classes.', 'we have adopted the f - measure  #TAUTHOR_TAG.', 'in our computation of the fmeasure, we construct a contingency table based on the presence or absence of individual elements in the two classes being compared, as opposed to basing it on pairs of words.', 'for example, suppose that class a is generated by the system and class b is provided by an expert ( as shown in table 1 ).', 'the contingency table obtained for this pair of classes is shown in table 2.', 'the three main steps in the evaluation process are the acquisition of "" correct "" classes from domain experts, mapping the experts\'clustering to that generated by the system, and generating an overall measure that represents the system\'s performance when compared against the expert']",5
['as explained in  #TAUTHOR_TAG'],['as explained in  #TAUTHOR_TAG'],['as explained in  #TAUTHOR_TAG'],"['', 'several such conflicts may exist, and re - mapping may lead to further conflicts.', 'the mapping algorithm iteratively searches for conflicts and resolves them till no more conflicts exist.', 'note also that a system class may map to an expert class only if the f - measure between them exceeds a certain threshold value.', 'this ensures that a certain degree of similarity must exist between two classes for them to map to each other.', 'we have used a threshold value of 0. 20.', 'this value is obtained purely by observations made on the f - measures between different pairs of classes with varying degrees of similarity.', 'table 2 ).', 'once all the mapped classes have been incorporated into this contingency table, add every element of all unmapped classes generated by the system to the yes - no cell and every element of all unmapped classes provided by the expert to the no - yes cell of this table.', 'once all classes in the two clusterings have been accounted for, calculate the precision, recall, and f - measure as explained in  #TAUTHOR_TAG']",5
"['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG.']","['new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image']","['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is']","['of the visual data is way different from the structure of the captions describing them. moreover, the simple cnn + rnn pipeline squash the whole input', 'image into a fixed - length embedding vector. this constitutes a major limitation of the basic encoder - decoder architecture. to overcome these limitations both for machine', 'translation and image captioning, some new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is now represented as a sequence of context vector where the length of the sequence depends on the number of words in the sentence to be generated. promising results has', 'been published since attention was introduced in [ 16 ] then later refined in  #TAUTHOR_TAG. another group of models [ 4 ] [ 5 ] [ 6 ] tried to', 'overcome the limitation of the basic encodedecoder framework by still representing the input image as a fixedlength vector but injecting external guiding information', '']",0
"['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG.']","['new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image']","['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is']","['of the visual data is way different from the structure of the captions describing them. moreover, the simple cnn + rnn pipeline squash the whole input', 'image into a fixed - length embedding vector. this constitutes a major limitation of the basic encoder - decoder architecture. to overcome these limitations both for machine', 'translation and image captioning, some new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is now represented as a sequence of context vector where the length of the sequence depends on the number of words in the sentence to be generated. promising results has', 'been published since attention was introduced in [ 16 ] then later refined in  #TAUTHOR_TAG. another group of models [ 4 ] [ 5 ] [ 6 ] tried to', 'overcome the limitation of the basic encodedecoder framework by still representing the input image as a fixedlength vector but injecting external guiding information', '']",0
"['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG.']","['new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image']","['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is']","['of the visual data is way different from the structure of the captions describing them. moreover, the simple cnn + rnn pipeline squash the whole input', 'image into a fixed - length embedding vector. this constitutes a major limitation of the basic encoder - decoder architecture. to overcome these limitations both for machine', 'translation and image captioning, some new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is now represented as a sequence of context vector where the length of the sequence depends on the number of words in the sentence to be generated. promising results has', 'been published since attention was introduced in [ 16 ] then later refined in  #TAUTHOR_TAG. another group of models [ 4 ] [ 5 ] [ 6 ] tried to', 'overcome the limitation of the basic encodedecoder framework by still representing the input image as a fixedlength vector but injecting external guiding information', '']",1
"['use of attention in sequence - to - sequence learning for machine translation [ 16,  #TAUTHOR_TAG, visual attention has been proved to be a very effective way of improving image cap']","['use of attention in sequence - to - sequence learning for machine translation [ 16,  #TAUTHOR_TAG, visual attention has been proved to be a very effective way of improving image captioning.', 'some early']","['injecting word occurrence prediction attributes [ 9 ] into the cnn - rnn framework.', 'inspired by the use of attention in sequence - to - sequence learning for machine translation [ 16,  #TAUTHOR_TAG, visual attention has been proved to be a very effective way of improving image captioning.', 'some early research follows this direction,']","['', 'during training at each time step they input the current word, compute a distribution over all the words of the vocabulary based on the hidden states and maximize the likelihood of the true next word using a negative log likelihood loss.', 'the work in [ 1 ] employs a more powerful rnn cell, and they incorporated the image features as first input word instead of using it as initial hidden state.', 'other similar approaches include [ 5, 10, 11 ].', '[ 5 ] were proposed as an extension of the lstm model by exploring different kind of semantic information that can be used as extra guiding input to the lstm during decoding steps.', '[ 4 ] followed this direction by injecting more powerful high - level image attributes into the decoder.', 'in their work, they investigate different architecture for injecting word occurrence prediction attributes [ 9 ] into the cnn - rnn framework.', 'inspired by the use of attention in sequence - to - sequence learning for machine translation [ 16,  #TAUTHOR_TAG, visual attention has been proved to be a very effective way of improving image captioning.', 'some early research follows this direction, e. g., the model proposed in [ 3 ] can focus on important parts of images while generating descriptions.', 'the captioning model in']",1
"['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG.']","['new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image']","['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is']","['of the visual data is way different from the structure of the captions describing them. moreover, the simple cnn + rnn pipeline squash the whole input', 'image into a fixed - length embedding vector. this constitutes a major limitation of the basic encoder - decoder architecture. to overcome these limitations both for machine', 'translation and image captioning, some new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is now represented as a sequence of context vector where the length of the sequence depends on the number of words in the sentence to be generated. promising results has', 'been published since attention was introduced in [ 16 ] then later refined in  #TAUTHOR_TAG. another group of models [ 4 ] [ 5 ] [ 6 ] tried to', 'overcome the limitation of the basic encodedecoder framework by still representing the input image as a fixedlength vector but injecting external guiding information', '']",4
"['d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h']","['d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h×d is a transformation matrix']","['c t ∈ r d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h×d is a transformation matrix']","['c t ∈ r d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h×d is a transformation matrix']",4
"['r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm']","['r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm [ 24 ] is a powerful']","['the recursive function r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm [ 24 ] is a powerful form of']","['form of the recursive function r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm [ 24 ] is a powerful form of recurrent neural network that is widely used now because of its ability to deal with issues like vanishing and exploding gradients.', 'the final form of r is described by the following equations :', 'here x t is input signal at time step t, m t and h t are respectively memory cell and hidden state of the lstm cell andh t represents attention vector.', 'the variables i t, f t, o t, g t stand respectively for input gate, forget gate, output gate and candidate memory cell.', 'the various w z xy and b z are respectively parameter matrices and bias terms to be optimized.', 'the non - linearity σ is element - wise sigmoid activation and is the element - wise dot product']",4
"['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG.']","['new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image']","['proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is']","['of the visual data is way different from the structure of the captions describing them. moreover, the simple cnn + rnn pipeline squash the whole input', 'image into a fixed - length embedding vector. this constitutes a major limitation of the basic encoder - decoder architecture. to overcome these limitations both for machine', 'translation and image captioning, some new models were proposed by using the attention mechanism [ 3, 16,  #TAUTHOR_TAG. for image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. with', 'this the input image is now represented as a sequence of context vector where the length of the sequence depends on the number of words in the sentence to be generated. promising results has', 'been published since attention was introduced in [ 16 ] then later refined in  #TAUTHOR_TAG. another group of models [ 4 ] [ 5 ] [ 6 ] tried to', 'overcome the limitation of the basic encodedecoder framework by still representing the input image as a fixedlength vector but injecting external guiding information', '']",6
"['d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h']","['d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h×d is a transformation matrix']","['c t ∈ r d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h×d is a transformation matrix']","['c t ∈ r d and α t 1, α t 2,..., α t k are alignment weights and the function φ is known as alignment function.', 'in our model, we use the general form described in  #TAUTHOR_TAG :', 'where w a ∈ r h×d is a transformation matrix']",6
"['r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm']","['r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm [ 24 ] is a powerful']","['the recursive function r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm [ 24 ] is a powerful form of']","['form of the recursive function r ( equation 3 ) is a critical design choice for generating sequences.', 'in this paper, we use a lstm cell wrapped with the attention mechanism described in  #TAUTHOR_TAG to form r. lstm [ 24 ] is a powerful form of recurrent neural network that is widely used now because of its ability to deal with issues like vanishing and exploding gradients.', 'the final form of r is described by the following equations :', 'here x t is input signal at time step t, m t and h t are respectively memory cell and hidden state of the lstm cell andh t represents attention vector.', 'the variables i t, f t, o t, g t stand respectively for input gate, forget gate, output gate and candidate memory cell.', 'the various w z xy and b z are respectively parameter matrices and bias terms to be optimized.', 'the non - linearity σ is element - wise sigmoid activation and is the element - wise dot product']",6
"['i, w i 1 : t−1 ; θ ) with a lstm cell wrapped with luong - style attention mechanism  #TAUTHOR_TAG.', ""a rnn cell with luong's attention""]","['1 : t−1.', 'in our work, we model the distribution p ( w i t | x i, w i 1 : t−1 ; θ ) with a lstm cell wrapped with luong - style attention mechanism  #TAUTHOR_TAG.', ""a rnn cell with luong's attention""]","['( w i t | x i, w i 1 : t−1 ; θ ) with a lstm cell wrapped with luong - style attention mechanism  #TAUTHOR_TAG.', ""a rnn cell with luong's attention""]","[', the ultimate goal of the neural cnn + rnn architecture for image captioning is to build an end - to - end model trainable by standard backpropagation that can generate a description s i given a certain image x i.', 'inspired by nmt, such a model can translate an image into a describing sentence.', 'a cnn is first used to obtain image features and a rnn decoder is conditioned on those cnn image features to generate the describing sentence.', 'given a training dataset consisting of ( s i, x i ) pairs, these models aim to directly maximize the log - probability of generating s i when x i is the input.', 'thus, the optimization problem can be formulated by :', 'where θ represents the set of parameters to be learned, x i is a single image and s i = [ w i 1, w i 2,..., w i n i ] is the corresponding caption which is a sequence of n i words.', 'because each caption represents a sequence of n i words, the log probability is calculated using bayes chain rule :', 'is the likelihood of generating the first word w i 1 given only the image x i and p ( w i t | x i ; w i 1 : t−1 ; θ ) represents the probability of emitting word w i t at time step t conditioned on the image x i and the words generated so far w i 1 : t−1.', 'in our work, we model the distribution p ( w i t | x i, w i 1 : t−1 ; θ ) with a lstm cell wrapped with luong - style attention mechanism  #TAUTHOR_TAG.', ""a rnn cell with luong's attention computes its hidden state h t at each time step based on the current input x t, the previous hidden state h t−1 and the previous attention vectorh t−1."", 'the current hidden state h t is combined with the image - side context vector c t to form the final output of the cell which is the current attention vectorh t.', 'finally, the distribution p ( w i t | x i ; w i 1 : t−1 ; θ ) is computed by applying a softmax layer on top of the current attention vectorh', 'where w s, w c are projection matrices, c t is the image - side context vector detailed in section 3. 1 and r is a recursive function whose details are given in section 3. 2.', 'states, our model computes a context vector c t which is a weighted sum of attention states and can be seen as a dynamic representation of the image']",3
"['access  #TAUTHOR_TAG, and']","['access  #TAUTHOR_TAG, and']","['database access  #TAUTHOR_TAG, and']","['language dialog has been used in many areas, such as for call - center / routing application ( carpenter & chu - carroll 1998 ), email routing ( walker, fromer & narayanan 1998 ), information retrieval and database access  #TAUTHOR_TAG, and for telephony banking ( zadrozny et al. 1998 ).', 'in this demonstration, we present a natural language dialog interface to online shopping.', ""our user studies show natural language dialog to be a very effective means for negotiating user's requests and intentions in this domain""]",0
"['access  #TAUTHOR_TAG, and']","['access  #TAUTHOR_TAG, and']","['database access  #TAUTHOR_TAG, and']","['language dialog has been used in many areas, such as for call - center / routing application ( carpenter & chu - carroll 1998 ), email routing ( walker, fromer & narayanan 1998 ), information retrieval and database access  #TAUTHOR_TAG, and for telephony banking ( zadrozny et al. 1998 ).', 'in this demonstration, we present a natural language dialog interface to online shopping.', ""our user studies show natural language dialog to be a very effective means for negotiating user's requests and intentions in this domain""]",6
"['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between the amount of the employed lexical information and the overall efficiency. in synthesis, along the ideas of the topic sensitive pagerank  #AUTHOR_TAG, p p r', 'suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer pagerank model over the graph to converge towards the proper senses in fewer steps', '. the basic idea behind the adoption of p p r is to impose a personalized vector that expresses the', 'contexts of all words targeted by the disambiguation. this method improves on the complexity of the previously presented', 'methods ( e. g.  #AUTHOR_TAG ) as it allows to contextual', '##ize the behaviors of pagerank over a sentence, without asking for a different graph : in this way the wordnet graph is always adopted, in a word or sentence oriented fashion. moreover, it is possible', 'to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. in  #TAUTHOR_TAG', ', a possible, and more accurate', 'alternative, is also presented called ppr word2word ( p p rw2w ) where a different personalization vector is used for each word in', 'a sentence. although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state -', 'ofthe art in unsupervised wsd. in this paper a different approach to personalization of the pagerank is presented, aiming at preserving the suitable efficiency of the sentence oriented ppr algorithm for ws', '##d but achieving an accuracy at least as high as the p p rw2w one. we propose to use distributional evidence that can be automatically', 'acquired from a corpus to define the topical information encoded by the personalization vector, in order', 'to amplify the bias on the resulting p p r and improve the performance of the sentence oriented version. the intuition is that distributional evidence is able to cover the gap between word oriented usages of the', 'p p r as for the p p rw2w defined in  #TAUTHOR_TAG, and its sentence oriented counterpart. in this way we can preserve higher accuracy levels while limiting the number', 'of pagerank runs, i. e. increasing efficiency. the paper is structured as follows. we', 'first give a more detailed overview of the pagerank and personalized pagerank algorithms in section 2. in section, 3 a description of our distributional approach', 'to the personalized pagerank is provided. a comparative evaluation with respect to previous works is then reported in section 4 while section 5 is left for conclusions']",0
"['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between the amount of the employed lexical information and the overall efficiency. in synthesis, along the ideas of the topic sensitive pagerank  #AUTHOR_TAG, p p r', 'suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer pagerank model over the graph to converge towards the proper senses in fewer steps', '. the basic idea behind the adoption of p p r is to impose a personalized vector that expresses the', 'contexts of all words targeted by the disambiguation. this method improves on the complexity of the previously presented', 'methods ( e. g.  #AUTHOR_TAG ) as it allows to contextual', '##ize the behaviors of pagerank over a sentence, without asking for a different graph : in this way the wordnet graph is always adopted, in a word or sentence oriented fashion. moreover, it is possible', 'to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. in  #TAUTHOR_TAG', ', a possible, and more accurate', 'alternative, is also presented called ppr word2word ( p p rw2w ) where a different personalization vector is used for each word in', 'a sentence. although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state -', 'ofthe art in unsupervised wsd. in this paper a different approach to personalization of the pagerank is presented, aiming at preserving the suitable efficiency of the sentence oriented ppr algorithm for ws', '##d but achieving an accuracy at least as high as the p p rw2w one. we propose to use distributional evidence that can be automatically', 'acquired from a corpus to define the topical information encoded by the personalization vector, in order', 'to amplify the bias on the resulting p p r and improve the performance of the sentence oriented version. the intuition is that distributional evidence is able to cover the gap between word oriented usages of the', 'p p r as for the p p rw2w defined in  #TAUTHOR_TAG, and its sentence oriented counterpart. in this way we can preserve higher accuracy levels while limiting the number', 'of pagerank runs, i. e. increasing efficiency. the paper is structured as follows. we', 'first give a more detailed overview of the pagerank and personalized pagerank algorithms in section 2. in section, 3 a description of our distributional approach', 'to the personalized pagerank is provided. a comparative evaluation with respect to previous works is then reported in section 4 while section 5 is left for conclusions']",0
"['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between the amount of the employed lexical information and the overall efficiency. in synthesis, along the ideas of the topic sensitive pagerank  #AUTHOR_TAG, p p r', 'suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer pagerank model over the graph to converge towards the proper senses in fewer steps', '. the basic idea behind the adoption of p p r is to impose a personalized vector that expresses the', 'contexts of all words targeted by the disambiguation. this method improves on the complexity of the previously presented', 'methods ( e. g.  #AUTHOR_TAG ) as it allows to contextual', '##ize the behaviors of pagerank over a sentence, without asking for a different graph : in this way the wordnet graph is always adopted, in a word or sentence oriented fashion. moreover, it is possible', 'to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. in  #TAUTHOR_TAG', ', a possible, and more accurate', 'alternative, is also presented called ppr word2word ( p p rw2w ) where a different personalization vector is used for each word in', 'a sentence. although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state -', 'ofthe art in unsupervised wsd. in this paper a different approach to personalization of the pagerank is presented, aiming at preserving the suitable efficiency of the sentence oriented ppr algorithm for ws', '##d but achieving an accuracy at least as high as the p p rw2w one. we propose to use distributional evidence that can be automatically', 'acquired from a corpus to define the topical information encoded by the personalization vector, in order', 'to amplify the bias on the resulting p p r and improve the performance of the sentence oriented version. the intuition is that distributional evidence is able to cover the gap between word oriented usages of the', 'p p r as for the p p rw2w defined in  #TAUTHOR_TAG, and its sentence oriented counterpart. in this way we can preserve higher accuracy levels while limiting the number', 'of pagerank runs, i. e. increasing efficiency. the paper is structured as follows. we', 'first give a more detailed overview of the pagerank and personalized pagerank algorithms in section 2. in section, 3 a description of our distributional approach', 'to the personalized pagerank is provided. a comparative evaluation with respect to previous works is then reported in section 4 while section 5 is left for conclusions']",0
"['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between the amount of the employed lexical information and the overall efficiency. in synthesis, along the ideas of the topic sensitive pagerank  #AUTHOR_TAG, p p r', 'suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer pagerank model over the graph to converge towards the proper senses in fewer steps', '. the basic idea behind the adoption of p p r is to impose a personalized vector that expresses the', 'contexts of all words targeted by the disambiguation. this method improves on the complexity of the previously presented', 'methods ( e. g.  #AUTHOR_TAG ) as it allows to contextual', '##ize the behaviors of pagerank over a sentence, without asking for a different graph : in this way the wordnet graph is always adopted, in a word or sentence oriented fashion. moreover, it is possible', 'to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. in  #TAUTHOR_TAG', ', a possible, and more accurate', 'alternative, is also presented called ppr word2word ( p p rw2w ) where a different personalization vector is used for each word in', 'a sentence. although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state -', 'ofthe art in unsupervised wsd. in this paper a different approach to personalization of the pagerank is presented, aiming at preserving the suitable efficiency of the sentence oriented ppr algorithm for ws', '##d but achieving an accuracy at least as high as the p p rw2w one. we propose to use distributional evidence that can be automatically', 'acquired from a corpus to define the topical information encoded by the personalization vector, in order', 'to amplify the bias on the resulting p p r and improve the performance of the sentence oriented version. the intuition is that distributional evidence is able to cover the gap between word oriented usages of the', 'p p r as for the p p rw2w defined in  #TAUTHOR_TAG, and its sentence oriented counterpart. in this way we can preserve higher accuracy levels while limiting the number', 'of pagerank runs, i. e. increasing efficiency. the paper is structured as follows. we', 'first give a more detailed overview of the pagerank and personalized pagerank algorithms in section 2. in section, 3 a description of our distributional approach', 'to the personalized pagerank is provided. a comparative evaluation with respect to previous works is then reported in section 4 while section 5 is left for conclusions']",0
['by  #TAUTHOR_TAG initialize'],['by  #TAUTHOR_TAG initialize the ranks of'],[' #TAUTHOR_TAG initialize'],[' #TAUTHOR_TAG'],0
['by  #TAUTHOR_TAG initialize'],['by  #TAUTHOR_TAG initialize the ranks of'],[' #TAUTHOR_TAG initialize'],[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize and use the graph g for disambiguating a sentence with respect to the overall graph ( hereafter gkb ) that represents the complete lexicon.', '']",0
"[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize and use the graph g for disambiguating a sentence with respect to the overall graph ( hereafter gkb ) that represents the complete lexicon.', '']",0
"[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize and use the graph g for disambiguating a sentence with respect to the overall graph ( hereafter gkb ) that represents the complete lexicon.', '']",0
"[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize']","[' #TAUTHOR_TAG, a novel use of pagerank for word sense disambiguation is presented.', 'it aims to present an optimized version of the algorithm previously discussed in  #AUTHOR_TAG.', 'the main difference concerns the method used to initialize and use the graph g for disambiguating a sentence with respect to the overall graph ( hereafter gkb ) that represents the complete lexicon.', '']",0
['key idea in  #TAUTHOR_TAG is to adapt the matrix initialization step in'],['key idea in  #TAUTHOR_TAG is to adapt the matrix initialization step in'],['key idea in  #TAUTHOR_TAG is to adapt the matrix initialization step in'],"['key idea in  #TAUTHOR_TAG is to adapt the matrix initialization step in order to exploit the available contextual evidence.', 'notice that personalization in word sense disambiguation is inspired by the topic - sensitive pagerank approach, proposed in  #AUTHOR_TAG, for web search tasks.', 'it exploits a context dependent definition of the vector p in eq. 1 to influence the linkbased sense ranking achievable over a sentence.', 'context is used as only words of the sentence ( or words co - occurring with the target w i in the w2w method ) are given non zero probability mass in p : this provides a topical bias to pagerank.', 'a variety of models of topical information have been proposed in ir ( e. g.  #AUTHOR_TAG ) to generalize documents or shorter texts ( e. g. query ).', 'they can be acquired through large scale corpus analysis in the so called distributional approaches to language modeling.', 'while contexts can be defined in different ways ( e. g as the set of words surrounding a target word ), their analysis over large corpora has been shown to effectively capture topical and paradigmatic relations  #AUTHOR_TAG.', 'we propose to use the topical information about a sentence σ, acquired through latent semantic analysis  #AUTHOR_TAG, as a source information for the initialization of the vector p in the p p r ( or p p rw2w ) disambiguation methods.', '']",0
"['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between the amount of the employed lexical information and the overall efficiency. in synthesis, along the ideas of the topic sensitive pagerank  #AUTHOR_TAG, p p r', 'suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer pagerank model over the graph to converge towards the proper senses in fewer steps', '. the basic idea behind the adoption of p p r is to impose a personalized vector that expresses the', 'contexts of all words targeted by the disambiguation. this method improves on the complexity of the previously presented', 'methods ( e. g.  #AUTHOR_TAG ) as it allows to contextual', '##ize the behaviors of pagerank over a sentence, without asking for a different graph : in this way the wordnet graph is always adopted, in a word or sentence oriented fashion. moreover, it is possible', 'to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. in  #TAUTHOR_TAG', ', a possible, and more accurate', 'alternative, is also presented called ppr word2word ( p p rw2w ) where a different personalization vector is used for each word in', 'a sentence. although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state -', 'ofthe art in unsupervised wsd. in this paper a different approach to personalization of the pagerank is presented, aiming at preserving the suitable efficiency of the sentence oriented ppr algorithm for ws', '##d but achieving an accuracy at least as high as the p p rw2w one. we propose to use distributional evidence that can be automatically', 'acquired from a corpus to define the topical information encoded by the personalization vector, in order', 'to amplify the bias on the resulting p p r and improve the performance of the sentence oriented version. the intuition is that distributional evidence is able to cover the gap between word oriented usages of the', 'p p r as for the p p rw2w defined in  #TAUTHOR_TAG, and its sentence oriented counterpart. in this way we can preserve higher accuracy levels while limiting the number', 'of pagerank runs, i. e. increasing efficiency. the paper is structured as follows. we', 'first give a more detailed overview of the pagerank and personalized pagerank algorithms in section 2. in section, 3 a description of our distributional approach', 'to the personalized pagerank is provided. a comparative evaluation with respect to previous works is then reported in section 4 while section 5 is left for conclusions']",1
"['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to']","['personalized pagerank ( p p r ) is', 'proposed  #TAUTHOR_TAG that tries to trade - off between the amount of the employed lexical information and the overall efficiency. in synthesis, along the ideas of the topic sensitive pagerank  #AUTHOR_TAG, p p r', 'suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer pagerank model over the graph to converge towards the proper senses in fewer steps', '. the basic idea behind the adoption of p p r is to impose a personalized vector that expresses the', 'contexts of all words targeted by the disambiguation. this method improves on the complexity of the previously presented', 'methods ( e. g.  #AUTHOR_TAG ) as it allows to contextual', '##ize the behaviors of pagerank over a sentence, without asking for a different graph : in this way the wordnet graph is always adopted, in a word or sentence oriented fashion. moreover, it is possible', 'to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. in  #TAUTHOR_TAG', ', a possible, and more accurate', 'alternative, is also presented called ppr word2word ( p p rw2w ) where a different personalization vector is used for each word in', 'a sentence. although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state -', 'ofthe art in unsupervised wsd. in this paper a different approach to personalization of the pagerank is presented, aiming at preserving the suitable efficiency of the sentence oriented ppr algorithm for ws', '##d but achieving an accuracy at least as high as the p p rw2w one. we propose to use distributional evidence that can be automatically', 'acquired from a corpus to define the topical information encoded by the personalization vector, in order', 'to amplify the bias on the resulting p p r and improve the performance of the sentence oriented version. the intuition is that distributional evidence is able to cover the gap between word oriented usages of the', 'p p r as for the p p rw2w defined in  #TAUTHOR_TAG, and its sentence oriented counterpart. in this way we can preserve higher accuracy levels while limiting the number', 'of pagerank runs, i. e. increasing efficiency. the paper is structured as follows. we', 'first give a more detailed overview of the pagerank and personalized pagerank algorithms in section 2. in section, 3 a description of our distributional approach', 'to the personalized pagerank is provided. a comparative evaluation with respect to previous works is then reported in section 4 while section 5 is left for conclusions']",4
"['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical']","['', 'two iterations ( 5 and 15 ) is reported to analyze also the overall trend during pagerank.', 'the best f1 scores between any pair', 'are emphasized in bold, to comparatively asses the results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical resources achieve different results', '. in general by adopting the graph derived from wn3. 0 ( i.', '']",4
"['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical']","['', 'two iterations ( 5 and 15 ) is reported to analyze also the overall trend during pagerank.', 'the best f1 scores between any pair', 'are emphasized in bold, to comparatively asses the results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical resources achieve different results', '. in general by adopting the graph derived from wn3. 0 ( i.', '']",5
"['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical']","['', 'two iterations ( 5 and 15 ) is reported to analyze also the overall trend during pagerank.', 'the best f1 scores between any pair', 'are emphasized in bold, to comparatively asses the results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical resources achieve different results', '. in general by adopting the graph derived from wn3. 0 ( i.', '']",5
"['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical']","['', 'two iterations ( 5 and 15 ) is reported to analyze also the overall trend during pagerank.', 'the best f1 scores between any pair', 'are emphasized in bold, to comparatively asses the results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical resources achieve different results', '. in general by adopting the graph derived from wn3. 0 ( i.', '']",5
"['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical']","['', 'two iterations ( 5 and 15 ) is reported to analyze also the overall trend during pagerank.', 'the best f1 scores between any pair', 'are emphasized in bold, to comparatively asses the results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical resources achieve different results', '. in general by adopting the graph derived from wn3. 0 ( i.', '']",3
"['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['. as a confirmation of the outcome in  #TAUTHOR_TAG,']","['results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical']","['', 'two iterations ( 5 and 15 ) is reported to analyze also the overall trend during pagerank.', 'the best f1 scores between any pair', 'are emphasized in bold, to comparatively asses the results. as a confirmation of the outcome in  #TAUTHOR_TAG, different lexical resources achieve different results', '. in general by adopting the graph derived from wn3. 0 ( i.', '']",3
['we use the long - distance agreement benchmark recently introduced by  #TAUTHOR_TAG'],['we use the long - distance agreement benchmark recently introduced by  #TAUTHOR_TAG'],"['', 'to this end we use the long - distance agreement benchmark recently introduced by  #TAUTHOR_TAG']","['', 'we focus on the case of language models ( lm ) trained on two languages, one of which ( l1 ) is over - resourced with respect to the other ( l2 ), and investigate whether the syntactic knowledge learned for l1 is transferred to l2.', 'to this end we use the long - distance agreement benchmark recently introduced by  #TAUTHOR_TAG']",5
"['of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are']","['of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are']","['train the model.', 'this is the least optimistic scenario for linguistic transfer but also the most controlled one.', 'in future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are evaluated on the italian section of the syntactic benchmark']","['consider the scenario where l1 is overresourced compared to l2 and train our bilingual models by joint training on a mixed l1 / l2 corpus so that supervision is provided simultaneously in the two languages ( ostling and  #AUTHOR_TAG.', 'we leave the evaluation of pre - training ( or transfer learning ) methods  #AUTHOR_TAG to future work.', 'the monolingual lm is trained on a small l2 corpus ( lm l2 ).', 'the bilingual lm is trained on a shuffled mix of the same small l2 corpus and a large l1 corpus, where l2 is oversampled to approximately match the amount of l1 sentences ( lm l1 + l2 ).', 'see table 1 for the actual training sizes.', 'for our preliminary experiments we have chosen french as the helper language ( l1 ) and italian as the target language ( l2 ).', 'since french and italian share many morphosyntactic patterns, accuracy on the italian agreement tasks is expected to benefit from adding french sentences to the training data if syntactic transfer occurs.', 'data and training details : we train our lms on french and italian wikipedia articles extracted using the wikiextractor tool.', '1 for each language, we maintain a vocabulary of the 50k most frequent tokens, and replace the remaining tokens by < unk >. for the bilingual lm, all words are prepended with a language tag so that vocabularies are completely disjoint.', 'their union ( 100k types ) is used to train the model.', 'this is the least optimistic scenario for linguistic transfer but also the most controlled one.', 'in future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are evaluated on the italian section of the syntactic benchmark provided by  #TAUTHOR_TAG, which includes various non - trivial number agreement constructions.', '2 note that all models are trained on a regular corpus likelihood objective and do not receive any specific supervision for the syntactic tasks.', 'table 1 shows the results of our preliminary experiments.', 'the unigram baseline simply picks, for each sentence, the most frequent word form between singular or plural.', 'as an upper - bound we report the agreement accuracy obtained by a monolingual model trained on a large l2 corpus.', 'the effect of mixing the small italian corpus with the large french one does not appear to be major.', 'agreement accuracy increases slightly in the original sentences, where the model is free to rely on collocational cues, but decreases slightly']",5
"['of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are']","['of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are']","['train the model.', 'this is the least optimistic scenario for linguistic transfer but also the most controlled one.', 'in future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are evaluated on the italian section of the syntactic benchmark']","['consider the scenario where l1 is overresourced compared to l2 and train our bilingual models by joint training on a mixed l1 / l2 corpus so that supervision is provided simultaneously in the two languages ( ostling and  #AUTHOR_TAG.', 'we leave the evaluation of pre - training ( or transfer learning ) methods  #AUTHOR_TAG to future work.', 'the monolingual lm is trained on a small l2 corpus ( lm l2 ).', 'the bilingual lm is trained on a shuffled mix of the same small l2 corpus and a large l1 corpus, where l2 is oversampled to approximately match the amount of l1 sentences ( lm l1 + l2 ).', 'see table 1 for the actual training sizes.', 'for our preliminary experiments we have chosen french as the helper language ( l1 ) and italian as the target language ( l2 ).', 'since french and italian share many morphosyntactic patterns, accuracy on the italian agreement tasks is expected to benefit from adding french sentences to the training data if syntactic transfer occurs.', 'data and training details : we train our lms on french and italian wikipedia articles extracted using the wikiextractor tool.', '1 for each language, we maintain a vocabulary of the 50k most frequent tokens, and replace the remaining tokens by < unk >. for the bilingual lm, all words are prepended with a language tag so that vocabularies are completely disjoint.', 'their union ( 100k types ) is used to train the model.', 'this is the least optimistic scenario for linguistic transfer but also the most controlled one.', 'in future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap.', 'following the setup of  #TAUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are evaluated on the italian section of the syntactic benchmark provided by  #TAUTHOR_TAG, which includes various non - trivial number agreement constructions.', '2 note that all models are trained on a regular corpus likelihood objective and do not receive any specific supervision for the syntactic tasks.', 'table 1 shows the results of our preliminary experiments.', 'the unigram baseline simply picks, for each sentence, the most frequent word form between singular or plural.', 'as an upper - bound we report the agreement accuracy obtained by a monolingual model trained on a large l2 corpus.', 'the effect of mixing the small italian corpus with the large french one does not appear to be major.', 'agreement accuracy increases slightly in the original sentences, where the model is free to rely on collocational cues, but decreases slightly']",5
"['- verb agreement task, while  #TAUTHOR_TAG extended']","['english subject - verb agreement task, while  #TAUTHOR_TAG extended']","['lstm lms at an english subject - verb agreement task, while  #TAUTHOR_TAG extended']","['recent advances in neural networks have opened the way to the design of architecturally simple multilingual models for various nlp tasks, such as language modeling or next word prediction  #AUTHOR_TAG ostling and  #AUTHOR_TAG, translation  #AUTHOR_TAG, morphological reinflection  #AUTHOR_TAG and more  #AUTHOR_TAG.', 'a practical benefit of training models multilingually is to transfer knowledge from high - resource languages to lowresource ones and improve task performance in the latter.', 'here we aim at understanding how linguistic knowledge is transferred among languages, specifically at the syntactic level, which to our knowledge has not been studied so far.', 'assessing the syntactic abilities of monolingual neural lms trained without explicit supervision has been the focus of several recent studies :  #AUTHOR_TAG analyzed the performance of lstm lms at an english subject - verb agreement task, while  #TAUTHOR_TAG extended the analysis to various long - range agreement patterns in different languages.', 'the latter study found that state - of - the - art lms trained on a standard loglikelihood objective capture non - trivial patterns of syntactic agreement and can approach the performance levels of humans, even when tested on syntactically well - formed but meaningless ( nonce ) sentences.', 'cross - language interaction during language production and comprehension by human subjects has been widely studied in the fields of bilingualism and second language acquisition ( kellerman and sharwood smith ;  #AUTHOR_TAG under the terms of language transfer or cross - linguistic influence.', 'numerous studies have shown that both the lexicons and the grammars of different languages are not stored independently but together in the mind of bilinguals and second - language learners, leading to observ - able lexical and syntactic transfer effects  #AUTHOR_TAG.', 'for instance, through a crosslingual syntactic priming experiment,  #AUTHOR_TAG showed that bilinguals recently exposed to a given syntactic construction ( passive voice ) in their l1 tend to reuse the same construction in their l2.', 'while the neural networks in this study are not designed to be plausible models of the human mind learning and processing multiple languages, we believe there is interesting potential at the intersection of these research fields']",0
"['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements']","['paper presents the introduction of wordnet semantic classes in a dependency parser, obtaining improvements on the full penn treebank for the first time.', 'we tried different combinations of some basic semantic classes and word sense disambiguation algorithms.', 'our experiments show that selecting the adequate combination of semantic features on development data is key for success.', 'given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements.', 'using semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing.', ' #AUTHOR_TAG tested a combined parsing / word sense disambiguation model based in wordnet which did not obtain improvements in parsing.', 'presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features.', 'they demonstrate the effectiveness of the approach in']",4
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing']",4
"['', ' #TAUTHOR_TAG used a simple method']","['over using a single feature.', ' #TAUTHOR_TAG used a simple method']","['tested the inclusion of several types of semantic information, in the form of wordnet semantic classes in a dependency parser, showing that :', '• semantic information gives an improvement on a transition - based deterministic dependency parsing.', '• feature combinations give an improvement over using a single feature.', ' #TAUTHOR_TAG used a simple method']","['tested the inclusion of several types of semantic information, in the form of wordnet semantic classes in a dependency parser, showing that :', '• semantic information gives an improvement on a transition - based deterministic dependency parsing.', '• feature combinations give an improvement over using a single feature.', ' #TAUTHOR_TAG used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature.', 'maltparser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech.', 'although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on gold and 1st in table 1.', 'due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature.', '• the present work presents a statistically significant improvement for the full treebank using wordnet - based semantic information for the first time.', 'our results extend those of  #TAUTHOR_TAG, which showed improvements on a subset of the ptb.', 'given the basic nature of the semantic classes and wsd algorithms, we think there is room for future improvements, incorporating new kinds of semantic information, such as wordnet base concepts, wikipedia concepts, or similarity measures']",4
"['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements']","['paper presents the introduction of wordnet semantic classes in a dependency parser, obtaining improvements on the full penn treebank for the first time.', 'we tried different combinations of some basic semantic classes and word sense disambiguation algorithms.', 'our experiments show that selecting the adequate combination of semantic features on development data is key for success.', 'given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements.', 'using semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing.', ' #AUTHOR_TAG tested a combined parsing / word sense disambiguation model based in wordnet which did not obtain improvements in parsing.', 'presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features.', 'they demonstrate the effectiveness of the approach in']",6
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing']",6
"['', ' #TAUTHOR_TAG used a simple method']","['over using a single feature.', ' #TAUTHOR_TAG used a simple method']","['tested the inclusion of several types of semantic information, in the form of wordnet semantic classes in a dependency parser, showing that :', '• semantic information gives an improvement on a transition - based deterministic dependency parsing.', '• feature combinations give an improvement over using a single feature.', ' #TAUTHOR_TAG used a simple method']","['tested the inclusion of several types of semantic information, in the form of wordnet semantic classes in a dependency parser, showing that :', '• semantic information gives an improvement on a transition - based deterministic dependency parsing.', '• feature combinations give an improvement over using a single feature.', ' #TAUTHOR_TAG used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature.', 'maltparser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech.', 'although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on gold and 1st in table 1.', 'due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature.', '• the present work presents a statistically significant improvement for the full treebank using wordnet - based semantic information for the first time.', 'our results extend those of  #TAUTHOR_TAG, which showed improvements on a subset of the ptb.', 'given the basic nature of the semantic classes and wsd algorithms, we think there is room for future improvements, incorporating new kinds of semantic information, such as wordnet base concepts, wikipedia concepts, or similarity measures']",6
"['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements']","['paper presents the introduction of wordnet semantic classes in a dependency parser, obtaining improvements on the full penn treebank for the first time.', 'we tried different combinations of some basic semantic classes and word sense disambiguation algorithms.', 'our experiments show that selecting the adequate combination of semantic features on development data is key for success.', 'given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements.', 'using semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing.', ' #AUTHOR_TAG tested a combined parsing / word sense disambiguation model based in wordnet which did not obtain improvements in parsing.', 'presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features.', 'they demonstrate the effectiveness of the approach in']",3
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing']",3
"['ptb intersection  #TAUTHOR_TAG.', '']","['semcor / ptb intersection  #TAUTHOR_TAG.', '']","['and the semcor / ptb intersection  #TAUTHOR_TAG.', 'the full ptb allows for comparison with']","['used two different datasets : the full ptb and the semcor / ptb intersection  #TAUTHOR_TAG.', '']",3
"['range of semantic representations used in  #TAUTHOR_TAG, all of which are based on']","['range of semantic representations used in  #TAUTHOR_TAG, all of which are based on']","['will experiment with the range of semantic representations used in  #TAUTHOR_TAG, all of which are based on']","['will experiment with the range of semantic representations used in  #TAUTHOR_TAG, all of which are based on wordnet 2. 1.', 'words in wordnet  #AUTHOR_TAG are organized into sets of synonyms, called synsets ( ss ).', 'each synset in turn belongs to a unique semantic file ( sf ).', 'there are a total of 45 sfs ( 1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns ), based on syntactic and semantic categories.', 'for example, noun semantic files ( sf _ n ) differentiate nouns denoting acts or actions, and nouns denoting animals, among others.', 'we experiment with both full synsets and sfs as instances of fine - grained and coarse - grained semantic representation, respectively.', 'as an example of the difference in these two representations, knife in its tool sense is in the edge tool used as a cutting instrument singleton synset, and also in the artifact sf along with thousands of other words including cutter.', 'note that these are the two extremes of semantic granularity in wordnet.', 'as a hybrid representation, we also tested the effect of merging words with their corresponding sf ( e. g. knife + artifact ).', '']",3
"['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant']","['in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements']","['paper presents the introduction of wordnet semantic classes in a dependency parser, obtaining improvements on the full penn treebank for the first time.', 'we tried different combinations of some basic semantic classes and word sense disambiguation algorithms.', 'our experiments show that selecting the adequate combination of semantic features on development data is key for success.', 'given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements.', 'using semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing.', ' #AUTHOR_TAG tested a combined parsing / word sense disambiguation model based in wordnet which did not obtain improvements in parsing.', 'presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features.', 'they demonstrate the effectiveness of the approach in']",0
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing']",0
"['', ' #TAUTHOR_TAG used a simple method']","['over using a single feature.', ' #TAUTHOR_TAG used a simple method']","['tested the inclusion of several types of semantic information, in the form of wordnet semantic classes in a dependency parser, showing that :', '• semantic information gives an improvement on a transition - based deterministic dependency parsing.', '• feature combinations give an improvement over using a single feature.', ' #TAUTHOR_TAG used a simple method']","['tested the inclusion of several types of semantic information, in the form of wordnet semantic classes in a dependency parser, showing that :', '• semantic information gives an improvement on a transition - based deterministic dependency parsing.', '• feature combinations give an improvement over using a single feature.', ' #TAUTHOR_TAG used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature.', 'maltparser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech.', 'although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on gold and 1st in table 1.', 'due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature.', '• the present work presents a statistically significant improvement for the full treebank using wordnet - based semantic information for the first time.', 'our results extend those of  #TAUTHOR_TAG, which showed improvements on a subset of the ptb.', 'given the basic nature of the semantic classes and wsd algorithms, we think there is room for future improvements, incorporating new kinds of semantic information, such as wordnet base concepts, wikipedia concepts, or similarity measures']",0
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['semantic information to improve parsing performance has been an interesting research avenue since the early days of nlp, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical pp attachment experiments  #AUTHOR_TAG.', 'although there have been some significant results ( see section 2 ), this issue continues to be elusive.', 'in principle, dependency parsing offers good prospects for experimenting with word - to - word - semantic relationships.', 'we present a set of experiments using semantic classes in dependency parsing of the penn treebank ( ptb ).', 'we extend the tests made in  #TAUTHOR_TAG, who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing.', 'as our baseline parser, we use maltparser  #AUTHOR_TAG.', 'we will evaluate the parser on both the full ptb ( marcus et al. 1993 ) and on a senseannotated subset of the brown corpus portion of ptb, in order to investigate the upper bound performance of the models given gold - standard sense information, as in  #TAUTHOR_TAG.', ' #TAUTHOR_TAG trained two state - of - the - art statistical parsers  #AUTHOR_TAG on semantically - enriched input, where content words had been substituted with their semantic classes.', 'this was done trying to overcome the limitations of lexicalized approaches to parsing  #AUTHOR_TAG, where related words, like scissors and knife cannot be generalized.', 'this simple method allowed incorporating lexical semantic information into the parser.', 'they tested the parsers in both a full parsing and a pp attachment context.', 'the experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance.', 'this work presented the first results over both wordnet and the penn treebank to show that semantic processing helps parsing']",5
"['ptb intersection  #TAUTHOR_TAG.', '']","['semcor / ptb intersection  #TAUTHOR_TAG.', '']","['and the semcor / ptb intersection  #TAUTHOR_TAG.', 'the full ptb allows for comparison with']","['used two different datasets : the full ptb and the semcor / ptb intersection  #TAUTHOR_TAG.', '']",5
"['ptb intersection  #TAUTHOR_TAG.', '']","['semcor / ptb intersection  #TAUTHOR_TAG.', '']","['and the semcor / ptb intersection  #TAUTHOR_TAG.', 'the full ptb allows for comparison with']","['used two different datasets : the full ptb and the semcor / ptb intersection  #TAUTHOR_TAG.', '']",5
"['range of semantic representations used in  #TAUTHOR_TAG, all of which are based on']","['range of semantic representations used in  #TAUTHOR_TAG, all of which are based on']","['will experiment with the range of semantic representations used in  #TAUTHOR_TAG, all of which are based on']","['will experiment with the range of semantic representations used in  #TAUTHOR_TAG, all of which are based on wordnet 2. 1.', 'words in wordnet  #AUTHOR_TAG are organized into sets of synonyms, called synsets ( ss ).', 'each synset in turn belongs to a unique semantic file ( sf ).', 'there are a total of 45 sfs ( 1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns ), based on syntactic and semantic categories.', 'for example, noun semantic files ( sf _ n ) differentiate nouns denoting acts or actions, and nouns denoting animals, among others.', 'we experiment with both full synsets and sfs as instances of fine - grained and coarse - grained semantic representation, respectively.', 'as an example of the difference in these two representations, knife in its tool sense is in the edge tool used as a cutting instrument singleton synset, and also in the artifact sf along with thousands of other words including cutter.', 'note that these are the two extremes of semantic granularity in wordnet.', 'as a hybrid representation, we also tested the effect of merging words with their corresponding sf ( e. g. knife + artifact ).', '']",5
['2006  #TAUTHOR_TAG involved structural correspondence'],['2006  #TAUTHOR_TAG involved structural correspondence'],"['. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in']","['direct importance to the discussion in this paper are results from domain adaptation in polarity detection.', 'one of the earlier successful approaches ( blitzer et al. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data.', 'in a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data.', 'daume ( 2007 ) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space.', 'instead of using a single, general, feature set for source and target, three distinct feature sets are created : the general set of features, a source - domain specific version of the feature set, and a target - specific version of the feature set.', 'li and zong ( nlp - ke 2008 ) explore a classifier combination technique they call "" multiplelabel consensus training "" which results in better accuracy than non - adapted models on the data sets used in  #TAUTHOR_TAG.', '']",0
['2006  #TAUTHOR_TAG involved structural correspondence'],['2006  #TAUTHOR_TAG involved structural correspondence'],"['. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in']","['direct importance to the discussion in this paper are results from domain adaptation in polarity detection.', 'one of the earlier successful approaches ( blitzer et al. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data.', 'in a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data.', 'daume ( 2007 ) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space.', 'instead of using a single, general, feature set for source and target, three distinct feature sets are created : the general set of features, a source - domain specific version of the feature set, and a target - specific version of the feature set.', 'li and zong ( nlp - ke 2008 ) explore a classifier combination technique they call "" multiplelabel consensus training "" which results in better accuracy than non - adapted models on the data sets used in  #TAUTHOR_TAG.', '']",0
['2006  #TAUTHOR_TAG involved structural correspondence'],['2006  #TAUTHOR_TAG involved structural correspondence'],"['. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in']","['direct importance to the discussion in this paper are results from domain adaptation in polarity detection.', 'one of the earlier successful approaches ( blitzer et al. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data.', 'in a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data.', 'daume ( 2007 ) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space.', 'instead of using a single, general, feature set for source and target, three distinct feature sets are created : the general set of features, a source - domain specific version of the feature set, and a target - specific version of the feature set.', 'li and zong ( nlp - ke 2008 ) explore a classifier combination technique they call "" multiplelabel consensus training "" which results in better accuracy than non - adapted models on the data sets used in  #TAUTHOR_TAG.', '']",0
['2006  #TAUTHOR_TAG involved structural correspondence'],['2006  #TAUTHOR_TAG involved structural correspondence'],"['. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in']","['direct importance to the discussion in this paper are results from domain adaptation in polarity detection.', 'one of the earlier successful approaches ( blitzer et al. 2006  #TAUTHOR_TAG involved structural correspondence learning ( scl ).', 'scl identifies "" pivot "" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data.', 'in a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data.', 'daume ( 2007 ) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space.', 'instead of using a single, general, feature set for source and target, three distinct feature sets are created : the general set of features, a source - domain specific version of the feature set, and a target - specific version of the feature set.', 'li and zong ( nlp - ke 2008 ) explore a classifier combination technique they call "" multiplelabel consensus training "" which results in better accuracy than non - adapted models on the data sets used in  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG employ the structural', 'correspondence']","[' #TAUTHOR_TAG employ the structural', 'correspondence']","['power of the feature augmentation approach.  #TAUTHOR_TAG employ the structural', 'correspondence learning ( sc']","['', 'm of the set of vectors vs created in step 1. c ) we employ either logistic regression or svm. we have used', 'svm light implementation 3 to train the ensemble using svm with the matrix m', 'and a radial basis kernel function. for k source domains, the augmented feature space consists of k + 1 copies of the original feature space. however,', 'creating three versions of each feature in both the source and the', ""target domains grows the feature space exponentially, which is prohibitive in a many - domain adaptation scenario such as ours which consists of a total of 27 domains. we addressed this challenge by considering the 26 source domains as a single source domain being adapted to the target domain. this setup along with feature reduction enabled us to apply daume's approach without too much of an inflation of the feature"", 'space. however, we also recognize that this likely compromises the power of the feature augmentation approach.  #TAUTHOR_TAG employ the structural', 'correspondence learning ( scl ) algorithm for sentiment domain adaptation. blitzer et al. evaluate the scl', 'domain adaptation on four publicly released datasets from amazon product reviews : books, dvds, electronics and kitchen appliances. in these four datasets, reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative', ', and the rest discarded because their polarity was ambiguous. 1000 positive and 1000 negative labeled examples were used for each', 'domain. some unlabeled data were additionally used including 3685 ( dvds ) and 5945 ( kitchen ). each labeled dataset', 'was split into 1600 instances for training and 400 instances for testing. the baseline in  #TAUTHOR_TAG is', 'a linear classifier trained without adaptation, while their ceiling reference is the same as ours', ', which is the in - domain classifier trained and tested on the same domain']",0
"['datasets as in  #TAUTHOR_TAG.', 'in']","['datasets as in  #TAUTHOR_TAG.', 'in']","['in daume ( 2007 ).', '4. we also compared the results of approaches 1 and 2 to published results on structural correspondence learning ( scl ) by using the same datasets as in  #TAUTHOR_TAG.', 'in']","['', 'an ensemble of classifiers, each trained on a source domain, combined into an ensemble.', '3.', 'the domain adaptation approach proposed in daume ( 2007 ).', '4. we also compared the results of approaches 1 and 2 to published results on structural correspondence learning ( scl ) by using the same datasets as in  #TAUTHOR_TAG.', 'in all our experiments, we employed maximum entropy - based classification with vanilla parameter settings and feature reduction using ure the performance of the all - in - one and ensemble classifiers.', 'the rest of the subsection illustrates the experimental setup for each of the above four approaches']",5
['reported by  #TAUTHOR_TAG baseline'],['reported by  #TAUTHOR_TAG baseline'],['reported by  #TAUTHOR_TAG baseline'],"['conducted a set of experiments employing the four datasets used for scl domain adaptation.', 'in these experiments, we compare the results of our all - in - one classifier and the ensemble classifier trained and tested on the four datasets to the results of scl and its variation scl - mi domain adaptation as reported by  #TAUTHOR_TAG baseline and ceiling in - domain classifiers for the four domains']",5
['employed the four domains datasets used in  #TAUTHOR_TAG'],['employed the four domains datasets used in  #TAUTHOR_TAG'],['employed the four domains datasets used in  #TAUTHOR_TAG'],"['employed the four domains datasets used in  #TAUTHOR_TAG to train and test the all - in one and the ensemble classifiers.', 'we also replicated the in - domain results of these four datasets using our maximum entropy classifier.', 'we compare the results of the all - in - one and the ensemble classifier to the scl and its variation scl - mi adaptation techniques using the four datasets used to evaluate scl and scl - mi in  #TAUTHOR_TAG.', ""note that the results published in blitzer's work represent pairwise domain - adaptation, while our ensemble and all - in - one results are based on training on three of blitzer's domains and testing on the held - out fourth domain."", 'this makes it impossible to draw a direct comparison, but we can still observe that in general, it is best to simply combine as many domains as possible in an all - in - one or ensemble approach as compared to carefully adapting a single domain.', 'table 2 summarizes the results of the comparison.', 'where is the transfer error defined as the test error obtained by a method trained on the source domain s and tested on the target domain t.', '']",5
['employed the four domains datasets used in  #TAUTHOR_TAG'],['employed the four domains datasets used in  #TAUTHOR_TAG'],['employed the four domains datasets used in  #TAUTHOR_TAG'],"['employed the four domains datasets used in  #TAUTHOR_TAG to train and test the all - in one and the ensemble classifiers.', 'we also replicated the in - domain results of these four datasets using our maximum entropy classifier.', 'we compare the results of the all - in - one and the ensemble classifier to the scl and its variation scl - mi adaptation techniques using the four datasets used to evaluate scl and scl - mi in  #TAUTHOR_TAG.', ""note that the results published in blitzer's work represent pairwise domain - adaptation, while our ensemble and all - in - one results are based on training on three of blitzer's domains and testing on the held - out fourth domain."", 'this makes it impossible to draw a direct comparison, but we can still observe that in general, it is best to simply combine as many domains as possible in an all - in - one or ensemble approach as compared to carefully adapting a single domain.', 'table 2 summarizes the results of the comparison.', 'where is the transfer error defined as the test error obtained by a method trained on the source domain s and tested on the target domain t.', '']",5
['datasets in  #TAUTHOR_TAG and that the'],['datasets in  #TAUTHOR_TAG and that the all - in - one'],['in  #TAUTHOR_TAG and that the all - in - one approach achieves comparable results in'],"[""results in the previous section indicate that both the all - in - one and the ensemble approaches exceed both daume's domain adaptation technique on the 27 datasets ( given our current implementation of daume's approach ) and scl on the four datasets in  #TAUTHOR_TAG and that the all - in - one approach achieves comparable results in terms of transfer ratio to  #AUTHOR_TAG."", 'the ensemble approach exceeds the all - in - one in some domains like apparel and automotive.', 'they both are very close in some domains like when comparing the all - in - one and the ensemble approaches on the four datasets in  #TAUTHOR_TAG, the all - in - one exceeds the ensemble only in the dvd domain.', 'the ensemble exceeds the all - in - one in electronics and kitchen & housewares.', 'they both perform at the same accuracy level on the books domain.', ""we have also employed ncnemar significance test between pairs of the all - in - one, the ensemble and daume's approaches on the 27 domains."", ""table 4 shows the significance difference between the approaches'combinations."", 'finally, we would like to do some initial exploration of the role of features across domains.', '']",5
"[' #TAUTHOR_TAG employ the structural', 'correspondence']","[' #TAUTHOR_TAG employ the structural', 'correspondence']","['power of the feature augmentation approach.  #TAUTHOR_TAG employ the structural', 'correspondence learning ( sc']","['', 'm of the set of vectors vs created in step 1. c ) we employ either logistic regression or svm. we have used', 'svm light implementation 3 to train the ensemble using svm with the matrix m', 'and a radial basis kernel function. for k source domains, the augmented feature space consists of k + 1 copies of the original feature space. however,', 'creating three versions of each feature in both the source and the', ""target domains grows the feature space exponentially, which is prohibitive in a many - domain adaptation scenario such as ours which consists of a total of 27 domains. we addressed this challenge by considering the 26 source domains as a single source domain being adapted to the target domain. this setup along with feature reduction enabled us to apply daume's approach without too much of an inflation of the feature"", 'space. however, we also recognize that this likely compromises the power of the feature augmentation approach.  #TAUTHOR_TAG employ the structural', 'correspondence learning ( scl ) algorithm for sentiment domain adaptation. blitzer et al. evaluate the scl', 'domain adaptation on four publicly released datasets from amazon product reviews : books, dvds, electronics and kitchen appliances. in these four datasets, reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative', ', and the rest discarded because their polarity was ambiguous. 1000 positive and 1000 negative labeled examples were used for each', 'domain. some unlabeled data were additionally used including 3685 ( dvds ) and 5945 ( kitchen ). each labeled dataset', 'was split into 1600 instances for training and 400 instances for testing. the baseline in  #TAUTHOR_TAG is', 'a linear classifier trained without adaptation, while their ceiling reference is the same as ours', ', which is the in - domain classifier trained and tested on the same domain']",3
['datasets in  #TAUTHOR_TAG and that the'],['datasets in  #TAUTHOR_TAG and that the all - in - one'],['in  #TAUTHOR_TAG and that the all - in - one approach achieves comparable results in'],"[""results in the previous section indicate that both the all - in - one and the ensemble approaches exceed both daume's domain adaptation technique on the 27 datasets ( given our current implementation of daume's approach ) and scl on the four datasets in  #TAUTHOR_TAG and that the all - in - one approach achieves comparable results in terms of transfer ratio to  #AUTHOR_TAG."", 'the ensemble approach exceeds the all - in - one in some domains like apparel and automotive.', 'they both are very close in some domains like when comparing the all - in - one and the ensemble approaches on the four datasets in  #TAUTHOR_TAG, the all - in - one exceeds the ensemble only in the dvd domain.', 'the ensemble exceeds the all - in - one in electronics and kitchen & housewares.', 'they both perform at the same accuracy level on the books domain.', ""we have also employed ncnemar significance test between pairs of the all - in - one, the ensemble and daume's approaches on the 27 domains."", ""table 4 shows the significance difference between the approaches'combinations."", 'finally, we would like to do some initial exploration of the role of features across domains.', '']",4
"['proposed in  #TAUTHOR_TAG, in which']","['proposed in  #TAUTHOR_TAG, in which']","['to neural data - to - text generation we proposed in  #TAUTHOR_TAG, in which the generation process is divided into a text - planning stage followed by a plan - realization stage.', 'we suggest four extensions to that framework : ( 1']","['follow the step - by - step approach to neural data - to - text generation we proposed in  #TAUTHOR_TAG, in which the generation process is divided into a text - planning stage followed by a plan - realization stage.', ""we suggest four extensions to that framework : ( 1 ) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than  #TAUTHOR_TAG's ability to deal with unseen relations and entities ; ( 3 ) we introduce a verification - by - reranking stage that substantially improves the faithfulness of the resulting texts ; ( 4 ) we incorporate a simple but effective referring expression generation module."", 'these extensions result in a generation process that is faster, more fluent, and more accurate']",5
[' #TAUTHOR_TAG proposed to adopt ideas'],[' #TAUTHOR_TAG proposed to adopt ideas'],"['', 'in recent work  #TAUTHOR_TAG proposed to adopt ideas']","['the data - to - text generation task ( d2t ), the input is data encoding facts ( e. g., a table, a set of tuples, or a small knowledge graph ), and the output is a natural language text representing those facts.', '1 in neural d2t, the common approaches train a neural end - to - end encoder - decoder system that encodes the input data and decodes an output text.', 'in recent work  #TAUTHOR_TAG proposed to adopt ideas from "" traditional "" language generation approaches ( i. e.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text.', 'we show that by breaking the task this way, one can achieve the same fluency of neural generation systems while being able to better control the form of the generated text and to improve its correctness by reducing missing facts and "" hallucinations "", common in neural systems.', 'in this work we adopt the step - by - step framework of  #TAUTHOR_TAG and propose four independent extensions that improve aspects of our original system : we suggest a new plan generation mechanism, based on a trainable - yetverifiable neural decoder, that is orders of magnitude faster than the original one ( § 3 ) ; we use knowledge of the plan structure to add typing information to plan elements.', ""this improves the system's performance on unseen relations and entities ( § 4 ) ; the separation of planning from realizations allows the incorporation of a simple output verification heuristic that drastically improves the correctness of the output ( § 5 ) ; and finally we incorporate a post - processing referring expression generation ( reg ) component, as proposed but not implemented in our previous work, to improve the naturalness of the resulting output ( § 6 )""]",5
['procedure of  #TAUTHOR_TAG :'],['procedure of  #TAUTHOR_TAG :'],['plan procedure of  #TAUTHOR_TAG : the linearized plan is generated incremental'],[' #TAUTHOR_TAG'],5
"['repeat the coverage experiment in  #TAUTHOR_TAG, counting the']","['repeat the coverage experiment in  #TAUTHOR_TAG, counting the']","['repeat the coverage experiment in  #TAUTHOR_TAG, counting the number of output texts that contain all the entities in the input graph, and, of these text, counting the ones in which the entities appear in the exact same order as the plan.', 'incorporating typing information reduced']","['repeat the coverage experiment in  #TAUTHOR_TAG, counting the number of output texts that contain all the entities in the input graph, and, of these text, counting the ones in which the entities appear in the exact same order as the plan.', 'incorporating typing information reduced the number of texts not containing all entities by 18 % for the seen part of the test set, and 16 % for the unseen part.', ""moreover, for the text containing all entities, the number of texts that did not follow the plan's entity order is reduced by 46 % for the seen part of the test set, and by 35 % for the unseen part."", 'we also observe a small drop in bleu scores, which we attribute to some relations being verbalized more freely ( though correctly )']",5
"['in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong']","['in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong']","[', reinforcing that automatic metrics are not sensitive enough to output accuracy.', 'we thus performed manual analysis, following the procedure in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong']","['addition of output verification resulted in negligible changes in bleu, reinforcing that automatic metrics are not sensitive enough to output accuracy.', 'we thus performed manual analysis, following the procedure in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over - generated ( hallucinated ) facts.', '5 we compare to the  #TAUTHOR_TAG.', 'results in table 1 indicate that the effectiveness of the verification process in ensuring correct output, reducing the already small number of ommited and overgenerated facts to 0 ( with the exhaustive planner ) and keeping it small ( with the fast neural planner )']",5
[' #TAUTHOR_TAG and extend'],[' #TAUTHOR_TAG and extend'],[' #TAUTHOR_TAG and extend'],"['adopt the planning - based neural generation framework of  #TAUTHOR_TAG and extend it to be orders of magnitude faster and produce more correct and more fluent text.', 'we conclude that these extensions not only improve  #TAUTHOR_TAG but also highlight the flexibility and advantages of the step - by - step framework for text generation']",5
[' #TAUTHOR_TAG proposed to adopt ideas'],[' #TAUTHOR_TAG proposed to adopt ideas'],"['', 'in recent work  #TAUTHOR_TAG proposed to adopt ideas']","['the data - to - text generation task ( d2t ), the input is data encoding facts ( e. g., a table, a set of tuples, or a small knowledge graph ), and the output is a natural language text representing those facts.', '1 in neural d2t, the common approaches train a neural end - to - end encoder - decoder system that encodes the input data and decodes an output text.', 'in recent work  #TAUTHOR_TAG proposed to adopt ideas from "" traditional "" language generation approaches ( i. e.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text.', 'we show that by breaking the task this way, one can achieve the same fluency of neural generation systems while being able to better control the form of the generated text and to improve its correctness by reducing missing facts and "" hallucinations "", common in neural systems.', 'in this work we adopt the step - by - step framework of  #TAUTHOR_TAG and propose four independent extensions that improve aspects of our original system : we suggest a new plan generation mechanism, based on a trainable - yetverifiable neural decoder, that is orders of magnitude faster than the original one ( § 3 ) ; we use knowledge of the plan structure to add typing information to plan elements.', ""this improves the system's performance on unseen relations and entities ( § 4 ) ; the separation of planning from realizations allows the incorporation of a simple output verification heuristic that drastically improves the correctness of the output ( § 5 ) ; and finally we incorporate a post - processing referring expression generation ( reg ) component, as proposed but not implemented in our previous work, to improve the naturalness of the resulting output ( § 6 )""]",0
['procedure of  #TAUTHOR_TAG :'],['procedure of  #TAUTHOR_TAG :'],['plan procedure of  #TAUTHOR_TAG : the linearized plan is generated incremental'],[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, the sentence plan trees were']","[' #TAUTHOR_TAG, the sentence plan trees were']","[' #TAUTHOR_TAG, the sentence plan trees were linearized into strings']","[' #TAUTHOR_TAG, the sentence plan trees were linearized into strings that were then fed to a neural machine translation decoder ( open - nmt )  #AUTHOR_TAG with a copy mecha - nism.', 'this linearization process is lossy, in the sense that the linearized strings do not explicitly distinguish between symbols that represent entities ( e. g., barack obama ) and symbols that represent relations ( e. g., works - for ).', 'while this information can be deduced from the position of the symbol within the structure, there is a benefit in making it more explicit.', 'in particular, the decoder needs to act differently when decoding relations and entities : entities are copied, while relations need to be verbalized.', 'by making the typing information explicit to the decoder, we make it easier for it to generalize this behavior distinction and apply it also for unseen entities and relations.', 'we thus expect the typing information to be especially useful for the unseen part of the evaluation set.', 'we incorporate typing information by concatenating to the embedding vector of each input symbol one of three embedding vectors, s, e or r, where s is concatenated to structural elements ( opening and closing brackets ), e to entity symbols and r to relation symbols']",0
"['these issues greatly,  #TAUTHOR_TAG still has 2']","['these issues greatly,  #TAUTHOR_TAG still has 2 % errors of these kinds.', 'existing approaches : soft encouragement via neural modules.', 'recent']","['these issues greatly,  #TAUTHOR_TAG still has 2 % errors of these kinds.', 'existing approaches : soft encouragement via neural modules.', 'recent work in neural text generation']","['the plan generation stage is guaranteed to be faithful to the input, the translation process from plans to text is based on a neural seq2seq model and may suffer from known issues with such models : hallucinating facts that do not exist in the input, repeating facts, or dropping facts.', 'while the clear mapping between plans and text helps to reduce these issues greatly,  #TAUTHOR_TAG still has 2 % errors of these kinds.', 'existing approaches : soft encouragement via neural modules.', 'recent work in neural text generation and summarization attempt to address these issues by trying to map the textual outputs back to structured predicates, and comparing these predicates to the input data.', ' #AUTHOR_TAG uses a neural checklist model to avoid the repetition of facts and improve coverage.', '']",0
"['', ' #TAUTHOR_TAG suggests the possibility of handling this with a']","['', ' #TAUTHOR_TAG suggests the possibility of handling this with a post - processing']","['', ' #TAUTHOR_TAG suggests the possibility of handling this with']","['', ' #TAUTHOR_TAG suggests the possibility of handling this with a post - processing referring - expression generation step ( reg ).', 'here, we propose a concrete reg module and demonstrate its effectiveness.', 'one option is to use a supervised reg module  #AUTHOR_TAG, that is trained to lexicalize in - context mentions.', 'such an approach is suboptimal for our setup as it is restricted to the entities and contexts it seen in training, and is prone to error on unseen entities and contexts.', '']",0
"['step - by - step system.', 'see  #TAUTHOR_TAG for further details.', '']","['step - by - step system.', 'see  #TAUTHOR_TAG for further details.', '']","['provide a brief overview of the step - by - step system.', 'see  #TAUTHOR_TAG for further details.', 'the system works in two stages.', 'the first stage ( planning )']","['provide a brief overview of the step - by - step system.', 'see  #TAUTHOR_TAG for further details.', 'the system works in two stages.', 'the first stage ( planning ) maps the input facts ( encoded as a directed, labeled graph, where nodes represent entities and edges represent relations ) to text plans, while the second stage ( realization ) maps the text plans to natural language text.', 'the text plans are a sequence of sentence plans - each of which is a tree - representing the ordering of facts and entities within the sentence.', 'in other words, the plans determine the separation of facts into sentences, the ordering of sentences, and the ordering of facts and entities within each sentence.', 'this stage is completely verifiable : the text plans are guaranteed to faithfully encode all and only the facts from the input.', 'the realization stage then translates the plans into natural language sentences, using a neural sequenceto - sequence system, resulting in fluent output']",7
"['these issues greatly,  #TAUTHOR_TAG still has 2']","['these issues greatly,  #TAUTHOR_TAG still has 2 % errors of these kinds.', 'existing approaches : soft encouragement via neural modules.', 'recent']","['these issues greatly,  #TAUTHOR_TAG still has 2 % errors of these kinds.', 'existing approaches : soft encouragement via neural modules.', 'recent work in neural text generation']","['the plan generation stage is guaranteed to be faithful to the input, the translation process from plans to text is based on a neural seq2seq model and may suffer from known issues with such models : hallucinating facts that do not exist in the input, repeating facts, or dropping facts.', 'while the clear mapping between plans and text helps to reduce these issues greatly,  #TAUTHOR_TAG still has 2 % errors of these kinds.', 'existing approaches : soft encouragement via neural modules.', 'recent work in neural text generation and summarization attempt to address these issues by trying to map the textual outputs back to structured predicates, and comparing these predicates to the input data.', ' #AUTHOR_TAG uses a neural checklist model to avoid the repetition of facts and improve coverage.', '']",7
"['- bystep systems described in  #TAUTHOR_TAG, which are state of the']","['step - bystep systems described in  #TAUTHOR_TAG, which are state of the art.', 'due to randomness inherent in neural training, our reported automatic evaluation measures are based on an average of 5 training runs of']","['evaluate on the webnlg dataset  #AUTHOR_TAG, comparing to the step - bystep systems described in  #TAUTHOR_TAG, which are state of the art.', 'due to randomness inherent in neural training, our reported automatic evaluation measures are based on an average of 5 training runs of each system ( neural planner and neural realizer ), each run with a different random seed']","['evaluate on the webnlg dataset  #AUTHOR_TAG, comparing to the step - bystep systems described in  #TAUTHOR_TAG, which are state of the art.', 'due to randomness inherent in neural training, our reported automatic evaluation measures are based on an average of 5 training runs of each system ( neural planner and neural realizer ), each run with a different random seed']",7
"['compare the exhaustive  #TAUTHOR_TAG.', 'moving to']","['compare the exhaustive  #TAUTHOR_TAG.', 'moving to']","['compare the exhaustive  #TAUTHOR_TAG.', 'moving to the neural planner exhibits a small drop in bleu (']","['compare the exhaustive  #TAUTHOR_TAG.', 'moving to the neural planner exhibits a small drop in bleu ( 46. 882 dropped to 46. 506 ).', 'however, figure 1 indicates 5 orders of magnitude ( 100, 000x ) speedup for graphs with 7 edges, and a linear growth in time for number of edges compared to exponential time for the exhaustive planner']",7
"['in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong']","['in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong']","[', reinforcing that automatic metrics are not sensitive enough to output accuracy.', 'we thus performed manual analysis, following the procedure in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong']","['addition of output verification resulted in negligible changes in bleu, reinforcing that automatic metrics are not sensitive enough to output accuracy.', 'we thus performed manual analysis, following the procedure in  #TAUTHOR_TAG.', 'we manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over - generated ( hallucinated ) facts.', '5 we compare to the  #TAUTHOR_TAG.', 'results in table 1 indicate that the effectiveness of the verification process in ensuring correct output, reducing the already small number of ommited and overgenerated facts to 0 ( with the exhaustive planner ) and keeping it small ( with the fast neural planner )']",7
['procedure of  #TAUTHOR_TAG :'],['procedure of  #TAUTHOR_TAG :'],['plan procedure of  #TAUTHOR_TAG : the linearized plan is generated incremental'],[' #TAUTHOR_TAG'],4
[' #TAUTHOR_TAG and extend'],[' #TAUTHOR_TAG and extend'],[' #TAUTHOR_TAG and extend'],"['adopt the planning - based neural generation framework of  #TAUTHOR_TAG and extend it to be orders of magnitude faster and produce more correct and more fluent text.', 'we conclude that these extensions not only improve  #TAUTHOR_TAG but also highlight the flexibility and advantages of the step - by - step framework for text generation']",4
[') decoding  #TAUTHOR_TAG is promising decoding algorithm for hierarchical phrase - based'],['( lr ) decoding  #TAUTHOR_TAG is promising decoding algorithm for hierarchical phrase - based'],[') decoding  #TAUTHOR_TAG is promising decoding algorithm for hierarchical phrase - based translation ('],"['- to - right ( lr ) decoding  #TAUTHOR_TAG is promising decoding algorithm for hierarchical phrase - based translation ( hiero ) that visits input spans in arbitrary order producing the output translation in left to right order.', 'this leads to far fewer language model calls, but while lr decoding is more efficient than cky decoding, it is unable to capture some hierarchical phrase alignments reachable using cky decoding and suffers from lower translation quality as a result.', 'this paper introduces two improvements to lr decoding that make it comparable in translation quality to cky - based hiero']",0
"['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed']","['phrase - based translation ( hiero )  #AUTHOR_TAG uses a lexicalized synchronous context - free grammar ( scfg ) extracted from word and phrase alignments of a bitext.', 'decoding for hiero is typically done with cky - style decoding with time complexity o ( n 3 ) for source input with n words.', 'computing the language model score for each hypothesis within cky decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside - out from sub - spans  #AUTHOR_TAG.', 'lr - decoding algorithms exist for phrasebased  #AUTHOR_TAG and syntax - based  #AUTHOR_TAG models and also for hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed left - toright ( lr ) decoding for hiero ( lr - hiero henceforth ) which uses beam search and runs in o ( n 2 b ) in practice where n is the length of source sentence and b is the size of beam  #AUTHOR_TAG.', 'to simplify target generation, scfg rules are constrained to be prefix - lexicalized on target side aka griebach normal form ( gnf ).', 'throughout this paper we abuse the notation for simplicity and use the term gnf grammars for such scfgs.', '']",0
"['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed']","['phrase - based translation ( hiero )  #AUTHOR_TAG uses a lexicalized synchronous context - free grammar ( scfg ) extracted from word and phrase alignments of a bitext.', 'decoding for hiero is typically done with cky - style decoding with time complexity o ( n 3 ) for source input with n words.', 'computing the language model score for each hypothesis within cky decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside - out from sub - spans  #AUTHOR_TAG.', 'lr - decoding algorithms exist for phrasebased  #AUTHOR_TAG and syntax - based  #AUTHOR_TAG models and also for hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #TAUTHOR_TAG first proposed left - toright ( lr ) decoding for hiero ( lr - hiero henceforth ) which uses beam search and runs in o ( n 2 b ) in practice where n is the length of source sentence and b is the size of beam  #AUTHOR_TAG.', 'to simplify target generation, scfg rules are constrained to be prefix - lexicalized on target side aka griebach normal form ( gnf ).', 'throughout this paper we abuse the notation for simplicity and use the term gnf grammars for such scfgs.', '']",0
['with beam search  #TAUTHOR_TAG'],['with beam search  #TAUTHOR_TAG'],['with beam search  #TAUTHOR_TAG does not'],[' #TAUTHOR_TAG'],0
"['algorithm in  #TAUTHOR_TAG.', 'lr - hier']","['algorithm in  #TAUTHOR_TAG.', 'lr - hiero decoding uses a top - down depth - first search, which strictly grows the hypotheses in target surface ordering.', 'search on']","['using the rule extraction algorithm in  #TAUTHOR_TAG.', 'lr - hiero decoding uses a top - down depth - first search, which strictly grows the hypotheses in target surface ordering.', 'search on the source side follows an earley - style search  #AUTHOR_TAG, the dot jumps around on the source side of the rules']","['- hiero uses a constrained lexicalized scfg which we call a gnf grammar : x → γ, b β where γ is a string of non - terminal and terminal symbols, b is a string of terminal symbols and β is a possibly empty sequence of non - terminals.', 'this ensures that as each rule is used in a derivation, add h to hyplist 29 :', 'return hyplist the target string is generated from left to right.', 'the rules are obtained from a word and phrase aligned bitext using the rule extraction algorithm in  #TAUTHOR_TAG.', 'lr - hiero decoding uses a top - down depth - first search, which strictly grows the hypotheses in target surface ordering.', 'search on the source side follows an earley - style search  #AUTHOR_TAG, the dot jumps around on the source side of the rules based on the order of nonterminals on the target side.', 'this search is integrated with beam search or cube pruning to find the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #AUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']",5
,,,,5
['with beam search  #TAUTHOR_TAG'],['with beam search  #TAUTHOR_TAG'],['with beam search  #TAUTHOR_TAG does not'],[' #TAUTHOR_TAG'],5
,,,,5
,,,,5
,,,,5
,,,,5
['nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types'],['nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types'],['machine translation studies on unsupervised methods have been conducted for both nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types'],"['machine translation studies on unsupervised methods have been conducted for both nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types for which our best system corrected errors well or mostly did not correct on the dev data.', 'top2 denotes the top two errors, and bottom2 denotes the lowest two errors in terms of the f 0. 5 10.', 'this study, we apply the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'gec with nmt / smt several studies that introduce sequence - to - sequence models in gec heavily rely on large amounts of training data.', ' #AUTHOR_TAG, who presented state - of - the - art results in gec, proposed a supervised nmt method trained on corpora of a total 5. 4 m sentence pairs.', 'we mainly use the monolingual corpus because the low resource track does not permit the use of the learner corpora.', 'despite the success of nmt, many studies on gec traditionally use smt  #AUTHOR_TAG junczys -  #AUTHOR_TAG.', 'these studies apply an offthe - shelf smt toolkit, moses, to gec.', 'junczys  #AUTHOR_TAG claimed that the smt system optimized for bleu learns to not change the source sentence.', 'instead of bleu, they proposed tuning an smt system using the m 2 score with annotated development data.', 'in this study, we also tune the weights with an f 0. 5 score measured by the m 2 scorer because the official score is an f 0. 5 score']",5
,,,,3
,,,,3
,,,,0
['nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types'],['nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types'],['machine translation studies on unsupervised methods have been conducted for both nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types'],"['machine translation studies on unsupervised methods have been conducted for both nmt  #AUTHOR_TAG and smt  #TAUTHOR_TAG table 4 : error types for which our best system corrected errors well or mostly did not correct on the dev data.', 'top2 denotes the top two errors, and bottom2 denotes the lowest two errors in terms of the f 0. 5 10.', 'this study, we apply the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'gec with nmt / smt several studies that introduce sequence - to - sequence models in gec heavily rely on large amounts of training data.', ' #AUTHOR_TAG, who presented state - of - the - art results in gec, proposed a supervised nmt method trained on corpora of a total 5. 4 m sentence pairs.', 'we mainly use the monolingual corpus because the low resource track does not permit the use of the learner corpora.', 'despite the success of nmt, many studies on gec traditionally use smt  #AUTHOR_TAG junczys -  #AUTHOR_TAG.', 'these studies apply an offthe - shelf smt toolkit, moses, to gec.', 'junczys  #AUTHOR_TAG claimed that the smt system optimized for bleu learns to not change the source sentence.', 'instead of bleu, they proposed tuning an smt system using the m 2 score with annotated development data.', 'in this study, we also tune the weights with an f 0. 5 score measured by the m 2 scorer because the official score is an f 0. 5 score']",0
,,,,6
"['', 'for example,  #TAUTHOR_TAG had to carefully discretiz']","['', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretize the real - valued pos tag score in']","['', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretiz']","['- based parsers  #AUTHOR_TAG are extremely popular because of their high accuracy and speed.', 'inspired by the greedy neural network transition - based parser of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG concurrently developed structured neural network parsers that use beam search and achieve state - of - the - art accuracies for english dependency parsing.', '1 while very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition - based framework : for example, all of these parsers use virtually identical atomic features and the arcstandard transition system.', ""in this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance : ( 1 ) a set - valued ( i. e., bag - of - words style ) feature for each word's morphological attributes, and ( 2 ) a weighted set - valued feature for each word's k - best pos tags."", 'these features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers.', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretize the real - valued pos tag score in order to combine it with the other discrete binary features in their system.', 'additionally, we also experiment with different transition systems, most notably the integrated parsing and part - of - speech ( pos ) tagging system of  #TAUTHOR_TAG and also the swap system of  #AUTHOR_TAG.', ""we evaluate our parser on the conll'09 shared task dependency treebanks, as well as on two english setups, achieving the best published numbers in many cases""]",0
"['', 'for example,  #TAUTHOR_TAG had to carefully discretiz']","['', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretize the real - valued pos tag score in']","['', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretiz']","['- based parsers  #AUTHOR_TAG are extremely popular because of their high accuracy and speed.', 'inspired by the greedy neural network transition - based parser of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG concurrently developed structured neural network parsers that use beam search and achieve state - of - the - art accuracies for english dependency parsing.', '1 while very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition - based framework : for example, all of these parsers use virtually identical atomic features and the arcstandard transition system.', ""in this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance : ( 1 ) a set - valued ( i. e., bag - of - words style ) feature for each word's morphological attributes, and ( 2 ) a weighted set - valued feature for each word's k - best pos tags."", 'these features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers.', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretize the real - valued pos tag score in order to combine it with the other discrete binary features in their system.', 'additionally, we also experiment with different transition systems, most notably the integrated parsing and part - of - speech ( pos ) tagging system of  #TAUTHOR_TAG and also the swap system of  #AUTHOR_TAG.', ""we evaluate our parser on the conll'09 shared task dependency treebanks, as well as on two english setups, achieving the best published numbers in many cases""]",3
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG,']","['the work of  #TAUTHOR_TAG, we embed the set']","['work using neural networks for dependency parsing has not ventured beyond the use of one - hot feature activations for each feature type - location pair.', 'in this work, we experiment with set - valued features, in which a set ( or bag ) of features for a given location fire at once, and are embedded into the same embedding space.', 'note that for both of the features we introduce, we extract features from the same 20 tokens as used in the tags and words features from  #AUTHOR_TAG, i. e. various locations on the stack and input buffer.', 'morphology.', 'it is well known that morphological information is very important for parsing morphologically rich languages ( see for example  #AUTHOR_TAG ).', 'we incorporate morphological information into our model using a setvalued feature function.', 'we define the feature group morph as the matrix x morph such that, for', 'where n f is the number of morphological features active on the token indexed by f.', 'in other words, we embed a bag of features into a shared embedding space by averaging the individual feature embeddings.', 'k - best tags.', 'the non - linear network models of  #AUTHOR_TAG and  #AUTHOR_TAG embed the 1 - best tag, according to a first - stage tagger, for a select set of tokens for any configuration.', 'inspired by the work of  #TAUTHOR_TAG, we embed the set of top tags according to a first - stage tagger.', 'specifically, we define the feature group ktags as the matrix x ktags such that, for', 'where p ( pos = v | f ) is the marginal probability that the token indexed by f has the tag indexed by v, according to the first - stage tagger.', 'results.', 'the contributions of our new features for pipelined arc - standard parsing are shown in table 1.', 'morphology features ( + morph ) contributed a labeled accuracy score ( las ) gain of 2. 9 % in czech, 1. 5 % in spanish, and 0. 9 % in catalan.', 'adding the k - best tag feature ( + morph + ktags ) provides modest gains ( and modest losses ), peaking at 0. 54 % las for spanish.', 'this feature proves more beneficial in the integrated transition system, discussed in the next section.', 'we note the ease with which we can obtain these gains in a multilayer embedding framework, without the need for any hand - tuning']",3
"['', 'for example,  #TAUTHOR_TAG had to carefully discretiz']","['', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretize the real - valued pos tag score in']","['', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretiz']","['- based parsers  #AUTHOR_TAG are extremely popular because of their high accuracy and speed.', 'inspired by the greedy neural network transition - based parser of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG concurrently developed structured neural network parsers that use beam search and achieve state - of - the - art accuracies for english dependency parsing.', '1 while very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition - based framework : for example, all of these parsers use virtually identical atomic features and the arcstandard transition system.', ""in this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance : ( 1 ) a set - valued ( i. e., bag - of - words style ) feature for each word's morphological attributes, and ( 2 ) a weighted set - valued feature for each word's k - best pos tags."", 'these features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers.', 'in contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking.', 'for example,  #TAUTHOR_TAG had to carefully discretize the real - valued pos tag score in order to combine it with the other discrete binary features in their system.', 'additionally, we also experiment with different transition systems, most notably the integrated parsing and part - of - speech ( pos ) tagging system of  #TAUTHOR_TAG and also the swap system of  #AUTHOR_TAG.', ""we evaluate our parser on the conll'09 shared task dependency treebanks, as well as on two english setups, achieving the best published numbers in many cases""]",5
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG,']","['the work of  #TAUTHOR_TAG, we embed the set']","['work using neural networks for dependency parsing has not ventured beyond the use of one - hot feature activations for each feature type - location pair.', 'in this work, we experiment with set - valued features, in which a set ( or bag ) of features for a given location fire at once, and are embedded into the same embedding space.', 'note that for both of the features we introduce, we extract features from the same 20 tokens as used in the tags and words features from  #AUTHOR_TAG, i. e. various locations on the stack and input buffer.', 'morphology.', 'it is well known that morphological information is very important for parsing morphologically rich languages ( see for example  #AUTHOR_TAG ).', 'we incorporate morphological information into our model using a setvalued feature function.', 'we define the feature group morph as the matrix x morph such that, for', 'where n f is the number of morphological features active on the token indexed by f.', 'in other words, we embed a bag of features into a shared embedding space by averaging the individual feature embeddings.', 'k - best tags.', 'the non - linear network models of  #AUTHOR_TAG and  #AUTHOR_TAG embed the 1 - best tag, according to a first - stage tagger, for a select set of tokens for any configuration.', 'inspired by the work of  #TAUTHOR_TAG, we embed the set of top tags according to a first - stage tagger.', 'specifically, we define the feature group ktags as the matrix x ktags such that, for', 'where p ( pos = v | f ) is the marginal probability that the token indexed by f has the tag indexed by v, according to the first - stage tagger.', 'results.', 'the contributions of our new features for pipelined arc - standard parsing are shown in table 1.', 'morphology features ( + morph ) contributed a labeled accuracy score ( las ) gain of 2. 9 % in czech, 1. 5 % in spanish, and 0. 9 % in catalan.', 'adding the k - best tag feature ( + morph + ktags ) provides modest gains ( and modest losses ), peaking at 0. 54 % las for spanish.', 'this feature proves more beneficial in the integrated transition system, discussed in the next section.', 'we note the ease with which we can obtain these gains in a multilayer embedding framework, without the need for any hand - tuning']",5
"['be obtained with more sophisticated transition systems that have a larger set of possible actions.', 'the integrated arc - standard transition system of  #TAUTHOR_TAG allows the parser to participate in tagging decisions, rather than being forced to treat']","['be obtained with more sophisticated transition systems that have a larger set of possible actions.', 'the integrated arc - standard transition system of  #TAUTHOR_TAG allows the parser to participate in tagging decisions, rather than being forced to treat']","['be obtained with more sophisticated transition systems that have a larger set of possible actions.', 'the integrated arc - standard transition system of  #TAUTHOR_TAG allows the parser to participate in tagging decisions, rather than being forced to treat']","['past work on neural network transitionbased parsing has focused exclusively on the arcstandard transition system, it is known that better results can often be obtained with more sophisticated transition systems that have a larger set of possible actions.', ""the integrated arc - standard transition system of  #TAUTHOR_TAG allows the parser to participate in tagging decisions, rather than being forced to treat the tagger's tags as given, as in the arc - standard system."", 'it does this by replacing the shift action in the arc - standard system with an action shift p, which, aside from shifting the top token on the buffer also assigns it one of the k best pos tags from a first - stage tagger.', 'we also experiment with the swap action of  #AUTHOR_TAG, which allows reordering of the tokens in the input sequence.', 'this transition system is able to produce non - projective parse trees, which is important for some languages.', 'results.', 'the effect of using the integrated transition system is quantified in the bottom part of table 1.', 'the use of both 1 ) + morph + kbest features and 2 ) integrated parsing and tagging achieves the best score for 5 out of 7 languages tested.', 'the use of integrated parsing and tagging provides, for example, a 0. 8 % las gain in german']",5
['deviate from  #TAUTHOR_TAG'],['deviate from  #TAUTHOR_TAG'],"['of experimentation, we deviate from  #TAUTHOR_TAG']","['train with predicted pos tags, we use a crfbased pos tagger to generate 5 - fold jack - knifed pos tags on the training set and predicted tags on the dev, test and tune sets ; our tagger gets comparable accuracy to the stanford pos tagger  #AUTHOR_TAG with 97. 44 % on the wsj test set.', 'the candidate tags allowed by the integrated transition system on every shift p action are chosen by taking the top 4 tags for a token according to the crf tagger, sorted by posterior probability, with no minimum posterior probability for a tag to be selected.', 'we report unlabeled attachment score ( uas ) and labeled attachment score ( las ).', 'whether punctuation is included in the evaluation is specified in each subsection.', 'we use 1024 units in all hidden layers, a choice made based on the development set.', 'we found network sizes to be of critical importance for the accuracy of our models.', ""for example, las improvements can be as high as 0. 98 % in conll'09 german when increasing the size of the two hidden layers from 200 to 1024."", 'we use b = 16 or b = 32 based on the development set performance per language.', 'for ease of experimentation, we deviate from  #TAUTHOR_TAG and use a single unstructured beam, rather than separate beams for pos tag and parse differences.', 'we train our neural networks on the standard training sets only, except for initializing with word embeddings generated by word2vec and using cluster features in our pos tagger.', ' #AUTHOR_TAG we train our model only on the treebank training set and do not use tri - training, which can likely further improve the results']",4
"['counterpart  #TAUTHOR_TAG + g + c can lead to even better results.', 'the improvements in pos tagging ( table 2 ) range from 0']","['counterpart  #TAUTHOR_TAG + g + c can lead to even better results.', 'the improvements in pos tagging ( table 2 ) range from 0. 3 % for english to 1. 4 % absolute']","['linear counterpart  #TAUTHOR_TAG + g + c can lead to even better results.', 'the improvements in pos tagging ( table 2 ) range from 0']","['', 'it is worth pointing out that  #AUTHOR_TAG is itself a neural net parser.', 'our models achieve higher labeled accuracy than the winning systems in the shared task in all languages.', 'additionally, our pipelined neural network parser always outperforms its linear counterpart, an in - house reimplementation of the system of  #AUTHOR_TAG, as well as the more recent and highly accurate parsers of zhang and mc  #AUTHOR_TAG and  #AUTHOR_TAG again outperforms its linear counterpart  #TAUTHOR_TAG + g + c can lead to even better results.', 'the improvements in pos tagging ( table 2 ) range from 0. 3 % for english to 1. 4 % absolute for chinese and are always higher for the neural network models compared to the linear models']",4
['of  #TAUTHOR_TAG on stanford dependencies uas by 0'],['of  #TAUTHOR_TAG on stanford dependencies uas by'],"['linear analog, the work of  #TAUTHOR_TAG on stanford dependencies uas by 0']","['experiment on english using the wall street journal ( wsj ) part of the penn treebank  #AUTHOR_TAG, with standard train / test splits.', 'we convert the constituency trees to stanford style dependencies  #AUTHOR_TAG using version 3. 3. 0 of the converter.', 'we use predicted pos tags and exclude punctuation from the evaluation, as is standard for english.', 'results.', 'the results shown in table 4, we find that our full model surpasses, to our knowledge, all previously reported supervised parsing models for the stanford dependency conversions.', 'it surpasses its linear analog, the work of  #TAUTHOR_TAG on stanford dependencies uas by 0. 9 % uas and by 1. 14 % las.', 'it also outperforms the pipeline neural net model of  #AUTHOR_TAG by a considerable margin and matches the semisupervised variant of  #AUTHOR_TAG']",4
"['rank pages by logistic regression and extra features like capitalization, sentence position and token matching.', 'keyword matching along with page - view statistics are used in  #AUTHOR_TAG.', 'ukp - athene  #TAUTHOR_TAG, the highest document retrieval scoring']","['rank pages by logistic regression and extra features like capitalization, sentence position and token matching.', 'keyword matching along with page - view statistics are used in  #AUTHOR_TAG.', 'ukp - athene  #TAUTHOR_TAG, the highest document retrieval scoring team, uses mediawiki api 1 to search the wikipedia']","[""with exact matching of the page titles with the claim's named entities."", 'the ucl team  #AUTHOR_TAG highlights the pages titles, and detect them in the claims.', 'they rank pages by logistic regression and extra features like capitalization, sentence position and token matching.', 'keyword matching along with page - view statistics are used in  #AUTHOR_TAG.', 'ukp - athene  #TAUTHOR_TAG, the highest document retrieval scoring']","['the fever benchmark  #AUTHOR_TAG, the drqa  #AUTHOR_TAG retrieval component is considered as the baseline.', 'they choose the k - nearest documents based on the cosine similarity of tf - idf feature vectors.', 'in addition to the drqa retrieval component, the sweeper team  #AUTHOR_TAG considers lexical and syntactic features for the claim and first two sentences in the pages.', ""the authors in  #AUTHOR_TAG use tf - idf along with exact matching of the page titles with the claim's named entities."", 'the ucl team  #AUTHOR_TAG highlights the pages titles, and detect them in the claims.', 'they rank pages by logistic regression and extra features like capitalization, sentence position and token matching.', 'keyword matching along with page - view statistics are used in  #AUTHOR_TAG.', 'ukp - athene  #TAUTHOR_TAG, the highest document retrieval scoring team, uses mediawiki api 1 to search the wikipedia database for the claims noun phrases']",0
"['##m )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG trains a logistic regression model on a heuristically set of features.', 'enhanced sequential inference model ( esim )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', '']","['##m )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', '']","['', 'the ucl team  #AUTHOR_TAG trains a logistic regression model on a heuristically set of features.', 'enhanced sequential inference model ( esim )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', 'esim encodes premises and hypotheses using one bidirectional long short - term memory ( bilstm ) with shared weights.', '']",0
"['##m )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG trains a logistic regression model on a heuristically set of features.', 'enhanced sequential inference model ( esim )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', '']","['##m )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', '']","['', 'the ucl team  #AUTHOR_TAG trains a logistic regression model on a heuristically set of features.', 'enhanced sequential inference model ( esim )  #AUTHOR_TAG with some small modifications has been used in  #TAUTHOR_TAG.', 'esim encodes premises and hypotheses using one bidirectional long short - term memory ( bilstm ) with shared weights.', '']",0
"['challenge participants  #TAUTHOR_TAG.', 'unc  #AUTHOR_TAG,']","['challenge participants  #TAUTHOR_TAG.', 'unc  #AUTHOR_TAG,']","['among the fever challenge participants  #TAUTHOR_TAG.', 'unc  #AUTHOR_TAG, the winner of the competition, proposes a modified esim']","['##posable attention ( da )  #AUTHOR_TAG, which compares and aggregates softaligned words in sentence pairs, is used in the fever benchmark paper  #AUTHOR_TAG.', 'the papelo team  #AUTHOR_TAG employs transformer networks with pre - trained weights  #AUTHOR_TAG.', 'esim has been widely used among the fever challenge participants  #TAUTHOR_TAG.', 'unc  #AUTHOR_TAG, the winner of the competition, proposes a modified esim that takes the concatenation of the retrieved evidence sentences and claim along with elmo embedding and three additional token - level features : word - net, number embedding, and semantic relatedness score from the document retrieval and sentence retrieval steps.', 'dream  #AUTHOR_TAG has the state of the art fever score.', 'the authors use a graph reasoning method based on xlnet  #AUTHOR_TAG and roberta  #AUTHOR_TAG, the two new bert variants that are supposed to provide better pre - trained embeddings']",0
"['.', 'following the ukp - athene promising document retrieval component  #TAUTHOR_TAG, which results in more than 93']","['retrieved.', 'following the ukp - athene promising document retrieval component  #TAUTHOR_TAG, which results in more than 93 % development set document recall, we exactly use their method to collect a set of top documents']","['.', 'following the ukp - athene promising document retrieval component  #TAUTHOR_TAG, which results in more than 93 % development set document recall, we exactly use their method to collect a set of top documents d c l top for the claim c l']","['the document retrieval step, the wikipedia documents containing the evidence supporting or refuting the claim are retrieved.', 'following the ukp - athene promising document retrieval component  #TAUTHOR_TAG, which results in more than 93 % development set document recall, we exactly use their method to collect a set of top documents d c l top for the claim c l']",5
"['in biasing on the claims with higher number of sentences.', 'in addition, we experiment with the modified hinge loss functions like  #TAUTHOR_TAG :', 'at testing time, for']","['in biasing on the claims with higher number of sentences.', 'in addition, we experiment with the modified hinge loss functions like  #TAUTHOR_TAG :', 'at testing time, for']","['the number of sentences per claim is significantly different and this difference might result in biasing on the claims with higher number of sentences.', 'in addition, we experiment with the modified hinge loss functions like  #TAUTHOR_TAG :', 'at testing time, for both pairwise loss functions, we sort the sentences by']","['the pairwise approach, a pair of positive and negative samples are compared against each other ( figure 4 ( right ) ).', 'we use the ranknet loss function  #AUTHOR_TAG :', 'where the mapping from the positive sample o pos and negative sample output o neg to probabilities are calculated using the softmax function p i = e opos−oneg / ( 1 + e opos−oneg ).', 'note that we do not force the positive and negative samples to be selected from the same claims because the number of sentences per claim is significantly different and this difference might result in biasing on the claims with higher number of sentences.', 'in addition, we experiment with the modified hinge loss functions like  #TAUTHOR_TAG :', 'at testing time, for both pairwise loss functions, we sort the sentences by their output value o and similarly choose s c l top for the claim c l']",5
['dream roberta scores  #TAUTHOR_TAG methods surpass'],['dream roberta scores  #TAUTHOR_TAG methods surpass'],['dream roberta scores  #TAUTHOR_TAG methods surpass the pairwise methods in'],"['the pairwise ranknet with hnm has the best recall score, we cannot conclude that pairwise methods are necessarily better for this task.', 'this is more clear in figure 5, which plots the recall - precision trade - off by applying a decision threshold on the output scores.', 'the pointwise model fever score ( % ) label accuracy ( % ) dream  #AUTHOR_TAG 70  #AUTHOR_TAG 64. 21 68. 21 ucl  #AUTHOR_TAG 62. 52 67. 62 ukp - athene  #AUTHOR_TAG 61. 58 65. 46 figure 5 : recall and precision results on the development set.', 'x shows the unc, ucl, upk - athene, dream xlnet, and dream roberta scores  #TAUTHOR_TAG methods surpass the pairwise methods in terms of recall - precision performance.', 'figure 5 also shows that hnm enhances both pairwise methods trained by the ranknet and hinge loss functions and preserves the pointwise performance.', 'in table 2, we compare the development set results of the state of the art methods with the bert model trained on different retrieved evidence sets.', '']",4
['dream roberta scores  #TAUTHOR_TAG methods surpass'],['dream roberta scores  #TAUTHOR_TAG methods surpass'],['dream roberta scores  #TAUTHOR_TAG methods surpass the pairwise methods in'],"['the pairwise ranknet with hnm has the best recall score, we cannot conclude that pairwise methods are necessarily better for this task.', 'this is more clear in figure 5, which plots the recall - precision trade - off by applying a decision threshold on the output scores.', 'the pointwise model fever score ( % ) label accuracy ( % ) dream  #AUTHOR_TAG 70  #AUTHOR_TAG 64. 21 68. 21 ucl  #AUTHOR_TAG 62. 52 67. 62 ukp - athene  #AUTHOR_TAG 61. 58 65. 46 figure 5 : recall and precision results on the development set.', 'x shows the unc, ucl, upk - athene, dream xlnet, and dream roberta scores  #TAUTHOR_TAG methods surpass the pairwise methods in terms of recall - precision performance.', 'figure 5 also shows that hnm enhances both pairwise methods trained by the ranknet and hinge loss functions and preserves the pointwise performance.', 'in table 2, we compare the development set results of the state of the art methods with the bert model trained on different retrieved evidence sets.', '']",4
['outperform  #TAUTHOR_TAG on'],['outperform  #TAUTHOR_TAG on'],['outperform  #TAUTHOR_TAG on'],"['', 'the extraction is more robust and complete than ad - hoc methods and maintained by a large community.', 'e. g., navigating the category hierarchy is much easier and reliable with dbpe - dia.', ""to summarize, eager's main contributions are"", '( 1 ) a novel gazetteer expansion algorithm that adds new entities from dbpedia.', 'eager adds entities that have several categories in common with the seed terms, addressing noisy categorizations through a sophisticated category pruning technique.', '( 2 ) eager also extracts categories from dbpe - dia abstracts using dependency analysis.', 'finally, eager extracts plural forms and synonyms from redirect information.', '( 3 ) for entity recognition, we integrate the gazetteer with a simple, but effective machine learning classifier, and experimentally show that the extended gazetteers improve the f 1 score between 7 % and 12 % over our baseline approach and outperform  #TAUTHOR_TAG on all learned concepts ( subject, location, temporal )']",4
"[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG,']","[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG,']","[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG, we reject']",[' #TAUTHOR_TAG'],4
"['impact of eager on entity recognition, we performed a large set of experiments ( on the archeology domain ).', 'the experiment domains and  #TAUTHOR_TAG, which we outperform']","['impact of eager on entity recognition, we performed a large set of experiments ( on the archeology domain ).', 'the experiment domains and  #TAUTHOR_TAG, which we outperform']","['evaluate the impact of eager on entity recognition, we performed a large set of experiments ( on the archeology domain ).', 'the experiment domains and  #TAUTHOR_TAG, which we outperform']","['evaluate the impact of eager on entity recognition, we performed a large set of experiments ( on the archeology domain ).', 'the experiment domains and  #TAUTHOR_TAG, which we outperform for all entity types, in some cases up to 5 % in f 1 score']",4
"['', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings )']","['category graph, and from redirection information.', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings ).', 'table 1 show']","['', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings ).', 'table 1 show the results of the comparison : ea - ger significantly']","['', 'for the latter, we consider full eager as described in section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information.', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings ).', 'table 1 show the results of the comparison : ea - ger significantly improves precision and recall over the baseline system and outperforms  #TAUTHOR_TAG in all cases.', 'furthermore, the impact of all three types of information ( dependencies from abstract, category, redirection ) of eager individually is quite notable with a slight disadvantage for category information.', 'however, in all cases the combination of all three types as proposed in eager shows a significant further increase in performance']",4
"[', the authors of  #TAUTHOR_TAG build an']","['without explicitly named concepts, arguing that consistent but anonymous labels are still useful.', 'most closely related to our own work, the authors of  #TAUTHOR_TAG build an']","['', 'most closely related to our own work, the authors of  #TAUTHOR_TAG build an approach solely on wikipedia which does not only exploit the article text']","['', 'as wordnet covers domain specific vocabularies only to a limited extent, this approach is also limited in its general applicability.', 'in ( toral and munoz, 2006 ), gazetteers are built from the noun phrases in the first sentences of wikipedia articles by mapping these phrases to wordnet and adding further terms found along the hypernymy relations.', 'the approach presented in  #AUTHOR_TAG relies solely on wikipedia, producing gazetteers without explicitly named concepts, arguing that consistent but anonymous labels are still useful.', 'most closely related to our own work, the authors of  #TAUTHOR_TAG build an approach solely on wikipedia which does not only exploit the article text but also analyzes the structural elements of wikipedia :', '']",3
"[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG,']","[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG,']","[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG, we reject']",[' #TAUTHOR_TAG'],3
"['dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor']","['dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor quality.', 'dbpedia improves little on that fact.', 'however, eager uses a sophisticated']","['dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor quality.', 'dbpedia improves little on that fact.', 'however, eager uses a sophisticated analysis of categories']","['addition to categories from the abstract analysis, we also use the category graph of dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor quality.', 'dbpedia improves little on that fact.', 'however, eager uses a sophisticated analysis of categories related to seed entities that allows us to prune most of the noise in the category graph.', 'biased towards precision over recall, section 4 shows that combined with the category extraction from abstracts it provides a significantly extended gazetteer without introducing substantial noise.', 'the fundamental contribution of eager is a category pruning based on finding a connected component in the graph of related categories that is supported by as many different entities from the seed list as possible.', 'figure 1 illustrates this further : from the articles for the seed entities, we compute ( line 12 ) the direct categories ( via subject edges ) and associate them to their seed entities e via cats ( e ).', 'we extent this set ( lines 13 - 14 ) with all categories in the k - neighbourhood ( here, we use k = 3 ), i. e., connected via up to k broader edges traversed in any direction, again maintaining via cats ( e ) which categories are reached from which seed entity e. in the resulting graph of all such categories, we identify the connected component with maximum support ( lines 15 - 19 ).', 'the support of a component is the sum of the support of its categories.', 'the support of a category c is the number of seed entities with c ∈ cats ( e ).', 'for figure 1, this yields the category graph of the blue and black categories of the figure.', 'the blue categories form the connected component with maximum support and are thus retained ( in p ), the black categories are dropped']",3
"[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG,']","[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG,']","[' #AUTHOR_TAG.', 'as  #TAUTHOR_TAG, we reject']",[' #TAUTHOR_TAG'],1
"['dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor']","['dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor quality.', 'dbpedia improves little on that fact.', 'however, eager uses a sophisticated']","['dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor quality.', 'dbpedia improves little on that fact.', 'however, eager uses a sophisticated analysis of categories']","['addition to categories from the abstract analysis, we also use the category graph of dbpedia.', 'it has been previously observed,  #TAUTHOR_TAG and  #AUTHOR_TAG, that the category graph of poor quality.', 'dbpedia improves little on that fact.', 'however, eager uses a sophisticated analysis of categories related to seed entities that allows us to prune most of the noise in the category graph.', 'biased towards precision over recall, section 4 shows that combined with the category extraction from abstracts it provides a significantly extended gazetteer without introducing substantial noise.', 'the fundamental contribution of eager is a category pruning based on finding a connected component in the graph of related categories that is supported by as many different entities from the seed list as possible.', 'figure 1 illustrates this further : from the articles for the seed entities, we compute ( line 12 ) the direct categories ( via subject edges ) and associate them to their seed entities e via cats ( e ).', 'we extent this set ( lines 13 - 14 ) with all categories in the k - neighbourhood ( here, we use k = 3 ), i. e., connected via up to k broader edges traversed in any direction, again maintaining via cats ( e ) which categories are reached from which seed entity e. in the resulting graph of all such categories, we identify the connected component with maximum support ( lines 15 - 19 ).', 'the support of a component is the sum of the support of its categories.', 'the support of a category c is the number of seed entities with c ∈ cats ( e ).', 'for figure 1, this yields the category graph of the blue and black categories of the figure.', 'the blue categories form the connected component with maximum support and are thus retained ( in p ), the black categories are dropped']",0
['setup as in  #TAUTHOR_TAG : a'],['setup as in  #TAUTHOR_TAG : a'],"['archaeological research ; subject ( sub ), temporal terms ( tem ), location ( loc ).', 'in this evaluation, we use the same setup as in  #TAUTHOR_TAG : a corpus of 30 full length uk archaeological reports archive']","['this experiment, we consider entity recognition in the domain of archaeology.', 'as part of this effort,  #AUTHOR_TAG identified three types of entities that are most useful for archaeological research ; subject ( sub ), temporal terms ( tem ), location ( loc ).', 'in this evaluation, we use the same setup as in  #TAUTHOR_TAG : a corpus of 30 full length uk archaeological reports archived by the arts and humanities data service ( ahds ).', 'the length of the documents varies from 4 to 120 pages.', 'the corpus is inter - annotated by three archaeologists']",5
"['', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings )']","['category graph, and from redirection information.', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings ).', 'table 1 show']","['', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings ).', 'table 1 show the results of the comparison : ea - ger significantly']","['', 'for the latter, we consider full eager as described in section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information.', 'finally, we also include the performance numbers report in  #TAUTHOR_TAG evaluation settings ).', 'table 1 show the results of the comparison : ea - ger significantly improves precision and recall over the baseline system and outperforms  #TAUTHOR_TAG in all cases.', 'furthermore, the impact of all three types of information ( dependencies from abstract, category, redirection ) of eager individually is quite notable with a slight disadvantage for category information.', 'however, in all cases the combination of all three types as proposed in eager shows a significant further increase in performance']",5
[')  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG for'],['( glmd )  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG for'],[')  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG for'],"['present results using support vector regression ( svr ) with rbf ( radial basis functions ) kernel ( smola and scholkopf, 2004 ) for sentence and document translation prediction tasks and global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd )  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG for word - level translation performance prediction.', 'we also use these learning models after a feature subset selection ( fs ) with recursive feature elimination ( rfe )  #AUTHOR_TAG or a dimensionality reduction and mapping step using partial least squares ( pls )  #AUTHOR_TAG, or pls after fs ( fs + pls ).', 'glm relies on viterbi decoding, perceptron learning, and flexible feature definitions.', 'glmd extends the glm framework by parallel perceptron training ( mc  #AUTHOR_TAG and dynamic learning with adaptive weight updates in the perceptron learning algorithm :', 'where φ returns a global representation for instance i and the weights are updated by α, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates.', 'the learning rate updates the weight values with weights in the range [ a, b ] using the following function taking error rate as the input :', 'learning rate curve for a = 0. 5 and b = 1. 0 is provided in figure 2']",5
"[';  #TAUTHOR_TAG.', '']","['( bicici, 2015 ;  #TAUTHOR_TAG.', 'maer is']","['( bicici, 2015 ;  #TAUTHOR_TAG.', '']","['use mean absolute error ( mae ), relative absolute error ( rae ), root mean squared error ( rmse ), and correlation ( r ) as well as relative mae ( maer ) and relative rae ( mraer ) to evaluate ( bicici, 2015 ;  #TAUTHOR_TAG.', 'maer is mean absolute error relative to the magnitude of the target and mraer is mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known ( bicici, 2015 2012 ) calculates the average quality difference between the top n−1 quartiles and the overall quality for the test set.', 'table 2 presents the training results for task 1 and task 3.', 'table 3 presents task 2 training results.', 'we refer to glmd parallelized over 4 splits as glmd s4 and glmd with 5 splits as glmd s5']",5
"['glmd model  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG,']","['glmd model  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG,']","['level translation quality task 2 is about binary classification of word - level quality.', 'we develop individual rtm models for each subtask and use glmd model  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG,']","['', 'we obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties.', 'rtms with fs followed by pls and learning with svr is able to achieve the top rank in this task.', 'task 2 : prediction of word - level translation quality task 2 is about binary classification of word - level quality.', 'we develop individual rtm models for each subtask and use glmd model  #TAUTHOR_TAG ; bicici and  #AUTHOR_TAG, for predicting the quality at the word - level.', 'the results on the test set are in table 5 where the ranks are out of about 17 submissions.', 'rtms with glmd becomes the second best system this task.', 'task 3 : predicting meteor of document translations task 3 is about predicting me - teor  #AUTHOR_TAG and their ranking.', 'the results on the test set are given in table 4 where the ranks are out of about 6 submissions using wf 1.', 'rtms achieve top rankings in this task.', 'table 5 : rtm - dcu task 2 results on the test set.', 'wf 1 is the average weighted f 1 score']",5
"['##13  #TAUTHOR_TAG.', 'the best results when predicting hter are']","['( bicici and  #AUTHOR_TAG, and qet13  #TAUTHOR_TAG.', 'the best results when predicting hter are']","['##13  #TAUTHOR_TAG.', 'the best results when predicting hter are obtained this year']","['compare the difficulty of tasks according to mraer levels achieved.', 'in table 6, we list the rtm test results for tasks and subtasks that predict hter or meteor from qet15, qet14 ( bicici and  #AUTHOR_TAG, and qet13  #TAUTHOR_TAG.', 'the best results when predicting hter are obtained this year']",5
['##13  #TAUTHOR_TAG'],['( bicici and  #AUTHOR_TAG and qet13  #TAUTHOR_TAG'],['##13  #TAUTHOR_TAG'],"['##tial translation machines achieve top performance in automatic, accurate, and language independent prediction of document -, sentence -, and word - level statistical machine translation ( smt ) performance.', 'rtms remove the need to access any smt system specific information or prior knowledge of the training data or models used when generating the translations.', 'rtms achieve top performance when predicting translation performance.', 'table 6 : test performance of the top individual rtm results when predicting hter or meteor also including results from qet14 ( bicici and  #AUTHOR_TAG and qet13  #TAUTHOR_TAG']",5
"['##ner model  #TAUTHOR_TAG.', 'it consists of a two - layer bi -']","['work is based on the summarunner model  #TAUTHOR_TAG.', 'it consists of a two - layer bi - directional']","['work is based on the summarunner model  #TAUTHOR_TAG.', 'it consists of']","['work is based on the summarunner model  #TAUTHOR_TAG.', 'it consists of a two - layer bi - directional gated recurrent unit ( gru ) recurrent neural network ( rnn ) which treats the summarization problem as a binary sequence classification problem, where each sentence is classified sequentially as sentence to be included or not in the summary.', 'however, we introduced two modifications to the original summarunner architecture, leading to better results while reducing complexity : arxiv : 1911. 06121v1 [ cs. cl ] 13 nov 2019 fig. 1.', 'our rnn - based sequence classifier ( based on [ 5 ] ).', 'all word embeddings from each sentence are averaged to generate a sentence embedding.', '']",6
"['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn']","['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn / daily mail corpus [ 2 ].', 'due']","['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn / daily mail corpus [ 2 ].', 'due']","['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn / daily mail corpus [ 2 ].', 'due to the limited number of provided news articles, we automatically annotated a large corpus of cnn articles from which an abstractive summary was available.', ""in a similar approach to  #TAUTHOR_TAG, we calculated the rouge - 1 f1 score between each sentence and its article's abstractive summary."", 'finally for each article, we sorted the sentences having the highest rouge - 1 f1 score and picked the top n = max ( 0. 1 * | | sentences | |, 3 ) sentences']",4
"['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn']","['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn / daily mail corpus [ 2 ].', 'due']","['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn / daily mail corpus [ 2 ].', 'due']","['contrast to  #TAUTHOR_TAG, we trained our model only on cnn articles from the cnn / daily mail corpus [ 2 ].', 'due to the limited number of provided news articles, we automatically annotated a large corpus of cnn articles from which an abstractive summary was available.', ""in a similar approach to  #TAUTHOR_TAG, we calculated the rouge - 1 f1 score between each sentence and its article's abstractive summary."", 'finally for each article, we sorted the sentences having the highest rouge - 1 f1 score and picked the top n = max ( 0. 1 * | | sentences | |, 3 ) sentences']",3
"['10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a']","['10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a']","['10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a focus on']","['capacity to understand and produce novel utterances by combining familiar primitives [ 5, 26 ]. the "" facebook slowly "" example', 'depends on knowledge of english, but people generalize compositionally in other domains too, such as learning novel commands and meanings in artificial languages [ 17 ]. a key challenge for cognitive science and artificial intelligence is to understand the computational underpinnings of human compositional learning and to build machines with similar capabilities. neural networks have long been criticized for lacking compositionality', ', leading critics to argue they are inappropriate for modeling language and thought [ 8, 23, 24 ]. nonetheless neural architectures have continued to advance and make important contributions in natural language', 'processing ( nlp ) [ 19 ]. recent work has revisited these classic critiques through studies of modern neural architectures [ 10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a focus on the sequence - to - sequence ( seq2seq ) models used successfully in machine translation and other', 'nlp tasks [ 32, 4, 36 ]. these studies show that powerful seq2seq approaches still have substantial', 'difficulties with compositional generalization, especially when combining a new concept ( "" to facebook "" ) with previous concepts ( "" slowly "" or "" eagerly "" ) [ 15, 3, 20 ]. new benchmarks have been proposed to encourage progress [ 10, 15, 2 ], including the scan dataset for compositional learning [', '15 ]. scan involves learning to follow instructions such as "" walk twice and look right "" by performing a sequence of appropriate output actions ; in this case, the correct response is to "" walk walk rturn look. "" a range of scan examples are shown', 'in table 1. seq2seq models are trained on thousands of instructions built compositionally from primitives ( "" look "", "" walk ""', ', "" run "", "" jump "", etc. ), modifiers ( "" twice "", "" around right, "" etc. ) and conjunctions ( "" and "" and "" after ""', '). after training, the aim is to execute, zero', '- shot, novel instructions such as "" walk around right after look twice. "" previous studies show that seq2seq recurrent neural networks ( rnn ) generalize well when the training and test sets are similar, but fail catastrophically when generalization requires systematic compositionality  #TAUTHOR_TAG 3, 20 ]. for instance, models often fail to understand', 'how to "" jump twice "" after learning how to "" run twice, "" "" walk twice, "" and how to "" jump. "" developing neural architectures with these compositional abilities remains an open problem']",0
"['10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a']","['10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a']","['10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a focus on']","['capacity to understand and produce novel utterances by combining familiar primitives [ 5, 26 ]. the "" facebook slowly "" example', 'depends on knowledge of english, but people generalize compositionally in other domains too, such as learning novel commands and meanings in artificial languages [ 17 ]. a key challenge for cognitive science and artificial intelligence is to understand the computational underpinnings of human compositional learning and to build machines with similar capabilities. neural networks have long been criticized for lacking compositionality', ', leading critics to argue they are inappropriate for modeling language and thought [ 8, 23, 24 ]. nonetheless neural architectures have continued to advance and make important contributions in natural language', 'processing ( nlp ) [ 19 ]. recent work has revisited these classic critiques through studies of modern neural architectures [ 10,  #TAUTHOR_TAG 3', ', 20, 22, 2, 6 ], with a focus on the sequence - to - sequence ( seq2seq ) models used successfully in machine translation and other', 'nlp tasks [ 32, 4, 36 ]. these studies show that powerful seq2seq approaches still have substantial', 'difficulties with compositional generalization, especially when combining a new concept ( "" to facebook "" ) with previous concepts ( "" slowly "" or "" eagerly "" ) [ 15, 3, 20 ]. new benchmarks have been proposed to encourage progress [ 10, 15, 2 ], including the scan dataset for compositional learning [', '15 ]. scan involves learning to follow instructions such as "" walk twice and look right "" by performing a sequence of appropriate output actions ; in this case, the correct response is to "" walk walk rturn look. "" a range of scan examples are shown', 'in table 1. seq2seq models are trained on thousands of instructions built compositionally from primitives ( "" look "", "" walk ""', ', "" run "", "" jump "", etc. ), modifiers ( "" twice "", "" around right, "" etc. ) and conjunctions ( "" and "" and "" after ""', '). after training, the aim is to execute, zero', '- shot, novel instructions such as "" walk around right after look twice. "" previous studies show that seq2seq recurrent neural networks ( rnn ) generalize well when the training and test sets are similar, but fail catastrophically when generalization requires systematic compositionality  #TAUTHOR_TAG 3, 20 ]. for instance, models often fail to understand', 'how to "" jump twice "" after learning how to "" run twice, "" "" walk twice, "" and how to "" jump. "" developing neural architectures with these compositional abilities remains an open problem']",0
"['test set  #TAUTHOR_TAG, standard seq2seq modeling completely']","['add jump', '"" test set  #TAUTHOR_TAG, standard seq2seq modeling completely']","['test set  #TAUTHOR_TAG, standard seq2seq modeling completely']","['', '"" test set  #TAUTHOR_TAG, standard seq2seq modeling completely fails to', '']",0
['longer sequences  #TAUTHOR_TAG'],['sequences  #TAUTHOR_TAG'],"['scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this']","[', meta seq2seq learns to treat the instruction as', 'a template "" x around right twice and y thrice "", where x and y are variables that can be filled arbitrarily. this approach is able to solve scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this way, meta seq2seq learners are several steps', 'closer to capturing the compositional abilities studied in synthetic learning tasks [ 17 ] and motivated in the "" to dax "" or "" to facebook "" thought experiments. meta seq2seq learning has implications for understanding', 'how people generalize compositionally. similarly to meta - training, people learn in dynamic rather than static environments, tackling a series of', 'changing learning problems rather than iterating repeatedly through a static dataset. there is natural pressure to generalize systematically after a single experience with a new verb like "" to facebook, "" and thus people are incentivized to generalize compositionally in ways that may resemble the meta - training loss introduced here. meta', 'learning is a powerful new toolbox for studying learning - to - learn and other elusive cognitive abilities [ 16, 35 ], although more work is needed to understand', 'its implications for cognitive science. the models studied here can learn variables that assign novel meanings to words at test time, using only the network dynamics and the external memory. although powerful, this is a limited concept of ""', 'variable "" since it requires familiarity with all of the possible input and output assignments', 'during meta - training. this limitation is shared by nearly all existing neural architectures [ 31, 11, 29 ] and shows that the meta seq2seq framework falls short', ""of addressing marcus's challenge of extrapolating outside the training space [ 23, 24, 22 ]. in future work, i intend to explore adding more symbolic machinery to the architecture [ 27 ] with the goal of handling"", 'genuinely new symbols. hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks  #TAUTHOR_TAG 3, 28 ] including meta seq2', '##seq learning. the meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [ 12 ] or', 'to graph traversal problems [ 11 ]. for traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that', 'alternates between standard training and meta - training to encourage compositional generalization. i am excited about the potential of the meta seq2seq approach both for solving practical problems and for illuminating the foundations of human compositional learning']",0
['longer sequences  #TAUTHOR_TAG'],['sequences  #TAUTHOR_TAG'],"['scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this']","[', meta seq2seq learns to treat the instruction as', 'a template "" x around right twice and y thrice "", where x and y are variables that can be filled arbitrarily. this approach is able to solve scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this way, meta seq2seq learners are several steps', 'closer to capturing the compositional abilities studied in synthetic learning tasks [ 17 ] and motivated in the "" to dax "" or "" to facebook "" thought experiments. meta seq2seq learning has implications for understanding', 'how people generalize compositionally. similarly to meta - training, people learn in dynamic rather than static environments, tackling a series of', 'changing learning problems rather than iterating repeatedly through a static dataset. there is natural pressure to generalize systematically after a single experience with a new verb like "" to facebook, "" and thus people are incentivized to generalize compositionally in ways that may resemble the meta - training loss introduced here. meta', 'learning is a powerful new toolbox for studying learning - to - learn and other elusive cognitive abilities [ 16, 35 ], although more work is needed to understand', 'its implications for cognitive science. the models studied here can learn variables that assign novel meanings to words at test time, using only the network dynamics and the external memory. although powerful, this is a limited concept of ""', 'variable "" since it requires familiarity with all of the possible input and output assignments', 'during meta - training. this limitation is shared by nearly all existing neural architectures [ 31, 11, 29 ] and shows that the meta seq2seq framework falls short', ""of addressing marcus's challenge of extrapolating outside the training space [ 23, 24, 22 ]. in future work, i intend to explore adding more symbolic machinery to the architecture [ 27 ] with the goal of handling"", 'genuinely new symbols. hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks  #TAUTHOR_TAG 3, 28 ] including meta seq2', '##seq learning. the meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [ 12 ] or', 'to graph traversal problems [ 11 ]. for traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that', 'alternates between standard training and meta - training to encourage compositional generalization. i am excited about the potential of the meta seq2seq approach both for solving practical problems and for illuminating the foundations of human compositional learning']",0
"['experiments in this paper.', 'the meta seq2seq architecture builds upon the seq2seq architecture from  #TAUTHOR_TAG that performed best across a range of scan evaluations.', 'the input and output sequence encoders are two -']","['experiments in this paper.', 'the meta seq2seq architecture builds upon the seq2seq architecture from  #TAUTHOR_TAG that performed best across a range of scan evaluations.', 'the input and output sequence encoders are two - layer bilstms with m = 200 hidden units per layer']","['1 architecture and training parameters i use a common architecture and training procedure for all experiments in this paper.', 'the meta seq2seq architecture builds upon the seq2seq architecture from  #TAUTHOR_TAG that performed best across a range of scan evaluations.', 'the input and output sequence encoders are two - layer bilstms with m = 200 hidden units per layer']","['. 1 architecture and training parameters i use a common architecture and training procedure for all experiments in this paper.', 'the meta seq2seq architecture builds upon the seq2seq architecture from  #TAUTHOR_TAG that performed best across a range of scan evaluations.', 'the input and output sequence encoders are two - layer bilstms with m = 200 hidden units per layer and produce m dimensional embeddings.', 'the output decoder is a two - layer lstm also with m = 200 hidden units per layer.', 'dropout is applied with probability 0. 5 to each symbol embedding and to each lstm.', ""a greedy decoder is used since it is effective on scan's deterministic outputs  #TAUTHOR_TAG."", 'in each experiment, the network is meta - trained for 10, 000 episodes with the adam optimizer [ 14 ].', 'halfway through training, the learning rate is reduced from 0. 001 to 0. 0001.', 'gradients with a l 2 - norm larger than 50 are clipped.', 'on the scan tasks, my meta seq2seq implementation takes less than an hour to train on a single nvidia titan x gpu.', 'for comparison, my seq2seq implementation takes less than 30 minutes.', 'all models were trained five times with different random initializations and random meta - training episodes']",6
"['test set  #TAUTHOR_TAG, standard seq2seq modeling completely']","['add jump', '"" test set  #TAUTHOR_TAG, standard seq2seq modeling completely']","['test set  #TAUTHOR_TAG, standard seq2seq modeling completely']","['', '"" test set  #TAUTHOR_TAG, standard seq2seq modeling completely fails to', '']",3
[' #TAUTHOR_TAG 99'],['( experiment 4. 3 and  #TAUTHOR_TAG 99'],[' #TAUTHOR_TAG 99 % accuracy during training ).'],"['', 'seq2seq learner takes advantage of the augmented training to generalize better than in standard scan training ( experiment 4. 3 and  #TAUTHOR_TAG 99 % accuracy during training ). the augmented task provides 23 fully compositional primitives during training, compared to the three in the original task. despite this salient compositionality, the basic seq2seq model is still largely', 'unable to make use of it. the lesion analyses show that the support loss is not critical in this setting', ', and the meta seq2', '##seq learner achieves 99. 48 % correct without it ( sd = 0. 37 ). in', 'contrast to experiment 4. 3, using many primitives more strongly guides the network to use the memory, since the network cannot substantially reduce the training loss without it. the decoder attention remains', 'critical in this setting, and the network attains merely 9. 29 % correct without it ( sd = 13. 07 ). these results demonstrate that only the full meta seq2seq learner is a satisfactory solution to both the learning problems in this experiment and the previous experiment ( table', '2 )']",4
['longer sequences  #TAUTHOR_TAG'],['sequences  #TAUTHOR_TAG'],"['scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this']","[', meta seq2seq learns to treat the instruction as', 'a template "" x around right twice and y thrice "", where x and y are variables that can be filled arbitrarily. this approach is able to solve scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this way, meta seq2seq learners are several steps', 'closer to capturing the compositional abilities studied in synthetic learning tasks [ 17 ] and motivated in the "" to dax "" or "" to facebook "" thought experiments. meta seq2seq learning has implications for understanding', 'how people generalize compositionally. similarly to meta - training, people learn in dynamic rather than static environments, tackling a series of', 'changing learning problems rather than iterating repeatedly through a static dataset. there is natural pressure to generalize systematically after a single experience with a new verb like "" to facebook, "" and thus people are incentivized to generalize compositionally in ways that may resemble the meta - training loss introduced here. meta', 'learning is a powerful new toolbox for studying learning - to - learn and other elusive cognitive abilities [ 16, 35 ], although more work is needed to understand', 'its implications for cognitive science. the models studied here can learn variables that assign novel meanings to words at test time, using only the network dynamics and the external memory. although powerful, this is a limited concept of ""', 'variable "" since it requires familiarity with all of the possible input and output assignments', 'during meta - training. this limitation is shared by nearly all existing neural architectures [ 31, 11, 29 ] and shows that the meta seq2seq framework falls short', ""of addressing marcus's challenge of extrapolating outside the training space [ 23, 24, 22 ]. in future work, i intend to explore adding more symbolic machinery to the architecture [ 27 ] with the goal of handling"", 'genuinely new symbols. hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks  #TAUTHOR_TAG 3, 28 ] including meta seq2', '##seq learning. the meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [ 12 ] or', 'to graph traversal problems [ 11 ]. for traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that', 'alternates between standard training and meta - training to encourage compositional generalization. i am excited about the potential of the meta seq2seq approach both for solving practical problems and for illuminating the foundations of human compositional learning']",4
['longer sequences  #TAUTHOR_TAG'],['sequences  #TAUTHOR_TAG'],"['scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this']","[', meta seq2seq learns to treat the instruction as', 'a template "" x around right twice and y thrice "", where x and y are variables that can be filled arbitrarily. this approach is able to solve scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this way, meta seq2seq learners are several steps', 'closer to capturing the compositional abilities studied in synthetic learning tasks [ 17 ] and motivated in the "" to dax "" or "" to facebook "" thought experiments. meta seq2seq learning has implications for understanding', 'how people generalize compositionally. similarly to meta - training, people learn in dynamic rather than static environments, tackling a series of', 'changing learning problems rather than iterating repeatedly through a static dataset. there is natural pressure to generalize systematically after a single experience with a new verb like "" to facebook, "" and thus people are incentivized to generalize compositionally in ways that may resemble the meta - training loss introduced here. meta', 'learning is a powerful new toolbox for studying learning - to - learn and other elusive cognitive abilities [ 16, 35 ], although more work is needed to understand', 'its implications for cognitive science. the models studied here can learn variables that assign novel meanings to words at test time, using only the network dynamics and the external memory. although powerful, this is a limited concept of ""', 'variable "" since it requires familiarity with all of the possible input and output assignments', 'during meta - training. this limitation is shared by nearly all existing neural architectures [ 31, 11, 29 ] and shows that the meta seq2seq framework falls short', ""of addressing marcus's challenge of extrapolating outside the training space [ 23, 24, 22 ]. in future work, i intend to explore adding more symbolic machinery to the architecture [ 27 ] with the goal of handling"", 'genuinely new symbols. hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks  #TAUTHOR_TAG 3, 28 ] including meta seq2', '##seq learning. the meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [ 12 ] or', 'to graph traversal problems [ 11 ]. for traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that', 'alternates between standard training and meta - training to encourage compositional generalization. i am excited about the potential of the meta seq2seq approach both for solving practical problems and for illuminating the foundations of human compositional learning']",1
['longer sequences  #TAUTHOR_TAG'],['sequences  #TAUTHOR_TAG'],"['scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this']","[', meta seq2seq learns to treat the instruction as', 'a template "" x around right twice and y thrice "", where x and y are variables that can be filled arbitrarily. this approach is able to solve scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this way, meta seq2seq learners are several steps', 'closer to capturing the compositional abilities studied in synthetic learning tasks [ 17 ] and motivated in the "" to dax "" or "" to facebook "" thought experiments. meta seq2seq learning has implications for understanding', 'how people generalize compositionally. similarly to meta - training, people learn in dynamic rather than static environments, tackling a series of', 'changing learning problems rather than iterating repeatedly through a static dataset. there is natural pressure to generalize systematically after a single experience with a new verb like "" to facebook, "" and thus people are incentivized to generalize compositionally in ways that may resemble the meta - training loss introduced here. meta', 'learning is a powerful new toolbox for studying learning - to - learn and other elusive cognitive abilities [ 16, 35 ], although more work is needed to understand', 'its implications for cognitive science. the models studied here can learn variables that assign novel meanings to words at test time, using only the network dynamics and the external memory. although powerful, this is a limited concept of ""', 'variable "" since it requires familiarity with all of the possible input and output assignments', 'during meta - training. this limitation is shared by nearly all existing neural architectures [ 31, 11, 29 ] and shows that the meta seq2seq framework falls short', ""of addressing marcus's challenge of extrapolating outside the training space [ 23, 24, 22 ]. in future work, i intend to explore adding more symbolic machinery to the architecture [ 27 ] with the goal of handling"", 'genuinely new symbols. hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks  #TAUTHOR_TAG 3, 28 ] including meta seq2', '##seq learning. the meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [ 12 ] or', 'to graph traversal problems [ 11 ]. for traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that', 'alternates between standard training and meta - training to encourage compositional generalization. i am excited about the potential of the meta seq2seq approach both for solving practical problems and for illuminating the foundations of human compositional learning']",7
['longer sequences  #TAUTHOR_TAG'],['sequences  #TAUTHOR_TAG'],"['scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this']","[', meta seq2seq learns to treat the instruction as', 'a template "" x around right twice and y thrice "", where x and y are variables that can be filled arbitrarily. this approach is able to solve scan tasks for compositional learning that have eluded standard nlp approaches, with the exception of generalizing to longer sequences  #TAUTHOR_TAG. in this way, meta seq2seq learners are several steps', 'closer to capturing the compositional abilities studied in synthetic learning tasks [ 17 ] and motivated in the "" to dax "" or "" to facebook "" thought experiments. meta seq2seq learning has implications for understanding', 'how people generalize compositionally. similarly to meta - training, people learn in dynamic rather than static environments, tackling a series of', 'changing learning problems rather than iterating repeatedly through a static dataset. there is natural pressure to generalize systematically after a single experience with a new verb like "" to facebook, "" and thus people are incentivized to generalize compositionally in ways that may resemble the meta - training loss introduced here. meta', 'learning is a powerful new toolbox for studying learning - to - learn and other elusive cognitive abilities [ 16, 35 ], although more work is needed to understand', 'its implications for cognitive science. the models studied here can learn variables that assign novel meanings to words at test time, using only the network dynamics and the external memory. although powerful, this is a limited concept of ""', 'variable "" since it requires familiarity with all of the possible input and output assignments', 'during meta - training. this limitation is shared by nearly all existing neural architectures [ 31, 11, 29 ] and shows that the meta seq2seq framework falls short', ""of addressing marcus's challenge of extrapolating outside the training space [ 23, 24, 22 ]. in future work, i intend to explore adding more symbolic machinery to the architecture [ 27 ] with the goal of handling"", 'genuinely new symbols. hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks  #TAUTHOR_TAG 3, 28 ] including meta seq2', '##seq learning. the meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [ 12 ] or', 'to graph traversal problems [ 11 ]. for traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that', 'alternates between standard training and meta - training to encourage compositional generalization. i am excited about the potential of the meta seq2seq approach both for solving practical problems and for illuminating the foundations of human compositional learning']",2
"['lexical level.', ' #TAUTHOR_TAG']","['lexical level.', ' #TAUTHOR_TAG']","['introduction several quantitative methods exist for thematically segmenting texts.', 'most of them are based on the following assumption : the thematic coherence of a text segment finds expression at the lexical level.', ' #TAUTHOR_TAG']","['introduction several quantitative methods exist for thematically segmenting texts.', 'most of them are based on the following assumption : the thematic coherence of a text segment finds expression at the lexical level.', ' #TAUTHOR_TAG and  #AUTHOR_TAG detect this coherence through patterns of lexical cooccurrence.', ' #AUTHOR_TAG and  #AUTHOR_TAG find topic boundaries in the texts by using lexical cohesion.', 'the first methods are applied to texts, such as expository texts, whose vocabulary is often very specific.', 'as a concept is always expressed by the same word, word repetitions are thematically significant in these texts.', 'the use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means.', 'however, this second approach requires knowledge about the cohesion between words.', ' #AUTHOR_TAG extract this knowledge from a thesaurus.', ' #AUTHOR_TAG exploits a lexical network built from a machine readable dictionary ( mrd ).', 'this article presents a method for thematically segmenting texts by using knowledge about lexical cohesion that has been automatically built.', 'this knowledge takes the form of a network of lexical collocations.', 'we claim that this network is as suitable as a thesaurus or a mrd for segmenting texts.', 'moreover, building it for a specific domain or for another language is quick']",0
"['first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in  #TAUTHOR_TAG.', 'the results of these tests are rather stable when parameters']","['first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in  #TAUTHOR_TAG.', 'the results of these tests are rather stable when parameters']","['first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in  #TAUTHOR_TAG.', 'the results of these tests are rather stable when parameters']","['first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in  #TAUTHOR_TAG.', 'the results of these tests are rather stable when parameters such as the size of the cohesion computing window or the size of the smoothing window are changed ( from 9 to 21 words ).', 'generally, the best results are obtained with a size of 19 words for the first window and 11 for the second one']",4
"['"" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', '']","['"" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', '']","['order to have a more objective evaluation, the method has been applied to the "" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', 'document breaks are supposed to be the boundaries that have the highest weights.', 'for the first from the corpus used']","['order to have a more objective evaluation, the method has been applied to the "" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', 'document breaks are supposed to be the boundaries that have the highest weights.', 'for the first from the corpus used for building the collocation network.', 'each text was 80 words long on average.', 'each boundary, which is a minimum of the cohesion graph, was weighted by the sum of the differences between its value and the values of the two maxima around it, as in  #TAUTHOR_TAG.', 'the match between a boundary and a document break was accepted if the boundary was no further than 9 words ( after pre - processing ).', ""globally, our results are not as good as hearst's ( with 44 texts ; nb : 10, p : 0. 8, r : 0. 19 ; nb : 70, p : 0. 59, r : 0. 95 )."", 'the first explanation for such a difference is the fact that the two methods do not apply to the same kind of texts.', 'hearst does not consider texts smaller than 10 sentences long.', 'all the texts of this evaluation are under this limit.', ""in fact, our method, as kozima's, is more convenient for closely tracking thematic evolutions than for detecting the major thematic shifts."", 'the second explanation for this difference is related to the way the document breaks are found, as shown by the precision values.', 'when nb increases, precision decreases as it generally does, but very slowly.', 'the decrease actually becomes significant only when nb becomes larger than n. it means that the weights associated to the boundaries are not very significant.', 'we have validated this hypothesis by changing the weighting policy of the boundaries without having significant changes in the results.', 'one way for increasing the performance would be to take as text boundary not the position of a minimum in the cohesion graph but the nearest sentence boundary from this position']",5
"['"" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', '']","['"" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', '']","['order to have a more objective evaluation, the method has been applied to the "" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', 'document breaks are supposed to be the boundaries that have the highest weights.', 'for the first from the corpus used']","['order to have a more objective evaluation, the method has been applied to the "" classical "" task of discovering boundaries between concatened texts.', 'results are shown in table 1.', 'as in  #TAUTHOR_TAG, boundaries found by the method are weighted and sorted in decreasing order.', 'document breaks are supposed to be the boundaries that have the highest weights.', 'for the first from the corpus used for building the collocation network.', 'each text was 80 words long on average.', 'each boundary, which is a minimum of the cohesion graph, was weighted by the sum of the differences between its value and the values of the two maxima around it, as in  #TAUTHOR_TAG.', 'the match between a boundary and a document break was accepted if the boundary was no further than 9 words ( after pre - processing ).', ""globally, our results are not as good as hearst's ( with 44 texts ; nb : 10, p : 0. 8, r : 0. 19 ; nb : 70, p : 0. 59, r : 0. 95 )."", 'the first explanation for such a difference is the fact that the two methods do not apply to the same kind of texts.', 'hearst does not consider texts smaller than 10 sentences long.', 'all the texts of this evaluation are under this limit.', ""in fact, our method, as kozima's, is more convenient for closely tracking thematic evolutions than for detecting the major thematic shifts."", 'the second explanation for this difference is related to the way the document breaks are found, as shown by the precision values.', 'when nb increases, precision decreases as it generally does, but very slowly.', 'the decrease actually becomes significant only when nb becomes larger than n. it means that the weights associated to the boundaries are not very significant.', 'we have validated this hypothesis by changing the weighting policy of the boundaries without having significant changes in the results.', 'one way for increasing the performance would be to take as text boundary not the position of a minimum in the cohesion graph but the nearest sentence boundary from this position']",5
"['of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a']","['the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a']","['the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a majority']","['', 'to this end, various sense - specific word embeddings have been proposed to account for the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', '']",0
"['of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a']","['the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a']","['the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a majority']","['', 'to this end, various sense - specific word embeddings have been proposed to account for the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', '']",0
"['obtain context sense representation  #TAUTHOR_TAG.', 'it was also used']","['obtain context sense representation  #TAUTHOR_TAG.', 'it was also used']","['obtain context sense representation  #TAUTHOR_TAG.', 'it was also used']","['unisense vectors, sense - specific vectors should be closely aligned to words in that sense.', 'this idea of local similarity has been widely used to obtain context sense representation  #TAUTHOR_TAG.', 'it was also used to decompose unisense vector into sense specific vectors  #AUTHOR_TAG.', 'in this paper, we exploit this intuition and model the contextual embedding of a word as a linear combination of its contexts.', '']",0
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",0
"['of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a']","['the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a']","['the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', 'a majority']","['', 'to this end, various sense - specific word embeddings have been proposed to account for the contextual subtlety of language  #AUTHOR_TAG b, a ;  #TAUTHOR_TAG.', '']",5
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",5
['4 ; also  #TAUTHOR_TAG'],['4 ; also  #TAUTHOR_TAG'],"['α = 1 is cosine similarity.', 'to show the effect of α when only relevant ( w, s ) pairs are used, figure 3. 2 plots the scws score ( see section 4 ; also  #TAUTHOR_TAG']","['propose to measure the contextual similarity using the geometric mean of the cosine similarity and dot product, as a tradeoff of these two extremes :', 'for 0 ≤ α ≤ 1.', 'specifically, d ( x, y ; 1 ) = x t y and d ( x, y ; 0 ) is the cosine distance between x and y. table 1 looks at the closest words to bank in its two contexts for different choice of α - distance measure.', 'for dot - product we see an overemphasis of popular words that are only marginally related ( gently, steeply ).', 'for cosine similarity, rare words are overpromoted ( saxony, sacramento ).', 'in general, α = 0. 75 and 0. 9 can reasonably measure contextual similarity.', 'table 1 : closest words to bank in the context of finance and geology, for various choices of α in ( 3 ).', 'recall that α = 0 is the dot product and α = 1 is cosine similarity.', 'to show the effect of α when only relevant ( w, s ) pairs are used, figure 3. 2 plots the scws score ( see section 4 ; also  #TAUTHOR_TAG for varying α, for the top - performing embedding from each method.', 'cos - distance works best for all embeddings.', 'however, for embeddings from  #TAUTHOR_TAG and  #AUTHOR_TAG, which all have norm ≈ 1, the choice of α makes little difference.', 'on the other hand, using the embeddings from and our method, which both have highly varying norms, the choice of α greatly affects performance']",7
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",7
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",7
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",7
['4 ; also  #TAUTHOR_TAG'],['4 ; also  #TAUTHOR_TAG'],"['α = 1 is cosine similarity.', 'to show the effect of α when only relevant ( w, s ) pairs are used, figure 3. 2 plots the scws score ( see section 4 ; also  #TAUTHOR_TAG']","['propose to measure the contextual similarity using the geometric mean of the cosine similarity and dot product, as a tradeoff of these two extremes :', 'for 0 ≤ α ≤ 1.', 'specifically, d ( x, y ; 1 ) = x t y and d ( x, y ; 0 ) is the cosine distance between x and y. table 1 looks at the closest words to bank in its two contexts for different choice of α - distance measure.', 'for dot - product we see an overemphasis of popular words that are only marginally related ( gently, steeply ).', 'for cosine similarity, rare words are overpromoted ( saxony, sacramento ).', 'in general, α = 0. 75 and 0. 9 can reasonably measure contextual similarity.', 'table 1 : closest words to bank in the context of finance and geology, for various choices of α in ( 3 ).', 'recall that α = 0 is the dot product and α = 1 is cosine similarity.', 'to show the effect of α when only relevant ( w, s ) pairs are used, figure 3. 2 plots the scws score ( see section 4 ; also  #TAUTHOR_TAG for varying α, for the top - performing embedding from each method.', 'cos - distance works best for all embeddings.', 'however, for embeddings from  #TAUTHOR_TAG and  #AUTHOR_TAG, which all have norm ≈ 1, the choice of α makes little difference.', 'on the other hand, using the embeddings from and our method, which both have highly varying norms, the choice of α greatly affects performance']",4
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",4
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",4
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",4
"['evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is']","['on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than']","['', 'across all words. discussion we have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from', 'existing test sources and formed ourselves through wordnet. overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a wordnet based model. one thing to note is that the scws spearman scores of the  #TAUTHOR_TAG listed here are', 'much smaller than that first reported. this is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. both are perfectly', 'valid metrics ; our choice is', 'solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used']",1
"['transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more']","['transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more appropriate.', 'both assume predictability is measured locally, relative to neighboring segment - pairs.', 'this paper replicates  #TAUTHOR_TAG mutualinformation model on a corpus of childdirected speech in modern greek, and introduces a variant model using a global threshold.', ""brent's finding regarding the superiority of mi is confirmed ; the relative""]","['transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more']","['computational simulations of how children solve the word segmentation problem have been proposed, but most have been applied only to a limited number of languages.', 'one model with some experimental support uses distributional statistics of sound sequence predictability ( saffran et al. 1996 ).', 'however, the experimental design does not fully specify how predictability is best measured or modeled in a simulation.', ' #AUTHOR_TAG assume transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more appropriate.', 'both assume predictability is measured locally, relative to neighboring segment - pairs.', 'this paper replicates  #TAUTHOR_TAG mutualinformation model on a corpus of childdirected speech in modern greek, and introduces a variant model using a global threshold.', ""brent's finding regarding the superiority of mi is confirmed ; the relative performance of local comparisons and global thresholds depends on the evaluation metric""]",3
"['study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs']","['study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs']","['in its general approach the study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs slightly in the details of']","['in its general approach the study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs slightly in the details of their use.', 'first, whereas brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram - type offline, over a separate training corpus.', '4 secondly, we compare the use of a global threshold ( described in more detail in section 2. 3, below ) to  #TAUTHOR_TAG use of the local context ( as described in section 1. 3 above ).', 'like  #TAUTHOR_TAG, but unlike  #AUTHOR_TAG, our model focuses on pairs of segments, not on pairs of syllables.', ""while modern greek syllabic structure is not as complicated as english's, it is still more complicated than the cv structure assumed in  #AUTHOR_TAG ; hence, access to syllabification cannot be assumed""]",3
"['as  #TAUTHOR_TAG.', 'in order']","['as  #TAUTHOR_TAG.', 'in order']","['as  #TAUTHOR_TAG.', 'in order']","['four model variants ( global mi, global tp, local mi, and local tp ) were each evaluated on three metrics : word boundaries, word tokens, and word types.', ""note that the first metric reported, simple boundary placement, considers only utterance - internal word - boundaries, rather than including those word boundaries which are detected'for free'by virtue of being utteranceboundaries also."", 'this boundary measure may be more conservative than that reported by other authors, but is easily convertible into other metrics.', 'the second metric, the percentage of word tokens detected, is the same as  #TAUTHOR_TAG.', '']",3
"['the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi']","['the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi']","['the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi and tp models suggest. quite possibly the best', 'model would involve']","['', 'local and global models was roughly equal, but recall for the global comparison model was 5. 5 % higher. not only were', 'the global models better at pulling out a variety of words, but', 'they also managed to learn longer ones ( especially the global tp variant ), including a few four - syllable words. the local model learned no four - syllable words, and relatively few three - syllable words. the mixed nature of these results suggests that evaluation depends fairly crucially on what performance metric needs', 'to be optimized. this demands stronger prior hypotheses regarding the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi and tp models suggest. quite possibly the best', ""model would involve either a hybrid of local and global comparisons, or a longer window, or even a'gradient'window where far neighbors count less than near ones in a computed average. however, further"", 'speculation on point this of less importance than considering how this cue interacts with others known experimentally to', 'be salient to infants.  #AUTHOR_TAG and  #AUTHOR_TAG have already began simulating and testing these interactions in english. however, more', 'work needs to be done to understand better the nature of these interactions cross - linguistically']",3
"['transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more']","['transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more appropriate.', 'both assume predictability is measured locally, relative to neighboring segment - pairs.', 'this paper replicates  #TAUTHOR_TAG mutualinformation model on a corpus of childdirected speech in modern greek, and introduces a variant model using a global threshold.', ""brent's finding regarding the superiority of mi is confirmed ; the relative""]","['transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more']","['computational simulations of how children solve the word segmentation problem have been proposed, but most have been applied only to a limited number of languages.', 'one model with some experimental support uses distributional statistics of sound sequence predictability ( saffran et al. 1996 ).', 'however, the experimental design does not fully specify how predictability is best measured or modeled in a simulation.', ' #AUTHOR_TAG assume transitional probability, but  #TAUTHOR_TAG claims mutual information ( mi ) is more appropriate.', 'both assume predictability is measured locally, relative to neighboring segment - pairs.', 'this paper replicates  #TAUTHOR_TAG mutualinformation model on a corpus of childdirected speech in modern greek, and introduces a variant model using a global threshold.', ""brent's finding regarding the superiority of mi is confirmed ; the relative performance of local comparisons and global thresholds depends on the evaluation metric""]",4
"['study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs']","['study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs']","['in its general approach the study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs slightly in the details of']","['in its general approach the study reported here replicates the mutual - information and transitional - probability models in  #TAUTHOR_TAG, it differs slightly in the details of their use.', 'first, whereas brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram - type offline, over a separate training corpus.', '4 secondly, we compare the use of a global threshold ( described in more detail in section 2. 3, below ) to  #TAUTHOR_TAG use of the local context ( as described in section 1. 3 above ).', 'like  #TAUTHOR_TAG, but unlike  #AUTHOR_TAG, our model focuses on pairs of segments, not on pairs of syllables.', ""while modern greek syllabic structure is not as complicated as english's, it is still more complicated than the cv structure assumed in  #AUTHOR_TAG ; hence, access to syllabification cannot be assumed""]",4
"['phoneme ( and phoneme - pair ) is modeled separately.', 'like  #TAUTHOR_TAG']","['phoneme ( and phoneme - pair ) is modeled separately.', 'like  #TAUTHOR_TAG']","['more minimal fsms ; each phoneme ( and phoneme - pair ) is modeled separately.', 'like  #TAUTHOR_TAG']","['as a solitary cue ( as it is in the tests run here ), comparison against a global threshold may be implemented within the same framework as  #AUTHOR_TAG tp and mi heuristics.', 'however, it may be implemented within a finite - state framework as well, with equivalent behavior.', ""this section will describe how the'global comparison'heuristic is modeled within a finite - state framework."", 'while such an implementation is not technically necessary here, one advantage of the finite - state framework is the compositionality of finite state machines, which allows for later composition of this approach with other heuristics depending on other cues, analogous to  #AUTHOR_TAG.', 'since the finite - state framework selects the best path over the whole utterance, it also allows for optimization over a sequence of decisions, rather than optimizing each local decision separately.', '6  #AUTHOR_TAG, where the actual fsm structure ( including classes of phonemes that could be group onto one arc ) was learned, here the structure of each fsm is determined in advance.', 'only the weight on each arc is derived from data.', 'no attempt is made to combine phonemes to produce more minimal fsms ; each phoneme ( and phoneme - pair ) is modeled separately.', 'like  #TAUTHOR_TAG and indeed most models in the literature, this model assumes ( for sake of convenience and simplicity ) that the child hears each segment produced within an utterance without error.', 'this assumption translates into the finitestate domain as a simple acceptor ( or equivalently, an identity transducer ) over the segment sequence for a given utterance.', '7 word boundaries are inserted by means of a transducer that computes the cost of word boundary insertion from the predictability scores.', 'in the mi model, the cost of inserting a word boundary is proportional to the mutual information.', 'for ease in modeling, this was represented with a finite state transducer with two paths between every pair of phonemes ( x, y ), with zero - counts modeled with a maximum weight of 99.', 'the direct path, representing a path with no word boundary inserted, costs −mi ( x, y ), which is positive for bigrams of low predictability ( negative mi ), where word boundaries are more likely.', 'the other path, representing a word boundary insertion, carries the cost of the global threshold, in this case arbitrarily set to zero ( although it could be optimized with held - out data ).', ""a small subset of the resulting fst, representing the connections over the alphabet { ab } is illustrated in figure 1, below : the best ( least - cost ) path over this subset model inserts boundaries between two adjacent a's and two adjacent""]",5
"['as  #TAUTHOR_TAG.', 'in order']","['as  #TAUTHOR_TAG.', 'in order']","['as  #TAUTHOR_TAG.', 'in order']","['four model variants ( global mi, global tp, local mi, and local tp ) were each evaluated on three metrics : word boundaries, word tokens, and word types.', ""note that the first metric reported, simple boundary placement, considers only utterance - internal word - boundaries, rather than including those word boundaries which are detected'for free'by virtue of being utteranceboundaries also."", 'this boundary measure may be more conservative than that reported by other authors, but is easily convertible into other metrics.', 'the second metric, the percentage of word tokens detected, is the same as  #TAUTHOR_TAG.', '']",6
"['the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi']","['the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi']","['the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi and tp models suggest. quite possibly the best', 'model would involve']","['', 'local and global models was roughly equal, but recall for the global comparison model was 5. 5 % higher. not only were', 'the global models better at pulling out a variety of words, but', 'they also managed to learn longer ones ( especially the global tp variant ), including a few four - syllable words. the local model learned no four - syllable words, and relatively few three - syllable words. the mixed nature of these results suggests that evaluation depends fairly crucially on what performance metric needs', 'to be optimized. this demands stronger prior hypotheses regarding the process and needed input of a vocabularyacquiring child. however', ', it cannot be blindly assumed that children are selecting low points over as short a window as  #TAUTHOR_TAG mi and tp models suggest. quite possibly the best', ""model would involve either a hybrid of local and global comparisons, or a longer window, or even a'gradient'window where far neighbors count less than near ones in a computed average. however, further"", 'speculation on point this of less importance than considering how this cue interacts with others known experimentally to', 'be salient to infants.  #AUTHOR_TAG and  #AUTHOR_TAG have already began simulating and testing these interactions in english. however, more', 'work needs to be done to understand better the nature of these interactions cross - linguistically']",7
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",0
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",0
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",0
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",0
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['be polarity shifters.', 'semantic similarity is modeled as cosine similarity in a word embedding space.', 'the word embeddings are created using word2vec  #AUTHOR_TAG on the german web corpus dewac  #AUTHOR_TAG, using the same hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['main requirement of the following features is a reasonably sized text corpus to detect syntactic patterns and word frequencies.', 'the text corpus was lemmatized using the treetagger  #AUTHOR_TAG and parsed for syntactic dependency structures with parzu  #AUTHOR_TAG.', '2 for features requiring knowledge of polarities we use the polart sentiment lexicon  #AUTHOR_TAG.', '3 distributional similarity ( sim ) : the distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters.', 'semantic similarity is modeled as cosine similarity in a word embedding space.', 'the word embeddings are created using word2vec  #AUTHOR_TAG on the german web corpus dewac  #AUTHOR_TAG, using the same hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will often occur when a polar verb modifies an expression of the opposite polarity, such as in ( 7 ).', 'the feature is further narrowed down to negative verbs that modify positive nouns, as polar verbal shifters are predominantly of negative polarity ( table 3 ).', 'particle verbs ( prt ) : certain verb particles indicate a complete transition to an end state  #AUTHOR_TAG.', ' #TAUTHOR_TAG hypothesize that this phenomenon correlates with shifting, which can be seen as producing a new ( negative ) end state.', 'therefore,  #TAUTHOR_TAG collect particle verbs containing relevant english particles, such as away, down and out.', 'for our german data we chose the following particles associated with negative end states : ab, aus, entgegen, fort, herunter, hinunter, weg and wider.', ""heuristic using'jeglich'( any ) : negative polarity items ( npis ) are known to occur in the context of negation  #AUTHOR_TAG."", ' #TAUTHOR_TAG showed that the english npi any co - occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter.', 'we expect the same for the german npi jeglich, as seen in ( 8 ).', 'we collect all verbs with a polar direct object that is modified by the lemma jeglich.', 'the resulting pattern matches are sorted by their frequency, normalized over their respective verb frequency and then reranked using personalised pagerank  #AUTHOR_TAG anti - shifter feature ( anti ) : this feature specifically targets anti - shifters, verbs that exhibit polar stability instead of causing polar shifting.', 'these are commonly verbs indicating creation or continued existence, such as live, introduce, construct or prepare.', 'such verbs often co - occur with the adverbs ausschließlich, zuerst, neu and extra, as seen in ( 9 )']",0
['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis'],"['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis.', 'to increase']","['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis.', 'to increase the size of our lexicon, we bootstrap additional shifters following  #TAUTHOR_TAG', 'we train our best classifier (']","['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis.', 'to increase the size of our lexicon, we bootstrap additional shifters following  #TAUTHOR_TAG', 'we train our best classifier ( table 5 ) on the 2000 verbs from our gold standard ( § 3 ) and then use it to classify the remaining 7262 germanet verbs that had not been labeled so far.', 'of these, the classifier labels 595 verbs as shifters.', '']",0
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",2
"['the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that such']","['the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that such']","['the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that']","['motivation behind the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that such a lexicon exists for english, it is an obvious resource to use when creating verbal shifter lexicons for other languages.', 'we hypothesize that a verb with the same meaning as an english verbal shifter will also function as a shifter in its own language.', 'all that is required is a mapping from english verbs to, in our case, german verbs.', 'we choose to use the bootstrapped lexicon of  #TAUTHOR_TAG, rather than the manually created one of  #AUTHOR_TAG, to show that bootstrapping is sufficient for all stages of the learning process.', 'one potential source for such a mapping is a bilingual dictionary.', 'we use the english - german dataset by dictcc 4, as it is large ( over one million translation pairs ) and publicly available.', 'it covers 76 % of german verbs found in germanet and 77 % of english verbs found in wordnet.', 'mapping the shifter labels of the english verbs to german verbs is performed as follows : for each german verb, all possible english translations are looked up.', '']",2
"['of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the']","['of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future, we']","['.', 'in reproducing the work of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future,']","['confirm that the bootstrapping process for creating a large lexicon of verbal polarity shifters can successfully be applied to german.', 'given appropriate resources, the effort for adjusting to a new language is minimal, mostly requiring translating seed words and adjusting syntactic patterns, while the underlying concepts of the features remain the same.', 'using a manually annotated sample of 2000 verbs taken from germanet, we train a supervised classifier with various data - and resource - driven features.', 'its performance is further improved by leveraging information from an existing english lexicon of verbal shifters using bilingual dictionaries and cross - lingual word embeddings.', 'the resulting improved classifier allows us to triple the number of confirmed german shifters in our lexicon.', 'we differentiate features by whether they require only unlabeled data and basic linguistic tools or whether they depend on rare semantic resources that may not be available for many languages.', 'in addition, we introduce the possibility of using cross - lingual resources to reduce the dependence on resources in the target language.', 'this shows promise, improving performance for both unsupervised and supervised classification, especially for scenarios where only small amounts of training data are available.', 'however, supervised learning that combines all features still provides the best results.', 'our recommendation for creating shifter lexicons in new languages is to start out with cross - lingual label transfer, but to also invest in annotating a random sample of verbs if possible, especially if advanced semantic resources like a wordnet are available, as they require supervised learning to be leveraged.', 'in reproducing the work of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future, we would like to investigate methods to extend the shifter lexicon to also cover nouns and adjectives.', 'while we have shown that the  #TAUTHOR_TAG for classifying verbal shifters works for german and english, future work will expand the number of languages, especially to verify that these methods can also be applied to non - indo - european languages, such as chinese, japanese or arabic.', 'in this context it will also be interesting to see whether using shifter lexicons from several languages can further improve the dictionary and cross - lingual word embedding classifiers']",2
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",4
"['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['similar to performance observed 5 bilbowa  #AUTHOR_TAG seeks to', 'improve the coverage problem of parallel corpora by incorporating additional monolingual corpora into the training process. however, our', 'experiments with it did not provide satisfactory results. this is in line with reports by  #AUTHOR_TAG and  #AUTHOR_TAG. 6 word2vec configuration : cbow,', '300 dimensions, context window of 5 words, sub - sampling at 1e − 05, negative samples at 10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure, as it has', 'a strong majority label bias. the no shifter label makes up 88. 8 % of our gold annotation ( table 2 ), which explains the strong performance of the', 'majority baseline on this metric. table 6 : features included in svm feature groups in table 5. all features in data and resource were also used in  #TAUTHOR_TAG. for english  #AUTHOR_TAG. cross - lingual embeddings and dictionaries as stand - alone classifiers both outperform the label propagation approach due to their better', 'recall coverage of shifters. interestingly, the cross', '- lingual embedding classifier performs far better than sim, despite both relying on word embeddings to', '']",4
"['classifier  #TAUTHOR_TAG.', 'covered.', '']","['classifier  #TAUTHOR_TAG.', 'covered.', '']","['1 : learning curve on gold standard.', 'svm data + resource represents the previously best classifier  #TAUTHOR_TAG.', 'covered.', '']","['dictionary mapping approach ( § 4. 2. 1 ) has been shown to be a strong stand - alone classifier and svm feature ( figure 1 : learning curve on gold standard.', 'svm data + resource represents the previously best classifier  #TAUTHOR_TAG.', 'covered.', 'for many other languages, finding a publicly available dictionary of comparable size may pose a challenge.', 'therefore, we investigate how smaller dictionaries may perform in our classifiers.', 'the english - german dictcc dictionary covers slightly over 8000 of the english verbs found in wordnet.', 'of the 2000 german verbs in our gold standard, dictcc covers 1368.', 'to simulate bilingual dictionaries of smaller size, we create a version of the dictcc dictionary with half the english vocabulary by limiting it to the 4000 most frequent verbs from wordnet ( dict voc _ size = 4k ).', 'we also create even smaller versions with only the 1000 ( dict voc _ size = 1k ) and 500 most frequent english verbs ( dict voc _ size = 0. 5k ).', 'as bilingual dictionaries provide a many - to - many mapping, having half the english vocabulary does not necessarily mean that we receive only half the german translations.', '']",4
"['classifier  #TAUTHOR_TAG.', 'covered.', '']","['classifier  #TAUTHOR_TAG.', 'covered.', '']","['1 : learning curve on gold standard.', 'svm data + resource represents the previously best classifier  #TAUTHOR_TAG.', 'covered.', '']","['dictionary mapping approach ( § 4. 2. 1 ) has been shown to be a strong stand - alone classifier and svm feature ( figure 1 : learning curve on gold standard.', 'svm data + resource represents the previously best classifier  #TAUTHOR_TAG.', 'covered.', 'for many other languages, finding a publicly available dictionary of comparable size may pose a challenge.', 'therefore, we investigate how smaller dictionaries may perform in our classifiers.', 'the english - german dictcc dictionary covers slightly over 8000 of the english verbs found in wordnet.', 'of the 2000 german verbs in our gold standard, dictcc covers 1368.', 'to simulate bilingual dictionaries of smaller size, we create a version of the dictcc dictionary with half the english vocabulary by limiting it to the 4000 most frequent verbs from wordnet ( dict voc _ size = 4k ).', 'we also create even smaller versions with only the 1000 ( dict voc _ size = 1k ) and 500 most frequent english verbs ( dict voc _ size = 0. 5k ).', 'as bilingual dictionaries provide a many - to - many mapping, having half the english vocabulary does not necessarily mean that we receive only half the german translations.', '']",4
"['adequately', ' #TAUTHOR_TAG']","['##ers adequately', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['a german lexicon of 677 verbal polarity shifters. we also improve the bootstrapping process by adding features that leverage polarity shifter resources across', 'languages. as is the case with negation, modeling polarity shifting is important for various tasks in nlp, such as relation extraction ( sanchez -  #AUTHOR_TAG, recognition of textual entailment  #AUTHOR_TAG and especially sentiment analysis  #AUTHOR_TAG. however, while there has been', 'significant research on negation in sentiment analysis  #AUTHOR_TAG, current classifiers fail', 'to handle polarity shifters adequately', ' #TAUTHOR_TAG']",3
"['hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['be polarity shifters.', 'semantic similarity is modeled as cosine similarity in a word embedding space.', 'the word embeddings are created using word2vec  #AUTHOR_TAG on the german web corpus dewac  #AUTHOR_TAG, using the same hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['main requirement of the following features is a reasonably sized text corpus to detect syntactic patterns and word frequencies.', 'the text corpus was lemmatized using the treetagger  #AUTHOR_TAG and parsed for syntactic dependency structures with parzu  #AUTHOR_TAG.', '2 for features requiring knowledge of polarities we use the polart sentiment lexicon  #AUTHOR_TAG.', '3 distributional similarity ( sim ) : the distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters.', 'semantic similarity is modeled as cosine similarity in a word embedding space.', 'the word embeddings are created using word2vec  #AUTHOR_TAG on the german web corpus dewac  #AUTHOR_TAG, using the same hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will often occur when a polar verb modifies an expression of the opposite polarity, such as in ( 7 ).', 'the feature is further narrowed down to negative verbs that modify positive nouns, as polar verbal shifters are predominantly of negative polarity ( table 3 ).', 'particle verbs ( prt ) : certain verb particles indicate a complete transition to an end state  #AUTHOR_TAG.', ' #TAUTHOR_TAG hypothesize that this phenomenon correlates with shifting, which can be seen as producing a new ( negative ) end state.', 'therefore,  #TAUTHOR_TAG collect particle verbs containing relevant english particles, such as away, down and out.', 'for our german data we chose the following particles associated with negative end states : ab, aus, entgegen, fort, herunter, hinunter, weg and wider.', ""heuristic using'jeglich'( any ) : negative polarity items ( npis ) are known to occur in the context of negation  #AUTHOR_TAG."", ' #TAUTHOR_TAG showed that the english npi any co - occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter.', 'we expect the same for the german npi jeglich, as seen in ( 8 ).', 'we collect all verbs with a polar direct object that is modified by the lemma jeglich.', 'the resulting pattern matches are sorted by their frequency, normalized over their respective verb frequency and then reranked using personalised pagerank  #AUTHOR_TAG anti - shifter feature ( anti ) : this feature specifically targets anti - shifters, verbs that exhibit polar stability instead of causing polar shifting.', 'these are commonly verbs indicating creation or continued existence, such as live, introduce, construct or prepare.', 'such verbs often co - occur with the adverbs ausschließlich, zuerst, neu and extra, as seen in ( 9 )']",3
"['hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['be polarity shifters.', 'semantic similarity is modeled as cosine similarity in a word embedding space.', 'the word embeddings are created using word2vec  #AUTHOR_TAG on the german web corpus dewac  #AUTHOR_TAG, using the same hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will']","['main requirement of the following features is a reasonably sized text corpus to detect syntactic patterns and word frequencies.', 'the text corpus was lemmatized using the treetagger  #AUTHOR_TAG and parsed for syntactic dependency structures with parzu  #AUTHOR_TAG.', '2 for features requiring knowledge of polarities we use the polart sentiment lexicon  #AUTHOR_TAG.', '3 distributional similarity ( sim ) : the distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters.', 'semantic similarity is modeled as cosine similarity in a word embedding space.', 'the word embeddings are created using word2vec  #AUTHOR_TAG on the german web corpus dewac  #AUTHOR_TAG, using the same hyperparameters as  #TAUTHOR_TAG and german translations of their negation seeds.', 'polarity clash ( clash ) : the polarity clash feature assesses that shifting will often occur when a polar verb modifies an expression of the opposite polarity, such as in ( 7 ).', 'the feature is further narrowed down to negative verbs that modify positive nouns, as polar verbal shifters are predominantly of negative polarity ( table 3 ).', 'particle verbs ( prt ) : certain verb particles indicate a complete transition to an end state  #AUTHOR_TAG.', ' #TAUTHOR_TAG hypothesize that this phenomenon correlates with shifting, which can be seen as producing a new ( negative ) end state.', 'therefore,  #TAUTHOR_TAG collect particle verbs containing relevant english particles, such as away, down and out.', 'for our german data we chose the following particles associated with negative end states : ab, aus, entgegen, fort, herunter, hinunter, weg and wider.', ""heuristic using'jeglich'( any ) : negative polarity items ( npis ) are known to occur in the context of negation  #AUTHOR_TAG."", ' #TAUTHOR_TAG showed that the english npi any co - occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter.', 'we expect the same for the german npi jeglich, as seen in ( 8 ).', 'we collect all verbs with a polar direct object that is modified by the lemma jeglich.', 'the resulting pattern matches are sorted by their frequency, normalized over their respective verb frequency and then reranked using personalised pagerank  #AUTHOR_TAG anti - shifter feature ( anti ) : this feature specifically targets anti - shifters, verbs that exhibit polar stability instead of causing polar shifting.', 'these are commonly verbs indicating creation or continued existence, such as live, introduce, construct or prepare.', 'such verbs often co - occur with the adverbs ausschließlich, zuerst, neu and extra, as seen in ( 9 )']",3
"['', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['following features rely on advanced semantic resources which are available in only a few languages.', 'germanet : wordnets are large lexical ontologies providing various kinds of semantic information and relations.', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['following features rely on advanced semantic resources which are available in only a few languages.', 'germanet : wordnets are large lexical ontologies providing various kinds of semantic information and relations.', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in the case of glosses, called paraphrases in germanet, germanet offers two variations : the paraphrases originally written for germanet, and a more extensive set of paraphrases harvested from wiktionary  #AUTHOR_TAG.', 'to improve coverage we use this paraphrase extension in our experiments.', 'salsa framenet : framenets provide semantic frames that group words with similar semantic behavior.', ' #TAUTHOR_TAG use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames.', 'we reproduce this feature using frames from the german framenet project salsa  #AUTHOR_TAG.', 'effektgermanet : wiebe and colleagues  #AUTHOR_TAG introduced the idea that events can have harmful or beneficial effects on their objects.', 'these effects are related but not identical to polarity shifting.', ' #AUTHOR_TAG provide lexical information on effects in their english resource effectwordnet.', 'we use its german counterpart, effektgermanet  #AUTHOR_TAG, to model the effect feature in our data']",3
"['', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['following features rely on advanced semantic resources which are available in only a few languages.', 'germanet : wordnets are large lexical ontologies providing various kinds of semantic information and relations.', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['following features rely on advanced semantic resources which are available in only a few languages.', 'germanet : wordnets are large lexical ontologies providing various kinds of semantic information and relations.', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in the case of glosses, called paraphrases in germanet, germanet offers two variations : the paraphrases originally written for germanet, and a more extensive set of paraphrases harvested from wiktionary  #AUTHOR_TAG.', 'to improve coverage we use this paraphrase extension in our experiments.', 'salsa framenet : framenets provide semantic frames that group words with similar semantic behavior.', ' #TAUTHOR_TAG use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames.', 'we reproduce this feature using frames from the german framenet project salsa  #AUTHOR_TAG.', 'effektgermanet : wiebe and colleagues  #AUTHOR_TAG introduced the idea that events can have harmful or beneficial effects on their objects.', 'these effects are related but not identical to polarity shifting.', ' #AUTHOR_TAG provide lexical information on effects in their english resource effectwordnet.', 'we use its german counterpart, effektgermanet  #AUTHOR_TAG, to model the effect feature in our data']",3
"['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['similar to performance observed 5 bilbowa  #AUTHOR_TAG seeks to', 'improve the coverage problem of parallel corpora by incorporating additional monolingual corpora into the training process. however, our', 'experiments with it did not provide satisfactory results. this is in line with reports by  #AUTHOR_TAG and  #AUTHOR_TAG. 6 word2vec configuration : cbow,', '300 dimensions, context window of 5 words, sub - sampling at 1e − 05, negative samples at 10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure, as it has', 'a strong majority label bias. the no shifter label makes up 88. 8 % of our gold annotation ( table 2 ), which explains the strong performance of the', 'majority baseline on this metric. table 6 : features included in svm feature groups in table 5. all features in data and resource were also used in  #TAUTHOR_TAG. for english  #AUTHOR_TAG. cross - lingual embeddings and dictionaries as stand - alone classifiers both outperform the label propagation approach due to their better', 'recall coverage of shifters. interestingly, the cross', '- lingual embedding classifier performs far better than sim, despite both relying on word embeddings to', '']",3
"['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['similar to performance observed 5 bilbowa  #AUTHOR_TAG seeks to', 'improve the coverage problem of parallel corpora by incorporating additional monolingual corpora into the training process. however, our', 'experiments with it did not provide satisfactory results. this is in line with reports by  #AUTHOR_TAG and  #AUTHOR_TAG. 6 word2vec configuration : cbow,', '300 dimensions, context window of 5 words, sub - sampling at 1e − 05, negative samples at 10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure, as it has', 'a strong majority label bias. the no shifter label makes up 88. 8 % of our gold annotation ( table 2 ), which explains the strong performance of the', 'majority baseline on this metric. table 6 : features included in svm feature groups in table 5. all features in data and resource were also used in  #TAUTHOR_TAG. for english  #AUTHOR_TAG. cross - lingual embeddings and dictionaries as stand - alone classifiers both outperform the label propagation approach due to their better', 'recall coverage of shifters. interestingly, the cross', '- lingual embedding classifier performs far better than sim, despite both relying on word embeddings to', '']",3
"['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['similar to performance observed 5 bilbowa  #AUTHOR_TAG seeks to', 'improve the coverage problem of parallel corpora by incorporating additional monolingual corpora into the training process. however, our', 'experiments with it did not provide satisfactory results. this is in line with reports by  #AUTHOR_TAG and  #AUTHOR_TAG. 6 word2vec configuration : cbow,', '300 dimensions, context window of 5 words, sub - sampling at 1e − 05, negative samples at 10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure, as it has', 'a strong majority label bias. the no shifter label makes up 88. 8 % of our gold annotation ( table 2 ), which explains the strong performance of the', 'majority baseline on this metric. table 6 : features included in svm feature groups in table 5. all features in data and resource were also used in  #TAUTHOR_TAG. for english  #AUTHOR_TAG. cross - lingual embeddings and dictionaries as stand - alone classifiers both outperform the label propagation approach due to their better', 'recall coverage of shifters. interestingly, the cross', '- lingual embedding classifier performs far better than sim, despite both relying on word embeddings to', '']",3
['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis'],"['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis.', 'to increase']","['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis.', 'to increase the size of our lexicon, we bootstrap additional shifters following  #TAUTHOR_TAG', 'we train our best classifier (']","['their intrinsic evaluation  #TAUTHOR_TAG showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis.', 'to increase the size of our lexicon, we bootstrap additional shifters following  #TAUTHOR_TAG', 'we train our best classifier ( table 5 ) on the 2000 verbs from our gold standard ( § 3 ) and then use it to classify the remaining 7262 germanet verbs that had not been labeled so far.', 'of these, the classifier labels 595 verbs as shifters.', '']",3
"['of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the']","['of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future, we']","['.', 'in reproducing the work of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future,']","['confirm that the bootstrapping process for creating a large lexicon of verbal polarity shifters can successfully be applied to german.', 'given appropriate resources, the effort for adjusting to a new language is minimal, mostly requiring translating seed words and adjusting syntactic patterns, while the underlying concepts of the features remain the same.', 'using a manually annotated sample of 2000 verbs taken from germanet, we train a supervised classifier with various data - and resource - driven features.', 'its performance is further improved by leveraging information from an existing english lexicon of verbal shifters using bilingual dictionaries and cross - lingual word embeddings.', 'the resulting improved classifier allows us to triple the number of confirmed german shifters in our lexicon.', 'we differentiate features by whether they require only unlabeled data and basic linguistic tools or whether they depend on rare semantic resources that may not be available for many languages.', 'in addition, we introduce the possibility of using cross - lingual resources to reduce the dependence on resources in the target language.', 'this shows promise, improving performance for both unsupervised and supervised classification, especially for scenarios where only small amounts of training data are available.', 'however, supervised learning that combines all features still provides the best results.', 'our recommendation for creating shifter lexicons in new languages is to start out with cross - lingual label transfer, but to also invest in annotating a random sample of verbs if possible, especially if advanced semantic resources like a wordnet are available, as they require supervised learning to be leveraged.', 'in reproducing the work of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future, we would like to investigate methods to extend the shifter lexicon to also cover nouns and adjectives.', 'while we have shown that the  #TAUTHOR_TAG for classifying verbal shifters works for german and english, future work will expand the number of languages, especially to verify that these methods can also be applied to non - indo - european languages, such as chinese, japanese or arabic.', 'in this context it will also be interesting to see whether using shifter lexicons from several languages can further improve the dictionary and cross - lingual word embedding classifiers']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG english gold standard.', 'an expert annot']","[' #TAUTHOR_TAG english gold standard.', 'an expert annotator,']","['create a gold standard for german verbal shifters, following the approach  #TAUTHOR_TAG english gold standard.', 'an expert annotator,']","['create a gold standard for german verbal shifters, following the approach  #TAUTHOR_TAG english gold standard.', 'an expert annotator, who is a native speaker of german, labeled 2000 verbs, randomly sampled from germanet  #AUTHOR_TAG, a german wordnet resource.', 'the remaining 7262 germanet verbs are used to bootstrap a larger lexicon in § 5. 3.', 'each verb is assigned a binary label of being a shifter or not.', 'to qualify as a shifter, a verb must permit polar expressions as its dependents and cause the polarity of the expression that embeds both verb and polar expression to move towards the opposite of the polar expression.', 'for example, in ( 6 ) verhindern shifts the negative polarity of its dependent ein gemetzel, resulting in a positive expression.', 'annotation is performed at the lemma level, as word - sense disambiguation tends to be insufficiently robust']",5
"['', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['following features rely on advanced semantic resources which are available in only a few languages.', 'germanet : wordnets are large lexical ontologies providing various kinds of semantic information and relations.', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in']","['following features rely on advanced semantic resources which are available in only a few languages.', 'germanet : wordnets are large lexical ontologies providing various kinds of semantic information and relations.', ' #TAUTHOR_TAG.', 'we use germanet  #AUTHOR_TAG, a german wordnet resource that provides all these features.', 'in the case of glosses, called paraphrases in germanet, germanet offers two variations : the paraphrases originally written for germanet, and a more extensive set of paraphrases harvested from wiktionary  #AUTHOR_TAG.', 'to improve coverage we use this paraphrase extension in our experiments.', 'salsa framenet : framenets provide semantic frames that group words with similar semantic behavior.', ' #TAUTHOR_TAG use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames.', 'we reproduce this feature using frames from the german framenet project salsa  #AUTHOR_TAG.', 'effektgermanet : wiebe and colleagues  #AUTHOR_TAG introduced the idea that events can have harmful or beneficial effects on their objects.', 'these effects are related but not identical to polarity shifting.', ' #AUTHOR_TAG provide lexical information on effects in their english resource effectwordnet.', 'we use its german counterpart, effektgermanet  #AUTHOR_TAG, to model the effect feature in our data']",5
"['the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that such']","['the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that such']","['the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that']","['motivation behind the work of  #TAUTHOR_TAG was to introduce a large lexicon of verbal polarity shifters.', 'now that such a lexicon exists for english, it is an obvious resource to use when creating verbal shifter lexicons for other languages.', 'we hypothesize that a verb with the same meaning as an english verbal shifter will also function as a shifter in its own language.', 'all that is required is a mapping from english verbs to, in our case, german verbs.', 'we choose to use the bootstrapped lexicon of  #TAUTHOR_TAG, rather than the manually created one of  #AUTHOR_TAG, to show that bootstrapping is sufficient for all stages of the learning process.', 'one potential source for such a mapping is a bilingual dictionary.', 'we use the english - german dataset by dictcc 4, as it is large ( over one million translation pairs ) and publicly available.', 'it covers 76 % of german verbs found in germanet and 77 % of english verbs found in wordnet.', 'mapping the shifter labels of the english verbs to german verbs is performed as follows : for each german verb, all possible english translations are looked up.', '']",5
"['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure,']","['similar to performance observed 5 bilbowa  #AUTHOR_TAG seeks to', 'improve the coverage problem of parallel corpora by incorporating additional monolingual corpora into the training process. however, our', 'experiments with it did not provide satisfactory results. this is in line with reports by  #AUTHOR_TAG and  #AUTHOR_TAG. 6 word2vec configuration : cbow,', '300 dimensions, context window of 5 words, sub - sampling at 1e − 05, negative samples at 10 and vocabulary restricted to the 200, 000 most frequent words. we also experimented with using the full vocabulary, but this resulted in lower quality embeddings. 7', 'as in  #TAUTHOR_TAG, accuracy proves to be a problematic measure, as it has', 'a strong majority label bias. the no shifter label makes up 88. 8 % of our gold annotation ( table 2 ), which explains the strong performance of the', 'majority baseline on this metric. table 6 : features included in svm feature groups in table 5. all features in data and resource were also used in  #TAUTHOR_TAG. for english  #AUTHOR_TAG. cross - lingual embeddings and dictionaries as stand - alone classifiers both outperform the label propagation approach due to their better', 'recall coverage of shifters. interestingly, the cross', '- lingual embedding classifier performs far better than sim, despite both relying on word embeddings to', '']",5
"['of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the']","['of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future, we']","['.', 'in reproducing the work of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future,']","['confirm that the bootstrapping process for creating a large lexicon of verbal polarity shifters can successfully be applied to german.', 'given appropriate resources, the effort for adjusting to a new language is minimal, mostly requiring translating seed words and adjusting syntactic patterns, while the underlying concepts of the features remain the same.', 'using a manually annotated sample of 2000 verbs taken from germanet, we train a supervised classifier with various data - and resource - driven features.', 'its performance is further improved by leveraging information from an existing english lexicon of verbal shifters using bilingual dictionaries and cross - lingual word embeddings.', 'the resulting improved classifier allows us to triple the number of confirmed german shifters in our lexicon.', 'we differentiate features by whether they require only unlabeled data and basic linguistic tools or whether they depend on rare semantic resources that may not be available for many languages.', 'in addition, we introduce the possibility of using cross - lingual resources to reduce the dependence on resources in the target language.', 'this shows promise, improving performance for both unsupervised and supervised classification, especially for scenarios where only small amounts of training data are available.', 'however, supervised learning that combines all features still provides the best results.', 'our recommendation for creating shifter lexicons in new languages is to start out with cross - lingual label transfer, but to also invest in annotating a random sample of verbs if possible, especially if advanced semantic resources like a wordnet are available, as they require supervised learning to be leveraged.', 'in reproducing the work of  #TAUTHOR_TAG, we limited ourselves to verbs.', 'in the future, we would like to investigate methods to extend the shifter lexicon to also cover nouns and adjectives.', 'while we have shown that the  #TAUTHOR_TAG for classifying verbal shifters works for german and english, future work will expand the number of languages, especially to verify that these methods can also be applied to non - indo - european languages, such as chinese, japanese or arabic.', 'in this context it will also be interesting to see whether using shifter lexicons from several languages can further improve the dictionary and cross - lingual word embedding classifiers']",5
"[') regardless of the input  #TAUTHOR_TAG.', 'while']","['output generated can be repetitive and generic leading to monotonous or uninteresting responses ( e. g "" i don\'t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while']","['\' t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while']","['models can be optimized to recognize syntax and semantics with great accuracy [ 1 ].', 'however, the output generated can be repetitive and generic leading to monotonous or uninteresting responses ( e. g "" i don\'t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while application of attention [ 3, 4 ] and advanced decoding mechanisms like beam search and variation sampling [ 5 ] have shown improvements, it does not solve the underlying problem.', 'in creative text generation, the objective is not strongly bound to the ground truth - instead the objective is to generate diverse, unique or original samples.', 'we attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens.', 'the contributions of this paper are in the usage of a gan framework to generate creative pieces of writing.', 'our experiments suggest that generative text models, while very good at encapsulating semantic, syntactic and domain information, perform better with external feedback from a discriminator for fine - tuning objectiveless decoding tasks like that of creative text.', 'we show this by evaluating our model on three very different creative datasets containing poetry, metaphors and lyrics.', 'previous work on handling the shortcomings of mle include length - normalizing sentence probability [ 6 ], future cost estimation [ 7 ], diversity - boosting objective function [ 8,  #TAUTHOR_TAG or penalizing repeating tokens [ 9 ].', 'when it comes to poetry generation using generative text models, zhang and lapata [ 10 ], yi et al. [ 11 ] and wang et al. [ 12 ] use language modeling to generate chinese poems.', 'however, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding.', 'for the task of text generation, maskgan [ 13 ] uses a reinforcement learning signal from the discriminator, fmd - gan [ 14 ] uses an optimal transport mechanism as an objective function.', 'gumbelgan [ 15 ] uses gumbel - softmax distribution that replaces the non - differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients.', 'li et al.  #TAUTHOR_TAG use a discriminator for a diversity promoting objective.', 'yu et al. [ 16 ] use seqgan to generate poetry and comment on the performance of seqgan over mle in human evaluations, encouraging our study of gans for creative text generation.', 'however, these studies do not focus solely on creative text.', 'using gans, we can train generative models in a two - player game setting between a']",0
"[') regardless of the input  #TAUTHOR_TAG.', 'while']","['output generated can be repetitive and generic leading to monotonous or uninteresting responses ( e. g "" i don\'t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while']","['\' t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while']","['models can be optimized to recognize syntax and semantics with great accuracy [ 1 ].', 'however, the output generated can be repetitive and generic leading to monotonous or uninteresting responses ( e. g "" i don\'t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while application of attention [ 3, 4 ] and advanced decoding mechanisms like beam search and variation sampling [ 5 ] have shown improvements, it does not solve the underlying problem.', 'in creative text generation, the objective is not strongly bound to the ground truth - instead the objective is to generate diverse, unique or original samples.', 'we attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens.', 'the contributions of this paper are in the usage of a gan framework to generate creative pieces of writing.', 'our experiments suggest that generative text models, while very good at encapsulating semantic, syntactic and domain information, perform better with external feedback from a discriminator for fine - tuning objectiveless decoding tasks like that of creative text.', 'we show this by evaluating our model on three very different creative datasets containing poetry, metaphors and lyrics.', 'previous work on handling the shortcomings of mle include length - normalizing sentence probability [ 6 ], future cost estimation [ 7 ], diversity - boosting objective function [ 8,  #TAUTHOR_TAG or penalizing repeating tokens [ 9 ].', 'when it comes to poetry generation using generative text models, zhang and lapata [ 10 ], yi et al. [ 11 ] and wang et al. [ 12 ] use language modeling to generate chinese poems.', 'however, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding.', 'for the task of text generation, maskgan [ 13 ] uses a reinforcement learning signal from the discriminator, fmd - gan [ 14 ] uses an optimal transport mechanism as an objective function.', 'gumbelgan [ 15 ] uses gumbel - softmax distribution that replaces the non - differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients.', 'li et al.  #TAUTHOR_TAG use a discriminator for a diversity promoting objective.', 'yu et al. [ 16 ] use seqgan to generate poetry and comment on the performance of seqgan over mle in human evaluations, encouraging our study of gans for creative text generation.', 'however, these studies do not focus solely on creative text.', 'using gans, we can train generative models in a two - player game setting between a']",0
"[') regardless of the input  #TAUTHOR_TAG.', 'while']","['output generated can be repetitive and generic leading to monotonous or uninteresting responses ( e. g "" i don\'t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while']","['\' t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while']","['models can be optimized to recognize syntax and semantics with great accuracy [ 1 ].', 'however, the output generated can be repetitive and generic leading to monotonous or uninteresting responses ( e. g "" i don\'t know "" ) regardless of the input  #TAUTHOR_TAG.', 'while application of attention [ 3, 4 ] and advanced decoding mechanisms like beam search and variation sampling [ 5 ] have shown improvements, it does not solve the underlying problem.', 'in creative text generation, the objective is not strongly bound to the ground truth - instead the objective is to generate diverse, unique or original samples.', 'we attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens.', 'the contributions of this paper are in the usage of a gan framework to generate creative pieces of writing.', 'our experiments suggest that generative text models, while very good at encapsulating semantic, syntactic and domain information, perform better with external feedback from a discriminator for fine - tuning objectiveless decoding tasks like that of creative text.', 'we show this by evaluating our model on three very different creative datasets containing poetry, metaphors and lyrics.', 'previous work on handling the shortcomings of mle include length - normalizing sentence probability [ 6 ], future cost estimation [ 7 ], diversity - boosting objective function [ 8,  #TAUTHOR_TAG or penalizing repeating tokens [ 9 ].', 'when it comes to poetry generation using generative text models, zhang and lapata [ 10 ], yi et al. [ 11 ] and wang et al. [ 12 ] use language modeling to generate chinese poems.', 'however, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding.', 'for the task of text generation, maskgan [ 13 ] uses a reinforcement learning signal from the discriminator, fmd - gan [ 14 ] uses an optimal transport mechanism as an objective function.', 'gumbelgan [ 15 ] uses gumbel - softmax distribution that replaces the non - differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients.', 'li et al.  #TAUTHOR_TAG use a discriminator for a diversity promoting objective.', 'yu et al. [ 16 ] use seqgan to generate poetry and comment on the performance of seqgan over mle in human evaluations, encouraging our study of gans for creative text generation.', 'however, these studies do not focus solely on creative text.', 'using gans, we can train generative models in a two - player game setting between a']",0
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",0
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",0
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",6
"['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG,']","['their experiments, naseem et al. also used a set of universal categories, however, with some differences to the tagset presented here.', 'their tagset does not have punctuation and catch - all categories, but includes a category for auxiliaries.', 'the auxiliary category helps define a syntactic rule that attaches verbs to an auxiliary head, which is beneficial for certain languages.', 'however, since this rule is reversed for other languages, we omit it in our tagset.', 'additionally, they also used refined categories in the form of conll treebank tags.', 'in our experiments, we did not make use of refined categories, as the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG, so that we can make use of their automatically projected pos tags.', 'for all languages, we used the treebanks released as a part of the conll - x  #AUTHOR_TAG shared task.', 'we only considered sentences of length 10 or less, after the removal of punctuations.', 'we performed bayesian inference on the whole treebank and report dependency attachment accuracy.', 'table 2 shows directed dependency accuracies for the dmv and pgi models using fine - grained gold pos tags.', 'for the usr model, it reports results on gold universal pos tags ( usr - g ) and automatically induced universal pos tags ( usr - i ).', 'the usr - i model falls short of the usr - g model, but has the advantage that it does not require any labeled data from the target language.', 'quite impressively, it does better than dmv for all languages, and is competitive with pgi, even though those models have access to fine - grained gold pos tags']",5
"['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG,']","['their experiments, naseem et al. also used a set of universal categories, however, with some differences to the tagset presented here.', 'their tagset does not have punctuation and catch - all categories, but includes a category for auxiliaries.', 'the auxiliary category helps define a syntactic rule that attaches verbs to an auxiliary head, which is beneficial for certain languages.', 'however, since this rule is reversed for other languages, we omit it in our tagset.', 'additionally, they also used refined categories in the form of conll treebank tags.', 'in our experiments, we did not make use of refined categories, as the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG, so that we can make use of their automatically projected pos tags.', 'for all languages, we used the treebanks released as a part of the conll - x  #AUTHOR_TAG shared task.', 'we only considered sentences of length 10 or less, after the removal of punctuations.', 'we performed bayesian inference on the whole treebank and report dependency attachment accuracy.', 'table 2 shows directed dependency accuracies for the dmv and pgi models using fine - grained gold pos tags.', 'for the usr model, it reports results on gold universal pos tags ( usr - g ) and automatically induced universal pos tags ( usr - i ).', 'the usr - i model falls short of the usr - g model, but has the advantage that it does not require any labeled data from the target language.', 'quite impressively, it does better than dmv for all languages, and is competitive with pgi, even though those models have access to fine - grained gold pos tags']",5
,,,,0
"['by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our']","['was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our']","['was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our results with a simple bidirectional ls', '##tm network and pre - trained word embeddings from word2vec. we can see that on all settings, the model using bert representations']","['further pre - training on the 1 https : / / github. com', '/ google - research / bert # pre - trained', '- models parts of the wall street journal corpus that do not have discourse relation annotation. the model version "" bert + wjs w / o nsp "" also performs pre - training on the wsj corpus, but only uses the masked language modelling task, not the next sentence prediction task in the pre - training. we added this variant to measure the benefit of in - domain nsp on discourse relation classification ( note though that', 'the downloaded pre - trained bert model contains the nsp task in the original pre - training ). we compared', 'the results with four state - of - theart systems :  #AUTHOR_TAG proposed a model that takes a step', 'towards calculating discourse expectations by using attention', 'over an encoding of the first argument, to generate the', 'representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse', 'relation arguments.  #AUTHOR_TAG fed external world knowledge ( conceptnet relations and coreferences )', 'explicitly into mage - gru  #AUTHOR_TAG and achieved improvements compared to only using the relational arguments. however, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better', 'argument representations due to being trained to explicitate the implicit connective. in addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. the current best performance was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our results with a simple bidirectional ls', '##tm network and pre - trained word embeddings from word2vec. we can see that on all settings, the model using bert representations outperformed all existing systems with a substantial margin. it obtained improvements of 7. 3 % points on pdtb - lin, 5. 5 % points on pdtb - ji, compared with the el', ""##mobased method proposed in  #TAUTHOR_TAG. what's more, the bert model outperformed on cross validation by around 8 %, with significance of p < 0."", '01. significance test was performed by estimating variance of the model from the performance on different folds in cross - validation', '( paired t - test ). for the lin and ji evaluations, we estimated variance due to random initialization by', 'running them 5 times and calculating the likelihood that the state - of - the - art model result would come from that', 'distribution']",0
,,,,1
"['by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our']","['was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our']","['was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our results with a simple bidirectional ls', '##tm network and pre - trained word embeddings from word2vec. we can see that on all settings, the model using bert representations']","['further pre - training on the 1 https : / / github. com', '/ google - research / bert # pre - trained', '- models parts of the wall street journal corpus that do not have discourse relation annotation. the model version "" bert + wjs w / o nsp "" also performs pre - training on the wsj corpus, but only uses the masked language modelling task, not the next sentence prediction task in the pre - training. we added this variant to measure the benefit of in - domain nsp on discourse relation classification ( note though that', 'the downloaded pre - trained bert model contains the nsp task in the original pre - training ). we compared', 'the results with four state - of - theart systems :  #AUTHOR_TAG proposed a model that takes a step', 'towards calculating discourse expectations by using attention', 'over an encoding of the first argument, to generate the', 'representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse', 'relation arguments.  #AUTHOR_TAG fed external world knowledge ( conceptnet relations and coreferences )', 'explicitly into mage - gru  #AUTHOR_TAG and achieved improvements compared to only using the relational arguments. however, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better', 'argument representations due to being trained to explicitate the implicit connective. in addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. the current best performance was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our results with a simple bidirectional ls', '##tm network and pre - trained word embeddings from word2vec. we can see that on all settings, the model using bert representations outperformed all existing systems with a substantial margin. it obtained improvements of 7. 3 % points on pdtb - lin, 5. 5 % points on pdtb - ji, compared with the el', ""##mobased method proposed in  #TAUTHOR_TAG. what's more, the bert model outperformed on cross validation by around 8 %, with significance of p < 0."", '01. significance test was performed by estimating variance of the model from the performance on different folds in cross - validation', '( paired t - test ). for the lin and ji evaluations, we estimated variance due to random initialization by', 'running them 5 times and calculating the likelihood that the state - of - the - art model result would come from that', 'distribution']",5
"['classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12']","['classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12 % points']","['the bert model on cross - domain data, we performed finetuning on pdtb while testing on biodrb.', 'we also tested the state of the art model of implicit discourse relation classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12']","['biomedical discourse relation bank  #AUTHOR_TAG also follows pdtb - style annotation.', 'it is a corpus annotated over 24 open access fulltext articles from the genia corpus  #AUTHOR_TAG in the biomedical domain.', 'compared with pdtb, some new discourse relations and changes have been introduced in the annotation of bio - drb.', 'in order to make the results comparable, we preprocessed the biodrb annotations to map the relations to the pdtb ones, following the instructions in  #AUTHOR_TAG.', 'the biomedical domain is very different from the wsj or the data on which the bert model was trained.', 'the biodrb contains a lot of professional words / phrases that are extremely hard to model.', 'in order to test the ability of the bert model on cross - domain data, we performed finetuning on pdtb while testing on biodrb.', 'we also tested the state of the art model of implicit discourse relation classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12 % points improvement over the bi - lstm baseline and 15 % points over  #TAUTHOR_TAG.', 'when fine - tuned on in - domain data in the crossvalidation setting, the improvement increases to around 17 % points']",5
"['by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our']","['was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our']","['was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our results with a simple bidirectional ls', '##tm network and pre - trained word embeddings from word2vec. we can see that on all settings, the model using bert representations']","['further pre - training on the 1 https : / / github. com', '/ google - research / bert # pre - trained', '- models parts of the wall street journal corpus that do not have discourse relation annotation. the model version "" bert + wjs w / o nsp "" also performs pre - training on the wsj corpus, but only uses the masked language modelling task, not the next sentence prediction task in the pre - training. we added this variant to measure the benefit of in - domain nsp on discourse relation classification ( note though that', 'the downloaded pre - trained bert model contains the nsp task in the original pre - training ). we compared', 'the results with four state - of - theart systems :  #AUTHOR_TAG proposed a model that takes a step', 'towards calculating discourse expectations by using attention', 'over an encoding of the first argument, to generate the', 'representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse', 'relation arguments.  #AUTHOR_TAG fed external world knowledge ( conceptnet relations and coreferences )', 'explicitly into mage - gru  #AUTHOR_TAG and achieved improvements compared to only using the relational arguments. however, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. used a seq2seq model that learns better', 'argument representations due to being trained to explicitate the implicit connective. in addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. the current best performance was achieved by  #TAUTHOR_TAG,', 'who combined representations from different grained em - beddings including contextualized word vectors from elmo  #AUTHOR_TAG, which has been proved very helpful. in addition, we compared our results with a simple bidirectional ls', '##tm network and pre - trained word embeddings from word2vec. we can see that on all settings, the model using bert representations outperformed all existing systems with a substantial margin. it obtained improvements of 7. 3 % points on pdtb - lin, 5. 5 % points on pdtb - ji, compared with the el', ""##mobased method proposed in  #TAUTHOR_TAG. what's more, the bert model outperformed on cross validation by around 8 %, with significance of p < 0."", '01. significance test was performed by estimating variance of the model from the performance on different folds in cross - validation', '( paired t - test ). for the lin and ji evaluations, we estimated variance due to random initialization by', 'running them 5 times and calculating the likelihood that the state - of - the - art model result would come from that', 'distribution']",4
"['classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12']","['classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12 % points']","['the bert model on cross - domain data, we performed finetuning on pdtb while testing on biodrb.', 'we also tested the state of the art model of implicit discourse relation classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12']","['biomedical discourse relation bank  #AUTHOR_TAG also follows pdtb - style annotation.', 'it is a corpus annotated over 24 open access fulltext articles from the genia corpus  #AUTHOR_TAG in the biomedical domain.', 'compared with pdtb, some new discourse relations and changes have been introduced in the annotation of bio - drb.', 'in order to make the results comparable, we preprocessed the biodrb annotations to map the relations to the pdtb ones, following the instructions in  #AUTHOR_TAG.', 'the biomedical domain is very different from the wsj or the data on which the bert model was trained.', 'the biodrb contains a lot of professional words / phrases that are extremely hard to model.', 'in order to test the ability of the bert model on cross - domain data, we performed finetuning on pdtb while testing on biodrb.', 'we also tested the state of the art model of implicit discourse relation classification proposed by  #TAUTHOR_TAG on biodrb.', 'from table 2, we can see that the bert base model achieved almost 12 % points improvement over the bi - lstm baseline and 15 % points over  #TAUTHOR_TAG.', 'when fine - tuned on in - domain data in the crossvalidation setting, the improvement increases to around 17 % points']",4
"['1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsource']","['1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsourced utterances from']","['of our neural generation models in figure 1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsourced utterances from']","['language generators for task - oriented dialog should be able to vary the style of the output while still effectively realizing the system dialog actions and their associated semantics.', 'the use of neural natural language generation ( nnlg ) for training the response generation component of conversational agents promises to simplify the process of producing high quality responses in new domains by relying on the neural architecture to automatically learn how to map an input meaning representation to an output utterance.', 'however, there has been little investigation of nnlgs for dialog that can vary their response style, and we know of no experiments on models that can generate responses that are different in style from those seen during training, while still maintaining semantic fidelity to the input meaning representation.', 'instead, work on stylistic transfer has focused on tasks where only coarse - grained semantic fidelity is needed, such as controlling the sentiment of the utterance ( positive or negative ), or the topic or entity under discussion [ 1, 2, 3 ].', 'consider for example a training instance for the restaurant domain consisting of a meaning representation ( mr ) from the end - to - end ( e2e ) generation challenge 1 and a sample output from one of our neural generation models in figure 1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsourced utterances from the e2e task achieved high semantic correctness, e. g. the bleu score for our best system on the dev set was 0. 72 [ 6 ].', 'however in the best case these models can only reproduce the 1 http : / / www. macs. hw. ac. uk / interactionlab / e2e / style of the training data, and in actuality the outputs have reduced stylistic variation, because when particular stylistic variations are less frequent, they are treated similarly to noise.', ""browns cambridge is a pub, also it is a moderately priced italian place near adriatic, also it is family friendly, you know and it's in the city centre."", 'in subsequent work, we showed that we could augment the e2e training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training  #TAUTHOR_TAG.', 'here, instead, we explore whether a model that is trained to achieve a single stylistic personality target can produce outputs that combine stylistic targets, to yield a novel style that is significantly different than what was seen in training, while still maintaining high semantic correctness.', 'we first train each stylistic model with a single latent']",0
"['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['damn, e. g. see the outputs based on disagreeable personality in table 2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG. figure 1 summarizes the model architecture. this model builds on the open - source sequence - tosequence ( seq2seq ) tgen system [ 11 ], which is implemented in tensorflow [ 12', ']. 2 the system is based on the seq2seq generation method with attention [ 14, 15 ], and uses a sequence of lstms [ 16 ] for the encoder and decoder, combined with beamsearch and an n - best list reranker for', 'output tuning. the inputs to the model are dialog acts for each system action ( such as inform ) and a set of attribute slots ( such as rating ) and their', 'values ( such as high for attribute rating ). to', 'prepro - 2 we refer the reader to tgen publications [ 11, 13 ] for model details. cess the', 'corpus of mr / utterance pairs, attributes that take on proper - noun', 'values are delexicalized during training i. e. name and near. we encode personality as an additional', 'dialog act, of type convert with personality as the key and the target personality as the value', '( see figure 1 ). for every input mr and a personality, we train the model with the corresponding per - sonage generated sentence. our model differs from the to - ken model used in our previous work  #TAUTHOR_TAG because it is trained on unsorted inputs to allow us to add multiple convert tags to the mr at generation time. note that we do', 'not train on multiple personalities, instead, we train', 'one model that uses all the data, where', 'each distinct single personality has a corresponding convert ( personality = x ) in the training instance. at generation time, we', 'generate singlevoice data for all the test mrs ( 1, 390 total realizations, 278 unique mrs,', 'realized for each of 5 personalities ). for the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test mrs, since the order of the convert tags matters. for a given order, the model produces a single output', '. we do not combine personalities that are exact opposites such as agreeable and disagreeable, yielding 8 combinations. the multivoice test set consists of 4, 448 total realizations ( 278 mrs and 8 × 2 outputs per mr )']",0
"['1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsource']","['1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsourced utterances from']","['of our neural generation models in figure 1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsourced utterances from']","['language generators for task - oriented dialog should be able to vary the style of the output while still effectively realizing the system dialog actions and their associated semantics.', 'the use of neural natural language generation ( nnlg ) for training the response generation component of conversational agents promises to simplify the process of producing high quality responses in new domains by relying on the neural architecture to automatically learn how to map an input meaning representation to an output utterance.', 'however, there has been little investigation of nnlgs for dialog that can vary their response style, and we know of no experiments on models that can generate responses that are different in style from those seen during training, while still maintaining semantic fidelity to the input meaning representation.', 'instead, work on stylistic transfer has focused on tasks where only coarse - grained semantic fidelity is needed, such as controlling the sentiment of the utterance ( positive or negative ), or the topic or entity under discussion [ 1, 2, 3 ].', 'consider for example a training instance for the restaurant domain consisting of a meaning representation ( mr ) from the end - to - end ( e2e ) generation challenge 1 and a sample output from one of our neural generation models in figure 1 [ 4,  #TAUTHOR_TAG.', 'systems using the training set of 50k crowdsourced utterances from the e2e task achieved high semantic correctness, e. g. the bleu score for our best system on the dev set was 0. 72 [ 6 ].', 'however in the best case these models can only reproduce the 1 http : / / www. macs. hw. ac. uk / interactionlab / e2e / style of the training data, and in actuality the outputs have reduced stylistic variation, because when particular stylistic variations are less frequent, they are treated similarly to noise.', ""browns cambridge is a pub, also it is a moderately priced italian place near adriatic, also it is family friendly, you know and it's in the city centre."", 'in subsequent work, we showed that we could augment the e2e training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training  #TAUTHOR_TAG.', 'here, instead, we explore whether a model that is trained to achieve a single stylistic personality target can produce outputs that combine stylistic targets, to yield a novel style that is significantly different than what was seen in training, while still maintaining high semantic correctness.', 'we first train each stylistic model with a single latent']",0
"['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['damn, e. g. see the outputs based on disagreeable personality in table 2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG. figure 1 summarizes the model architecture. this model builds on the open - source sequence - tosequence ( seq2seq ) tgen system [ 11 ], which is implemented in tensorflow [ 12', ']. 2 the system is based on the seq2seq generation method with attention [ 14, 15 ], and uses a sequence of lstms [ 16 ] for the encoder and decoder, combined with beamsearch and an n - best list reranker for', 'output tuning. the inputs to the model are dialog acts for each system action ( such as inform ) and a set of attribute slots ( such as rating ) and their', 'values ( such as high for attribute rating ). to', 'prepro - 2 we refer the reader to tgen publications [ 11, 13 ] for model details. cess the', 'corpus of mr / utterance pairs, attributes that take on proper - noun', 'values are delexicalized during training i. e. name and near. we encode personality as an additional', 'dialog act, of type convert with personality as the key and the target personality as the value', '( see figure 1 ). for every input mr and a personality, we train the model with the corresponding per - sonage generated sentence. our model differs from the to - ken model used in our previous work  #TAUTHOR_TAG because it is trained on unsorted inputs to allow us to add multiple convert tags to the mr at generation time. note that we do', 'not train on multiple personalities, instead, we train', 'one model that uses all the data, where', 'each distinct single personality has a corresponding convert ( personality = x ) in the training instance. at generation time, we', 'generate singlevoice data for all the test mrs ( 1, 390 total realizations, 278 unique mrs,', 'realized for each of 5 personalities ). for the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test mrs, since the order of the convert tags matters. for a given order, the model produces a single output', '. we do not combine personalities that are exact opposites such as agreeable and disagreeable, yielding 8 combinations. the multivoice test set consists of 4, 448 total realizations ( 278 mrs and 8 × 2 outputs per mr )']",5
"['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['damn, e. g. see the outputs based on disagreeable personality in table 2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG. figure 1 summarizes the model architecture. this model builds on the open - source sequence - tosequence ( seq2seq ) tgen system [ 11 ], which is implemented in tensorflow [ 12', ']. 2 the system is based on the seq2seq generation method with attention [ 14, 15 ], and uses a sequence of lstms [ 16 ] for the encoder and decoder, combined with beamsearch and an n - best list reranker for', 'output tuning. the inputs to the model are dialog acts for each system action ( such as inform ) and a set of attribute slots ( such as rating ) and their', 'values ( such as high for attribute rating ). to', 'prepro - 2 we refer the reader to tgen publications [ 11, 13 ] for model details. cess the', 'corpus of mr / utterance pairs, attributes that take on proper - noun', 'values are delexicalized during training i. e. name and near. we encode personality as an additional', 'dialog act, of type convert with personality as the key and the target personality as the value', '( see figure 1 ). for every input mr and a personality, we train the model with the corresponding per - sonage generated sentence. our model differs from the to - ken model used in our previous work  #TAUTHOR_TAG because it is trained on unsorted inputs to allow us to add multiple convert tags to the mr at generation time. note that we do', 'not train on multiple personalities, instead, we train', 'one model that uses all the data, where', 'each distinct single personality has a corresponding convert ( personality = x ) in the training instance. at generation time, we', 'generate singlevoice data for all the test mrs ( 1, 390 total realizations, 278 unique mrs,', 'realized for each of 5 personalities ). for the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test mrs, since the order of the convert tags matters. for a given order, the model produces a single output', '. we do not combine personalities that are exact opposites such as agreeable and disagreeable, yielding 8 combinations. the multivoice test set consists of 4, 448 total realizations ( 278 mrs and 8 × 2 outputs per mr )']",5
"['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['damn, e. g. see the outputs based on disagreeable personality in table 2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG. figure 1 summarizes the model architecture. this model builds on the open - source sequence - tosequence ( seq2seq ) tgen system [ 11 ], which is implemented in tensorflow [ 12', ']. 2 the system is based on the seq2seq generation method with attention [ 14, 15 ], and uses a sequence of lstms [ 16 ] for the encoder and decoder, combined with beamsearch and an n - best list reranker for', 'output tuning. the inputs to the model are dialog acts for each system action ( such as inform ) and a set of attribute slots ( such as rating ) and their', 'values ( such as high for attribute rating ). to', 'prepro - 2 we refer the reader to tgen publications [ 11, 13 ] for model details. cess the', 'corpus of mr / utterance pairs, attributes that take on proper - noun', 'values are delexicalized during training i. e. name and near. we encode personality as an additional', 'dialog act, of type convert with personality as the key and the target personality as the value', '( see figure 1 ). for every input mr and a personality, we train the model with the corresponding per - sonage generated sentence. our model differs from the to - ken model used in our previous work  #TAUTHOR_TAG because it is trained on unsorted inputs to allow us to add multiple convert tags to the mr at generation time. note that we do', 'not train on multiple personalities, instead, we train', 'one model that uses all the data, where', 'each distinct single personality has a corresponding convert ( personality = x ) in the training instance. at generation time, we', 'generate singlevoice data for all the test mrs ( 1, 390 total realizations, 278 unique mrs,', 'realized for each of 5 personalities ). for the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test mrs, since the order of the convert tags matters. for a given order, the model produces a single output', '. we do not combine personalities that are exact opposites such as agreeable and disagreeable, yielding 8 combinations. the multivoice test set consists of 4, 448 total realizations ( 278 mrs and 8 × 2 outputs per mr )']",4
"['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG']","['damn, e. g. see the outputs based on disagreeable personality in table 2. model description. our nnlg model uses a single token to represent', 'personality encoding, following the use of single language labels used in machine translation and other work on neural generation', '[ 10,  #TAUTHOR_TAG. figure 1 summarizes the model architecture. this model builds on the open - source sequence - tosequence ( seq2seq ) tgen system [ 11 ], which is implemented in tensorflow [ 12', ']. 2 the system is based on the seq2seq generation method with attention [ 14, 15 ], and uses a sequence of lstms [ 16 ] for the encoder and decoder, combined with beamsearch and an n - best list reranker for', 'output tuning. the inputs to the model are dialog acts for each system action ( such as inform ) and a set of attribute slots ( such as rating ) and their', 'values ( such as high for attribute rating ). to', 'prepro - 2 we refer the reader to tgen publications [ 11, 13 ] for model details. cess the', 'corpus of mr / utterance pairs, attributes that take on proper - noun', 'values are delexicalized during training i. e. name and near. we encode personality as an additional', 'dialog act, of type convert with personality as the key and the target personality as the value', '( see figure 1 ). for every input mr and a personality, we train the model with the corresponding per - sonage generated sentence. our model differs from the to - ken model used in our previous work  #TAUTHOR_TAG because it is trained on unsorted inputs to allow us to add multiple convert tags to the mr at generation time. note that we do', 'not train on multiple personalities, instead, we train', 'one model that uses all the data, where', 'each distinct single personality has a corresponding convert ( personality = x ) in the training instance. at generation time, we', 'generate singlevoice data for all the test mrs ( 1, 390 total realizations, 278 unique mrs,', 'realized for each of 5 personalities ). for the multivoice experiments, we generate 2 references per combination of two personalities for each of the 278 test mrs, since the order of the convert tags matters. for a given order, the model produces a single output', '. we do not combine personalities that are exact opposites such as agreeable and disagreeable, yielding 8 combinations. the multivoice test set consists of 4, 448 total realizations ( 278 mrs and 8 × 2 outputs per mr )']",4
"[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['first stage in training a machine translation system is typically that of aligning bilingual text.', 'the quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages.', 'in string - based statistical machine translation, the alignment space is typically restricted by the n - grams considered in the underlying language model, but in syntax - based machine translation the alignment space is restricted by very different and less transparent structural contraints.', 'while it is easy to estimate the consequences of restrictions to n - grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed by a particular syntax - based formalism consists in finding what is often called "" empirical lower bounds "" on the coverage of the formalism  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['first stage in training a machine translation system is typically that of aligning bilingual text.', 'the quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages.', 'in string - based statistical machine translation, the alignment space is typically restricted by the n - grams considered in the underlying language model, but in syntax - based machine translation the alignment space is restricted by very different and less transparent structural contraints.', 'while it is easy to estimate the consequences of restrictions to n - grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed by a particular syntax - based formalism consists in finding what is often called "" empirical lower bounds "" on the coverage of the formalism  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['first stage in training a machine translation system is typically that of aligning bilingual text.', 'the quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages.', 'in string - based statistical machine translation, the alignment space is typically restricted by the n - grams considered in the underlying language model, but in syntax - based machine translation the alignment space is restricted by very different and less transparent structural contraints.', 'while it is easy to estimate the consequences of restrictions to n - grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed by a particular syntax - based formalism consists in finding what is often called "" empirical lower bounds "" on the coverage of the formalism  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', '']",0
"['- out alignments were first described by  #AUTHOR_TAG, and their frequency has been a matter of some debate  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', 'cross - serial dtus are']","['- out alignments were first described by  #AUTHOR_TAG, and their frequency has been a matter of some debate  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', 'cross - serial dtus are']","['- out alignments were first described by  #AUTHOR_TAG, and their frequency has been a matter of some debate  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', 'cross - serial dtus are made of two dtus noncontiguous to the same side such that both have material in the gap of each other.', 'bonbons are similar, except the dtus are']","['- out alignments were first described by  #AUTHOR_TAG, and their frequency has been a matter of some debate  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', 'cross - serial dtus are made of two dtus noncontiguous to the same side such that both have material in the gap of each other.', ""bonbons are similar, except the dtus are non - contiguous to different sides, i. e. d has a gap in the source side that contains at least one token in e, and e has a gap in the target side that contains at least one token in d. here's an example of a bonbon configuration from  #AUTHOR_TAG multigap dtus with mixed transfer are, as already mentioned multigap dtus with crossing alignments from material in two distinct gaps""]",0
"[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","[' #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed']","['first stage in training a machine translation system is typically that of aligning bilingual text.', 'the quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages.', 'in string - based statistical machine translation, the alignment space is typically restricted by the n - grams considered in the underlying language model, but in syntax - based machine translation the alignment space is restricted by very different and less transparent structural contraints.', 'while it is easy to estimate the consequences of restrictions to n - grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax - based machine translation formalisms.', 'consequently, much work has been devoted to this task  #TAUTHOR_TAG søgaard and  #AUTHOR_TAG.', 'the task of estimating the consequences of the structural constraints imposed by a particular syntax - based formalism consists in finding what is often called "" empirical lower bounds "" on the coverage of the formalism  #TAUTHOR_TAG ; søgaard and  #AUTHOR_TAG.', '']",1
"['.', 'in  #TAUTHOR_TAG, we propose a']","['system development, comparison and analysis.', 'in  #TAUTHOR_TAG, we propose a']","['.', 'in  #TAUTHOR_TAG, we propose a framework for error analysis for core']","['##ference resolution is the task of determining which mentions in a text refer to the same entity.', 'both the natural language processing engineer ( who needs a coreference resolution system for the problem at hand ) and the coreference resolution researcher need tools to facilitate and support system development, comparison and analysis.', 'in  #TAUTHOR_TAG, we propose a framework for error analysis for coreference resolution.', 'in this paper, we present cort 1, an implementation of this framework, and show how it can be useful for engineers and researchers.', 'cort is released as open source and is available for download 2']",1
"['entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['an entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['to the set - based nature of coreference resolution, it is not clear how to extract errors when an entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning trees in a graph - based entity representation.', '1 short for coreference resolution toolkit.', '2 http : / / smartschat. de / software figure 1 summarizes their approach.', 'they represent reference and system entities as complete onedirectional graphs ( figures 1a and 1b ).', '']",0
['is a deterministic approach using a few strong features  #TAUTHOR_TAG'],['is a deterministic approach using a few strong features  #TAUTHOR_TAG'],"['is a deterministic approach using a few strong features  #TAUTHOR_TAG.', ""second, it includes a mention - pair approach  #AUTHOR_TAG with a large feature set, trained via a perceptron on the conll'12 english training data."", 'in table 1, we compare']","['ships with two coreference resolution approaches.', 'first, it includes multigraph, which is a deterministic approach using a few strong features  #TAUTHOR_TAG.', ""second, it includes a mention - pair approach  #AUTHOR_TAG with a large feature set, trained via a perceptron on the conll'12 english training data."", 'in table 1, we compare both approaches with stanfordsieve  #AUTHOR_TAG, the winner of the conll - 2011 shared task, and berkeleycoref  #AUTHOR_TAG, a state - of - the - art structured machine learning approach.', 'the systems are evaluated via the conll scorer  #AUTHOR_TAG.', 'both implemented approaches achieve competitive performance.', 'due to their modular implementation, both approaches are easily extensible with new features and with training or inference schemes.', 'they therefore can serve as a good starting point for system development and analysis']",0
"['entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['an entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['to the set - based nature of coreference resolution, it is not clear how to extract errors when an entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning trees in a graph - based entity representation.', '1 short for coreference resolution toolkit.', '2 http : / / smartschat. de / software figure 1 summarizes their approach.', 'they represent reference and system entities as complete onedirectional graphs ( figures 1a and 1b ).', '']",3
"['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality']","['core of this module is the erroranalysis class, which extracts and manages errors extracted from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality to', '• categorize and filter sets of errors,', '• visualize these sets,', '']",3
"['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality']","['core of this module is the erroranalysis class, which extracts and manages errors extracted from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality to', '• categorize and filter sets of errors,', '• visualize these sets,', '']",3
"[', following  #TAUTHOR_TAG, we categor']","['a suitable error categorization.', 'suppose the user is interested in improving recall for non - pronominal coreference.', 'hence, following  #TAUTHOR_TAG, we categorize all errors by coarse mention type of anaphor and antecedent ( proper name, noun, pronoun, demonstrative pronoun or verb ) 4']","[', following  #TAUTHOR_TAG, we categorize all errors by coarse mention type of anaph']","['get an initial assessment, the user can extract all errors made by the system and then make use of the plotting component to compare these errors with the maximum possible number of errors 3.', 'for a meaningful analysis, we have to find a suitable error categorization.', 'suppose the user is interested in improving recall for non - pronominal coreference.', 'hence, following  #TAUTHOR_TAG, we categorize all errors by coarse mention type of anaphor and antecedent ( proper name, noun, pronoun, demonstrative pronoun or verb ) 4']",3
"['entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['an entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning']","['to the set - based nature of coreference resolution, it is not clear how to extract errors when an entity is not correctly identified.', 'the idea underlying the analysis framework of  #TAUTHOR_TAG is to employ spanning trees in a graph - based entity representation.', '1 short for coreference resolution toolkit.', '2 http : / / smartschat. de / software figure 1 summarizes their approach.', 'they represent reference and system entities as complete onedirectional graphs ( figures 1a and 1b ).', '']",5
"['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality']","['core of this module is the erroranalysis class, which extracts and manages errors extracted from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality to', '• categorize and filter sets of errors,', '• visualize these sets,', '']",5
"['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this']","['from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality']","['core of this module is the erroranalysis class, which extracts and manages errors extracted from one or more systems.', 'the user can define own spanning tree algorithms to extract errors.', 'we already implemented the algorithms discussed in  #TAUTHOR_TAG.', 'furthermore, this module provides functionality to', '• categorize and filter sets of errors,', '• visualize these sets,', '']",5
"[', following  #TAUTHOR_TAG, we categor']","['a suitable error categorization.', 'suppose the user is interested in improving recall for non - pronominal coreference.', 'hence, following  #TAUTHOR_TAG, we categorize all errors by coarse mention type of anaphor and antecedent ( proper name, noun, pronoun, demonstrative pronoun or verb ) 4']","[', following  #TAUTHOR_TAG, we categorize all errors by coarse mention type of anaph']","['get an initial assessment, the user can extract all errors made by the system and then make use of the plotting component to compare these errors with the maximum possible number of errors 3.', 'for a meaningful analysis, we have to find a suitable error categorization.', 'suppose the user is interested in improving recall for non - pronominal coreference.', 'hence, following  #TAUTHOR_TAG, we categorize all errors by coarse mention type of anaphor and antecedent ( proper name, noun, pronoun, demonstrative pronoun or verb ) 4']",5
"['framework  #TAUTHOR_TAG,']","['framework  #TAUTHOR_TAG,']","['original implementation of the error analysis framework  #TAUTHOR_TAG,']","['to our original implementation of the error analysis framework  #TAUTHOR_TAG, we made the analysis interface more userfriendly and provide more analysis functionality.', 'furthermore, while our original implementation did not include any visualization capabilities, we now allow for both data visualization and document visualization.', 'we are aware of two other software packages for coreference resolution error analysis.', 'our toolkit complements these.', ' #AUTHOR_TAG present a toolkit which extracts errors from transformation of reference to system entities.', 'hence, their definition of what an error is not rooted in a pairwise representation, and is therefore conceptually different from our definition.', 'they do not provide any visualization components.', 'ice ( gartner et al., 2014 ) is a toolkit for coreference visualization and corpus analysis.', 'in particular, the toolkit visualizes recall and precision errors in a tree - based visualization of coreference clusters.', 'compared to ice, we provide more extensive functionality for error analysis and can accommodate for different notions of errors']",4
[' #TAUTHOR_TAG and speech'],[' #TAUTHOR_TAG and speech'],"[' #AUTHOR_TAG, machine translation  #TAUTHOR_TAG and speech recognition']","['##ing languages  #AUTHOR_TAG, such as german, dutch, danish, norwegian, swedish, greek or finnish, allow the generation of complex words by merging together simpler ones.', 'so, for instance, the flower bouquet can be expressed in german as blumenstrauße, made up of blumen ( flower ) and strauße ( bouquet ), and in finnish as kukkakimppu, from kukka ( flower ) and kimppu ( bunch, collection ).', 'for many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out - of - vocabulary terms.', 'decompounders have been used successfully in information retrieval  #AUTHOR_TAG, machine translation  #TAUTHOR_TAG and speech recognition ( adda -  #AUTHOR_TAG.', 'the cross language evaluation forum ( clef ) competitions have shown that very simple approaches can produce big gains in cross language information retrieval ( clir ) for german and dutch ( monz and de  #AUTHOR_TAG and for finnish  #AUTHOR_TAG.', 'when working with web data, which has not necessarily been reviewed for correctness, many of the words are more difficult to analyze than when working with standard texts.', 'there are more words with spelling mistakes, and many texts mix words from different languages.', 'this problem exists to a larger degree when handling user queries : they are written quickly, not paying attention to mistakes.', 'however, being able to identify that achzigerjahre should be decompounded as achzig + jahre ( where achzig is a misspelled variation of achtzig ) is still useful in obtaining some meaning from the user query and in helping the spelling correction system.', 'this paper evaluates a state - of - the - art procedure for german splitting  #AUTHOR_TAG, robust enough to handle query data, on different languages, and shows that it is possible to have a single decompounding model that can be applied to all the languages under study']",0
"['in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we']","['in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we']","['substantial amount of compounds : we started by building a very naive decompounder that splits a word in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we obtain two random samples with possibly repeated words : one with words that are considered non - compounds, and the other with']","['that are not really grammatical compounds, but due to the user forgetting to input the blankspace between the words ( like', 'desktopcomputer ) are split. for the evaluation, we have built and manually annotated gold standard sets for german, dutch, danish, norwegian, swedish and finnish from fully anonymized search query logs. because people do not', 'use capitalization consistently when writing queries, all the', 'query logs are lowercased. by randomly sampling keywords we would get few compounds ( as their frequency is small compared to that of', 'non - compounds ), so we have proceeded in the following way to ensure that the gold - standards contain', 'a substantial amount of compounds : we started by building a very naive decompounder that splits a word in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we obtain two random samples with possibly repeated words : one with words that are considered non - compounds, and the other with words that are considered compounds', 'by this naive approach. next, we removed all the duplicates from the previous list, and we had them', 'annotated manually as compounds or non - compounds, including the correct splittings. the sizes of the final training sets vary between 2, 000 and 3, 600 words depending on the language. each compound was annotated by two human judges who had received the previous instructions on when to consider that a keyword is a compound. for all the languages considered, exactly one of the two', 'judges was a native speaker living in a country where it is the official language 1. table 1 shows the percentage of agreement in classifying words as compounds or non - compounds ( compound classification agreement,', 'cca ) for each language and the kappa score  #AUTHOR_TAG obtained from it, and the percentage of words for which also the decomposition provided was identical ( decompound', '##ing agreement, da ). the most common source of disagreement were long words that could be split into two or', 'more parts. the evaluation is done using the metrics precision, recall and accuracy, defined in the following way  #TAUTHOR_TAG : • correct', '']",5
"['in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we']","['in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we']","['substantial amount of compounds : we started by building a very naive decompounder that splits a word in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we obtain two random samples with possibly repeated words : one with words that are considered non - compounds, and the other with']","['that are not really grammatical compounds, but due to the user forgetting to input the blankspace between the words ( like', 'desktopcomputer ) are split. for the evaluation, we have built and manually annotated gold standard sets for german, dutch, danish, norwegian, swedish and finnish from fully anonymized search query logs. because people do not', 'use capitalization consistently when writing queries, all the', 'query logs are lowercased. by randomly sampling keywords we would get few compounds ( as their frequency is small compared to that of', 'non - compounds ), so we have proceeded in the following way to ensure that the gold - standards contain', 'a substantial amount of compounds : we started by building a very naive decompounder that splits a word in several parts using a frequency - based compound splitting method  #TAUTHOR_TAG.', 'using this procedure, we obtain two random samples with possibly repeated words : one with words that are considered non - compounds, and the other with words that are considered compounds', 'by this naive approach. next, we removed all the duplicates from the previous list, and we had them', 'annotated manually as compounds or non - compounds, including the correct splittings. the sizes of the final training sets vary between 2, 000 and 3, 600 words depending on the language. each compound was annotated by two human judges who had received the previous instructions on when to consider that a keyword is a compound. for all the languages considered, exactly one of the two', 'judges was a native speaker living in a country where it is the official language 1. table 1 shows the percentage of agreement in classifying words as compounds or non - compounds ( compound classification agreement,', 'cca ) for each language and the kappa score  #AUTHOR_TAG obtained from it, and the percentage of words for which also the decomposition provided was identical ( decompound', '##ing agreement, da ). the most common source of disagreement were long words that could be split into two or', 'more parts. the evaluation is done using the metrics precision, recall and accuracy, defined in the following way  #TAUTHOR_TAG : • correct', '']",5
"['described by  #TAUTHOR_TAG.', 'when those resources']","['described by  #TAUTHOR_TAG.', 'when those resources']","['described by  #TAUTHOR_TAG.', 'when those resources are not available, the']","['approaches for decompounding can be considered as having this general structure : given a word w, calculate every possible way of splitting w in one or more parts, and score those parts according to some weighting function.', 'if the highest scoring splitting contains just one part, it means that w is not a compound.', 'for the first step ( calculating every possible splitting ), it is common to take into account that modifiers inside compound words sometimes need linking morphemes.', 'table 2 lists the ones used in our system  #AUTHOR_TAG concerning the second step, there is some work that uses, for scoring, additional information such as rules for cognate recognition  #AUTHOR_TAG or sentence - aligned parallel corpora and a translation model, as in the full system described by  #TAUTHOR_TAG.', ""when those resources are not available, the most common methods used for compound splitting are using features such as the geometric mean of the frequencies of compound parts in a corpus, as in  #TAUTHOR_TAG's back - off method, or learning a language model from a corpus and estimating the probability of each sequence of possible compound parts  #AUTHOR_TAG."", '']",5
"['described by  #TAUTHOR_TAG.', 'when those resources']","['described by  #TAUTHOR_TAG.', 'when those resources']","['described by  #TAUTHOR_TAG.', 'when those resources are not available, the']","['approaches for decompounding can be considered as having this general structure : given a word w, calculate every possible way of splitting w in one or more parts, and score those parts according to some weighting function.', 'if the highest scoring splitting contains just one part, it means that w is not a compound.', 'for the first step ( calculating every possible splitting ), it is common to take into account that modifiers inside compound words sometimes need linking morphemes.', 'table 2 lists the ones used in our system  #AUTHOR_TAG concerning the second step, there is some work that uses, for scoring, additional information such as rules for cognate recognition  #AUTHOR_TAG or sentence - aligned parallel corpora and a translation model, as in the full system described by  #TAUTHOR_TAG.', ""when those resources are not available, the most common methods used for compound splitting are using features such as the geometric mean of the frequencies of compound parts in a corpus, as in  #TAUTHOR_TAG's back - off method, or learning a language model from a corpus and estimating the probability of each sequence of possible compound parts  #AUTHOR_TAG."", '']",5
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. extensive', 'evaluation shows that']","['use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes']","['relations between words. second, by depending solely on', 'word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare', 'words where word embeddings are poorly trained. we seek to address these issues by proposing a more powerful neural network model. a well - studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network ( rnn )  #AUTHOR_TAG. recently, rnns have shown great success', 'in diverse nlp tasks such as speech recognition  #AUTHOR_TAG, machine', 'translation  #AUTHOR_TAG, and language modeling  #AUTHOR_TAG. the long - short term memory ( lstm ) unit with the forget gate allows highly non - trivial long - distance dependencies to be easily learned  #AUTHOR_TAG. for sequential', 'labelling tasks such as ner and speech recognition, a bi - directional lstm model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context', 'that applies to any feed - forward model  #AUTHOR_TAG. while lstms have', 'been studied in the past for the ner task by  #AUTHOR_TAG, the lack of computational power ( which led to the use of very small models ) and quality', 'word embeddings limited their effectiveness. convolutional neural networks ( cnn ) have also been investigated for modeling character - level information, among other nlp tasks', '.  #AUTHOR_TAG and  #AUTHOR_TAG successfully employed cnns to extract character - level features for use in ner and pos - tagging respectively.  #AUTHOR_TAG b ) also applied cnn', '##s to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures  #AUTHOR_TAG. however, the effectiveness of character - level cnns has not been evaluated for english ner. while we considered using character - level bi -', 'directional lstms, which was recently proposed by  #AUTHOR_TAG for postagging, preliminary evaluation shows that it does not perform significantly better than cnns while being more computationally expensive to train. our main contribution lies in', 'combining these neural network models for the ner task. we present a hybrid model of bi - directional lstms and', 'cnns that learns both character - and word - level features, presenting the first evaluation of such an architecture on well', '- established english language evaluation datasets. furthermore, as lexicons are crucial to ner performance, we propose a new lexicon encoding scheme and matching algorithm that can', 'make use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes 5. 0 datasets']",0
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. extensive', 'evaluation shows that']","['use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes']","['relations between words. second, by depending solely on', 'word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare', 'words where word embeddings are poorly trained. we seek to address these issues by proposing a more powerful neural network model. a well - studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network ( rnn )  #AUTHOR_TAG. recently, rnns have shown great success', 'in diverse nlp tasks such as speech recognition  #AUTHOR_TAG, machine', 'translation  #AUTHOR_TAG, and language modeling  #AUTHOR_TAG. the long - short term memory ( lstm ) unit with the forget gate allows highly non - trivial long - distance dependencies to be easily learned  #AUTHOR_TAG. for sequential', 'labelling tasks such as ner and speech recognition, a bi - directional lstm model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context', 'that applies to any feed - forward model  #AUTHOR_TAG. while lstms have', 'been studied in the past for the ner task by  #AUTHOR_TAG, the lack of computational power ( which led to the use of very small models ) and quality', 'word embeddings limited their effectiveness. convolutional neural networks ( cnn ) have also been investigated for modeling character - level information, among other nlp tasks', '.  #AUTHOR_TAG and  #AUTHOR_TAG successfully employed cnns to extract character - level features for use in ner and pos - tagging respectively.  #AUTHOR_TAG b ) also applied cnn', '##s to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures  #AUTHOR_TAG. however, the effectiveness of character - level cnns has not been evaluated for english ner. while we considered using character - level bi -', 'directional lstms, which was recently proposed by  #AUTHOR_TAG for postagging, preliminary evaluation shows that it does not perform significantly better than cnns while being more computationally expensive to train. our main contribution lies in', 'combining these neural network models for the ner task. we present a hybrid model of bi - directional lstms and', 'cnns that learns both character - and word - level features, presenting the first evaluation of such an architecture on well', '- established english language evaluation datasets. furthermore, as lexicons are crucial to ner performance, we propose a new lexicon encoding scheme and matching algorithm that can', 'make use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes 5. 0 datasets']",0
"['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag']","['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag - transition matrix a where a i, j represents the score of jumping from tag i to tag j in successive tokens, and a 0, i as the score']","['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag - transition matrix a where a i, j represents the score of jumping from tag i to tag j in successive tokens, and a 0, i as the score']","['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag - transition matrix a where a i, j represents the score of jumping from tag i to tag j in successive tokens, and a 0, i as the score for starting with tag i. this matrix of parameters are also learned.', 'define θ as the set of parameters for the neural network, and θ = θ ∪ { a i, j ∀i, j } as the set of all parameters to be trained.', 'given an example sentence, [ x ] t 1, of length t, and define [ f θ ] i, t as the score outputted by the neural network for the t th word and i th tag given parameters θ, then the score of a sequence of tags [ i ] t 1 is given as the sum of network and transition scores : then, letting [ y ] t 1 be the true tag sequence, the sentence - level log - likelihood is obtained by normalizing the above score over all possible tag - sequences [ j ] t 1 using a softmax :', 'this objective function and its gradients can be efficiently computed by dynamic programming  #TAUTHOR_TAG.', 'at inference time, given neural network outputs [ f θ ] i, t we use the viterbi algorithm to find the tag sequence [ i ] t 1 that maximizes the score', 'the output tags are annotated with bioes ( which stand for begin, inside, outside, end, single, indicating the position of the token in the 18 ontonotes results taken from  #AUTHOR_TAG 19 evaluation on ontonotes 5. 0 done by  #AUTHOR_TAG 20 not directly comparable as they evaluated on an earlier version of the corpus with a different data split.', '21 numbers taken from the original paper  #AUTHOR_TAG.', '']",0
"['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['of neural word embeddings,  #TAUTHOR_TAG presented senna, which']","['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['many approaches involve crf models, there has also been a long history of research involving neural networks.', 'early attempts were hindered by table 10 : per genre f1 scores on ontonotes.', 'bc = broadcast conversation, bn = broadcast news, mz = magazine, nw = newswire, tc = telephone conversation, wb = blogs and newsgroups lack of computational power, scalable learning algorithms, and high quality word embeddings.', ' #AUTHOR_TAG used a feed - forward neural network with one hidden layer on ner and achieved state - of - the - art results on the muc6 dataset.', 'their approach used only pos tag and gazetteer tags for each word, with no word embeddings.', ' #AUTHOR_TAG attempted ner with a singledirection lstm network and a combination of word vectors trained using self - organizing maps and context vectors obtained using principle component analysis.', 'however, while our method optimizes loglikelihood and uses softmax, they used a different output encoding and optimized an unspecified objective function.', ' #AUTHOR_TAG reported results were only slightly above baseline models.', 'much later, with the advent of neural word embeddings,  #TAUTHOR_TAG presented senna, which employs a deep ffnn and word embeddings to achieve near state of the art results on pos tagging, chunking, ner, and srl.', 'we build on their approach, sharing the word embeddings, feature encoding method, and objective functions.', ' #AUTHOR_TAG presented their charwnn network, which augments the neural network of  #TAUTHOR_TAG with character level cnns, and they reported improved performance on spanish and portuguese ner.', 'we have successfully incorporated character - level cnns into our model.', 'there have been various other similar architecture proposed for various sequential labeling nlp tasks.', 'used a blstm for the pos - tagging, chunking, and ner tasks, but they employed heavy feature engineering instead of using a cnn to automatically extract characterlevel features.', ' #AUTHOR_TAG used a brnn with character - level cnns to perform german postagging ; our model differs in that we use the more powerful lstm unit, which we found to perform better than rnns in preliminary experiments, and that we employ word embeddings, which is much more important in ner than in pos tagging.', ' #AUTHOR_TAG used both word - and character - level blstms to establish the current state of the art for english pos tagging.', 'while using blstms instead of cnns allows extraction of more sophisticated character - level features, we found in preliminary experiments that for ner it did not perform significantly better than cnns and was substantially more computationally expensive to train']",0
"['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['of neural word embeddings,  #TAUTHOR_TAG presented senna, which']","['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['many approaches involve crf models, there has also been a long history of research involving neural networks.', 'early attempts were hindered by table 10 : per genre f1 scores on ontonotes.', 'bc = broadcast conversation, bn = broadcast news, mz = magazine, nw = newswire, tc = telephone conversation, wb = blogs and newsgroups lack of computational power, scalable learning algorithms, and high quality word embeddings.', ' #AUTHOR_TAG used a feed - forward neural network with one hidden layer on ner and achieved state - of - the - art results on the muc6 dataset.', 'their approach used only pos tag and gazetteer tags for each word, with no word embeddings.', ' #AUTHOR_TAG attempted ner with a singledirection lstm network and a combination of word vectors trained using self - organizing maps and context vectors obtained using principle component analysis.', 'however, while our method optimizes loglikelihood and uses softmax, they used a different output encoding and optimized an unspecified objective function.', ' #AUTHOR_TAG reported results were only slightly above baseline models.', 'much later, with the advent of neural word embeddings,  #TAUTHOR_TAG presented senna, which employs a deep ffnn and word embeddings to achieve near state of the art results on pos tagging, chunking, ner, and srl.', 'we build on their approach, sharing the word embeddings, feature encoding method, and objective functions.', ' #AUTHOR_TAG presented their charwnn network, which augments the neural network of  #TAUTHOR_TAG with character level cnns, and they reported improved performance on spanish and portuguese ner.', 'we have successfully incorporated character - level cnns into our model.', 'there have been various other similar architecture proposed for various sequential labeling nlp tasks.', 'used a blstm for the pos - tagging, chunking, and ner tasks, but they employed heavy feature engineering instead of using a cnn to automatically extract characterlevel features.', ' #AUTHOR_TAG used a brnn with character - level cnns to perform german postagging ; our model differs in that we use the more powerful lstm unit, which we found to perform better than rnns in preliminary experiments, and that we employ word embeddings, which is much more important in ner than in pos tagging.', ' #AUTHOR_TAG used both word - and character - level blstms to establish the current state of the art for english pos tagging.', 'while using blstms instead of cnns allows extraction of more sophisticated character - level features, we found in preliminary experiments that for ner it did not perform significantly better than cnns and was substantially more computationally expensive to train']",0
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. extensive', 'evaluation shows that']","['use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes']","['relations between words. second, by depending solely on', 'word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare', 'words where word embeddings are poorly trained. we seek to address these issues by proposing a more powerful neural network model. a well - studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network ( rnn )  #AUTHOR_TAG. recently, rnns have shown great success', 'in diverse nlp tasks such as speech recognition  #AUTHOR_TAG, machine', 'translation  #AUTHOR_TAG, and language modeling  #AUTHOR_TAG. the long - short term memory ( lstm ) unit with the forget gate allows highly non - trivial long - distance dependencies to be easily learned  #AUTHOR_TAG. for sequential', 'labelling tasks such as ner and speech recognition, a bi - directional lstm model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context', 'that applies to any feed - forward model  #AUTHOR_TAG. while lstms have', 'been studied in the past for the ner task by  #AUTHOR_TAG, the lack of computational power ( which led to the use of very small models ) and quality', 'word embeddings limited their effectiveness. convolutional neural networks ( cnn ) have also been investigated for modeling character - level information, among other nlp tasks', '.  #AUTHOR_TAG and  #AUTHOR_TAG successfully employed cnns to extract character - level features for use in ner and pos - tagging respectively.  #AUTHOR_TAG b ) also applied cnn', '##s to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures  #AUTHOR_TAG. however, the effectiveness of character - level cnns has not been evaluated for english ner. while we considered using character - level bi -', 'directional lstms, which was recently proposed by  #AUTHOR_TAG for postagging, preliminary evaluation shows that it does not perform significantly better than cnns while being more computationally expensive to train. our main contribution lies in', 'combining these neural network models for the ner task. we present a hybrid model of bi - directional lstms and', 'cnns that learns both character - and word - level features, presenting the first evaluation of such an architecture on well', '- established english language evaluation datasets. furthermore, as lexicons are crucial to ner performance, we propose a new lexicon encoding scheme and matching algorithm that can', 'make use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes 5. 0 datasets']",1
"['of  #TAUTHOR_TAG, where lookup tables']","['of  #TAUTHOR_TAG, where lookup tables']","['neural network is inspired by the work of  #TAUTHOR_TAG, where lookup tables transform discrete features such as words and characters']","['neural network is inspired by the work of  #TAUTHOR_TAG, where lookup tables transform discrete features such as words and characters into continuous vector representations, which are then concatenated and fed into a neural network.', 'instead of a feed - forward network, we use the bi - directional long - short term memory ( blstm ) network.', 'to induce character - level features, we use a convolutional neural network, which has been successfully applied to spanish and portuguese ner  #AUTHOR_TAG and german pos - tagging  #AUTHOR_TAG']",1
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. extensive', 'evaluation shows that']","['use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes']","['relations between words. second, by depending solely on', 'word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare', 'words where word embeddings are poorly trained. we seek to address these issues by proposing a more powerful neural network model. a well - studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network ( rnn )  #AUTHOR_TAG. recently, rnns have shown great success', 'in diverse nlp tasks such as speech recognition  #AUTHOR_TAG, machine', 'translation  #AUTHOR_TAG, and language modeling  #AUTHOR_TAG. the long - short term memory ( lstm ) unit with the forget gate allows highly non - trivial long - distance dependencies to be easily learned  #AUTHOR_TAG. for sequential', 'labelling tasks such as ner and speech recognition, a bi - directional lstm model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context', 'that applies to any feed - forward model  #AUTHOR_TAG. while lstms have', 'been studied in the past for the ner task by  #AUTHOR_TAG, the lack of computational power ( which led to the use of very small models ) and quality', 'word embeddings limited their effectiveness. convolutional neural networks ( cnn ) have also been investigated for modeling character - level information, among other nlp tasks', '.  #AUTHOR_TAG and  #AUTHOR_TAG successfully employed cnns to extract character - level features for use in ner and pos - tagging respectively.  #AUTHOR_TAG b ) also applied cnn', '##s to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures  #AUTHOR_TAG. however, the effectiveness of character - level cnns has not been evaluated for english ner. while we considered using character - level bi -', 'directional lstms, which was recently proposed by  #AUTHOR_TAG for postagging, preliminary evaluation shows that it does not perform significantly better than cnns while being more computationally expensive to train. our main contribution lies in', 'combining these neural network models for the ner task. we present a hybrid model of bi - directional lstms and', 'cnns that learns both character - and word - level features, presenting the first evaluation of such an architecture on well', '- established english language evaluation datasets. furthermore, as lexicons are crucial to ner performance, we propose a new lexicon encoding scheme and matching algorithm that can', 'make use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes 5. 0 datasets']",5
"['available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the re']","['available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the reuters rcv - 1 corpus.', 'we also experimented with two other sets of published embeddings,']","['best model uses the publicly available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the reuters rcv - 1 corpus.', 'we also experimented with two other sets of published embeddings,']","['best model uses the publicly available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the reuters rcv - 1 corpus.', ""we also experimented with two other sets of published embeddings, namely stanford's glove embeddings 3 trained on 6 billion words from wikipedia and web text  #AUTHOR_TAG and google's word2vec embeddings 4 trained on 100 billion words from google news  #AUTHOR_TAG."", 'in addition, as we hypothesized that word embeddings trained on in - domain text may perform better, we also used the publicly available glove  #AUTHOR_TAG program and an in - house re - implementation 5 of the word2vec  #AUTHOR_TAG program to train word embeddings on wikipedia and reuters rcv1 datasets as well.', '6 following  #TAUTHOR_TAG, all words are lower - cased before passing through the lookup table text hayao tada, commander of the japanese north china area army to convert to their corresponding embeddings.', 'the pre - trained embeddings are allowed to be modified during training.', '']",5
"['available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the re']","['available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the reuters rcv - 1 corpus.', 'we also experimented with two other sets of published embeddings,']","['best model uses the publicly available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the reuters rcv - 1 corpus.', 'we also experimented with two other sets of published embeddings,']","['best model uses the publicly available 50dimensional word embeddings released by  #TAUTHOR_TAG 2, which were trained on wikipedia and the reuters rcv - 1 corpus.', ""we also experimented with two other sets of published embeddings, namely stanford's glove embeddings 3 trained on 6 billion words from wikipedia and web text  #AUTHOR_TAG and google's word2vec embeddings 4 trained on 100 billion words from google news  #AUTHOR_TAG."", 'in addition, as we hypothesized that word embeddings trained on in - domain text may perform better, we also used the publicly available glove  #AUTHOR_TAG program and an in - house re - implementation 5 of the word2vec  #AUTHOR_TAG program to train word embeddings on wikipedia and reuters rcv1 datasets as well.', '6 following  #TAUTHOR_TAG, all words are lower - cased before passing through the lookup table text hayao tada, commander of the japanese north china area army to convert to their corresponding embeddings.', 'the pre - trained embeddings are allowed to be modified during training.', '']",5
"['add a capitalization feature with the following options : allcaps, upperinitial, lowercase, mixedcaps, noinfo  #TAUTHOR_TAG.', 'this method is compared with the character type feature ( section 2. 5 ) and character - level cnns']","['add a capitalization feature with the following options : allcaps, upperinitial, lowercase, mixedcaps, noinfo  #TAUTHOR_TAG.', 'this method is compared with the character type feature ( section 2. 5 ) and character - level cnns']","['add a capitalization feature with the following options : allcaps, upperinitial, lowercase, mixedcaps, noinfo  #TAUTHOR_TAG.', 'this method is compared with the character type feature ( section 2. 5 ) and character - level cnns']","[""capitalization information is erased during lookup of the word embedding, we evaluate collobert's method of using a separate lookup table to add a capitalization feature with the following options : allcaps, upperinitial, lowercase, mixedcaps, noinfo  #TAUTHOR_TAG."", 'this method is compared with the character type feature ( section 2. 5 ) and character - level cnns']",5
"['5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['4. 5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['', 'all matches are case insensitive.', 'for each token in the match, the feature is en - coded in bioes annotation ( begin, inside, outside, end, single ), indicating the position of the token in the matched entry.', 'in other words, b will not appear in a suffix - only partial match, and e will not appear in a prefix - only partial match.', 'as we will see in section 4. 5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with yes / no.', 'in addition, since  #TAUTHOR_TAG released their lexicon with their senna system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features.', 'we found that the two lexicons complement each other and improve performance on the conll - 2003 dataset.', 'our best model uses the senna lexicon with exact matching and']",5
"['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag']","['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag - transition matrix a where a i, j represents the score of jumping from tag i to tag j in successive tokens, and a 0, i as the score']","['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag - transition matrix a where a i, j represents the score of jumping from tag i to tag j in successive tokens, and a 0, i as the score']","['train our network to maximize the sentencelevel log - likelihood from  #TAUTHOR_TAG.', '17 first, we define a tag - transition matrix a where a i, j represents the score of jumping from tag i to tag j in successive tokens, and a 0, i as the score for starting with tag i. this matrix of parameters are also learned.', 'define θ as the set of parameters for the neural network, and θ = θ ∪ { a i, j ∀i, j } as the set of all parameters to be trained.', 'given an example sentence, [ x ] t 1, of length t, and define [ f θ ] i, t as the score outputted by the neural network for the t th word and i th tag given parameters θ, then the score of a sequence of tags [ i ] t 1 is given as the sum of network and transition scores : then, letting [ y ] t 1 be the true tag sequence, the sentence - level log - likelihood is obtained by normalizing the above score over all possible tag - sequences [ j ] t 1 using a softmax :', 'this objective function and its gradients can be efficiently computed by dynamic programming  #TAUTHOR_TAG.', 'at inference time, given neural network outputs [ f θ ] i, t we use the viterbi algorithm to find the tag sequence [ i ] t 1 that maximizes the score', 'the output tags are annotated with bioes ( which stand for begin, inside, outside, end, single, indicating the position of the token in the 18 ontonotes results taken from  #AUTHOR_TAG 19 evaluation on ontonotes 5. 0 done by  #AUTHOR_TAG 20 not directly comparable as they evaluated on an earlier version of the corpus with a different data split.', '21 numbers taken from the original paper  #AUTHOR_TAG.', '']",5
['re - implemented the ffnn model of  #TAUTHOR_TAG as a baseline for comparison'],"['re - implemented the ffnn model of  #TAUTHOR_TAG as a baseline for comparison.', 'table 5 shows that while performing reasonably']",['re - implemented the ffnn model of  #TAUTHOR_TAG as a baseline for comparison'],"['re - implemented the ffnn model of  #TAUTHOR_TAG as a baseline for comparison.', 'table 5 shows that while performing reasonably well on conll - 2003, ffnns are clearly inadequate for ontonotes, which has a larger domain, showing that lstm models are essential for ner']",5
"['a direct comparison to  #TAUTHOR_TAG, we']","['a direct comparison to  #TAUTHOR_TAG, we']","['a direct comparison to  #TAUTHOR_TAG, we do not exclude']","['possible reason that collobert embeddings perform better than other publicly available embeddings on conll - 2003 is that they are trained on the reuters rcv - 1 corpus, the source of the conll - 2003 dataset, whereas the other embeddings are not 28.', ""on the other hand, we suspect that google's embeddings perform poorly because of vocabulary mismatch - in particular, google's embeddings were trained in a case - sensitive manner, and embeddings for many common punctuations and 27 wilcoxon rank sum test, p < 0. 001 28 to make a direct comparison to  #TAUTHOR_TAG, we do not exclude the conll - 2003 ner task test data from the word vector training data."", 'while it is possible that this difference could be responsible for the disparate performance of word vectors, the conll - 2003 training data comprises only 20k out of 800 million words, or 0. 00002 % of the total data ; in an unsupervised training scheme, the effects are likely negligible.', 'symbols were not provided.', 'to test these hypotheses, we performed experiments with new word embeddings trained using glove and word2vec, with vocabulary list and corpus similar to collobert et.', 'al. ( 2011b ).', '']",5
"['method as  #TAUTHOR_TAG,']","['method as  #TAUTHOR_TAG,']","['as  #TAUTHOR_TAG,']","['order to isolate the contribution of each lexicon and matching method, we compare different sources and matching methods on a blstm - cnn model with randomly initialized word embeddings and no 32 wilcoxon rank sum test, p < 0. 001.', 'other features or sources of external knowledge.', 'table 9 shows the results.', 'in this weakened model, both lexicons contribute significant 33 improvements over the baseline.', 'compared to the senna lexicon, our dbpedia lexicon is noisier but has broader coverage, which explains why when applying it using the same method as  #TAUTHOR_TAG, it performs worse on conll - 2003 but better on ontonotesa dataset containing many more obscure named entities.', 'however, we suspect that the method of  #TAUTHOR_TAG is not noise resistant and therefore unsuitable for our lexicon because it fails to distinguish exact and partial matches 34 and does not set a minimum length for partial matching.', '35 instead, when we apply our superior partial matching algorithm and bioes encoding with our dbpedia lexicon, we gain a significant 36 improvement, allowing our lexicon to perform similarly to the senna lexicon.', 'unfortunately, as we could not reliably remove partial entries from the senna lexicon, we were unable to investigate whether or not our lexicon matching method would help in that lexicon.', '']",5
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. extensive', 'evaluation shows that']","['use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes']","['relations between words. second, by depending solely on', 'word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare', 'words where word embeddings are poorly trained. we seek to address these issues by proposing a more powerful neural network model. a well - studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network ( rnn )  #AUTHOR_TAG. recently, rnns have shown great success', 'in diverse nlp tasks such as speech recognition  #AUTHOR_TAG, machine', 'translation  #AUTHOR_TAG, and language modeling  #AUTHOR_TAG. the long - short term memory ( lstm ) unit with the forget gate allows highly non - trivial long - distance dependencies to be easily learned  #AUTHOR_TAG. for sequential', 'labelling tasks such as ner and speech recognition, a bi - directional lstm model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context', 'that applies to any feed - forward model  #AUTHOR_TAG. while lstms have', 'been studied in the past for the ner task by  #AUTHOR_TAG, the lack of computational power ( which led to the use of very small models ) and quality', 'word embeddings limited their effectiveness. convolutional neural networks ( cnn ) have also been investigated for modeling character - level information, among other nlp tasks', '.  #AUTHOR_TAG and  #AUTHOR_TAG successfully employed cnns to extract character - level features for use in ner and pos - tagging respectively.  #AUTHOR_TAG b ) also applied cnn', '##s to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures  #AUTHOR_TAG. however, the effectiveness of character - level cnns has not been evaluated for english ner. while we considered using character - level bi -', 'directional lstms, which was recently proposed by  #AUTHOR_TAG for postagging, preliminary evaluation shows that it does not perform significantly better than cnns while being more computationally expensive to train. our main contribution lies in', 'combining these neural network models for the ner task. we present a hybrid model of bi - directional lstms and', 'cnns that learns both character - and word - level features, presenting the first evaluation of such an architecture on well', '- established english language evaluation datasets. furthermore, as lexicons are crucial to ner performance, we propose a new lexicon encoding scheme and matching algorithm that can', 'make use of partial matches, and we compare it to the simpler approach of  #TAUTHOR_TAG. extensive', 'evaluation shows that our proposed method establishes a new state of the art on both the conll - 2003 ner shared task and the ontonotes 5. 0 datasets']",4
"['5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['4. 5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['', 'all matches are case insensitive.', 'for each token in the match, the feature is en - coded in bioes annotation ( begin, inside, outside, end, single ), indicating the position of the token in the matched entry.', 'in other words, b will not appear in a suffix - only partial match, and e will not appear in a prefix - only partial match.', 'as we will see in section 4. 5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with yes / no.', 'in addition, since  #TAUTHOR_TAG released their lexicon with their senna system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features.', 'we found that the two lexicons complement each other and improve performance on the conll - 2003 dataset.', 'our best model uses the senna lexicon with exact matching and']",4
"['5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['4. 5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches,']","['', 'all matches are case insensitive.', 'for each token in the match, the feature is en - coded in bioes annotation ( begin, inside, outside, end, single ), indicating the position of the token in the matched entry.', 'in other words, b will not appear in a suffix - only partial match, and e will not appear in a prefix - only partial match.', 'as we will see in section 4. 5, we found that this more sophisticated method outperforms the method presented by  #TAUTHOR_TAG, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with yes / no.', 'in addition, since  #TAUTHOR_TAG released their lexicon with their senna system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features.', 'we found that the two lexicons complement each other and improve performance on the conll - 2003 dataset.', 'our best model uses the senna lexicon with exact matching and']",3
"['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['of neural word embeddings,  #TAUTHOR_TAG presented senna, which']","['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['many approaches involve crf models, there has also been a long history of research involving neural networks.', 'early attempts were hindered by table 10 : per genre f1 scores on ontonotes.', 'bc = broadcast conversation, bn = broadcast news, mz = magazine, nw = newswire, tc = telephone conversation, wb = blogs and newsgroups lack of computational power, scalable learning algorithms, and high quality word embeddings.', ' #AUTHOR_TAG used a feed - forward neural network with one hidden layer on ner and achieved state - of - the - art results on the muc6 dataset.', 'their approach used only pos tag and gazetteer tags for each word, with no word embeddings.', ' #AUTHOR_TAG attempted ner with a singledirection lstm network and a combination of word vectors trained using self - organizing maps and context vectors obtained using principle component analysis.', 'however, while our method optimizes loglikelihood and uses softmax, they used a different output encoding and optimized an unspecified objective function.', ' #AUTHOR_TAG reported results were only slightly above baseline models.', 'much later, with the advent of neural word embeddings,  #TAUTHOR_TAG presented senna, which employs a deep ffnn and word embeddings to achieve near state of the art results on pos tagging, chunking, ner, and srl.', 'we build on their approach, sharing the word embeddings, feature encoding method, and objective functions.', ' #AUTHOR_TAG presented their charwnn network, which augments the neural network of  #TAUTHOR_TAG with character level cnns, and they reported improved performance on spanish and portuguese ner.', 'we have successfully incorporated character - level cnns into our model.', 'there have been various other similar architecture proposed for various sequential labeling nlp tasks.', 'used a blstm for the pos - tagging, chunking, and ner tasks, but they employed heavy feature engineering instead of using a cnn to automatically extract characterlevel features.', ' #AUTHOR_TAG used a brnn with character - level cnns to perform german postagging ; our model differs in that we use the more powerful lstm unit, which we found to perform better than rnns in preliminary experiments, and that we employ word embeddings, which is much more important in ner than in pos tagging.', ' #AUTHOR_TAG used both word - and character - level blstms to establish the current state of the art for english pos tagging.', 'while using blstms instead of cnns allows extraction of more sophisticated character - level features, we found in preliminary experiments that for ner it did not perform significantly better than cnns and was substantially more computationally expensive to train']",3
"['method as  #TAUTHOR_TAG,']","['method as  #TAUTHOR_TAG,']","['as  #TAUTHOR_TAG,']","['order to isolate the contribution of each lexicon and matching method, we compare different sources and matching methods on a blstm - cnn model with randomly initialized word embeddings and no 32 wilcoxon rank sum test, p < 0. 001.', 'other features or sources of external knowledge.', 'table 9 shows the results.', 'in this weakened model, both lexicons contribute significant 33 improvements over the baseline.', 'compared to the senna lexicon, our dbpedia lexicon is noisier but has broader coverage, which explains why when applying it using the same method as  #TAUTHOR_TAG, it performs worse on conll - 2003 but better on ontonotesa dataset containing many more obscure named entities.', 'however, we suspect that the method of  #TAUTHOR_TAG is not noise resistant and therefore unsuitable for our lexicon because it fails to distinguish exact and partial matches 34 and does not set a minimum length for partial matching.', '35 instead, when we apply our superior partial matching algorithm and bioes encoding with our dbpedia lexicon, we gain a significant 36 improvement, allowing our lexicon to perform similarly to the senna lexicon.', 'unfortunately, as we could not reliably remove partial entries from the senna lexicon, we were unable to investigate whether or not our lexicon matching method would help in that lexicon.', '']",7
"['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['of neural word embeddings,  #TAUTHOR_TAG presented senna, which']","['of neural word embeddings,  #TAUTHOR_TAG presented sen']","['many approaches involve crf models, there has also been a long history of research involving neural networks.', 'early attempts were hindered by table 10 : per genre f1 scores on ontonotes.', 'bc = broadcast conversation, bn = broadcast news, mz = magazine, nw = newswire, tc = telephone conversation, wb = blogs and newsgroups lack of computational power, scalable learning algorithms, and high quality word embeddings.', ' #AUTHOR_TAG used a feed - forward neural network with one hidden layer on ner and achieved state - of - the - art results on the muc6 dataset.', 'their approach used only pos tag and gazetteer tags for each word, with no word embeddings.', ' #AUTHOR_TAG attempted ner with a singledirection lstm network and a combination of word vectors trained using self - organizing maps and context vectors obtained using principle component analysis.', 'however, while our method optimizes loglikelihood and uses softmax, they used a different output encoding and optimized an unspecified objective function.', ' #AUTHOR_TAG reported results were only slightly above baseline models.', 'much later, with the advent of neural word embeddings,  #TAUTHOR_TAG presented senna, which employs a deep ffnn and word embeddings to achieve near state of the art results on pos tagging, chunking, ner, and srl.', 'we build on their approach, sharing the word embeddings, feature encoding method, and objective functions.', ' #AUTHOR_TAG presented their charwnn network, which augments the neural network of  #TAUTHOR_TAG with character level cnns, and they reported improved performance on spanish and portuguese ner.', 'we have successfully incorporated character - level cnns into our model.', 'there have been various other similar architecture proposed for various sequential labeling nlp tasks.', 'used a blstm for the pos - tagging, chunking, and ner tasks, but they employed heavy feature engineering instead of using a cnn to automatically extract characterlevel features.', ' #AUTHOR_TAG used a brnn with character - level cnns to perform german postagging ; our model differs in that we use the more powerful lstm unit, which we found to perform better than rnns in preliminary experiments, and that we employ word embeddings, which is much more important in ner than in pos tagging.', ' #AUTHOR_TAG used both word - and character - level blstms to establish the current state of the art for english pos tagging.', 'while using blstms instead of cnns allows extraction of more sophisticated character - level features, we found in preliminary experiments that for ner it did not perform significantly better than cnns and was substantially more computationally expensive to train']",6
"['', 'recent papers  #TAUTHOR_TAG,']","['high computational requirements.', 'recent papers  #TAUTHOR_TAG, [ 4 ] in neural machine']","['high computational requirements.', 'recent papers  #TAUTHOR_TAG,']","['', 'since their emergence, attention mechanisms [ 2 ] have added to the effectiveness of the encoder decoder model and have been at the forefront of machine translation.', 'attention mechanisms help the neural system focus on parts of the input, and possibly the output as it learns to translate.', 'this concentration facilitates the capturing of dependencies between parts of the input and the output.', 'after training the network, the attention mechanism enables the system to perform translations that can handle issues such as the movement of words and phrases, and fertility.', 'however, even with these attention mechanisms, nmt models have their drawbacks, which include long training time and high computational requirements.', 'recent papers  #TAUTHOR_TAG, [ 4 ] in neural machine translation have proposed the strict use of attention mechanisms in networks such as the transformer over previous approaches such as recurrent neural networks ( rnns ) [ 5 ] and convolutional neural networks ( cnns ) [ 6 ].', 'in other words, these approaches dispense with recurrences and convolutions entirely.', 'in practice, attention mechanisms have mostly been used with recurrent architectures because removing the recurrent nature of the architecture makes the training more efficient by the removal of necessary sequential steps.', 'this paper contributes by continuing to pursue the removal of sequential operations within encoder - decoder models.', '']",0
['.  #TAUTHOR_TAG proposed the reduction in the sequential steps seen in cnns and rn'],['al.  #TAUTHOR_TAG proposed the reduction in the sequential steps seen in cnns and'],['.  #TAUTHOR_TAG proposed the reduction in the sequential steps seen in cnns and rn'],"['has been a plethora of work in the past several years on end - to - end neural translation.', 'bytenet [ 7 ] uses cnns with dilated convolutions for both encoding and decoding.', 'zhou et al. [ 8 ] use stacked interleaved bi - directional lstm layers ( up to 16 layers ) with skipped connections ; ensembling gives the best results.', ""google's earlier and path - breaking endto - end translation approach [ 9 ] uses 16 lstm layers with attention ; once again, ensembling produces the best results."", ""facebook's end - to - end translation approach [ 10 ] depends entirely on cnns with attention mechanism."", 'our work reported in this paper is based on another translation work by google.', ""google's vaswani et al.  #TAUTHOR_TAG proposed the reduction in the sequential steps seen in cnns and rnns."", 'the sole use of attention mechanisms and feed - forward networks within the common encoder - decoder sequential model replaces the necessity of deep convolutions for distant dependent relationships, and the memory and computation intensive operations required within recurrent networks.', 'original training and testing by vaswani et al. were over both the wmt 2014 english - french ( en - fe ) and english - german ( en - de ) data sets, while this paper uses only the wmt 2014 en - de set and the iwslt 2014 en - de and en - fr data sets.', 'this model is discussed later in the paper.', 'works in the field of nmt recommend a particular focus on the encoder.', 'analysis by domhan [ 11 ] poses two questions : what type of attention is needed, and where.', 'in this analysis, self - attention had a higher correspondence with accuracy when placed in the encoder section of the architecture than the decoder, even claiming that the decoder, when replaced with a cnn or rnn, retained the same accuracy with little to no loss in robustness.', ""imamura, fujita, and sumita's [ 12 ] study shows that the current paradigm of using high - volume sets of parallel corpora are sufficient for decoders but are unreliable for the encoder."", 'these conclusions encourage further research in the manipulation of position and design of the encoder and attention mechanisms within them']",0
"['.  #TAUTHOR_TAG, seen in']","['al.  #TAUTHOR_TAG, seen in']","["".  #TAUTHOR_TAG, seen in figure 1, inspires this paper's work."", 'we have made modifications to this architecture, to']","[""transformer architectures proposed by vaswani et al.  #TAUTHOR_TAG, seen in figure 1, inspires this paper's work."", 'we have made modifications to this architecture, to make it more efficient.', 'however, our modifications can be applied to any encoder - decoder based model and is architecture - agnostic.', 'these alterations follow from the following two hypotheses.', '1 ) reduction in the number of required sequential operations throughout the encoder section is likely to reduce training time without reducing performance.', '2 ) replacing the subsequent encoder attention stack is expected to result in discarding of inter - dependencies, and possibly incorrect, assumptions of encoder attention mechanisms and layers, improving performance.', 'for simplification, but without loss of generalization, this paper discusses the use and modification of transformer based - models.', 'the original transformer model is composed of stacked self - attention layers.', 'these self - attention mechanisms compare and relate multiple positions of one sequence in order to find a representation of itself.', '']",0
['and cnns  #TAUTHOR_TAG'],['rnns and cnns  #TAUTHOR_TAG'],"['and cnns  #TAUTHOR_TAG.', ""this was done by simplifying and limiting sequential operations and computational requirements while also increasing the model's ability to exploit current hardware architecture."", 'this paper proposes that removal of']","['motivation for creating the transformer model was the sluggish training and generation times of other common sequence - to - sequence models such as rnns and cnns  #TAUTHOR_TAG.', ""this was done by simplifying and limiting sequential operations and computational requirements while also increasing the model's ability to exploit current hardware architecture."", ""this paper proposes that removal of the previously stacked branches of the encoder ( there is a stack of n encoder and other blocks on the left side of figure 1 ), parallelizing these separate encoder'trees ', and incorporating their learned results for the decoder, will further eliminate sequential steps and accelerate learning within current sequence - to - sequence models."", 'the architectures discussed are modeled in figure 2.', 'alterations to this parallel transformer model were made and the following models were trained, tested, and are discussed in this paper :', '• additive parallel attention ( apa ), • attended concatenated parallel attention ( acpa ), and', '• attended additive parallel attention ( aapa )']",0
['proposed architectures including the base transformer model  #TAUTHOR_TAG are'],['proposed architectures including the base transformer model  #TAUTHOR_TAG are'],['proposed architectures including the base transformer model  #TAUTHOR_TAG are trained over the international workshop on spoken language translation ( iwslt ) 2016 corpus'],"['proposed architectures including the base transformer model  #TAUTHOR_TAG are trained over the international workshop on spoken language translation ( iwslt ) 2016 corpus and tested similarly over the iwslt 2014 test corpus [ 15 ].', 'the training corpus includes over 200, 000 parallel sentence pairs, and 4 million tokens for each language.', 'the testing set contains 1, 250 sentences, and 20 - 30 thousand tokens for french and german.', 'this paper also performed experiments over the larger wmt data set including 4. 5 and 36 million training sentence pairs for the en - de and en - fr tasks respectively.', 'the testing set for these experiments was the standard newstest 2014 test set including around 3000 sentence pairs for each language task.', 'these statistics are noted in table i a full measure of the tested models and robustness to both short and long input.', 'across all models, a greedy - decoding function for both training and testing time, the kullback - leibler divergence loss function, the adam optimizer [ 16 ], and the number of training epochs ( 10 ) were kept constant.', 'the training and testing were done using the nmt task of english to german ( en - de ) and iwslt english to french and english to german translation and each network was trained using one graphics processing unit ( gpu ).', 'the utilized machine gpu configuration was one nvidia gtx 1070.', 'for the assessment of each model and translation task this paper uses the bilingual evaluation understudy ( bleu ) metric [ 17 ].', 'this is a modified precision calculation using ngrams such as unigram, grouped unigrams, and bigrams.', 'the bleu metric claims to have a high correlation to translation quality judgments made by humans.', 'bleu computes scores for individual sentences by comparing them with good quality reference translations.', 'the individual scores are averaged over the the entire corpus, without taking intelligibility or grammatical correctness into account']",0
"['.  #TAUTHOR_TAG, seen in']","['al.  #TAUTHOR_TAG, seen in']","["".  #TAUTHOR_TAG, seen in figure 1, inspires this paper's work."", 'we have made modifications to this architecture, to']","[""transformer architectures proposed by vaswani et al.  #TAUTHOR_TAG, seen in figure 1, inspires this paper's work."", 'we have made modifications to this architecture, to make it more efficient.', 'however, our modifications can be applied to any encoder - decoder based model and is architecture - agnostic.', 'these alterations follow from the following two hypotheses.', '1 ) reduction in the number of required sequential operations throughout the encoder section is likely to reduce training time without reducing performance.', '2 ) replacing the subsequent encoder attention stack is expected to result in discarding of inter - dependencies, and possibly incorrect, assumptions of encoder attention mechanisms and layers, improving performance.', 'for simplification, but without loss of generalization, this paper discusses the use and modification of transformer based - models.', 'the original transformer model is composed of stacked self - attention layers.', 'these self - attention mechanisms compare and relate multiple positions of one sequence in order to find a representation of itself.', '']",3
"['.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of']","['al.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of 62. 69 compared to 60. 95 and 61. 00 for the two transformers shown in table iii.', 'our approach also takes considerably less time']","['.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of']","['the much larger wmt english - german test set, all our models achieve better results then vaswani et al.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of 62. 69 compared to 60. 95 and 61. 00 for the two transformers shown in table iii.', 'our approach also takes considerably less time than the large transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller transformer model reported by vaswani et al.  #TAUTHOR_TAG.', 'in terms of the bleu metric, we establish state - of - the - art performance for both en - de and en - fr translation considering the iwslt 2014, and comparable results for the wmt data sets.', 'since our results came up very good, surpassing state of the art for the iwslt 2014 dataset, we ran our experiments multiple times to ensure the results are correct.', ""during the transformer and attended parallel model's training lifetime, it can be seen that loss was consistently lower for our modified parallel model with five parallel stacks as seen in figure 3."", ""in this task, loss doesn't always correspond to a higher metric, in this case our model also shows a continuous higher score in the bleu metric over the validation set while the transformer shows signs of plateauing early on figure 4."", 'however, our parallelized model did have a slightly higher training time over a single gpu.', 'one final experiment conducted to improve this drawback, also seen in the same table, is the reduction of number of parallel branches in the encoder.', 'by reducing the number incrementally, our bleu score stays equivalent to higher perplexity layers, but linearly reduces the run - time']",4
"['.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of']","['al.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of 62. 69 compared to 60. 95 and 61. 00 for the two transformers shown in table iii.', 'our approach also takes considerably less time']","['.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of']","['the much larger wmt english - german test set, all our models achieve better results then vaswani et al.  #TAUTHOR_TAG.', 'our model with five parallel encoding branches has a bleu score of 62. 69 compared to 60. 95 and 61. 00 for the two transformers shown in table iii.', 'our approach also takes considerably less time than the large transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller transformer model reported by vaswani et al.  #TAUTHOR_TAG.', 'in terms of the bleu metric, we establish state - of - the - art performance for both en - de and en - fr translation considering the iwslt 2014, and comparable results for the wmt data sets.', 'since our results came up very good, surpassing state of the art for the iwslt 2014 dataset, we ran our experiments multiple times to ensure the results are correct.', ""during the transformer and attended parallel model's training lifetime, it can be seen that loss was consistently lower for our modified parallel model with five parallel stacks as seen in figure 3."", ""in this task, loss doesn't always correspond to a higher metric, in this case our model also shows a continuous higher score in the bleu metric over the validation set while the transformer shows signs of plateauing early on figure 4."", 'however, our parallelized model did have a slightly higher training time over a single gpu.', 'one final experiment conducted to improve this drawback, also seen in the same table, is the reduction of number of parallel branches in the encoder.', 'by reducing the number incrementally, our bleu score stays equivalent to higher perplexity layers, but linearly reduces the run - time']",4
"['cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural']","['cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural']","['cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model']","['proposed a seminal neural architecture for sequence labeling.', 'it captures word sequence information with a one - layer cnn based on pretrained word embeddings and handcrafted neural features, followed with a crf output layer.', 'dos  #AUTHOR_TAG extended this model by integrating character - level cnn features.', ' #AUTHOR_TAG built a deeper dilated cnn architecture to capture larger local features.', ' #AUTHOR_TAG was the first to exploit lstm for sequence labeling.', 'built a bilstm - crf structure, which has been extended by adding character - level lstm  #AUTHOR_TAG, gru  #AUTHOR_TAG, and cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model to improve ner models.', 'these models achieve state - of - the - art results in the literature.', ' #AUTHOR_TAG b ) compared several word - based lstm models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.', 'they investigated the influence of various hyperparameters and configurations.', 'our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects : 1 ) their experiments are based on a bilstm with handcrafted word features, while our experiments are based on end - to - end neural models without human knowledge.', '2 ) their system gives relatively low performances on standard benchmarks 2, while ours can give comparable or better results with state - of - the - art models, rendering our observations more informative for practitioners.', '3 ) our findings are more consistent with most previous work on configurations such as usefulness of character information  #TAUTHOR_TAG and tag scheme  #AUTHOR_TAG.', 'in contrast, many results of  #AUTHOR_TAG b ) contradict existing reports.', '4 ) we conduct a wider range of comparison for word sequence representations, including all combinations of character cnn / lstm and word cnn / lstm structures, while  #AUTHOR_TAG b ) studied the word lstm models only']",3
"['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits by using sections 0 - 18 as training set, sections 19 - 21 as development set and sections 22 - 24 as test set.', 'no preprocessing is performed on either dataset except for normalizing digits.', 'the dataset statistics are listed in table 2.', 'hyperparameters.', 'table 3 shows the hyperparameters used in our experiments, which mostly follow  #TAUTHOR_TAG, including the learning rate η = 0. 015 for word lstm models.', 'for word cnn based models, a large η leads to convergence problem.', 'we take η = 0. 005 with more epochs ( 200 ) instead.', 'glove 100 - dimension  #AUTHOR_TAG is used to initialize word embeddings and character embeddings are randomly initialized.', 'we use mini - batch stochastic gradient descent ( sgd ) with a decayed learning rate to update parameters.', 'for ner and chunking, we the bioes tag scheme.', 'evaluation.', '']",3
"['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']","['4, 5 and 6 show the results of the twelve models on ner, chunking and pos datasets, respectively.', 'existing work has also been listed in the tables for comparison.', 'to simplify the description, we use "" clstm "" and "" ccnn "" to represent character lstm and character cnn encoder, respectively.', 'similarly, "" wlstm "" and "" wcnn "" represent word lstm and word cnn structure, respectively.', 'as shown in table 4, most ner work focuses on wlstm + crf structures with different character sequence representations.', 'we re - implement the structure of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']",3
"['of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine']","['of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine']","['of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine two different tag schemes : bio and bioes  #AUTHOR_TAG.', 'the results are shown in figure 4 ( b ).', 'in our experiments, models using bioes are significantly (']","['addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance.', 'we investigate a set of external factors on the ner dataset with the two best models : clstm + wlstm + crf and ccnn + wlstm + crf.', 'pretrained embedding.', 'figure 4 ( a ) shows the f1 - scores of the two best models on the ner test set with two different pretrained embeddings, as well as the random initialization.', 'compared with the random initialization, models using pretrained embeddings give significant improvements ( p < 0. 01 ).', 'the glove 100 - dimension embeddings give higher f1 - scores than senna  #AUTHOR_TAG on both models, which is consistent with the observation of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine two different tag schemes : bio and bioes  #AUTHOR_TAG.', 'the results are shown in figure 4 ( b ).', 'in our experiments, models using bioes are significantly ( p < 0. 05 ) better than bio.', 'our observation is consistent with most literature  #AUTHOR_TAG.', ' #AUTHOR_TAG b ) report that the difference between the schemes is insignificant.', 'running environment.', ' #AUTHOR_TAG observe that neural sequence labeling models can give better results on gpu rather than cpu.', 'we conduct repeated experiments on both gpu and cpu environments.', 'the results are shown in figure 4 ( b ).', 'models run on cpu give a lower mean f1 - score than models run on gpu, while the difference is insignificant ( p > 0. 2 ).', 'optimizer.', 'we compare different optimizers including sgd, adagrad  #AUTHOR_TAG, adadelta  #AUTHOR_TAG rmsprop  #AUTHOR_TAG and adam  #AUTHOR_TAG.', 'the results are shown in figure 5 5.', 'in contrast to  #AUTHOR_TAG b ), who reported that sgd is the worst optimizer, our results show that sgd outperforms all other optimizers significantly ( p < 0. 01 ), with a slower convergence process during training.', 'our observation is consistent with most literature  #TAUTHOR_TAG']",3
"['of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine']","['of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine']","['of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine two different tag schemes : bio and bioes  #AUTHOR_TAG.', 'the results are shown in figure 4 ( b ).', 'in our experiments, models using bioes are significantly (']","['addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance.', 'we investigate a set of external factors on the ner dataset with the two best models : clstm + wlstm + crf and ccnn + wlstm + crf.', 'pretrained embedding.', 'figure 4 ( a ) shows the f1 - scores of the two best models on the ner test set with two different pretrained embeddings, as well as the random initialization.', 'compared with the random initialization, models using pretrained embeddings give significant improvements ( p < 0. 01 ).', 'the glove 100 - dimension embeddings give higher f1 - scores than senna  #AUTHOR_TAG on both models, which is consistent with the observation of  #TAUTHOR_TAG.', 'tag scheme.', 'we examine two different tag schemes : bio and bioes  #AUTHOR_TAG.', 'the results are shown in figure 4 ( b ).', 'in our experiments, models using bioes are significantly ( p < 0. 05 ) better than bio.', 'our observation is consistent with most literature  #AUTHOR_TAG.', ' #AUTHOR_TAG b ) report that the difference between the schemes is insignificant.', 'running environment.', ' #AUTHOR_TAG observe that neural sequence labeling models can give better results on gpu rather than cpu.', 'we conduct repeated experiments on both gpu and cpu environments.', 'the results are shown in figure 4 ( b ).', 'models run on cpu give a lower mean f1 - score than models run on gpu, while the difference is insignificant ( p > 0. 2 ).', 'optimizer.', 'we compare different optimizers including sgd, adagrad  #AUTHOR_TAG, adadelta  #AUTHOR_TAG rmsprop  #AUTHOR_TAG and adam  #AUTHOR_TAG.', 'the results are shown in figure 5 5.', 'in contrast to  #AUTHOR_TAG b ), who reported that sgd is the worst optimizer, our results show that sgd outperforms all other optimizers significantly ( p < 0. 01 ), with a slower convergence process during training.', 'our observation is consistent with most literature  #TAUTHOR_TAG']",3
"['sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being']","['sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being']","['sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being']","['neural sequence labeling framework contains three layers, i. e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in figure 1.', 'character information has been proven to be critical for sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being used to model character sequence information ( "" char rep. "" ).', 'similarly, on the word level, lstm or cnn structures can be leveraged to capture long - term information or local features ( "" word rep. "" ), respectively.', 'subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations']",5
"['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['features such as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure to encode character sequences was firstly proposed by  #AUTHOR_TAG, and followed by many subsequent investigations ( dos  #TAUTHOR_TAG.', 'in our experiments, we take the same structure as  #TAUTHOR_TAG, using one layer cnn structure with max - pooling to capture character - level representations.', 'figure 2 ( a ) shows the cnn structure on representing word "" mexico "".', 'character lstm.', '']",5
"['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['features such as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure to encode character sequences was firstly proposed by  #AUTHOR_TAG, and followed by many subsequent investigations ( dos  #TAUTHOR_TAG.', 'in our experiments, we take the same structure as  #TAUTHOR_TAG, using one layer cnn structure with max - pooling to capture character - level representations.', 'figure 2 ( a ) shows the cnn structure on representing word "" mexico "".', 'character lstm.', '']",5
"['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits by using sections 0 - 18 as training set, sections 19 - 21 as development set and sections 22 - 24 as test set.', 'no preprocessing is performed on either dataset except for normalizing digits.', 'the dataset statistics are listed in table 2.', 'hyperparameters.', 'table 3 shows the hyperparameters used in our experiments, which mostly follow  #TAUTHOR_TAG, including the learning rate η = 0. 015 for word lstm models.', 'for word cnn based models, a large η leads to convergence problem.', 'we take η = 0. 005 with more epochs ( 200 ) instead.', 'glove 100 - dimension  #AUTHOR_TAG is used to initialize word embeddings and character embeddings are randomly initialized.', 'we use mini - batch stochastic gradient descent ( sgd ) with a decayed learning rate to update parameters.', 'for ner and chunking, we the bioes tag scheme.', 'evaluation.', '']",5
"['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits by using sections 0 - 18 as training set, sections 19 - 21 as development set and sections 22 - 24 as test set.', 'no preprocessing is performed on either dataset except for normalizing digits.', 'the dataset statistics are listed in table 2.', 'hyperparameters.', 'table 3 shows the hyperparameters used in our experiments, which mostly follow  #TAUTHOR_TAG, including the learning rate η = 0. 015 for word lstm models.', 'for word cnn based models, a large η leads to convergence problem.', 'we take η = 0. 005 with more epochs ( 200 ) instead.', 'glove 100 - dimension  #AUTHOR_TAG is used to initialize word embeddings and character embeddings are randomly initialized.', 'we use mini - batch stochastic gradient descent ( sgd ) with a decayed learning rate to update parameters.', 'for ner and chunking, we the bioes tag scheme.', 'evaluation.', '']",5
"['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']","['4, 5 and 6 show the results of the twelve models on ner, chunking and pos datasets, respectively.', 'existing work has also been listed in the tables for comparison.', 'to simplify the description, we use "" clstm "" and "" ccnn "" to represent character lstm and character cnn encoder, respectively.', 'similarly, "" wlstm "" and "" wcnn "" represent word lstm and word cnn structure, respectively.', 'as shown in table 4, most ner work focuses on wlstm + crf structures with different character sequence representations.', 'we re - implement the structure of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']",5
"['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']","['4, 5 and 6 show the results of the twelve models on ner, chunking and pos datasets, respectively.', 'existing work has also been listed in the tables for comparison.', 'to simplify the description, we use "" clstm "" and "" ccnn "" to represent character lstm and character cnn encoder, respectively.', 'similarly, "" wlstm "" and "" wcnn "" represent word lstm and word cnn structure, respectively.', 'as shown in table 4, most ner work focuses on wlstm + crf structures with different character sequence representations.', 'we re - implement the structure of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']",5
"['out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are']","['out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are']","['out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are divided into four subsets : in - vocabulary words, out - of - training - vocabulary words']","['', 'we conduct error analysis on in - vocabulary and out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are divided into four subsets : in - vocabulary words, out - of - training - vocabulary words ( ootv ), out - of - embedding - vocabulary words ( ooev ) and out - of - both - vocabulary words ( oobv ).', '']",5
"['out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are']","['out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are']","['out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are divided into four subsets : in - vocabulary words, out - of - training - vocabulary words']","['', 'we conduct error analysis on in - vocabulary and out - of - vocabulary words with the crf based models 6.', 'following  #TAUTHOR_TAG, words in the test set are divided into four subsets : in - vocabulary words, out - of - training - vocabulary words ( ootv ), out - of - embedding - vocabulary words ( ooev ) and out - of - both - vocabulary words ( oobv ).', '']",5
[' #TAUTHOR_TAG computes the distance of'],['to fsd  #TAUTHOR_TAG computes the distance of'],"['', 'the traditional approach to fsd  #TAUTHOR_TAG computes the distance of each incoming document 1 e']","['story detection ( fsd ), also called new event detection, is the task of identifying the very first document in a stream to mention a new event 1.', 'fsd was introduced as port of the tdt 2 initiative and has direct applications in finance, news and government security.', 'the most accurate approaches to fsd involve a runtime of o ( n 2 ) and cannot scale to unbounded high volume streams such as twitter.', 'we present a novel approach to fsd that operates in o ( 1 ) per tweet.', 'our method is able to process the load of the average twitter firehose 3 stream on a single core of modest hardware while retaining effectiveness on par with one of the most accurate fsd systems.', 'during the tdt program, fsd was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability.', 'the traditional approach to fsd  #TAUTHOR_TAG computes the distance of each incoming document 1 e. g. a natural disaster or a scandal 2 tdt by nist - 1998 nist - - 2004.', '']",0
"[',  #TAUTHOR_TAG additionally compared']","['number of tweets per bucket constant.', 'because lsh alone performed ineffectively,  #TAUTHOR_TAG additionally compared']","[',  #TAUTHOR_TAG additionally compared each incoming tweet with the k most recent tweets.', ' #AUTHOR_TAG analysed scoring functions for novelty detection while focusing on their effectiveness.', 'they presented a language - model ( lm ) based novelty measure using the kl divergence between the lm of a document and a single lm built on all previously scored']","['', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, scale their systems by only considering tweets containing hashtags.', ""although efficient, this method don't consider 90 % of the tweets  #AUTHOR_TAG, which limits their scope."", ' #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG use the degree of burstiness of terms during a time interval to detect new events.', 'this approach is not suitable for fsd as events are detected with a time lag, once they grow in popularity.', ' #AUTHOR_TAG were the first to demonstrate fsd on twitter in constant time and space, while maintaining effectiveness comparable to those of pair - wise comparison systems.', 'the key was to reduce the search space using locality sensitive hashing ( lsh ).', 'each tweet was hashed, placing it into buckets that contain other similar tweets, which are subsequently compared.', 'operation in constant space was ensured by keeping the number of tweets per bucket constant.', 'because lsh alone performed ineffectively,  #TAUTHOR_TAG additionally compared each incoming tweet with the k most recent tweets.', ' #AUTHOR_TAG analysed scoring functions for novelty detection while focusing on their effectiveness.', 'they presented a language - model ( lm ) based novelty measure using the kl divergence between the lm of a document and a single lm built on all previously scored documents, which they referred to as an aggregate measure language model.', 'the idea of maintaining a single representation covering all previously seen documents, instead of performing pairwise comparisons with every document is closely related to the approach presented in this paper']",0
"['systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['compare our system ( k - term ) against 3 baselines.', 'umass is a state - of - the - art fsd system, developed by  #AUTHOR_TAG.', 'it is known for its high effectiveness in the tdt2 and tdt3 competitions  #AUTHOR_TAG and widely used as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', 'umass makes use of an inverted index and k - nearest - neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.', 'to maximise efficiency, we set - up umass to operate in - memory by turning off its default memory mapping to disk.', 'this ensures fair comparisons, as all algorithms operate in memory.', 'lsh - fsd is a highly - scalable system by  #TAUTHOR_TAG.', 'it is based on locality sensitive hashing ( lsh ) and claims to operate in constant time and space while performing on a comparable level of accuracy as umass.', 'we configure their system using the default parameters  #TAUTHOR_TAG']",0
"['systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['compare our system ( k - term ) against 3 baselines.', 'umass is a state - of - the - art fsd system, developed by  #AUTHOR_TAG.', 'it is known for its high effectiveness in the tdt2 and tdt3 competitions  #AUTHOR_TAG and widely used as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', 'umass makes use of an inverted index and k - nearest - neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.', 'to maximise efficiency, we set - up umass to operate in - memory by turning off its default memory mapping to disk.', 'this ensures fair comparisons, as all algorithms operate in memory.', 'lsh - fsd is a highly - scalable system by  #TAUTHOR_TAG.', 'it is based on locality sensitive hashing ( lsh ) and claims to operate in constant time and space while performing on a comparable level of accuracy as umass.', 'we configure their system using the default parameters  #TAUTHOR_TAG']",0
"['systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', '']","['compare our system ( k - term ) against 3 baselines.', 'umass is a state - of - the - art fsd system, developed by  #AUTHOR_TAG.', 'it is known for its high effectiveness in the tdt2 and tdt3 competitions  #AUTHOR_TAG and widely used as a benchmark for fsd systems  #TAUTHOR_TAG petrovic 2013 ; ).', 'umass makes use of an inverted index and k - nearest - neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.', 'to maximise efficiency, we set - up umass to operate in - memory by turning off its default memory mapping to disk.', 'this ensures fair comparisons, as all algorithms operate in memory.', 'lsh - fsd is a highly - scalable system by  #TAUTHOR_TAG.', 'it is based on locality sensitive hashing ( lsh ) and claims to operate in constant time and space while performing on a comparable level of accuracy as umass.', 'we configure their system using the default parameters  #TAUTHOR_TAG']",5
"['at different points in the stream.', 'although  #TAUTHOR_TAG designed their system ( lsh - fs']","['throughput to decrease drastically with every new document.', 'in figure 2 we compare the memory requirements of k - term and lsh - fsd at different points in the stream.', 'although  #TAUTHOR_TAG designed their system ( lsh - fsd ) to operate in constant space, we found that the memory requirement gradually increases with']","['at different points in the stream.', 'although  #TAUTHOR_TAG designed their system ( lsh - fs']","['- volume streams require operation in constant time and space.', 'figure 2 compares the change in throughput of lsh - fsd, umass and kterm as we process more and more tweets in the stream.', 'additionally, the plot also shows the average rate of tweets in the twitter firehose 6 at 5, 787 tweets per second.', 'note that our system processes the equivalent of the full twitter stream on a single core of modest hardware.', 'this surpasses the throughput of lsh - fsd, a system known for high efficiency, by more than an order of magnitude.', 'the throughput of lsh - fsd and k - term increases up until 20k documents because both approaches require initialisation of their data structures, which makes them slow when the number of documents is low.', 'umass has no initialization and performs the fastest when the number of documents is kept low.', ""the pair - wise comparison of umass causes it's throughput to decrease drastically with every new document."", 'in figure 2 we compare the memory requirements of k - term and lsh - fsd at different points in the stream.', 'although  #TAUTHOR_TAG designed their system ( lsh - fsd ) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in figure 3.', 'we hypothesise that this increase results from new terms added to the vocabulary.', 'our system has a strictly constant memory footprint']",4
[' #TAUTHOR_TAG was'],[' #TAUTHOR_TAG was'],"[' #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet']","['text would become an invaluable component of many semantics - oriented nlp applications. the majority of previous computational approaches to metaphor rely', 'on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. handcoded knowledge has proved useful', 'for both metaphor identification, i. e. distinguishing between literal and metaphorical', 'language in text  #AUTHOR_TAG and metaphor interpretation, i. e. identifying the intended literal meaning of a metaphor', '##ical expression  #AUTHOR_TAG. however, to be applicable in a real - world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. the recent metaphor paraphrasing', 'approach of  #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet  #AUTHOR_TAG database to generate the initial set of paraphrases.', 'in this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. in our method', ', candidate substitutes for the metaphorical term are generated using a vector space model. vector space models have been previously used in the general lexical substitution task  #AUTHOR_TAG pado, 2008, 2009 ;  #AUTHOR_TAG thater et al.,,', '2010 erk and pado, 2010 ; van de  #AUTHOR_TAG. however, ( to the best of our knowledge ) they have not yet been deployed in tasks', 'involving figurative meaning transfers, such as interpretation of metonymy or metaphor. in this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphr', '##asing, appropriately adapting it to the task. in comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely', 'that of discriminating between literal and metaphorical substitutes.  #TAUTHOR_TAG used a selectional preference - based model', 'for this purpose, obtaining encouraging results in a supervised setting. we evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it', 'with a selectional preference - based model similar to that of  #TAUTHOR_TAG and thus evaluating the latter in an unsupervised setting. our system thus operates in two steps. it first computes candidate', 'paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using', 'a selectional preference model. we focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of  #TAUTHOR_TAG especially designed', 'for this task. the comparison against a paraphrasing gold standard provided by  #TAUTHOR_TAG is complemented by an evaluation', 'against direct human judgements of system output']",0
[' #TAUTHOR_TAG was'],[' #TAUTHOR_TAG was'],"[' #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet']","['text would become an invaluable component of many semantics - oriented nlp applications. the majority of previous computational approaches to metaphor rely', 'on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. handcoded knowledge has proved useful', 'for both metaphor identification, i. e. distinguishing between literal and metaphorical', 'language in text  #AUTHOR_TAG and metaphor interpretation, i. e. identifying the intended literal meaning of a metaphor', '##ical expression  #AUTHOR_TAG. however, to be applicable in a real - world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. the recent metaphor paraphrasing', 'approach of  #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet  #AUTHOR_TAG database to generate the initial set of paraphrases.', 'in this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. in our method', ', candidate substitutes for the metaphorical term are generated using a vector space model. vector space models have been previously used in the general lexical substitution task  #AUTHOR_TAG pado, 2008, 2009 ;  #AUTHOR_TAG thater et al.,,', '2010 erk and pado, 2010 ; van de  #AUTHOR_TAG. however, ( to the best of our knowledge ) they have not yet been deployed in tasks', 'involving figurative meaning transfers, such as interpretation of metonymy or metaphor. in this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphr', '##asing, appropriately adapting it to the task. in comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely', 'that of discriminating between literal and metaphorical substitutes.  #TAUTHOR_TAG used a selectional preference - based model', 'for this purpose, obtaining encouraging results in a supervised setting. we evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it', 'with a selectional preference - based model similar to that of  #TAUTHOR_TAG and thus evaluating the latter in an unsupervised setting. our system thus operates in two steps. it first computes candidate', 'paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using', 'a selectional preference model. we focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of  #TAUTHOR_TAG especially designed', 'for this task. the comparison against a paraphrasing gold standard provided by  #TAUTHOR_TAG is complemented by an evaluation', 'against direct human judgements of system output']",0
"['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct']","['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions']","['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions']","['our knowledge, the only metaphor paraphrasing dataset and gold standard available to date is that of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions and 41 were verb - direct object constructions']",0
"['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct']","['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions']","['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions']","['our knowledge, the only metaphor paraphrasing dataset and gold standard available to date is that of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions and 41 were verb - direct object constructions']",0
[' #TAUTHOR_TAG was'],[' #TAUTHOR_TAG was'],"[' #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet']","['text would become an invaluable component of many semantics - oriented nlp applications. the majority of previous computational approaches to metaphor rely', 'on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. handcoded knowledge has proved useful', 'for both metaphor identification, i. e. distinguishing between literal and metaphorical', 'language in text  #AUTHOR_TAG and metaphor interpretation, i. e. identifying the intended literal meaning of a metaphor', '##ical expression  #AUTHOR_TAG. however, to be applicable in a real - world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. the recent metaphor paraphrasing', 'approach of  #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet  #AUTHOR_TAG database to generate the initial set of paraphrases.', 'in this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. in our method', ', candidate substitutes for the metaphorical term are generated using a vector space model. vector space models have been previously used in the general lexical substitution task  #AUTHOR_TAG pado, 2008, 2009 ;  #AUTHOR_TAG thater et al.,,', '2010 erk and pado, 2010 ; van de  #AUTHOR_TAG. however, ( to the best of our knowledge ) they have not yet been deployed in tasks', 'involving figurative meaning transfers, such as interpretation of metonymy or metaphor. in this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphr', '##asing, appropriately adapting it to the task. in comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely', 'that of discriminating between literal and metaphorical substitutes.  #TAUTHOR_TAG used a selectional preference - based model', 'for this purpose, obtaining encouraging results in a supervised setting. we evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it', 'with a selectional preference - based model similar to that of  #TAUTHOR_TAG and thus evaluating the latter in an unsupervised setting. our system thus operates in two steps. it first computes candidate', 'paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using', 'a selectional preference model. we focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of  #TAUTHOR_TAG especially designed', 'for this task. the comparison against a paraphrasing gold standard provided by  #TAUTHOR_TAG is complemented by an evaluation', 'against direct human judgements of system output']",6
[' #TAUTHOR_TAG was'],[' #TAUTHOR_TAG was'],"[' #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet']","['text would become an invaluable component of many semantics - oriented nlp applications. the majority of previous computational approaches to metaphor rely', 'on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. handcoded knowledge has proved useful', 'for both metaphor identification, i. e. distinguishing between literal and metaphorical', 'language in text  #AUTHOR_TAG and metaphor interpretation, i. e. identifying the intended literal meaning of a metaphor', '##ical expression  #AUTHOR_TAG. however, to be applicable in a real - world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. the recent metaphor paraphrasing', 'approach of  #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet  #AUTHOR_TAG database to generate the initial set of paraphrases.', 'in this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. in our method', ', candidate substitutes for the metaphorical term are generated using a vector space model. vector space models have been previously used in the general lexical substitution task  #AUTHOR_TAG pado, 2008, 2009 ;  #AUTHOR_TAG thater et al.,,', '2010 erk and pado, 2010 ; van de  #AUTHOR_TAG. however, ( to the best of our knowledge ) they have not yet been deployed in tasks', 'involving figurative meaning transfers, such as interpretation of metonymy or metaphor. in this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphr', '##asing, appropriately adapting it to the task. in comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely', 'that of discriminating between literal and metaphorical substitutes.  #TAUTHOR_TAG used a selectional preference - based model', 'for this purpose, obtaining encouraging results in a supervised setting. we evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it', 'with a selectional preference - based model similar to that of  #TAUTHOR_TAG and thus evaluating the latter in an unsupervised setting. our system thus operates in two steps. it first computes candidate', 'paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using', 'a selectional preference model. we focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of  #TAUTHOR_TAG especially designed', 'for this task. the comparison against a paraphrasing gold standard provided by  #TAUTHOR_TAG is complemented by an evaluation', 'against direct human judgements of system output']",3
[' #TAUTHOR_TAG was'],[' #TAUTHOR_TAG was'],"[' #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet']","['text would become an invaluable component of many semantics - oriented nlp applications. the majority of previous computational approaches to metaphor rely', 'on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. handcoded knowledge has proved useful', 'for both metaphor identification, i. e. distinguishing between literal and metaphorical', 'language in text  #AUTHOR_TAG and metaphor interpretation, i. e. identifying the intended literal meaning of a metaphor', '##ical expression  #AUTHOR_TAG. however, to be applicable in a real - world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. the recent metaphor paraphrasing', 'approach of  #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet  #AUTHOR_TAG database to generate the initial set of paraphrases.', 'in this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. in our method', ', candidate substitutes for the metaphorical term are generated using a vector space model. vector space models have been previously used in the general lexical substitution task  #AUTHOR_TAG pado, 2008, 2009 ;  #AUTHOR_TAG thater et al.,,', '2010 erk and pado, 2010 ; van de  #AUTHOR_TAG. however, ( to the best of our knowledge ) they have not yet been deployed in tasks', 'involving figurative meaning transfers, such as interpretation of metonymy or metaphor. in this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphr', '##asing, appropriately adapting it to the task. in comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely', 'that of discriminating between literal and metaphorical substitutes.  #TAUTHOR_TAG used a selectional preference - based model', 'for this purpose, obtaining encouraging results in a supervised setting. we evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it', 'with a selectional preference - based model similar to that of  #TAUTHOR_TAG and thus evaluating the latter in an unsupervised setting. our system thus operates in two steps. it first computes candidate', 'paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using', 'a selectional preference model. we focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of  #TAUTHOR_TAG especially designed', 'for this task. the comparison against a paraphrasing gold standard provided by  #TAUTHOR_TAG is complemented by an evaluation', 'against direct human judgements of system output']",5
[' #TAUTHOR_TAG was'],[' #TAUTHOR_TAG was'],"[' #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet']","['text would become an invaluable component of many semantics - oriented nlp applications. the majority of previous computational approaches to metaphor rely', 'on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. handcoded knowledge has proved useful', 'for both metaphor identification, i. e. distinguishing between literal and metaphorical', 'language in text  #AUTHOR_TAG and metaphor interpretation, i. e. identifying the intended literal meaning of a metaphor', '##ical expression  #AUTHOR_TAG. however, to be applicable in a real - world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. the recent metaphor paraphrasing', 'approach of  #TAUTHOR_TAG was designed with this requirement in mind and used statistical methods, but still relied on the wordnet  #AUTHOR_TAG database to generate the initial set of paraphrases.', 'in this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. in our method', ', candidate substitutes for the metaphorical term are generated using a vector space model. vector space models have been previously used in the general lexical substitution task  #AUTHOR_TAG pado, 2008, 2009 ;  #AUTHOR_TAG thater et al.,,', '2010 erk and pado, 2010 ; van de  #AUTHOR_TAG. however, ( to the best of our knowledge ) they have not yet been deployed in tasks', 'involving figurative meaning transfers, such as interpretation of metonymy or metaphor. in this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphr', '##asing, appropriately adapting it to the task. in comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely', 'that of discriminating between literal and metaphorical substitutes.  #TAUTHOR_TAG used a selectional preference - based model', 'for this purpose, obtaining encouraging results in a supervised setting. we evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it', 'with a selectional preference - based model similar to that of  #TAUTHOR_TAG and thus evaluating the latter in an unsupervised setting. our system thus operates in two steps. it first computes candidate', 'paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using', 'a selectional preference model. we focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of  #TAUTHOR_TAG especially designed', 'for this task. the comparison against a paraphrasing gold standard provided by  #TAUTHOR_TAG is complemented by an evaluation', 'against direct human judgements of system output']",5
"['refined.', 'following  #TAUTHOR_TAG, we use a selection']","['refined.', 'following  #TAUTHOR_TAG, we use a selectional preference']","['refined.', 'following  #TAUTHOR_TAG, we use a selectional preference model to discriminate between literally and metaphorically used substitutes.', 'verbs used metaphorically']","['', 'as the task is to identify the literal interpretation, this ranking still needs to be refined.', 'following  #TAUTHOR_TAG, we use a selectional preference model to discriminate between literally and metaphorically used substitutes.', 'verbs used metaphorically are likely to demonstrate semantic preference for the source domain, e. g. speed up would select for machines, or vehicles, rather than change ( the target domain ), whereas the ones used literally for the target domain, e. g. facilitate would select for processes ( including change ).', 'we therefore expect that selecting the verbs whose preferences the noun in the metaphorical expression matches best should allow us to filter out non - literalness.', '']",5
['of the sp model of  #TAUTHOR_TAG in an unsupervised setting and in combination with vs'],['of the sp model of  #TAUTHOR_TAG in an unsupervised setting and in combination with vs'],"['literal paraphrases, as well as the effectiveness of the sp model of  #TAUTHOR_TAG in an unsupervised setting and in combination with vs']","['compared the rankings of the initial candidate generation by the vector space model ( vs ) and the selectional preference - based reranking ( sp ) to that of an unsupervised paraphrasing baseline.', 'we thus evaluated the ability of vs on its own to detect literal paraphrases, as well as the effectiveness of the sp model of  #TAUTHOR_TAG in an unsupervised setting and in combination with vs']",5
"['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct']","['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions']","['of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions']","['our knowledge, the only metaphor paraphrasing dataset and gold standard available to date is that of  #TAUTHOR_TAG.', 'we used this dataset to develop and test our system.', ' #TAUTHOR_TAG annotated metaphorical expressions in a subset of the bnc sampling various genres : literature, newspaper / journal articles, essays on politics, international relations and sociology, radio broadcast ( transcribed speech ).', 'the dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object.', 'subject - verb constructions ).', '10 phrases in the dataset were used during development to observe the behavior of the system, and the remaining 52 constituted the test set.', '11 of them were subject - verb constructions and 41 were verb - direct object constructions']",5
"['standard was created by  #TAUTHOR_TAG as follows.', 'five independent annotators were presented with a set of sentences containing metaphorical however, given that the metaphor paraphrasing task is open - ended, it is hard to construct a comprehensive gold standard.', 'for example, for the phrase stir excitement the gold']","['standard was created by  #TAUTHOR_TAG as follows.', 'five independent annotators were presented with a set of sentences containing metaphorical however, given that the metaphor paraphrasing task is open - ended, it is hard to construct a comprehensive gold standard.', 'for example, for the phrase stir excitement the gold']","['baseline rankings against a human - constructed paraphrasing gold standard.', 'the gold standard was created by  #TAUTHOR_TAG as follows.', 'five independent annotators were presented with a set of sentences containing metaphorical however, given that the metaphor paraphrasing task is open - ended, it is hard to construct a comprehensive gold standard.', 'for example, for the phrase stir excitement the gold standard includes the paraphrase create excitement, but not provoke excitement or stimulate excitement, which are more precise paraphrases.', 'thus the gold standard evaluation']","['then also evaluated vs, sp and baseline rankings against a human - constructed paraphrasing gold standard.', 'the gold standard was created by  #TAUTHOR_TAG as follows.', 'five independent annotators were presented with a set of sentences containing metaphorical however, given that the metaphor paraphrasing task is open - ended, it is hard to construct a comprehensive gold standard.', 'for example, for the phrase stir excitement the gold standard includes the paraphrase create excitement, but not provoke excitement or stimulate excitement, which are more precise paraphrases.', 'thus the gold standard evaluation may unfairly penalise the system, which motivates our two - phase evaluation against both the gold standard and direct judgements of system output.', 'the system output was compared against the gold standard using mean average precision ( map ) as a measure.', 'map is defined as follows :', 'where m is the number of metaphorical expressions, n j is the number of correct paraphrases for the metaphorical expression, p ji is the precision at each correct paraphrase ( the number of correct paraphrases among the top i ranks ).', 'first, average precision is estimated for individual metaphorical expressions, and then the mean is computed across the dataset.', 'this measure allows us to assess ranking quality beyond rank 1, as well as the recall of the system.', 'as compared to the gold standard, map of vs is 0. 40, map of sp is 0. 41 and that of the baseline is 0. 37']",5
"['supervised system of  #TAUTHOR_TAG.', 'note, however, that our results are in line with the']","['supervised system of  #TAUTHOR_TAG.', 'note, however, that our results are in line with the']","['the supervised system of  #TAUTHOR_TAG.', 'note, however, that our results are in line with the performance']","['system consistently produces better results than the baseline, with an improvement of 12 % in precision on our human evaluation ( sp ) and an improvement of 4 % map on the gold standard ( sp ).', 'at first sight, these improvements of our unsupervised system may not seem very high, in particular when compared to the results of the supervised system of  #TAUTHOR_TAG.', 'note, however, that our results are in line with the performance of unsupervised approaches on the lexical substitution task.', 'unsupervised approaches to lexical substitution perform well below their supervised counterparts ( which are usually based on wordnet ), and often have difficulties getting significant improvements over a baseline of a simple dependency - based vector space model of semantic similarity ( erk and pado, 2008 ; van de  #AUTHOR_TAG.', 'we therefore think that the method presented here takes a promising step in the direction of unsupervised metaphor paraphrasing.', 'the sp re - ranking of the candidates yields an improvement over the vs model used on its own, as expected.', '']",5
"['.', 'following  #TAUTHOR_TAG, the current experimental design and test set focuses on']","['this a promising result.', 'following  #TAUTHOR_TAG, the current experimental design and test set focuses on subject - verb and verb - object metaphors only, but']","['and considering the state - of - the - art in unsupervised lexical substitution, we consider this a promising result.', 'following  #TAUTHOR_TAG, the current experimental design and test set focuses on']","['this paper we presented the first fully unsupervised approach to metaphor interpretation.', 'our system produces literal paraphrases for metaphorical expressions in unrestricted text.', 'producing metaphorical interpretations in textual format makes our system directly usable by other nlp applications that can benefit from a metaphor processing component.', 'the fact that, unlike all previous approaches to this problem, our system does not use any supervision makes it easily scalable to new domains and applications, as well as portable to a wider range of languages.', 'our method identifies literal paraphrases for metaphorical expressions with a precision of 0. 52 measured at top - ranked paraphrases.', 'given the unsupervised nature of our system and considering the state - of - the - art in unsupervised lexical substitution, we consider this a promising result.', 'following  #TAUTHOR_TAG, the current experimental design and test set focuses on subject - verb and verb - object metaphors only, but we expect the method to be equally applicable to other parts of speech and a wider range of syntactic constructions.', 'our context - based vector space model is suited to all part - of - speech classes and types of relations.', 'selectional preferences have been previously successfully acquired not only for verbs, but also for nouns, adjectives and even prepositions  #AUTHOR_TAG o seaghdha, 2010 ).', 'extending the system to deal with further syntactic constructions is thus part of our future work']",5
,,,,0
,,,,0
,,,,0
"['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable']","['', 'closest word along the rank - 1 subspace spanned by the relation vector ( restricted by c in eq. 4 ). for relconst, the missing word is predicted by translating the first word by the relation vector', 'and then searching for nearest word. the accuracy results on wordrep data are shown in table 1. relaxing the constant translation to rank - 1', 'subspace assumption results in significant improvements on this task. in the analogy task, we want to predict the missing word in an', 'analogy tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable gains with relsub over cbow for semantic categories. the accuracy of knowledge regularized methods on syntactic categories is a little worse than', 'cbow and only slightly better than relconst, which is contrary to our observation on the', 'knowledge - base completion task. this is due to the fact that analogy task uses the difference vector (', 'b − a ) instead of the learned relation vector which is assumed to be unknown']",0
,,,,5
,,,,5
"['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable']","['', 'closest word along the rank - 1 subspace spanned by the relation vector ( restricted by c in eq. 4 ). for relconst, the missing word is predicted by translating the first word by the relation vector', 'and then searching for nearest word. the accuracy results on wordrep data are shown in table 1. relaxing the constant translation to rank - 1', 'subspace assumption results in significant improvements on this task. in the analogy task, we want to predict the missing word in an', 'analogy tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable gains with relsub over cbow for semantic categories. the accuracy of knowledge regularized methods on syntactic categories is a little worse than', 'cbow and only slightly better than relconst, which is contrary to our observation on the', 'knowledge - base completion task. this is due to the fact that analogy task uses the difference vector (', 'b − a ) instead of the learned relation vector which is assumed to be unknown']",5
"['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable']","['', 'closest word along the rank - 1 subspace spanned by the relation vector ( restricted by c in eq. 4 ). for relconst, the missing word is predicted by translating the first word by the relation vector', 'and then searching for nearest word. the accuracy results on wordrep data are shown in table 1. relaxing the constant translation to rank - 1', 'subspace assumption results in significant improvements on this task. in the analogy task, we want to predict the missing word in an', 'analogy tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable gains with relsub over cbow for semantic categories. the accuracy of knowledge regularized methods on syntactic categories is a little worse than', 'cbow and only slightly better than relconst, which is contrary to our observation on the', 'knowledge - base completion task. this is due to the fact that analogy task uses the difference vector (', 'b − a ) instead of the learned relation vector which is assumed to be unknown']",5
"['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable']","['', 'closest word along the rank - 1 subspace spanned by the relation vector ( restricted by c in eq. 4 ). for relconst, the missing word is predicted by translating the first word by the relation vector', 'and then searching for nearest word. the accuracy results on wordrep data are shown in table 1. relaxing the constant translation to rank - 1', 'subspace assumption results in significant improvements on this task. in the analogy task, we want to predict the missing word in an', 'analogy tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable gains with relsub over cbow for semantic categories. the accuracy of knowledge regularized methods on syntactic categories is a little worse than', 'cbow and only slightly better than relconst, which is contrary to our observation on the', 'knowledge - base completion task. this is due to the fact that analogy task uses the difference vector (', 'b − a ) instead of the learned relation vector which is assumed to be unknown']",5
"['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '']","['tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable']","['', 'closest word along the rank - 1 subspace spanned by the relation vector ( restricted by c in eq. 4 ). for relconst, the missing word is predicted by translating the first word by the relation vector', 'and then searching for nearest word. the accuracy results on wordrep data are shown in table 1. relaxing the constant translation to rank - 1', 'subspace assumption results in significant improvements on this task. in the analogy task, we want to predict the missing word in an', 'analogy tuple a : b : : c :?. we use the google word - analogy data  #TAUTHOR_TAG for this evaluation', '. we observe considerable gains with relsub over cbow for semantic categories. the accuracy of knowledge regularized methods on syntactic categories is a little worse than', 'cbow and only slightly better than relconst, which is contrary to our observation on the', 'knowledge - base completion task. this is due to the fact that analogy task uses the difference vector (', 'b − a ) instead of the learned relation vector which is assumed to be unknown']",4
"['of nlp tasks  #TAUTHOR_TAG 8, 9 ]']","['of nlp tasks  #TAUTHOR_TAG 8, 9 ].', 'the word2vec [ 10 ] is among the']","['a source of features in a broad range of nlp tasks  #TAUTHOR_TAG 8, 9 ].', 'the word2vec [ 10 ] is among the']","['', 'learning knowledge from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks  #TAUTHOR_TAG 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts']",0
"[' #TAUTHOR_TAG 8, 9 ].', 'the word2']","[' #TAUTHOR_TAG 8, 9 ].', 'the word2vec [ 10 ] is among the']","['nlp tasks  #TAUTHOR_TAG 8, 9 ].', 'the word2vec [ 10 ] is among the']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks  #TAUTHOR_TAG 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', '']",0
"['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bd']","['papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual word embeddings.', 'pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties ( for an adversarial approach, see  #TAUTHOR_TAG.', 'alternatively, weak supervision can be provided in the form of numerals  #AUTHOR_TAG or identically spelled words.', 'successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross - lingual learning.', 'in addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,  #TAUTHOR_TAG present a supervised alignment algorithm that assumes a gold - standard seed dictionary and performs procrustes analysis ( schonemann, 1966 ).', 'show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i. e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.', 'we therefore focus on weakly - supervised procrustes analysis ( pa ) for bdi here.', '']",0
"['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bd']","['papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual word embeddings.', 'pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties ( for an adversarial approach, see  #TAUTHOR_TAG.', 'alternatively, weak supervision can be provided in the form of numerals  #AUTHOR_TAG or identically spelled words.', 'successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross - lingual learning.', 'in addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,  #TAUTHOR_TAG present a supervised alignment algorithm that assumes a gold - standard seed dictionary and performs procrustes analysis ( schonemann, 1966 ).', 'show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i. e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.', 'we therefore focus on weakly - supervised procrustes analysis ( pa ) for bdi here.', '']",0
"['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bd']","['papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual word embeddings.', 'pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties ( for an adversarial approach, see  #TAUTHOR_TAG.', 'alternatively, weak supervision can be provided in the form of numerals  #AUTHOR_TAG or identically spelled words.', 'successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross - lingual learning.', 'in addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,  #TAUTHOR_TAG present a supervised alignment algorithm that assumes a gold - standard seed dictionary and performs procrustes analysis ( schonemann, 1966 ).', 'show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i. e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.', 'we therefore focus on weakly - supervised procrustes analysis ( pa ) for bdi here.', '']",0
"['##tes analysis  #AUTHOR_TAG artetxe et al.,, 2018  #TAUTHOR_TAG.', '']","['language space  #AUTHOR_TAG.', 'in most recent work, this mapping is constrained to be orthogonal and solved using procrustes analysis  #AUTHOR_TAG artetxe et al.,, 2018  #TAUTHOR_TAG.', '']","['##tes analysis  #AUTHOR_TAG artetxe et al.,, 2018  #TAUTHOR_TAG.', 'the approach most similar']","['embeddings many diverse crosslingual word embedding models have been proposed.', 'the most popular kind learns a linear transformation from source to target language space  #AUTHOR_TAG.', 'in most recent work, this mapping is constrained to be orthogonal and solved using procrustes analysis  #AUTHOR_TAG artetxe et al.,, 2018  #TAUTHOR_TAG.', 'the approach most similar to ours,  #AUTHOR_TAG, uses canonical correlation analysis ( cca ) to project both source and target language spaces into a third, joint space.', 'in this setup, similarly to gpa, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t − 1.', 'the objective that drives the updates of the mapping matrices is to maximize the correlation between the projected embeddings of translational equivalents ( where the latter are taken from a gold - standard seed dictionary ).', 'in their analysis of the transformed embedding spaces,  #AUTHOR_TAG focus on the improved quality of monolingual embedding spaces themselves and do not perform evaluation of the task of bdi.', 'they find that the transformed monolingual spaces better encode the difference between synonyms and antonyms : in the original monolingual english space, synonyms and antonyms of beautiful are all mapped close to each other in a mixed fashion ; in the transformed space the synonyms of beautiful are mapped in a cluster around the query word and its antonyms are mapped in a separate cluster.', 'this finding is in line with our observation that gpa - learned alignments are more precise in distinguishing between synonyms and antonyms.', 'multilingual embeddings several approaches extend existing methods to space alignments between more than two languages  #AUTHOR_TAG.', ' #AUTHOR_TAG project all vocabularies into the english space.', 'in some cases, multilingual training has been shown to lead to improvements over bilingually trained embedding spaces ( vulic et al., 2017 ), similar to our findings']",0
"['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bd']","['papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual word embeddings.', 'pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties ( for an adversarial approach, see  #TAUTHOR_TAG.', 'alternatively, weak supervision can be provided in the form of numerals  #AUTHOR_TAG or identically spelled words.', 'successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross - lingual learning.', 'in addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,  #TAUTHOR_TAG present a supervised alignment algorithm that assumes a gold - standard seed dictionary and performs procrustes analysis ( schonemann, 1966 ).', 'show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i. e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.', 'we therefore focus on weakly - supervised procrustes analysis ( pa ) for bdi here.', '']",7
"['nearest neighbors in the english embedding space. p @ 1 over the bosnianenglish test set of  #TAUTHOR_TAG is 31', '']","['nearest neighbors in the english embedding space. p @ 1 over the bosnianenglish test set of  #TAUTHOR_TAG is 31', '']","['lists example translational pairs as induced from alignments between english and bosnian, learned with pa, gpa and mg', '##pa +. for interpretability, we query the system with words in bosnian and seek', 'their nearest neighbors in the english embedding space. p @ 1 over the bosnianenglish test set of  #TAUTHOR_TAG is 31', '. 33, 34. 80, and 34']","['', 'we observe therefore appears to be an instance of the poincare paradox ( poin', '##care, 1902 ). while gpa is not more expressive than pa, it may still be easier', 'to align each monolingual space to an intermediate space, as the latter constitutes a more similar target ( albeit a nonisomorphic one ) ; for example, the loss landscape', 'of aligning a source and target language word embedding with an average of the two may be much smoother than when aligning source directly with target. our work is in this way similar in spirit to  #AUTHOR_TAG, who use simple linear transforms to make learning of non - linear problems easier. table 5 lists example translational pairs as induced from alignments between english and bosnian, learned with pa, gpa and mg', '##pa +. for interpretability, we query the system with words in bosnian and seek', 'their nearest neighbors in the english embedding space. p @ 1 over the bosnianenglish test set of  #TAUTHOR_TAG is 31', '. 33, 34. 80, and 34. 47 for pa, gpa and mgpa +, respectively. the examples are grouped in three blocks,', 'based on success and failure of pa and gpa alignments to retrieve a valid translation']",7
"['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual']","['bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bd']","['papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction ( bdi )  #TAUTHOR_TAG ;, the task of identifying translational equivalents across two languages.', 'these approaches cast bdi as a problem of aligning monolingual word embeddings.', 'pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties ( for an adversarial approach, see  #TAUTHOR_TAG.', 'alternatively, weak supervision can be provided in the form of numerals  #AUTHOR_TAG or identically spelled words.', 'successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross - lingual learning.', 'in addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,  #TAUTHOR_TAG present a supervised alignment algorithm that assumes a gold - standard seed dictionary and performs procrustes analysis ( schonemann, 1966 ).', 'show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i. e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.', 'we therefore focus on weakly - supervised procrustes analysis ( pa ) for bdi here.', '']",4
"['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15, 000.', 'we do not put any restrictions on the initial seed dictionaries, based on cross - lingual homographs : those vary considerably in size, from 17, 012 for hebrew to 85, 912 for spanish.', 'instead of doing a single training epoch, however, we run pa and gpa with early stopping, until five epochs of no improvement in the validation criterion as used in  #TAUTHOR_TAG, i. e. the average cosine similarity between the top 10, 000 most frequent words in the source language and their candidate translations as induced with csls.', 'our metric is precision at k×100 ( p @ k ), i. e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set  #TAUTHOR_TAG.', 'unless stated otherwise, experiments were carried out using the publicly available pre - trained fasttext embeddings, trained on wikipedia data, 5 and bilingual dictionaries - consisting of 5000 and 1500 unique word pairs for training and testing, respectively - provided by  #TAUTHOR_TAG 6']",5
"['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15, 000.', 'we do not put any restrictions on the initial seed dictionaries, based on cross - lingual homographs : those vary considerably in size, from 17, 012 for hebrew to 85, 912 for spanish.', 'instead of doing a single training epoch, however, we run pa and gpa with early stopping, until five epochs of no improvement in the validation criterion as used in  #TAUTHOR_TAG, i. e. the average cosine similarity between the top 10, 000 most frequent words in the source language and their candidate translations as induced with csls.', 'our metric is precision at k×100 ( p @ k ), i. e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set  #TAUTHOR_TAG.', 'unless stated otherwise, experiments were carried out using the publicly available pre - trained fasttext embeddings, trained on wikipedia data, 5 and bilingual dictionaries - consisting of 5000 and 1500 unique word pairs for training and testing, respectively - provided by  #TAUTHOR_TAG 6']",5
"['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15, 000.', 'we do not put any restrictions on the initial seed dictionaries, based on cross - lingual homographs : those vary considerably in size, from 17, 012 for hebrew to 85, 912 for spanish.', 'instead of doing a single training epoch, however, we run pa and gpa with early stopping, until five epochs of no improvement in the validation criterion as used in  #TAUTHOR_TAG, i. e. the average cosine similarity between the top 10, 000 most frequent words in the source language and their candidate translations as induced with csls.', 'our metric is precision at k×100 ( p @ k ), i. e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set  #TAUTHOR_TAG.', 'unless stated otherwise, experiments were carried out using the publicly available pre - trained fasttext embeddings, trained on wikipedia data, 5 and bilingual dictionaries - consisting of 5000 and 1500 unique word pairs for training and testing, respectively - provided by  #TAUTHOR_TAG 6']",5
"['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15, 000.', 'we do not put any restrictions on the initial seed dictionaries, based on cross - lingual homographs : those vary considerably in size, from 17, 012 for hebrew to 85, 912 for spanish.', 'instead of doing a single training epoch, however, we run pa and gpa with early stopping, until five epochs of no improvement in the validation criterion as used in  #TAUTHOR_TAG, i. e. the average cosine similarity between the top 10, 000 most frequent words in the source language and their candidate translations as induced with csls.', 'our metric is precision at k×100 ( p @ k ), i. e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set  #TAUTHOR_TAG.', 'unless stated otherwise, experiments were carried out using the publicly available pre - trained fasttext embeddings, trained on wikipedia data, 5 and bilingual dictionaries - consisting of 5000 and 1500 unique word pairs for training and testing, respectively - provided by  #TAUTHOR_TAG 6']",5
"['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15, 000.', 'we do not put any restrictions on the initial seed dictionaries, based on cross - lingual homographs : those vary considerably in size, from 17, 012 for hebrew to 85, 912 for spanish.', 'instead of doing a single training epoch, however, we run pa and gpa with early stopping, until five epochs of no improvement in the validation criterion as used in  #TAUTHOR_TAG, i. e. the average cosine similarity between the top 10, 000 most frequent words in the source language and their candidate translations as induced with csls.', 'our metric is precision at k×100 ( p @ k ), i. e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set  #TAUTHOR_TAG.', 'unless stated otherwise, experiments were carried out using the publicly available pre - trained fasttext embeddings, trained on wikipedia data, 5 and bilingual dictionaries - consisting of 5000 and 1500 unique word pairs for training and testing, respectively - provided by  #TAUTHOR_TAG 6']",5
"[', italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best']","['work, we include results on english - { finnish, german, italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best']","['finnish, german, italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best']","['resource setting we first present a direct comparison of pa and gpa on bdi from english to five fairly high - resource languages : arabic, finnish, german, russian, and spanish.', 'the wikipedia corpus sizes for these languages are reported in table 1.', 'results are listed in table 2.', 'gpa improves over pa consistently for all five languages.', 'most notably, for finnish it scores 2. 5 % higher than pa.', 'common benchmarks for a more extensive comparison with previous work, we include results on english - { finnish, german, italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best approach to bdi known to us, which also uses procrustes analysis.', 'we conduct experiments using three forms of supervision : gold - standard seed dictionaries of 5000 word pairs, cross - lingual homographs, and numerals.', 'we use train and test bilingual dictionaries from  #AUTHOR_TAG for english - italian and from  #AUTHOR_TAG for english - { finnish, german }. following  #TAUTHOR_TAG, we report results with a set of cbow embeddings trained on the wacky corpus  #AUTHOR_TAG, and with wikipedia embeddings.', 'results are reported in table 3.', 'we observe that gpa outperforms pa consistently on italian and german with the wacky embeddings, and on all languages with the wikipedia embeddings.', 'notice that once more, finnish benefits the most from a switch to gpa in the wikipedia embeddings setting, but it is also the only language to suffer from that switch in the wacky setup.', 'interestingly, pa fails to learn a good alignment for italian and finnish when supervised with numerals, while gpa performs comparably with numerals as with other forms of supervision.', ' #AUTHOR_TAG point out that improvement from subsequent iterations of pa is generally negligible, which we also found to be the case.', 'we also found that while pa learned a slightly poorer alignment than gpa, it did so faster.', 'with our criterion for early stopping, pa converged in 5 to 10 epochs, while gpa did so within 10 to 15 epochs 7.', 'in the case of italian and finnish alignment supervised by numerals, pa converged in 8 and 5 epochs, respectively, but clearly got stuck in local minima.', 'gpa took considerably longer to converge : 27 and 74 epochs, respectively, but also managed to find a reasonable alignment between the language spaces.', 'this points to an important difference in the learning properties of pa and gpa - 7 notice that one epoch with both pa and gpa takes less than half a minute, so the slower convergence of gpa is in no way prohibitive.', 'unlike pa, gpa has a two - fold objective of opposing forces :']",5
"[', italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best']","['work, we include results on english - { finnish, german, italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best']","['finnish, german, italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best']","['resource setting we first present a direct comparison of pa and gpa on bdi from english to five fairly high - resource languages : arabic, finnish, german, russian, and spanish.', 'the wikipedia corpus sizes for these languages are reported in table 1.', 'results are listed in table 2.', 'gpa improves over pa consistently for all five languages.', 'most notably, for finnish it scores 2. 5 % higher than pa.', 'common benchmarks for a more extensive comparison with previous work, we include results on english - { finnish, german, italian } dictionaries used in  #TAUTHOR_TAG and  #AUTHOR_TAG - the second best approach to bdi known to us, which also uses procrustes analysis.', 'we conduct experiments using three forms of supervision : gold - standard seed dictionaries of 5000 word pairs, cross - lingual homographs, and numerals.', 'we use train and test bilingual dictionaries from  #AUTHOR_TAG for english - italian and from  #AUTHOR_TAG for english - { finnish, german }. following  #TAUTHOR_TAG, we report results with a set of cbow embeddings trained on the wacky corpus  #AUTHOR_TAG, and with wikipedia embeddings.', 'results are reported in table 3.', 'we observe that gpa outperforms pa consistently on italian and german with the wacky embeddings, and on all languages with the wikipedia embeddings.', 'notice that once more, finnish benefits the most from a switch to gpa in the wikipedia embeddings setting, but it is also the only language to suffer from that switch in the wacky setup.', 'interestingly, pa fails to learn a good alignment for italian and finnish when supervised with numerals, while gpa performs comparably with numerals as with other forms of supervision.', ' #AUTHOR_TAG point out that improvement from subsequent iterations of pa is generally negligible, which we also found to be the case.', 'we also found that while pa learned a slightly poorer alignment than gpa, it did so faster.', 'with our criterion for early stopping, pa converged in 5 to 10 epochs, while gpa did so within 10 to 15 epochs 7.', 'in the case of italian and finnish alignment supervised by numerals, pa converged in 8 and 5 epochs, respectively, but clearly got stuck in local minima.', 'gpa took considerably longer to converge : 27 and 74 epochs, respectively, but also managed to find a reasonable alignment between the language spaces.', 'this points to an important difference in the learning properties of pa and gpa - 7 notice that one epoch with both pa and gpa takes less than half a minute, so the slower convergence of gpa is in no way prohibitive.', 'unlike pa, gpa has a two - fold objective of opposing forces :']",5
"['a procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of  #TAUTHOR_TAG 9 for']","['a procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of  #TAUTHOR_TAG 9 for']","['explore the latter issue and to further compare the capabilities of pa and gpa, we perform a procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of  #TAUTHOR_TAG 9 for both training']","['explore the latter issue and to further compare the capabilities of pa and gpa, we perform a procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of  #TAUTHOR_TAG 9 for both training and evaluation 10.', 'in the ideal case, i. e. if the subspaces defined by the words in the seed dictionaries are perfectly alignable, this setup should result in precision of 100 %.', 'we found the difference between the fit with pa and gpa to be negligible, 0. 20 on average across all 10 languages ( 5 low - resource and 5 high - source languages ).', 'it is not surprising that pa and gpa results in almost equivalent fits - the two algorithms both rely on linear transformations, i. e. they are equal in expressivity.', 'as pointed out earlier, the superiority of gpa over pa stems from its tables 2 and 4.', 'more robust learning procedure, not from higher expressivity.', 'figure 3 thus only visualizes the procrustes fit as obtained with gpa.', 'the procrustes fit of all languages is indeed lower than 100 %, showing that there is a ceiling on the linear alignability between the source and target spaces.', 'we attribute this ceiling effect to variable degrees of linguistic difference between source and target language and possibly to differences in the contents of cross - lingual wikipedias ( recall that the embeddings we use are trained on wikipedia corpora ).', 'an apparent correlation emerges between the procrustes fit and precision scores for weakly - supervised gpa, i. e. between the circles and the xs in the plot.', 'the only language that does not conform here is occitan, which has the highest procrustes fit and the lowest gpa precision out of all languages, but this result has an important caveat : our dictionary for occitan comes from a different source and is much smaller than all the other dictionaries.', 'for some of the high - resource languages, weakly - supervised gpa takes us rather close to the best possible fit : e. g. for spanish gpa scores 81. 93 %, and the procrustes fit is 90. 07 %.', 'while low - resource languages do not necessarily have lower procrustes fits than high - resource ones ( compare estonian and finnish, for example ), the gap between the procrustes fit and gpa precision is on average much higher within low - resource languages than within high - resource ones ( 52. 46 11 compared to 25. 47, respectively ).', 'this finding is in line with the common understanding that the quality of distributional word vectors depends']",5
"['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure,']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric']","['our experiments, we generally use the same hyper - parameters as used in  #TAUTHOR_TAG, unless otherwise stated.', 'when extracting dictionaries for the bootstrapping procedure, we use cross - domain local scaling ( csls, see  #TAUTHOR_TAG for details ) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15, 000.', 'we do not put any restrictions on the initial seed dictionaries, based on cross - lingual homographs : those vary considerably in size, from 17, 012 for hebrew to 85, 912 for spanish.', 'instead of doing a single training epoch, however, we run pa and gpa with early stopping, until five epochs of no improvement in the validation criterion as used in  #TAUTHOR_TAG, i. e. the average cosine similarity between the top 10, 000 most frequent words in the source language and their candidate translations as induced with csls.', 'our metric is precision at k×100 ( p @ k ), i. e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set  #TAUTHOR_TAG.', 'unless stated otherwise, experiments were carried out using the publicly available pre - trained fasttext embeddings, trained on wikipedia data, 5 and bilingual dictionaries - consisting of 5000 and 1500 unique word pairs for training and testing, respectively - provided by  #TAUTHOR_TAG 6']",3
"['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['function labelling is commonly integrated into syntactic parsing.', 'few studies have adressed the issue as a separate classification task.', 'while most of them assign grammatical functions on top of constituency trees  #AUTHOR_TAG jijkoun and de  #AUTHOR_TAG chrupała and van  #AUTHOR_TAG, less work has tried to predict gf labels for unlabelled dependency trees.', 'one of them is mc  #AUTHOR_TAG who first generate the unlabelled trees using a graph - based parser, and then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.', 'then, in a post - processing step, they assign labels to each head - dependent pair, using a two - layer rectifier network.', 'dependency parsing as head selection our labelling model is an extension of the parsing model of  #TAUTHOR_TAG.', 'we use our own implementation of the head - selection parser and focus on the grammatical function labelling part.', 'the parser uses a bidirectional lstm to extract a dense, positional representation a i of the word w i at position i in a sentence :', 'x i is the input at position i, which is the concatenation of the word embeddings and the tag embeddings of word w i.', 'an artificial root token w 0 is added at the beginning of each sentence.', '']",0
"['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['function labelling is commonly integrated into syntactic parsing.', 'few studies have adressed the issue as a separate classification task.', 'while most of them assign grammatical functions on top of constituency trees  #AUTHOR_TAG jijkoun and de  #AUTHOR_TAG chrupała and van  #AUTHOR_TAG, less work has tried to predict gf labels for unlabelled dependency trees.', 'one of them is mc  #AUTHOR_TAG who first generate the unlabelled trees using a graph - based parser, and then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.', 'then, in a post - processing step, they assign labels to each head - dependent pair, using a two - layer rectifier network.', 'dependency parsing as head selection our labelling model is an extension of the parsing model of  #TAUTHOR_TAG.', 'we use our own implementation of the head - selection parser and focus on the grammatical function labelling part.', 'the parser uses a bidirectional lstm to extract a dense, positional representation a i of the word w i at position i in a sentence :', 'x i is the input at position i, which is the concatenation of the word embeddings and the tag embeddings of word w i.', 'an artificial root token w 0 is added at the beginning of each sentence.', '']",0
"['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only']","['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only']","['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only']","['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages.', 'first, some labels are easier to predict when we also take context into account, e. g. the parent and grandparent nodes or the siblings of the head or dependent.', '']",0
"['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model']","['function labelling is commonly integrated into syntactic parsing.', 'few studies have adressed the issue as a separate classification task.', 'while most of them assign grammatical functions on top of constituency trees  #AUTHOR_TAG jijkoun and de  #AUTHOR_TAG chrupała and van  #AUTHOR_TAG, less work has tried to predict gf labels for unlabelled dependency trees.', 'one of them is mc  #AUTHOR_TAG who first generate the unlabelled trees using a graph - based parser, and then model the assignment of dependency labels as a sequence labelling task.', 'another approach has been proposed by  #TAUTHOR_TAG who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.', 'then, in a post - processing step, they assign labels to each head - dependent pair, using a two - layer rectifier network.', 'dependency parsing as head selection our labelling model is an extension of the parsing model of  #TAUTHOR_TAG.', 'we use our own implementation of the head - selection parser and focus on the grammatical function labelling part.', 'the parser uses a bidirectional lstm to extract a dense, positional representation a i of the word w i at position i in a sentence :', 'x i is the input at position i, which is the concatenation of the word embeddings and the tag embeddings of word w i.', 'an artificial root token w 0 is added at the beginning of each sentence.', '']",6
"['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only']","['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only']","['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only']","['the labelling approach in  #TAUTHOR_TAG is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages.', 'first, some labels are easier to predict when we also take context into account, e. g. the parent and grandparent nodes or the siblings of the head or dependent.', '']",1
"['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG']","['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG']","['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG']","['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG and report results also for english, which has a configurational word order, and for czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than german.', 'for english, we use the penn treebank ( ptb )  #AUTHOR_TAG with standard training / dev / test splits.', 'the pos tags are assigned using the stanford pos tagger  #AUTHOR_TAG with ten - way jackknifing, and constituency trees are converted to stanford basic dependencies  #AUTHOR_TAG.', 'the german and czech data come from the conll - x shared task  #AUTHOR_TAG and our data split follows  #TAUTHOR_TAG.', 'as the conll - x testsets are rather small ( ∼ 360 sentences ), we']",5
"['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG']","['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG']","['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG']","['interest is focussed on german, but to put our work in context, we follow  #TAUTHOR_TAG and report results also for english, which has a configurational word order, and for czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than german.', 'for english, we use the penn treebank ( ptb )  #AUTHOR_TAG with standard training / dev / test splits.', 'the pos tags are assigned using the stanford pos tagger  #AUTHOR_TAG with ten - way jackknifing, and constituency trees are converted to stanford basic dependencies  #AUTHOR_TAG.', 'the german and czech data come from the conll - x shared task  #AUTHOR_TAG and our data split follows  #TAUTHOR_TAG.', 'as the conll - x testsets are rather small ( ∼ 360 sentences ), we']",5
"['according to  #TAUTHOR_TAG, and tag embedding size was']","['according to  #TAUTHOR_TAG, and tag embedding size was']","['- implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that']","['test different labelling models on top of the unlabelled trees produced by our re - implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that we do not use pre - trained embeddings in our experiments.', 'in the next step, we train four different labelling models : the labeller of  #TAUTHOR_TAG that uses a rectifier neural network with two hidden layers ( baseline ), two bidirectional lstm models ( bilstm ( l ) and bilstm ( b ) ), and one tree lstm model ( treelstm ) ( § 3 ).', 'the hidden layer dimension in all lstm models was set to 200.', 'the models were trained for 10 epochs, and were optimized using adam  #AUTHOR_TAG with default parameters ( initial learning rate 0. 001, first momentum coefficient 0. 9, second momentum coefficient 0. 999 ).', 'we used l2 regularization with a coefficient of 10 −3 and max - norm regularization with an upper bound of 5. 0.', '']",5
"['according to  #TAUTHOR_TAG, and tag embedding size was']","['according to  #TAUTHOR_TAG, and tag embedding size was']","['- implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that']","['test different labelling models on top of the unlabelled trees produced by our re - implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that we do not use pre - trained embeddings in our experiments.', 'in the next step, we train four different labelling models : the labeller of  #TAUTHOR_TAG that uses a rectifier neural network with two hidden layers ( baseline ), two bidirectional lstm models ( bilstm ( l ) and bilstm ( b ) ), and one tree lstm model ( treelstm ) ( § 3 ).', 'the hidden layer dimension in all lstm models was set to 200.', 'the models were trained for 10 epochs, and were optimized using adam  #AUTHOR_TAG with default parameters ( initial learning rate 0. 001, first momentum coefficient 0. 9, second momentum coefficient 0. 999 ).', 'we used l2 regularization with a coefficient of 10 −3 and max - norm regularization with an upper bound of 5. 0.', '']",5
"['according to  #TAUTHOR_TAG, and tag embedding size was']","['according to  #TAUTHOR_TAG, and tag embedding size was']","['- implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that']","['test different labelling models on top of the unlabelled trees produced by our re - implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that we do not use pre - trained embeddings in our experiments.', 'in the next step, we train four different labelling models : the labeller of  #TAUTHOR_TAG that uses a rectifier neural network with two hidden layers ( baseline ), two bidirectional lstm models ( bilstm ( l ) and bilstm ( b ) ), and one tree lstm model ( treelstm ) ( § 3 ).', 'the hidden layer dimension in all lstm models was set to 200.', 'the models were trained for 10 epochs, and were optimized using adam  #AUTHOR_TAG with default parameters ( initial learning rate 0. 001, first momentum coefficient 0. 9, second momentum coefficient 0. 999 ).', 'we used l2 regularization with a coefficient of 10 −3 and max - norm regularization with an upper bound of 5. 0.', '']",4
"['according to  #TAUTHOR_TAG, and tag embedding size was']","['according to  #TAUTHOR_TAG, and tag embedding size was']","['- implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that']","['test different labelling models on top of the unlabelled trees produced by our re - implementation of the parsing as head selection model ( § 2 ).', 'we first train the unlabelled parsing models for the three languages.', 'unless stated otherwise, all parameters are set according to  #TAUTHOR_TAG, and tag embedding size was set to 40 for all languages.', 'please note that we do not use pre - trained embeddings in our experiments.', 'in the next step, we train four different labelling models : the labeller of  #TAUTHOR_TAG that uses a rectifier neural network with two hidden layers ( baseline ), two bidirectional lstm models ( bilstm ( l ) and bilstm ( b ) ), and one tree lstm model ( treelstm ) ( § 3 ).', 'the hidden layer dimension in all lstm models was set to 200.', 'the models were trained for 10 epochs, and were optimized using adam  #AUTHOR_TAG with default parameters ( initial learning rate 0. 001, first momentum coefficient 0. 9, second momentum coefficient 0. 999 ).', 'we used l2 regularization with a coefficient of 10 −3 and max - norm regularization with an upper bound of 5. 0.', '']",4
['of  #TAUTHOR_TAG ( 96. 15'],['original labeller of  #TAUTHOR_TAG ( 96. 15 % ) by more'],[') outperforms the original labeller of  #TAUTHOR_TAG ( 96. 15'],"['the spmrl 2014 shared task data, our results are only 0. 3 % lower than the ones of the winning system ( bjorkelund et al., 2014 ) fectiveness of our models, we also ran our labeller on the unlabelled output of the spmrl 2014 winning system and on unlabelled gold trees.', 'on the output of the blended system las slightly improves from 88. 62 % to 88. 76 % ( treelstm ).', '3 when applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history - based models ( bilstm ( b ), 97. 38 % ) outperforms the original labeller of  #TAUTHOR_TAG ( 96. 15 % ) by more than 1 %.', 'we would like to emphasize that our historybased lstm labeller is practically simple and computationally inexpensive ( as compared to global training or inference ), so our model manages to preserve simplicity while significantly improving labelling performance']",4
['original labeller of  #TAUTHOR_TAG'],['original labeller of  #TAUTHOR_TAG'],"['have shown that gf labelling, which is of crucial importance for languages like german, can be improved by combining lstm models with a decision history.', 'all our models outperform the original labeller of  #TAUTHOR_TAG']","['have shown that gf labelling, which is of crucial importance for languages like german, can be improved by combining lstm models with a decision history.', 'all our models outperform the original labeller of  #TAUTHOR_TAG and give results in the same range as the best system from the spmrl - 2014 shared task ( without the reranker ), but with a much simpler model.', 'our results show that the history is especially important for languages that show more word order variation.', 'here, presenting the input in a structured bfs order not only significantly outperforms the baseline, but also yields improvements over the other lstm models on core grammatical functions']",4
"['local action classifier.', 'recently,  #TAUTHOR_TAG use']","['local action classifier.', 'recently,  #TAUTHOR_TAG use']","['the local action classifier.', 'recently,  #TAUTHOR_TAG use']","['', 'they are attractive by very fast speeds.', 'traditionally, a linear model has been used for the local action classifier.', 'recently,  #TAUTHOR_TAG use a neural network ( nn ) to replace linear models, and report improved accuracies.', 'a contrast between a neural network model and a linear model is shown in figure 1 a neural network model takes continuous vector representations of words as inputs, which can be pre - trained using large amounts of unlabeled data, thus containing more information.', 'in addition, using an extra hidden layer, a neural network is capable of learning non - linear relations between automatic features, achieving feature combinations automatically.', 'discrete manual features and continuous features complement each other.', '']",0
"['embeddings.', ' #TAUTHOR_TAG fine -']","['embeddings.', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of']","['embeddings.', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of']","['', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of large - scale pre - training, because out - of - vocabulary ( oov ) words do not have their embeddings fine - tuned.', 'in this sense, the method of chen and manning resembles a traditional supervised sparse linear model, which can be weak on oov.', 'on the other hand, the semi - supervised learning methods such as  #AUTHOR_TAG table 2 : main results on sancl.', 'all systems are deterministic.', 'parameters.', '']",0
"['take  #TAUTHOR_TAG,']","['take  #TAUTHOR_TAG,']","['take  #TAUTHOR_TAG,']","['take  #TAUTHOR_TAG, which uses the arc - standard transition system  #AUTHOR_TAG.', 'given an pos - tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search.', ' #AUTHOR_TAG can be viewed as a neutral alternative of maltparser  #AUTHOR_TAG.', 'although not giving state - of - the - art accuracies, deterministic parsing is attractive for its high parsing speed ( 1000 + sentences per second ).', 'our incorporation of discrete features does not harm the overall speed significantly.', 'in addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences']",5
"[' #TAUTHOR_TAG, training of']","[' #TAUTHOR_TAG, training of']","[' #TAUTHOR_TAG, training of all the models using a cross - entropy loss objective with a l2 - regularization, and mini - batched adagrad  #AUTHOR_TAG.', 'we unify below the five deterministic parsing models in figure 1']","[' #TAUTHOR_TAG, training of all the models using a cross - entropy loss objective with a l2 - regularization, and mini - batched adagrad  #AUTHOR_TAG.', 'we unify below the five deterministic parsing models in figure 1']",5
['take the neural model of  #TAUTHOR_TAG as another baseline ('],"['take the neural model of  #TAUTHOR_TAG as another baseline ( figure 1 ( b ) ).', 'given a parsing state x, the words are first mapped into continuous vectors by using a set of pre - trained word embeddings.', 'denote the mapping']",['take the neural model of  #TAUTHOR_TAG as another baseline ('],"['take the neural model of  #TAUTHOR_TAG as another baseline ( figure 1 ( b ) ).', 'given a parsing state x, the words are first mapped into continuous vectors by using a set of pre - trained word embeddings.', 'denote the mapping as φ e ( x ).', 'in addition, denote the hidden layer as φ h, and the ith node in the hidden as φ h, i ( 0 ≤ i ≤ | φ h | ).', 'the hidden layer is defined as', 'where − → θ h is the set of parameters between the input and hidden layers.', 'the score of an action a is defined as', 'where − → θ c, a is the set of parameters between the hidden and output layers.', 'we use the arc - standard features φ e as  #TAUTHOR_TAG, which is also based on the arc - eager templates of  #AUTHOR_TAG, similar to those of the baseline model l']",5
['take the neural model of  #TAUTHOR_TAG as another baseline ('],"['take the neural model of  #TAUTHOR_TAG as another baseline ( figure 1 ( b ) ).', 'given a parsing state x, the words are first mapped into continuous vectors by using a set of pre - trained word embeddings.', 'denote the mapping']",['take the neural model of  #TAUTHOR_TAG as another baseline ('],"['take the neural model of  #TAUTHOR_TAG as another baseline ( figure 1 ( b ) ).', 'given a parsing state x, the words are first mapped into continuous vectors by using a set of pre - trained word embeddings.', 'denote the mapping as φ e ( x ).', 'in addition, denote the hidden layer as φ h, and the ith node in the hidden as φ h, i ( 0 ≤ i ≤ | φ h | ).', 'the hidden layer is defined as', 'where − → θ h is the set of parameters between the input and hidden layers.', 'the score of an action a is defined as', 'where − → θ c, a is the set of parameters between the hidden and output layers.', 'we use the arc - standard features φ e as  #TAUTHOR_TAG, which is also based on the arc - eager templates of  #AUTHOR_TAG, similar to those of the baseline model l']",5
"['assigned automatically on the training corpus by ten - fold jackknifing.', 'following  #TAUTHOR_TAG, we use the pre - trained word embedding released by  #AUTHOR_TAG, and set h = 200 for the hidden layer size, λ']","['assigned automatically on the training corpus by ten - fold jackknifing.', 'following  #TAUTHOR_TAG, we use the pre - trained word embedding released by  #AUTHOR_TAG, and set h = 200 for the hidden layer size, λ']","['on the wsj training corpus.', 'the pos tags are assigned automatically on the training corpus by ten - fold jackknifing.', 'following  #TAUTHOR_TAG, we use the pre - trained word embedding released by  #AUTHOR_TAG, and set h = 200 for the hidden layer size, λ']","['perform experiments on the sancl 2012 web data ( petrov and mc  #AUTHOR_TAG, using the wall street journal ( wsj ) training corpus to train the models and the wsj development corpus to tune parameters.', 'we clean the web domain texts following the method of  #AUTHOR_TAG b ).', 'automatic pos tags are produced by using a crf model trained on the wsj training corpus.', 'the pos tags are assigned automatically on the training corpus by ten - fold jackknifing.', 'following  #TAUTHOR_TAG, we use the pre - trained word embedding released by  #AUTHOR_TAG, and set h = 200 for the hidden layer size, λ = 10 −8 for l2 regularization, and α = 0. 01 for the initial learning rate of adagrad']",5
"['embeddings.', ' #TAUTHOR_TAG fine -']","['embeddings.', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of']","['embeddings.', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of']","['', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of large - scale pre - training, because out - of - vocabulary ( oov ) words do not have their embeddings fine - tuned.', 'in this sense, the method of chen and manning resembles a traditional supervised sparse linear model, which can be weak on oov.', 'on the other hand, the semi - supervised learning methods such as  #AUTHOR_TAG table 2 : main results on sancl.', 'all systems are deterministic.', 'parameters.', '']",5
"['embeddings.', ' #TAUTHOR_TAG fine -']","['embeddings.', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of']","['embeddings.', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of']","['', ' #TAUTHOR_TAG fine - tune word embeddings in supervised training, consistent with  #AUTHOR_TAG.', 'intuitively, fine - tuning embeddings allows in - vocabulary words to join the parameter space, thereby giving better fitting to in - domain data.', 'however, it also forfeits the benefit of large - scale pre - training, because out - of - vocabulary ( oov ) words do not have their embeddings fine - tuned.', 'in this sense, the method of chen and manning resembles a traditional supervised sparse linear model, which can be weak on oov.', 'on the other hand, the semi - supervised learning methods such as  #AUTHOR_TAG table 2 : main results on sancl.', 'all systems are deterministic.', 'parameters.', '']",5
['- implementation of  #TAUTHOR_TAG give comparable'],"['', 'our logistic regression linear parser and re - implementation of  #TAUTHOR_TAG give comparable']",['- implementation of  #TAUTHOR_TAG give comparable accuracies to'],"['final results across web domains are shown in table 2.', 'our logistic regression linear parser and re - implementation of  #TAUTHOR_TAG give comparable accuracies to the perceptron zpar 2 and stanford nn parser 3, respectively.', 'it can be seen from the table that both turian and guo 4 outperform l by incorporating embed - ding features.', 'guo gives overall higher improvements, consistent with the observation of  #AUTHOR_TAG on ner.', 'our methods gives significantly 5 better results compared with turian and guo, thanks to the extra hidden layer.', 'our oov performance is higher than nn, because the embeddings of oov words are not tuned, and hence the model can handle them effectively.', 'interestingly, nn gives higher accuracies on web domain out - of - embeddingvocabulary ( ooe ) words, out of which 54 % are in - vocabulary.', 'note that the accuracies of our parsers are lower than the best systems in the sancl shared task, which use ensemble models.', 'our parser enjoys the fast speed of deterministic parsers, and in particular the baseline nn parser  #TAUTHOR_TAG']",5
"['.', 'in particular, the method of  #TAUTHOR_TAG is']","['literature.', 'in particular, the method of  #TAUTHOR_TAG is']","['', 'in particular, the method of  #TAUTHOR_TAG is']","['comparison with related work, we conduct experiments on penn treebank corpus also.', 'we use the wsj sections 2 - 21 for training, section 22 for development and section 23 for testing.', 'wsj constituent trees are converted to dependency trees using penn2malt 6.', 'we use auto pos tags consistent with previous work.', 'the zpar pos - tagger is used to assign pos tags.', 'ten - fold jackknifing is performed on the training data to assign pos automatically.', 'for this set of experiments, the parser hyper - parameters are taken directly from the best settings in the web domain experiments.', 'the results are shown in table 3, together with some state - of - the - art deterministic parsers.', 'comparing the l, nn and this models, the observations are consistent with the web domain.', ' #AUTHOR_TAG 91. 30 90. 00  #AUTHOR_TAG a ) 91. 32 - table 3 : main results on wsj.', 'all systems are deterministic.', 'our combined parser gives accuracies competitive to state - of - the - art deterministic parsers in the literature.', 'in particular, the method of  #TAUTHOR_TAG is the same as our nn baseline.', 'note that  #AUTHOR_TAG reports a uas of 91. 47 % by this parser, which is higher than the results we obtained.', 'the main results include the use of different batch size during, while  #AUTHOR_TAG used a batch size of 100, 000, we used a batch size of 10, 000 in all experiments.', ' #AUTHOR_TAG applies dynamic oracle to the deterministic transition - based parsing, giving a uas of 91. 30 %.', ' #AUTHOR_TAG a ) is similar to zpar local, except that they use the arc - standard transitions, while zpar - local is based on arc - eager transitions.', ' #AUTHOR_TAG a ) uses a special method to process punctuations, leading to about 1 % uas improvements over the vanilla system.', ' #AUTHOR_TAG proposed a deterministic transition - based parser using lstm, which gives a uas of 93. 1 % on stanford conversion of the penn treebank.', 'their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers.', 'their work is orthogonal to ours']",5
['- implementation of  #TAUTHOR_TAG give comparable'],"['', 'our logistic regression linear parser and re - implementation of  #TAUTHOR_TAG give comparable']",['- implementation of  #TAUTHOR_TAG give comparable accuracies to'],"['final results across web domains are shown in table 2.', 'our logistic regression linear parser and re - implementation of  #TAUTHOR_TAG give comparable accuracies to the perceptron zpar 2 and stanford nn parser 3, respectively.', 'it can be seen from the table that both turian and guo 4 outperform l by incorporating embed - ding features.', 'guo gives overall higher improvements, consistent with the observation of  #AUTHOR_TAG on ner.', 'our methods gives significantly 5 better results compared with turian and guo, thanks to the extra hidden layer.', 'our oov performance is higher than nn, because the embeddings of oov words are not tuned, and hence the model can handle them effectively.', 'interestingly, nn gives higher accuracies on web domain out - of - embeddingvocabulary ( ooe ) words, out of which 54 % are in - vocabulary.', 'note that the accuracies of our parsers are lower than the best systems in the sancl shared task, which use ensemble models.', 'our parser enjoys the fast speed of deterministic parsers, and in particular the baseline nn parser  #TAUTHOR_TAG']",3
['- implementation of  #TAUTHOR_TAG give comparable'],"['', 'our logistic regression linear parser and re - implementation of  #TAUTHOR_TAG give comparable']",['- implementation of  #TAUTHOR_TAG give comparable accuracies to'],"['final results across web domains are shown in table 2.', 'our logistic regression linear parser and re - implementation of  #TAUTHOR_TAG give comparable accuracies to the perceptron zpar 2 and stanford nn parser 3, respectively.', 'it can be seen from the table that both turian and guo 4 outperform l by incorporating embed - ding features.', 'guo gives overall higher improvements, consistent with the observation of  #AUTHOR_TAG on ner.', 'our methods gives significantly 5 better results compared with turian and guo, thanks to the extra hidden layer.', 'our oov performance is higher than nn, because the embeddings of oov words are not tuned, and hence the model can handle them effectively.', 'interestingly, nn gives higher accuracies on web domain out - of - embeddingvocabulary ( ooe ) words, out of which 54 % are in - vocabulary.', 'note that the accuracies of our parsers are lower than the best systems in the sancl shared task, which use ensemble models.', 'our parser enjoys the fast speed of deterministic parsers, and in particular the baseline nn parser  #TAUTHOR_TAG']",3
"['.', 'in particular, the method of  #TAUTHOR_TAG is']","['literature.', 'in particular, the method of  #TAUTHOR_TAG is']","['', 'in particular, the method of  #TAUTHOR_TAG is']","['comparison with related work, we conduct experiments on penn treebank corpus also.', 'we use the wsj sections 2 - 21 for training, section 22 for development and section 23 for testing.', 'wsj constituent trees are converted to dependency trees using penn2malt 6.', 'we use auto pos tags consistent with previous work.', 'the zpar pos - tagger is used to assign pos tags.', 'ten - fold jackknifing is performed on the training data to assign pos automatically.', 'for this set of experiments, the parser hyper - parameters are taken directly from the best settings in the web domain experiments.', 'the results are shown in table 3, together with some state - of - the - art deterministic parsers.', 'comparing the l, nn and this models, the observations are consistent with the web domain.', ' #AUTHOR_TAG 91. 30 90. 00  #AUTHOR_TAG a ) 91. 32 - table 3 : main results on wsj.', 'all systems are deterministic.', 'our combined parser gives accuracies competitive to state - of - the - art deterministic parsers in the literature.', 'in particular, the method of  #TAUTHOR_TAG is the same as our nn baseline.', 'note that  #AUTHOR_TAG reports a uas of 91. 47 % by this parser, which is higher than the results we obtained.', 'the main results include the use of different batch size during, while  #AUTHOR_TAG used a batch size of 100, 000, we used a batch size of 10, 000 in all experiments.', ' #AUTHOR_TAG applies dynamic oracle to the deterministic transition - based parsing, giving a uas of 91. 30 %.', ' #AUTHOR_TAG a ) is similar to zpar local, except that they use the arc - standard transitions, while zpar - local is based on arc - eager transitions.', ' #AUTHOR_TAG a ) uses a special method to process punctuations, leading to about 1 % uas improvements over the vanilla system.', ' #AUTHOR_TAG proposed a deterministic transition - based parser using lstm, which gives a uas of 93. 1 % on stanford conversion of the penn treebank.', 'their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers.', 'their work is orthogonal to ours']",3
"['the best results  #TAUTHOR_TAG for', 'chinese ws']","['best results  #TAUTHOR_TAG for', 'chinese ws']","[', we combine the contextual features in the target context with the pre', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese ws']","['', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese wsd. instead of including bi - gram features as part of discrimination features, in our system, we consider both topical contextual', 'features as well as local collocation features. these features are extracted form the 60mb human sense - tagged', ""people's daily news with segmentation information""]",0
"['the best results  #TAUTHOR_TAG for', 'chinese ws']","['best results  #TAUTHOR_TAG for', 'chinese ws']","[', we combine the contextual features in the target context with the pre', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese ws']","['', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese wsd. instead of including bi - gram features as part of discrimination features, in our system, we consider both topical contextual', 'features as well as local collocation features. these features are extracted form the 60mb human sense - tagged', ""people's daily news with segmentation information""]",0
"['the best results  #TAUTHOR_TAG for', 'chinese ws']","['best results  #TAUTHOR_TAG for', 'chinese ws']","[', we combine the contextual features in the target context with the pre', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese ws']","['', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese wsd. instead of including bi - gram features as part of discrimination features, in our system, we consider both topical contextual', 'features as well as local collocation features. these features are extracted form the 60mb human sense - tagged', ""people's daily news with segmentation information""]",0
"['the best results  #TAUTHOR_TAG for', 'chinese ws']","['best results  #TAUTHOR_TAG for', 'chinese ws']","[', we combine the contextual features in the target context with the pre', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese ws']","['', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese wsd. instead of including bi - gram features as part of discrimination features, in our system, we consider both topical contextual', 'features as well as local collocation features. these features are extracted form the 60mb human sense - tagged', ""people's daily news with segmentation information""]",0
"['s  #TAUTHOR_TAG work, topical features as well as']","['collocations applied in our system.', 'the sources of the collocations will be explained in section 4. 1.', ""in both niu [ 11 ] and dang's  #TAUTHOR_TAG work, topical features as well as""]","['s  #TAUTHOR_TAG work, topical features as well as the so called collocational features were used.', 'however, as discussed in section 2,']","['chose collocations as the local features.', 'a collocation is a recurrent and conventional fixed expression of words which holds syntactic and semantic relations [ 21 ].', 'collocations can be classified as fully fixed collocations, fixed collocations, strong collocations and loose collocations.', 'fixed collocations means the appearance of one word implies the co - occurrence of another one such as "" "" ( "" burden of history "" ), while strong collocations allows very limited substitution of the components, for example, ""', '"" ( "" local college "" ), or "" "" ( "" local university "" ).', 'the sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system.', 'the sources of the collocations will be explained in section 4. 1.', ""in both niu [ 11 ] and dang's  #TAUTHOR_TAG work, topical features as well as the so called collocational features were used."", 'however, as discussed in section 2, they both used bi - gram cooccurrences as the additional local features.', 'however, bi - gram co - occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations.', 'thus instead of using co - occurrences of bigrams, we take the true bi - gram collocations extracted from our system and use this data to compare with bi - gram co - occurrences to test the usefulness of collocation for wsd.', 'the local features in our system make use of the collocations using the template ( w i, w ) within a window size of ten ( where i = ± 5 ).', 'for example, "" "" ( "" government departments and local government commanded that "" ) fits the bi - gram collocation template ( w, w 1 ) with the value of ( ).', 'during the training and the testing processes, the counting of frequency value of the collocation feature will be increased by 1 if a collocation containing the ambiguous word occurs in a sentence.', 'to have a good analysis on collocation features, we have also developed an algorithm using lonely adjacent bi - gram as locals features ( named sys - adjacent bi - gram as locals features ( named system a ) and another using collocation as local features ( named system b )']",0
"['the best results  #TAUTHOR_TAG for', 'chinese ws']","['best results  #TAUTHOR_TAG for', 'chinese ws']","[', we combine the contextual features in the target context with the pre', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese ws']","['', '- prepared collocations list to build our classifier. as stated early, an important issue is what features will be used to construct the classifier in wsd. early researches have proven', 'that using lexical statistical information, such as bi - gram co - occurrences was sufficient to produce close to the best results  #TAUTHOR_TAG for', 'chinese wsd. instead of including bi - gram features as part of discrimination features, in our system, we consider both topical contextual', 'features as well as local collocation features. these features are extracted form the 60mb human sense - tagged', ""people's daily news with segmentation information""]",4
"['s  #TAUTHOR_TAG work, topical features as well as']","['collocations applied in our system.', 'the sources of the collocations will be explained in section 4. 1.', ""in both niu [ 11 ] and dang's  #TAUTHOR_TAG work, topical features as well as""]","['s  #TAUTHOR_TAG work, topical features as well as the so called collocational features were used.', 'however, as discussed in section 2,']","['chose collocations as the local features.', 'a collocation is a recurrent and conventional fixed expression of words which holds syntactic and semantic relations [ 21 ].', 'collocations can be classified as fully fixed collocations, fixed collocations, strong collocations and loose collocations.', 'fixed collocations means the appearance of one word implies the co - occurrence of another one such as "" "" ( "" burden of history "" ), while strong collocations allows very limited substitution of the components, for example, ""', '"" ( "" local college "" ), or "" "" ( "" local university "" ).', 'the sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system.', 'the sources of the collocations will be explained in section 4. 1.', ""in both niu [ 11 ] and dang's  #TAUTHOR_TAG work, topical features as well as the so called collocational features were used."", 'however, as discussed in section 2, they both used bi - gram cooccurrences as the additional local features.', 'however, bi - gram co - occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations.', 'thus instead of using co - occurrences of bigrams, we take the true bi - gram collocations extracted from our system and use this data to compare with bi - gram co - occurrences to test the usefulness of collocation for wsd.', 'the local features in our system make use of the collocations using the template ( w i, w ) within a window size of ten ( where i = ± 5 ).', 'for example, "" "" ( "" government departments and local government commanded that "" ) fits the bi - gram collocation template ( w, w 1 ) with the value of ( ).', 'during the training and the testing processes, the counting of frequency value of the collocation feature will be increased by 1 if a collocation containing the ambiguous word occurs in a sentence.', 'to have a good analysis on collocation features, we have also developed an algorithm using lonely adjacent bi - gram as locals features ( named sys - adjacent bi - gram as locals features ( named system a ) and another using collocation as local features ( named system b )']",4
['in the large  #TAUTHOR_TAG.'],['in the large  #TAUTHOR_TAG.'],['in the large  #TAUTHOR_TAG.'],"['', 'the penalty for not handling non - projective constructions is almost negligible. still, from a theoretical point of view, projective parsing of non - projective structures has the drawback that it rules out perfect', 'accuracy even as an asymptotic goal. there exist a few robust broad - coverage parsers that produce non - projective dependency structures, notably tapanainen and jarvinen ( 1997 ) and  #AUTHOR_TAG for  #AUTHOR_TAG', 'for  #AUTHOR_TAG for czech. in addition, there are several approaches to non - projective dependency parsing that', 'are still to be evaluated in the large  #TAUTHOR_TAG. finally, since non - projective constructions often involve long - distance dependencies, the problem is closely related to the recovery of empty categories and non - local', 'dependencies in constituency - based parsing  #AUTHOR_TAG jijkoun and de  #AUTHOR_TAG. in this paper, we show how non - projective', 'dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. first, the training data for the parser is projectivized by applying a minimal number of lifting operations  #TAUTHOR_TAG and encoding information about these lifts in arc labels. when the parser is trained on the', 'transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. by applying an inverse transformation to', '']",0
['in the large  #TAUTHOR_TAG.'],['in the large  #TAUTHOR_TAG.'],['in the large  #TAUTHOR_TAG.'],"['', 'the penalty for not handling non - projective constructions is almost negligible. still, from a theoretical point of view, projective parsing of non - projective structures has the drawback that it rules out perfect', 'accuracy even as an asymptotic goal. there exist a few robust broad - coverage parsers that produce non - projective dependency structures, notably tapanainen and jarvinen ( 1997 ) and  #AUTHOR_TAG for  #AUTHOR_TAG', 'for  #AUTHOR_TAG for czech. in addition, there are several approaches to non - projective dependency parsing that', 'are still to be evaluated in the large  #TAUTHOR_TAG. finally, since non - projective constructions often involve long - distance dependencies, the problem is closely related to the recovery of empty categories and non - local', 'dependencies in constituency - based parsing  #AUTHOR_TAG jijkoun and de  #AUTHOR_TAG. in this paper, we show how non - projective', 'dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. first, the training data for the parser is projectivized by applying a minimal number of lifting operations  #TAUTHOR_TAG and encoding information about these lifts in arc labels. when the parser is trained on the', 'transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. by applying an inverse transformation to', '']",0
,,,,0
['in the large  #TAUTHOR_TAG.'],['in the large  #TAUTHOR_TAG.'],['in the large  #TAUTHOR_TAG.'],"['', 'the penalty for not handling non - projective constructions is almost negligible. still, from a theoretical point of view, projective parsing of non - projective structures has the drawback that it rules out perfect', 'accuracy even as an asymptotic goal. there exist a few robust broad - coverage parsers that produce non - projective dependency structures, notably tapanainen and jarvinen ( 1997 ) and  #AUTHOR_TAG for  #AUTHOR_TAG', 'for  #AUTHOR_TAG for czech. in addition, there are several approaches to non - projective dependency parsing that', 'are still to be evaluated in the large  #TAUTHOR_TAG. finally, since non - projective constructions often involve long - distance dependencies, the problem is closely related to the recovery of empty categories and non - local', 'dependencies in constituency - based parsing  #AUTHOR_TAG jijkoun and de  #AUTHOR_TAG. in this paper, we show how non - projective', 'dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. first, the training data for the parser is projectivized by applying a minimal number of lifting operations  #TAUTHOR_TAG and encoding information about these lifts in arc labels. when the parser is trained on the', 'transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. by applying an inverse transformation to', '']",7
,,,,7
,,,,5
,,,,4
"['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['in natural language processing ( nlp ) tasks, yet few attempts have been made', 'to learn more task - specific embeddings. for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe ) to address the aa task on the asa', '##p dataset. their embeddings are constructed by ranking correct ngrams against their "" noisy "" counterparts, in addition to capturing words\'informative', '##ness measured by their contribution to the overall score of the essay. we propose a task - specific approach to pre - train word embeddings, utilized by neural aa models, in an error - oriented fashion. writing errors are strong indicators of the quality', ""of writing competence and good predictors for the overall script score, especially in scripts written by language learners, which is the case for the fce dataset. for example, the spearman's rank correlation coefficient between the fce script scores and the ratio of errors is −0"", '. 63 which is indicative of the importance of errors in writing evaluation', ': ratio of errors = number of erroneous script words script length this correlation could even be higher if error severity is accounted for as some errors could', 'be more serious than others. therefore, it seems plausible to exploit writing errors and integrate them into aa systems, as was successfully done by  #AUTHOR_TAG', 'and, but not by capturing this information directly in word embeddings in a neural aa model. our pre - training model learns to predict a score for each ngram based on the errors it contains', '']",0
"['of the lstm layer.', ' #TAUTHOR_TAG assessed']","['of the lstm layer.', ' #TAUTHOR_TAG assessed']","['averages the output of the lstm layer.', ' #TAUTHOR_TAG assessed the same dataset by building a bidirectional double - layer lstm which']","['have been various attempts to employ neural networks to assess the essays in the asap dataset.', ' #AUTHOR_TAG compared the performance of a few neural network variants and obtained the best results with an lstm followed by a mean over time layer that averages the output of the lstm layer.', ' #TAUTHOR_TAG assessed the same dataset by building a bidirectional double - layer lstm which outperformed distributed memory model of paragraph vectors ( pv - dm )  #AUTHOR_TAG and support vector machines ( svm ) baselines.', ' #AUTHOR_TAG implemented a cnn where the first layer convolves a filter of weights over the words in each sentence followed by an aggregative pooling function to construct sentence representations.', '']",0
"['of the lstm layer.', ' #TAUTHOR_TAG assessed']","['of the lstm layer.', ' #TAUTHOR_TAG assessed']","['averages the output of the lstm layer.', ' #TAUTHOR_TAG assessed the same dataset by building a bidirectional double - layer lstm which']","['have been various attempts to employ neural networks to assess the essays in the asap dataset.', ' #AUTHOR_TAG compared the performance of a few neural network variants and obtained the best results with an lstm followed by a mean over time layer that averages the output of the lstm layer.', ' #TAUTHOR_TAG assessed the same dataset by building a bidirectional double - layer lstm which outperformed distributed memory model of paragraph vectors ( pv - dm )  #AUTHOR_TAG and support vector machines ( svm ) baselines.', ' #AUTHOR_TAG implemented a cnn where the first layer convolves a filter of weights over the words in each sentence followed by an aggregative pooling function to construct sentence representations.', '']",0
"['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['in natural language processing ( nlp ) tasks, yet few attempts have been made', 'to learn more task - specific embeddings. for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe ) to address the aa task on the asa', '##p dataset. their embeddings are constructed by ranking correct ngrams against their "" noisy "" counterparts, in addition to capturing words\'informative', '##ness measured by their contribution to the overall score of the essay. we propose a task - specific approach to pre - train word embeddings, utilized by neural aa models, in an error - oriented fashion. writing errors are strong indicators of the quality', ""of writing competence and good predictors for the overall script score, especially in scripts written by language learners, which is the case for the fce dataset. for example, the spearman's rank correlation coefficient between the fce script scores and the ratio of errors is −0"", '. 63 which is indicative of the importance of errors in writing evaluation', ': ratio of errors = number of erroneous script words script length this correlation could even be higher if error severity is accounted for as some errors could', 'be more serious than others. therefore, it seems plausible to exploit writing errors and integrate them into aa systems, as was successfully done by  #AUTHOR_TAG', 'and, but not by capturing this information directly in word embeddings in a neural aa model. our pre - training model learns to predict a score for each ngram based on the errors it contains', '']",1
"['of the lstm layer.', ' #TAUTHOR_TAG assessed']","['of the lstm layer.', ' #TAUTHOR_TAG assessed']","['averages the output of the lstm layer.', ' #TAUTHOR_TAG assessed the same dataset by building a bidirectional double - layer lstm which']","['have been various attempts to employ neural networks to assess the essays in the asap dataset.', ' #AUTHOR_TAG compared the performance of a few neural network variants and obtained the best results with an lstm followed by a mean over time layer that averages the output of the lstm layer.', ' #TAUTHOR_TAG assessed the same dataset by building a bidirectional double - layer lstm which outperformed distributed memory model of paragraph vectors ( pv - dm )  #AUTHOR_TAG and support vector machines ( svm ) baselines.', ' #AUTHOR_TAG implemented a cnn where the first layer convolves a filter of weights over the words in each sentence followed by an aggregative pooling function to construct sentence representations.', '']",1
"['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['in natural language processing ( nlp ) tasks, yet few attempts have been made', 'to learn more task - specific embeddings. for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe ) to address the aa task on the asa', '##p dataset. their embeddings are constructed by ranking correct ngrams against their "" noisy "" counterparts, in addition to capturing words\'informative', '##ness measured by their contribution to the overall score of the essay. we propose a task - specific approach to pre - train word embeddings, utilized by neural aa models, in an error - oriented fashion. writing errors are strong indicators of the quality', ""of writing competence and good predictors for the overall script score, especially in scripts written by language learners, which is the case for the fce dataset. for example, the spearman's rank correlation coefficient between the fce script scores and the ratio of errors is −0"", '. 63 which is indicative of the importance of errors in writing evaluation', ': ratio of errors = number of erroneous script words script length this correlation could even be higher if error severity is accounted for as some errors could', 'be more serious than others. therefore, it seems plausible to exploit writing errors and integrate them into aa systems, as was successfully done by  #AUTHOR_TAG', 'and, but not by capturing this information directly in word embeddings in a neural aa model. our pre - training model learns to predict a score for each ngram based on the errors it contains', '']",3
"['sswe developed by  #TAUTHOR_TAG.', 'their method is inspired by']","['sswe developed by  #TAUTHOR_TAG.', 'their method is inspired by']","['pre - training models to the sswe developed by  #TAUTHOR_TAG.', 'their method is inspired by']","['compare our pre - training models to the sswe developed by  #TAUTHOR_TAG.', ""their method is inspired by the work of  #AUTHOR_TAG which learns word representations by distinguishing between a target word's context ( window of surrounding words ) and its noisy counterparts."", 'these counterparts are generated by replacing the target word with a randomly selected word from the vocabulary.', '']",3
"['parameters as  #TAUTHOR_TAG.', '10 tuning the filter sizes was']","['parameters as  #TAUTHOR_TAG.', '10 tuning the filter sizes was']","['as  #TAUTHOR_TAG.', '10 tuning the filter sizes was done for each model separately ; for']","['public fce results shown in table 2 reveal that aa - specific embedding pre - training offers further gains in performance over the traditional embeddings trained on large corpora ( google and glove embeddings ), which suggests that they are more suited for the aa task.', 'the table also demonstrates that the eswe model outperforms the sswe one on correlation metrics, with a slight difference in the rmse value.', 'while the variance in the correlations between the two models is noticeable and suggests that the eswe model is a more powerful one, the rmse values weaken this assumption.', 'this result could be attributed to the fact that public fce is a small dataset with sparse error representations and sswe is trained on 20 times more data as each ngram is paired with 20 randomly generated counterparts.', 'therefore, a more relevant comparison is needed and could be achieved by either training on more data, as will be discussed later, or further enriching the embedding space with corrections ( ecswe ).', 'table 2 demonstrates that learning from the er - 9 using the same parameters as  #TAUTHOR_TAG.', '10 tuning the filter sizes was done for each model separately ; for the glove and word2vec models, a filter of size 3 performed better than 9, on both datasets.', '']",3
"['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe )']","['in natural language processing ( nlp ) tasks, yet few attempts have been made', 'to learn more task - specific embeddings. for instance,  #TAUTHOR_TAG developed score - specific word embeddings ( sswe ) to address the aa task on the asa', '##p dataset. their embeddings are constructed by ranking correct ngrams against their "" noisy "" counterparts, in addition to capturing words\'informative', '##ness measured by their contribution to the overall score of the essay. we propose a task - specific approach to pre - train word embeddings, utilized by neural aa models, in an error - oriented fashion. writing errors are strong indicators of the quality', ""of writing competence and good predictors for the overall script score, especially in scripts written by language learners, which is the case for the fce dataset. for example, the spearman's rank correlation coefficient between the fce script scores and the ratio of errors is −0"", '. 63 which is indicative of the importance of errors in writing evaluation', ': ratio of errors = number of erroneous script words script length this correlation could even be higher if error severity is accounted for as some errors could', 'be more serious than others. therefore, it seems plausible to exploit writing errors and integrate them into aa systems, as was successfully done by  #AUTHOR_TAG', 'and, but not by capturing this information directly in word embeddings in a neural aa model. our pre - training model learns to predict a score for each ngram based on the errors it contains', '']",5
"['model implemented by  #TAUTHOR_TAG and the two error - oriented models we propose in this work.', ""the models'output embeddings -""]","['model implemented by  #TAUTHOR_TAG and the two error - oriented models we propose in this work.', ""the models'output embeddings - referred""]","['this section, we describe three different neural networks to pre - train word representations : the model implemented by  #TAUTHOR_TAG and the two error - oriented models we propose in this work.', ""the models'output embeddings -""]","['this section, we describe three different neural networks to pre - train word representations : the model implemented by  #TAUTHOR_TAG and the two error - oriented models we propose in this work.', ""the models'output embeddings - referred to as aa - specific embeddings - are used later to bootstrap the aa system""]",5
"['parameters as  #TAUTHOR_TAG.', '10 tuning the filter sizes was']","['parameters as  #TAUTHOR_TAG.', '10 tuning the filter sizes was']","['as  #TAUTHOR_TAG.', '10 tuning the filter sizes was done for each model separately ; for']","['public fce results shown in table 2 reveal that aa - specific embedding pre - training offers further gains in performance over the traditional embeddings trained on large corpora ( google and glove embeddings ), which suggests that they are more suited for the aa task.', 'the table also demonstrates that the eswe model outperforms the sswe one on correlation metrics, with a slight difference in the rmse value.', 'while the variance in the correlations between the two models is noticeable and suggests that the eswe model is a more powerful one, the rmse values weaken this assumption.', 'this result could be attributed to the fact that public fce is a small dataset with sparse error representations and sswe is trained on 20 times more data as each ngram is paired with 20 randomly generated counterparts.', 'therefore, a more relevant comparison is needed and could be achieved by either training on more data, as will be discussed later, or further enriching the embedding space with corrections ( ecswe ).', 'table 2 demonstrates that learning from the er - 9 using the same parameters as  #TAUTHOR_TAG.', '10 tuning the filter sizes was done for each model separately ; for the glove and word2vec models, a filter of size 3 performed better than 9, on both datasets.', '']",5
"['- root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility']","['inverse square - root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility']","['. g., the inverse square - root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility']","['', 'optimizers update the model parameters based on the gradients.', 'we provide wrappers around most pytorch optimizers and an implementation of adafactor  #AUTHOR_TAG, which is a memory - efficient variant of adam.', 'learning rate schedulers update the learning rate over the course of training.', 'we provide several popular schedulers, e. g., the inverse square - root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility and forward compatibility.', 'fairseq includes features designed to improve reproducibility and forward compatibility.', 'for example, checkpoints contain the full state of the model, optimizer and dataloader, so that results are reproducible if training is interrupted and resumed.', 'fairseq also provides forward compatibility, i. e., models trained using old versions of the toolkit will continue to run on the latest version through automatic checkpoint upgrading']",3
"['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big ""']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to german ( en - de ) and wmt english to french ( en - fr ).', ""for en - de we replicate the setup of  #TAUTHOR_TAG which relies on wmt'16 for training with 4. 5m sentence pairs, we validate on newstest13 and test on newstest14."", 'the 32k vocabulary is based on a joint source and target byte pair encoding ( bpe ; sennrich et al. 2016 ).', ""for en - fr, we train on wmt'14 and borrow the setup of  #AUTHOR_TAG with 36m training sentence pairs."", 'we use newstest12 + 13 for validation and newstest14 for test.', 'the 40k vocabulary is based on a joint source and target bpe.', 'we measure case - sensitive tokenized bleu with multi - bleu  #AUTHOR_TAG and detokenized bleu with sacrebleu 1  #AUTHOR_TAG.', 'all results use beam search with a beam width of 4 and length penalty of 0. 6, following  #TAUTHOR_TAG.', 'fairseq results are summarized in table 2.', 'we reported improved bleu scores over  #TAUTHOR_TAG by training with a bigger batch size and an increased learning rate']",3
"['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big ""']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to german ( en - de ) and wmt english to french ( en - fr ).', ""for en - de we replicate the setup of  #TAUTHOR_TAG which relies on wmt'16 for training with 4. 5m sentence pairs, we validate on newstest13 and test on newstest14."", 'the 32k vocabulary is based on a joint source and target byte pair encoding ( bpe ; sennrich et al. 2016 ).', ""for en - fr, we train on wmt'14 and borrow the setup of  #AUTHOR_TAG with 36m training sentence pairs."", 'we use newstest12 + 13 for validation and newstest14 for test.', 'the 40k vocabulary is based on a joint source and target bpe.', 'we measure case - sensitive tokenized bleu with multi - bleu  #AUTHOR_TAG and detokenized bleu with sacrebleu 1  #AUTHOR_TAG.', 'all results use beam search with a beam width of 4 and length penalty of 0. 6, following  #TAUTHOR_TAG.', 'fairseq results are summarized in table 2.', 'we reported improved bleu scores over  #TAUTHOR_TAG by training with a bigger batch size and an increased learning rate']",3
"['- root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility']","['inverse square - root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility']","['. g., the inverse square - root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility']","['', 'optimizers update the model parameters based on the gradients.', 'we provide wrappers around most pytorch optimizers and an implementation of adafactor  #AUTHOR_TAG, which is a memory - efficient variant of adam.', 'learning rate schedulers update the learning rate over the course of training.', 'we provide several popular schedulers, e. g., the inverse square - root scheduler from  #TAUTHOR_TAG and cyclical schedulers based on warm restarts  #AUTHOR_TAG.', 'reproducibility and forward compatibility.', 'fairseq includes features designed to improve reproducibility and forward compatibility.', 'for example, checkpoints contain the full state of the model, optimizer and dataloader, so that results are reproducible if training is interrupted and resumed.', 'fairseq also provides forward compatibility, i. e., models trained using old versions of the toolkit will continue to run on the latest version through automatic checkpoint upgrading']",5
"['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big ""']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to german ( en - de ) and wmt english to french ( en - fr ).', ""for en - de we replicate the setup of  #TAUTHOR_TAG which relies on wmt'16 for training with 4. 5m sentence pairs, we validate on newstest13 and test on newstest14."", 'the 32k vocabulary is based on a joint source and target byte pair encoding ( bpe ; sennrich et al. 2016 ).', ""for en - fr, we train on wmt'14 and borrow the setup of  #AUTHOR_TAG with 36m training sentence pairs."", 'we use newstest12 + 13 for validation and newstest14 for test.', 'the 40k vocabulary is based on a joint source and target bpe.', 'we measure case - sensitive tokenized bleu with multi - bleu  #AUTHOR_TAG and detokenized bleu with sacrebleu 1  #AUTHOR_TAG.', 'all results use beam search with a beam width of 4 and length penalty of 0. 6, following  #TAUTHOR_TAG.', 'fairseq results are summarized in table 2.', 'we reported improved bleu scores over  #TAUTHOR_TAG by training with a bigger batch size and an increased learning rate']",5
"['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big ""']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to german ( en - de ) and wmt english to french ( en - fr ).', ""for en - de we replicate the setup of  #TAUTHOR_TAG which relies on wmt'16 for training with 4. 5m sentence pairs, we validate on newstest13 and test on newstest14."", 'the 32k vocabulary is based on a joint source and target byte pair encoding ( bpe ; sennrich et al. 2016 ).', ""for en - fr, we train on wmt'14 and borrow the setup of  #AUTHOR_TAG with 36m training sentence pairs."", 'we use newstest12 + 13 for validation and newstest14 for test.', 'the 40k vocabulary is based on a joint source and target bpe.', 'we measure case - sensitive tokenized bleu with multi - bleu  #AUTHOR_TAG and detokenized bleu with sacrebleu 1  #AUTHOR_TAG.', 'all results use beam search with a beam width of 4 and length penalty of 0. 6, following  #TAUTHOR_TAG.', 'fairseq results are summarized in table 2.', 'we reported improved bleu scores over  #TAUTHOR_TAG by training with a bigger batch size and an increased learning rate']",5
"['inference. fairseq provides fast inference for non - recurrent models  #TAUTHOR_TAG b ;  #AUTHOR_TAG through incremental decoding, where the model states of previously generated tokens are cache']","['inference. fairseq provides fast inference for non - recurrent models  #TAUTHOR_TAG b ;  #AUTHOR_TAG through incremental decoding, where the model states of previously generated tokens are cache']","['weights. inference. fairseq provides fast inference for non - recurrent models  #TAUTHOR_TAG b ;  #AUTHOR_TAG through incremental decoding, where the model states of previously generated tokens are cache']","['', 'update the weights. inference. fairseq provides fast inference for non - recurrent models  #TAUTHOR_TAG b ;  #AUTHOR_TAG through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re - used. this can', 'speed up a naive implementation without caching by up to an order of magnitude, since only new', 'states are computed', 'for each token. for some models, this requires a component - specific caching', 'implementation, e. g., multi - head attention in the transformer architecture. during inference we build', 'batches with a variable number of examples up to a user - specified number of tokens, similar to training. fairseq', 'also supports inference in fp16 which increases decoding speed by 54 %', 'compared to fp32 with no loss in accuracy ( table 1 )']",0
"['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big ""']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to german ( en - de ) and wmt english to french ( en - fr ).', ""for en - de we replicate the setup of  #TAUTHOR_TAG which relies on wmt'16 for training with 4. 5m sentence pairs, we validate on newstest13 and test on newstest14."", 'the 32k vocabulary is based on a joint source and target byte pair encoding ( bpe ; sennrich et al. 2016 ).', ""for en - fr, we train on wmt'14 and borrow the setup of  #AUTHOR_TAG with 36m training sentence pairs."", 'we use newstest12 + 13 for validation and newstest14 for test.', 'the 40k vocabulary is based on a joint source and target bpe.', 'we measure case - sensitive tokenized bleu with multi - bleu  #AUTHOR_TAG and detokenized bleu with sacrebleu 1  #AUTHOR_TAG.', 'all results use beam search with a beam width of 4 and length penalty of 0. 6, following  #TAUTHOR_TAG.', 'fairseq results are summarized in table 2.', 'we reported improved bleu scores over  #TAUTHOR_TAG by training with a bigger batch size and an increased learning rate']",0
"['transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of']","['transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of']","['transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of']","['##seq supports language modeling with gated convolutional models and transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings ( kim 1 en - de en - fr a.  #AUTHOR_TAG 25. 2 40. 5 b.  #TAUTHOR_TAG 28. 4 41. 0 c.  #AUTHOR_TAG 28. 9 41. 4 d.  #AUTHOR_TAG 29 et al., 2016 ), adaptive softmax  #AUTHOR_TAG, and adaptive inputs.', 'we also provide tutorials and pre - trained models that replicate the results of']",0
"['transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of']","['transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of']","['transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of']","['##seq supports language modeling with gated convolutional models and transformer models  #TAUTHOR_TAG.', 'models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings ( kim 1 en - de en - fr a.  #AUTHOR_TAG 25. 2 40. 5 b.  #TAUTHOR_TAG 28. 4 41. 0 c.  #AUTHOR_TAG 28. 9 41. 4 d.  #AUTHOR_TAG 29 et al., 2016 ), adaptive softmax  #AUTHOR_TAG, and adaptive inputs.', 'we also provide tutorials and pre - trained models that replicate the results of']",0
"['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big ""']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to']","['provide reference implementations of several popular sequence - to - sequence models which can be used for machine translation, including lstm  #AUTHOR_TAG, convolutional models  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we evaluate a "" big "" transformer encoderdecoder model on two language pairs, wmt english to german ( en - de ) and wmt english to french ( en - fr ).', ""for en - de we replicate the setup of  #TAUTHOR_TAG which relies on wmt'16 for training with 4. 5m sentence pairs, we validate on newstest13 and test on newstest14."", 'the 32k vocabulary is based on a joint source and target byte pair encoding ( bpe ; sennrich et al. 2016 ).', ""for en - fr, we train on wmt'14 and borrow the setup of  #AUTHOR_TAG with 36m training sentence pairs."", 'we use newstest12 + 13 for validation and newstest14 for test.', 'the 40k vocabulary is based on a joint source and target bpe.', 'we measure case - sensitive tokenized bleu with multi - bleu  #AUTHOR_TAG and detokenized bleu with sacrebleu 1  #AUTHOR_TAG.', 'all results use beam search with a beam width of 4 and length penalty of 0. 6, following  #TAUTHOR_TAG.', 'fairseq results are summarized in table 2.', 'we reported improved bleu scores over  #TAUTHOR_TAG by training with a bigger batch size and an increased learning rate']",4
"['of  #TAUTHOR_TAG, confirms that neural speech -']","[', in line with previous findings of  #TAUTHOR_TAG, confirms that neural speech - image models can capture a cross - lingual semantic signal, a first step in the perspective of learning speech -']","[', in line with previous findings of  #TAUTHOR_TAG, confirms that neural speech - image models']","['amount of phonological information is preserved in higher layers, and suggested that the attention layer focuses on semantic information. such computational models', 'can be used to emulate child language acquisition and could shed light on the inner cognitive pro - this work was supported by grants from neurocog idex', 'uga as part of of the "" investissements d\'avenir "" program ( anr - 15 - idex - 02 ) cesses at work in humans as suggested by [ 15 ]. while [ 11, 7, 4 ] focused on analyzing speech representations learnt by speech - image neural models from a phonological and semantic point of view, the present work focuses on lexical acquisition and the way speech utterances are segmented', 'into lexical units and processed by a computational model of visually grounded speech. we analyze a key component of the neural model - the attention mechanism - and we observe its behaviour and draw parallels between artificial neural attention and human attention. attention', 'indeed plays a key role in human perceptual learning, as stated by [ 16 ]. contributions. we', 'enrich an existing speech - image corpus in english with forced alignments and part - of - speech ( pos ) tags and analyse which parts of the spoken utterances the neural model attends to. in order to put these experiments in a cross - lingual perspective, we also experiment on a similar corpus in japanese. 1 we show', 'that the attention mechanism mostly focuses on nouns for both languages. we also show that our japanese model developed a language -', 'specific behaviour to detect relevant information by paying attention to particles, as japanese toddlers do. moreover, the bilingual corpus allows us to demonstrate that', 'images can be used as pivots to automatically align spoken utterances in two different languages ( english and japanese ) without using any transcripts. this preliminary result, in line with previous findings of  #TAUTHOR_TAG, confirms that neural speech - image models can capture a cross - lingual semantic signal, a first step in the perspective of learning speech - to - speech translation systems without text supervision. fig.', '2 : attention weights over an english ( 2a ) and japanese caption ( 2c ), both describing the same picture ( 2b ). attention peaks in the english caption are located above "" airport "" and "" jets "". attention peaks in the japanese caption are located above', '"" ni "" ( particle indicating location ) and "" ga "" ( particule indicating the subject of the sentence ). red dotted lines show token boundaries. large', 'orange markers show automatically detected peaks. japanese caption reads : "" several planes are stopped at', 'the airport', '']",0
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",0
"['of  #TAUTHOR_TAG, confirms that neural speech -']","[', in line with previous findings of  #TAUTHOR_TAG, confirms that neural speech - image models can capture a cross - lingual semantic signal, a first step in the perspective of learning speech -']","[', in line with previous findings of  #TAUTHOR_TAG, confirms that neural speech - image models']","['amount of phonological information is preserved in higher layers, and suggested that the attention layer focuses on semantic information. such computational models', 'can be used to emulate child language acquisition and could shed light on the inner cognitive pro - this work was supported by grants from neurocog idex', 'uga as part of of the "" investissements d\'avenir "" program ( anr - 15 - idex - 02 ) cesses at work in humans as suggested by [ 15 ]. while [ 11, 7, 4 ] focused on analyzing speech representations learnt by speech - image neural models from a phonological and semantic point of view, the present work focuses on lexical acquisition and the way speech utterances are segmented', 'into lexical units and processed by a computational model of visually grounded speech. we analyze a key component of the neural model - the attention mechanism - and we observe its behaviour and draw parallels between artificial neural attention and human attention. attention', 'indeed plays a key role in human perceptual learning, as stated by [ 16 ]. contributions. we', 'enrich an existing speech - image corpus in english with forced alignments and part - of - speech ( pos ) tags and analyse which parts of the spoken utterances the neural model attends to. in order to put these experiments in a cross - lingual perspective, we also experiment on a similar corpus in japanese. 1 we show', 'that the attention mechanism mostly focuses on nouns for both languages. we also show that our japanese model developed a language -', 'specific behaviour to detect relevant information by paying attention to particles, as japanese toddlers do. moreover, the bilingual corpus allows us to demonstrate that', 'images can be used as pivots to automatically align spoken utterances in two different languages ( english and japanese ) without using any transcripts. this preliminary result, in line with previous findings of  #TAUTHOR_TAG, confirms that neural speech - image models can capture a cross - lingual semantic signal, a first step in the perspective of learning speech - to - speech translation systems without text supervision. fig.', '2 : attention weights over an english ( 2a ) and japanese caption ( 2c ), both describing the same picture ( 2b ). attention peaks in the english caption are located above "" airport "" and "" jets "". attention peaks in the japanese caption are located above', '"" ni "" ( particle indicating location ) and "" ga "" ( particule indicating the subject of the sentence ). red dotted lines show token boundaries. large', 'orange markers show automatically detected peaks. japanese caption reads : "" several planes are stopped at', 'the airport', '']",3
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",3
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",3
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",3
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",4
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",4
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",6
['of our test corpus to be comparable with  #TAUTHOR_TAG'],"['on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['approach on 1k', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given']","['', 'captions of our test corpus to be comparable with  #TAUTHOR_TAG. 11 at the time of the evaluation', ', given a speech query in language src which we know 10 since both image encoders ( from english and japanese ) are trained separately', '']",5
"['to compare our results with  #TAUTHOR_TAG.', 'results are averaged over 10 random samples']","['to compare our results with  #TAUTHOR_TAG.', 'results are averaged over 10 random samples']","['to compare our results with  #TAUTHOR_TAG.', 'results are averaged over 10 random samples']","['thank g. chrupała and his team for sharing their code and dataset, as well as for helping us with technical issues.', 'ces so that there would be only one target caption for each query in order to compare our results with  #TAUTHOR_TAG.', 'results are averaged over 10 random samples']",2
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['this paper, we study direct transfer methods for multilingual named entity recognition.', 'specifically, we extend the method recently proposed by  #TAUTHOR_TAG, which is based on cross - lingual word cluster features.', 'first, we show that by using multiple source languages, combined with self - training for target language adaptation, we can achieve significant improvements compared to using only single source direct transfer.', 'second, we investigate how the direct transfer system fares against a supervised target language system and conclude that between 8, 000 and 16, 000 word tokens need to be annotated in each target language to match the best direct transfer system.', 'finally, we show that we can significantly improve target language performance, even after annotating up to 64, 000 tokens in the target language, by simply concatenating source and target language annotations']",4
"['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['', 'although semi - supervised approaches have been shown to reduce the need for manual annotation  #TAUTHOR_TAG, these methods still require a substantial amount of manual annotation for each target language.', 'manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an herculean task.', 'in this study, we turn to direct transfer methods  #TAUTHOR_TAG as a way to combat the need for annotated resources in all languages.', 'these methods allow one to train a system for a target language, using only annotations in some source language, as long as all source language features also have support in the target languages.', 'specifically, we extend the direct transfer method proposed by  #TAUTHOR_TAG in two ways.', 'first, in § 3, we use multiple source languages for training.', 'we then propose a self - training algorithm, which allows for the inclusion of additional target language specific features, in § 4.', 'by combining these extensions, we achieve significant error reductions on all tested languages.', 'finally, in § 5, we assess the viability of the different direct transfer systems compared to a supervised system trained on target language']",4
"['feature templates as  #TAUTHOR_TAG, with the exception that the transition factors are not conditioned on the input.', '3 the features used are']","['feature templates as  #TAUTHOR_TAG, with the exception that the transition factors are not conditioned on the input.', '3 the features used are']","['- speech  #AUTHOR_TAG berg  #AUTHOR_TAG and in transfer learning of dependency syntax  #AUTHOR_TAG 256 cross - lingual word clusters and the same feature templates as  #TAUTHOR_TAG, with the exception that the transition factors are not conditioned on the input.', '3 the features used are']","['from multiple languages have been shown to be of benefit both in unsupervised learning of syntax and part - of - speech  #AUTHOR_TAG berg  #AUTHOR_TAG and in transfer learning of dependency syntax  #AUTHOR_TAG 256 cross - lingual word clusters and the same feature templates as  #TAUTHOR_TAG, with the exception that the transition factors are not conditioned on the input.', '3 the features used are similar to those used by  #AUTHOR_TAG, but include cross - lingual rather than monolingual word clusters.', 'we remove the capitalization features when transferring to german, but keep them in all other cases, even when german is included in the set of source languages.', 'we use the training, development and test data sets provided by the conll 2002 / 2003 shared tasks  #AUTHOR_TAG.', ""the multi - source training sets are created by concatenating each of the source languages'training sets."", 'in order to have equivalent label sets across languages, we use the io ( inside / outside ) encoding, rather than the bio ( begin / inside / outside ) encoding, since the latter is available only for spanish and dutch.', 'the models are trained using crfsuite 0. 12  #AUTHOR_TAG, by running stochastic gradient descent for a maximum of 100 iterations.', 'table 1 shows the result of using different source languages for different target languages.', 'we see that multi - source transfer is somewhat helpful in general, but that the results are sensitive to the combination of source and target languages.', '']",4
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['this paper, we study direct transfer methods for multilingual named entity recognition.', 'specifically, we extend the method recently proposed by  #TAUTHOR_TAG, which is based on cross - lingual word cluster features.', 'first, we show that by using multiple source languages, combined with self - training for target language adaptation, we can achieve significant improvements compared to using only single source direct transfer.', 'second, we investigate how the direct transfer system fares against a supervised target language system and conclude that between 8, 000 and 16, 000 word tokens need to be annotated in each target language to match the best direct transfer system.', 'finally, we show that we can significantly improve target language performance, even after annotating up to 64, 000 tokens in the target language, by simply concatenating source and target language annotations']",6
"['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['', 'although semi - supervised approaches have been shown to reduce the need for manual annotation  #TAUTHOR_TAG, these methods still require a substantial amount of manual annotation for each target language.', 'manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an herculean task.', 'in this study, we turn to direct transfer methods  #TAUTHOR_TAG as a way to combat the need for annotated resources in all languages.', 'these methods allow one to train a system for a target language, using only annotations in some source language, as long as all source language features also have support in the target languages.', 'specifically, we extend the direct transfer method proposed by  #TAUTHOR_TAG in two ways.', 'first, in § 3, we use multiple source languages for training.', 'we then propose a self - training algorithm, which allows for the inclusion of additional target language specific features, in § 4.', 'by combining these extensions, we achieve significant error reductions on all tested languages.', 'finally, in § 5, we assess the viability of the different direct transfer systems compared to a supervised system trained on target language']",6
"['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['', 'although semi - supervised approaches have been shown to reduce the need for manual annotation  #TAUTHOR_TAG, these methods still require a substantial amount of manual annotation for each target language.', 'manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an herculean task.', 'in this study, we turn to direct transfer methods  #TAUTHOR_TAG as a way to combat the need for annotated resources in all languages.', 'these methods allow one to train a system for a target language, using only annotations in some source language, as long as all source language features also have support in the target languages.', 'specifically, we extend the direct transfer method proposed by  #TAUTHOR_TAG in two ways.', 'first, in § 3, we use multiple source languages for training.', 'we then propose a self - training algorithm, which allows for the inclusion of additional target language specific features, in § 4.', 'by combining these extensions, we achieve significant error reductions on all tested languages.', 'finally, in § 5, we assess the viability of the different direct transfer systems compared to a supervised system trained on target language']",0
['direct transfer methods  #TAUTHOR_TAG'],['direct transfer methods  #TAUTHOR_TAG'],['the heart of both direct transfer methods  #TAUTHOR_TAG'],"['than starting from scratch when creating systems that predict linguistic structure in one language, we should be able to take advantage of any corresponding annotations that are available in other languages.', 'this idea is at the heart of both direct transfer methods  #TAUTHOR_TAG and of annotation projection methods  #AUTHOR_TAG.', 'while the aim of the latter is to transfer annotations across languages, direct transfer methods instead aim to transfer systems, trained on some source language, directly to other languages.', 'in this paper, we focus on direct transfer methods, however, we briefly discuss the relationship between these approaches in § 6.', 'considering the substantial differences between languages at the grammatical and lexical level, the prospect of directly applying a system trained on one language to another language may seem bleak.', 'however, showed that a language independent dependency parser can indeed be created by training on a delexicalized treebank and by only incorporating features defined on universal part - of - speech tags  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG developed an algorithm for inducing cross - lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems.', 'the richer set of cross - lingual features was shown to substantially improve on direct transfer of both dependency parsing and ner from english to other languages.', 'cross - lingual word clusters are clusterings of words in two ( or more ) languages, such that the clusters are adequate in each language and at the same time consistent across languages.', 'for cross - lingual word clusters to be useful in direct transfer of linguistic structure, the clusters should capture crosslingual properties on both the semantic and syntactic level.', 'tackstrom et al. ( 2012 ) showed that this is, at least to some degree, achievable by coupling monolingual class - based language models, via word alignments.', 'the basic building block is the following simple monolingual class - based language model  #AUTHOR_TAG :', 'where l ( w ; c ) is the likelihood of a sequence of words, w, and c is a ( hard ) clustering function, which maps words to cluster identities.', 'these monolingual models are coupled through word alignments, which constrains the clusterings to be consistent across languages, and optimized by approximately maximizing the joint likelihood across languages.', 'just as monolingual word clusters are broadly applicable as features in monolingual models for linguistic structure prediction  #AUTHOR_TAG, the resulting cross - lingual word clusters can be used as features in various crosslingual direct transfer models.', 'we believe that the extensions that we propose are likely to be useful for other tasks as well, e. g., direct transfer dependency parsing, in this paper, we focus solely']",0
['direct transfer methods  #TAUTHOR_TAG'],['direct transfer methods  #TAUTHOR_TAG'],['the heart of both direct transfer methods  #TAUTHOR_TAG'],"['than starting from scratch when creating systems that predict linguistic structure in one language, we should be able to take advantage of any corresponding annotations that are available in other languages.', 'this idea is at the heart of both direct transfer methods  #TAUTHOR_TAG and of annotation projection methods  #AUTHOR_TAG.', 'while the aim of the latter is to transfer annotations across languages, direct transfer methods instead aim to transfer systems, trained on some source language, directly to other languages.', 'in this paper, we focus on direct transfer methods, however, we briefly discuss the relationship between these approaches in § 6.', 'considering the substantial differences between languages at the grammatical and lexical level, the prospect of directly applying a system trained on one language to another language may seem bleak.', 'however, showed that a language independent dependency parser can indeed be created by training on a delexicalized treebank and by only incorporating features defined on universal part - of - speech tags  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG developed an algorithm for inducing cross - lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems.', 'the richer set of cross - lingual features was shown to substantially improve on direct transfer of both dependency parsing and ner from english to other languages.', 'cross - lingual word clusters are clusterings of words in two ( or more ) languages, such that the clusters are adequate in each language and at the same time consistent across languages.', 'for cross - lingual word clusters to be useful in direct transfer of linguistic structure, the clusters should capture crosslingual properties on both the semantic and syntactic level.', 'tackstrom et al. ( 2012 ) showed that this is, at least to some degree, achievable by coupling monolingual class - based language models, via word alignments.', 'the basic building block is the following simple monolingual class - based language model  #AUTHOR_TAG :', 'where l ( w ; c ) is the likelihood of a sequence of words, w, and c is a ( hard ) clustering function, which maps words to cluster identities.', 'these monolingual models are coupled through word alignments, which constrains the clusterings to be consistent across languages, and optimized by approximately maximizing the joint likelihood across languages.', 'just as monolingual word clusters are broadly applicable as features in monolingual models for linguistic structure prediction  #AUTHOR_TAG, the resulting cross - lingual word clusters can be used as features in various crosslingual direct transfer models.', 'we believe that the extensions that we propose are likely to be useful for other tasks as well, e. g., direct transfer dependency parsing, in this paper, we focus solely']",0
"['of linguistic structure, in the same way that monolingual word clusters have been shown to be a robust way to bring improvements in many monolingual applications  #TAUTHOR_TAG']","['of linguistic structure, in the same way that monolingual word clusters have been shown to be a robust way to bring improvements in many monolingual applications  #TAUTHOR_TAG']","['of general use in multilingual learning of linguistic structure, in the same way that monolingual word clusters have been shown to be a robust way to bring improvements in many monolingual applications  #TAUTHOR_TAG']","['', 'we also found that combining native and crosslingual word clusters leads to improved results across the board.', 'finally, we showed that direct transfer methods can aid even in the supervised target language scenario.', 'by simply mixing annotated source language data with target language data, we can significantly reduce the annotation burden required to reach a given level of performance in the target language, even with up to 64, 000 tokens annotated in the target language.', 'we hypothesize that more elaborate domain adaptation techniques, such as that proposed by  #AUTHOR_TAG, can lead to further improvements in these scenarios.', 'our use of cross - lingual word clusters is orthogonal to several other approaches discussed in this paper.', 'we therefore suggest that such clusters could be of general use in multilingual learning of linguistic structure, in the same way that monolingual word clusters have been shown to be a robust way to bring improvements in many monolingual applications  #TAUTHOR_TAG']",0
"['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['', 'although semi - supervised approaches have been shown to reduce the need for manual annotation  #TAUTHOR_TAG, these methods still require a substantial amount of manual annotation for each target language.', 'manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an herculean task.', 'in this study, we turn to direct transfer methods  #TAUTHOR_TAG as a way to combat the need for annotated resources in all languages.', 'these methods allow one to train a system for a target language, using only annotations in some source language, as long as all source language features also have support in the target languages.', 'specifically, we extend the direct transfer method proposed by  #TAUTHOR_TAG in two ways.', 'first, in § 3, we use multiple source languages for training.', 'we then propose a self - training algorithm, which allows for the inclusion of additional target language specific features, in § 4.', 'by combining these extensions, we achieve significant error reductions on all tested languages.', 'finally, in § 5, we assess the viability of the different direct transfer systems compared to a supervised system trained on target language']",3
"['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['manual annotation  #TAUTHOR_TAG, these methods']","['', 'although semi - supervised approaches have been shown to reduce the need for manual annotation  #TAUTHOR_TAG, these methods still require a substantial amount of manual annotation for each target language.', 'manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an herculean task.', 'in this study, we turn to direct transfer methods  #TAUTHOR_TAG as a way to combat the need for annotated resources in all languages.', 'these methods allow one to train a system for a target language, using only annotations in some source language, as long as all source language features also have support in the target languages.', 'specifically, we extend the direct transfer method proposed by  #TAUTHOR_TAG in two ways.', 'first, in § 3, we use multiple source languages for training.', 'we then propose a self - training algorithm, which allows for the inclusion of additional target language specific features, in § 4.', 'by combining these extensions, we achieve significant error reductions on all tested languages.', 'finally, in § 5, we assess the viability of the different direct transfer systems compared to a supervised system trained on target language']",5
"[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","['learning techniques  #AUTHOR_TAG, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. the second factor is the formulation', 'of standard machine comprehension benchmarks based on cloze - style queries  #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental evaluation. cloze - style queries', ' #AUTHOR_TAG are created by deleting a particular word in a natural - language statement. the task is to guess which word was deleted. in a pragmatic approach, recent work', ' #TAUTHOR_TAG formed such questions by extracting a sentence from a larger document. in contrast to considering a stand -', 'alone statement, the system is now required to handle', '']",0
"[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","['learning techniques  #AUTHOR_TAG, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. the second factor is the formulation', 'of standard machine comprehension benchmarks based on cloze - style queries  #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental evaluation. cloze - style queries', ' #AUTHOR_TAG are created by deleting a particular word in a natural - language statement. the task is to guess which word was deleted. in a pragmatic approach, recent work', ' #TAUTHOR_TAG formed such questions by extracting a sentence from a larger document. in contrast to considering a stand -', 'alone statement, the system is now required to handle', '']",0
"['without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', 'the cbt 1 corpus was']","['without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', 'the cbt 1 corpus was']","['of the advantages of using cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', 'the cbt 1 corpus was generated from']","['of the advantages of using cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', ""the cbt 1 corpus was generated from well - known children's books available through project gutenberg."", '']",0
"[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","['learning techniques  #AUTHOR_TAG, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. the second factor is the formulation', 'of standard machine comprehension benchmarks based on cloze - style queries  #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental evaluation. cloze - style queries', ' #AUTHOR_TAG are created by deleting a particular word in a natural - language statement. the task is to guess which word was deleted. in a pragmatic approach, recent work', ' #TAUTHOR_TAG formed such questions by extracting a sentence from a larger document. in contrast to considering a stand -', 'alone statement, the system is now required to handle', '']",4
"['used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each']","['used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each']","['used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.', 'this simple bilinear attention has been successfully used in  #AUTHOR_TAG.', 'second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1.', 'this is similar to']","['phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer.', 'the inference is modelled by an additional recurrent gru network.', 'the recurrent network iteratively performs an alternating search step to gather information that may be useful to predict the answer.', 'in particular, at each time step : ( 1 ) it performs an attentive read on the query encodings, resulting in a query glimpse, q t, and ( 2 ) given the current query glimpse, it extracts a conditional document glimpse, d t, representing the parts of the document that are relevant to the current query glimpse.', 'in turn, both attentive reads are conditioned on the previous hidden state of the inference gru s t−1, summarizing the information that has been gathered from the query and the document up to time t. the inference gru uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process.', 'query attentive read given the query encodings { q i }, we formulate a query glimpse q t at timestep t by :', 'where q i, t are the query attention weights and a q ∈ r 2h×s, where s is the dimensionality of the inference gru state, and a q ∈ r 2h.', 'the attention we use here is similar to the formulation used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.', 'this simple bilinear attention has been successfully used in  #AUTHOR_TAG.', 'second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1.', 'this is similar to what is achieved by the original attention mechanism proposed in  #AUTHOR_TAG without the burden of the additional tanh layer']",4
"[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","[' #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental']","['learning techniques  #AUTHOR_TAG, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. the second factor is the formulation', 'of standard machine comprehension benchmarks based on cloze - style queries  #TAUTHOR_TAG, which permit fast integration loops between model conception and experimental evaluation. cloze - style queries', ' #AUTHOR_TAG are created by deleting a particular word in a natural - language statement. the task is to guess which word was deleted. in a pragmatic approach, recent work', ' #TAUTHOR_TAG formed such questions by extracting a sentence from a larger document. in contrast to considering a stand -', 'alone statement, the system is now required to handle', '']",7
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['train our model, we used stochastic gradient descent with the adam optimizer  #AUTHOR_TAG, with an initial learning rate of 0. 001.', 'we set the batch size to 32 and we decay the learning rate by 0. 8 if the accuracy on the validation set does not increase after a half - epoch, i. e. 2000 batches ( for cbt ) and 5000 batches for ( cnn ).', 'we initialize all weights of our model by sampling from the normal distribution n ( 0, 0. 05 ).', 'following  #AUTHOR_TAG, the gru recurrent weights are initialized to be orthogonal and biases are initialized to zero.', 'in order to stabilize the learning, we clip the gradients if their norm is greater than 5  #AUTHOR_TAG.', 'we performed a hyperparameter search with embedding regularization in { 0. 001, 0. 0001 }, inference steps t ∈ { 3, 5, 8 }, embedding size d ∈ { 256, 384 }, encoder size h ∈ { 128, 256 } and the inference gru size s ∈ { 256, 512 }. we regularize our model by applying a dropout  #AUTHOR_TAG table 2 : results on the cbt - ne ( named entity ) and cbt - cn ( common noun ) datasets.', 'results marked with 1 are from  #TAUTHOR_TAG and those marked with 2 are from  #AUTHOR_TAG.', 'the inputs to both the query and the document attention mechanisms.', 'we found that setting embedding regularization to 0. 0001, t = 8, d = 384, h = 128, s = 512 worked robustly across the datasets.', 'our model is implemented in theano  #AUTHOR_TAG, using the keras  #AUTHOR_TAG library.', 'computational complexity similar to previous state - of - the - art models  #AUTHOR_TAG which use a bidirectional encoder, the major bottleneck of our method is computing the document and query encodings.', 'the alternating attention mechanism runs only for a fixed number of steps ( t = 8 in our tests ), which is orders of magnitude smaller than a typical document or query in our datasets ( see table 1 ).', 'the repeated attentions each require a softmax over ∼1000 locations which is typically fast on recent gpu architectures.', 'thus, our computation cost is comparable to  #AUTHOR_TAG, but we outperform the latter models on the datasets tested']",7
"['without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', 'the cbt 1 corpus was']","['without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', 'the cbt 1 corpus was']","['of the advantages of using cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', 'the cbt 1 corpus was generated from']","['of the advantages of using cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.', 'the cbt  #TAUTHOR_TAG and cnn  #AUTHOR_TAG corpora are two such datasets.', ""the cbt 1 corpus was generated from well - known children's books available through project gutenberg."", '']",1
"['used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each']","['used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each']","['used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.', 'this simple bilinear attention has been successfully used in  #AUTHOR_TAG.', 'second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1.', 'this is similar to']","['phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer.', 'the inference is modelled by an additional recurrent gru network.', 'the recurrent network iteratively performs an alternating search step to gather information that may be useful to predict the answer.', 'in particular, at each time step : ( 1 ) it performs an attentive read on the query encodings, resulting in a query glimpse, q t, and ( 2 ) given the current query glimpse, it extracts a conditional document glimpse, d t, representing the parts of the document that are relevant to the current query glimpse.', 'in turn, both attentive reads are conditioned on the previous hidden state of the inference gru s t−1, summarizing the information that has been gathered from the query and the document up to time t. the inference gru uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process.', 'query attentive read given the query encodings { q i }, we formulate a query glimpse q t at timestep t by :', 'where q i, t are the query attention weights and a q ∈ r 2h×s, where s is the dimensionality of the inference gru state, and a q ∈ r 2h.', 'the attention we use here is similar to the formulation used in  #TAUTHOR_TAG, but with two differences.', 'first, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.', 'this simple bilinear attention has been successfully used in  #AUTHOR_TAG.', 'second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1.', 'this is similar to what is achieved by the original attention mechanism proposed in  #AUTHOR_TAG without the burden of the additional tanh layer']",3
"['to  #TAUTHOR_TAG, which']","['to  #TAUTHOR_TAG, which']","['to  #TAUTHOR_TAG, which were also']",[' #TAUTHOR_TAG'],3
"['to  #TAUTHOR_TAG, which']","['to  #TAUTHOR_TAG, which']","['to  #TAUTHOR_TAG, which were also']",[' #TAUTHOR_TAG'],3
['from  #TAUTHOR_TAG and the attention - sum reader ( as reader ) is a'],['from  #TAUTHOR_TAG and the attention - sum reader ( as reader ) is a state - of - the - art result recently'],"['report the results of our model on the cbt - cn, cbt - ne and cnn datasets, previously described in section 2.', 'table 2 reports our results on the cbt - cn and cbt - ne dataset.', 'the humans, lstms and memory networks ( memnns ) results are taken from  #TAUTHOR_TAG and the attention - sum reader ( as reader ) is a state - of - the - art result recently']","['report the results of our model on the cbt - cn, cbt - ne and cnn datasets, previously described in section 2.', 'table 2 reports our results on the cbt - cn and cbt - ne dataset.', 'the humans, lstms and memory networks ( memnns ) results are taken from  #TAUTHOR_TAG and the attention - sum reader ( as reader ) is a state - of - the - art result recently obtained by  #AUTHOR_TAG']",5
['taken from theory  #TAUTHOR_TAG and one with 17'],['taken from theory  #TAUTHOR_TAG and one with 17'],"['practical approaches.', 'section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well - defined quality dimensions taken from theory  #TAUTHOR_TAG and one with 17 reasons for quality differences phrased spontaneously in practice  #AUTHOR_TAG a ).', 'in a crowdsourcing study, we test']","['', 'section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well - defined quality dimensions taken from theory  #TAUTHOR_TAG and one with 17 reasons for quality differences phrased spontaneously in practice  #AUTHOR_TAG a ).', 'in a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions ( section 4 ).', 'we find that assessments of overall argumentation quality largely match in theory and practice.', 'nearly all phrased reasons are adequately represented in theory.', 'however, some theoretical quality dimensions seem hard to separate in practice.', 'most importantly, we provide evidence that the observed relative quality differences are reflected in absolute quality ratings.', 'still, our study underpins the fact that the theory - based argumentation quality assessment remains complex.', 'our results do not']",5
['- 3 9 -  #TAUTHOR_TAG'],['9 - 3 9 -  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"[' #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', '']","['hypotheses 1 and 2, we consider all 736 pairs of arguments from  #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', 'for each pair ( a, b ) with a being 1 source code and annotated data : http : / / www. arguana. com more convincing than b, we check whether the ratings of a and b for each dimension ( averaged over all annotators ) show a concordant difference ( i. e., a higher rating for a ), a disconcordant difference ( lower ), or a tie.', 'this way, we can correlate each dimension with all reason labels in table 2 including conv.', '']",5
"[' #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', '']","['hypotheses 1 and 2, we consider all 736 pairs of arguments from  #AUTHOR_TAG a ) where both have been annotated by  #TAUTHOR_TAG.', 'for each pair ( a, b ) with a being 1 source code and annotated data : http : / / www. arguana. com more convincing than b, we check whether the ratings of a and b for each dimension ( averaged over all annotators ) show a concordant difference ( i. e., a higher rating for a ), a disconcordant difference ( lower ), or a tie.', 'this way, we can correlate each dimension with all reason labels in table 2 including conv.', '']",5
['of all arguments from  #TAUTHOR_TAG with a'],['of all arguments from  #TAUTHOR_TAG with a'],"['of all arguments from  #TAUTHOR_TAG with a particular reason label from  #AUTHOR_TAG a ).', 'as each reason refers']","['correlations found imply that the relative quality differences captured are reflected in absolute differences.', 'for explicitness, we computed the mean rating for each quality dimension of all arguments from  #TAUTHOR_TAG with a particular reason label from  #AUTHOR_TAG a ).', 'as each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.', '']",5
['by  #TAUTHOR_TAG on crowd'],['by  #TAUTHOR_TAG on crowdflower in order to'],['by  #TAUTHOR_TAG on crowd'],"['emulated the expert annotation process carried out by  #TAUTHOR_TAG on crowdflower in order to evaluate whether lay annotators suffice for a theory - based quality assessment.', 'in particular, we asked the crowd to rate the same 304 arguments as the experts for all 15 given quality dimensions with scores from 1 to 3 ( or choose "" cannot judge "" ).', 'each argument was rated 10 times at an offered price of $ 0. 10 for each rating ( 102 annotators in total ).', 'given the crowd ratings, we then performed two comparisons as detailed in the following']",5
"['10 crowd ratings to the mean of the three ratings of  #TAUTHOR_TAG.', 'on the other hand, we estimated a reliable']","['10 crowd ratings to the mean of the three ratings of  #TAUTHOR_TAG.', 'on the other hand, we estimated a reliable']","['one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of  #TAUTHOR_TAG.', 'on the other hand, we estimated a reliable rating from the crowd ratings using mace  #AUTHOR_TAG and compared']","["", we checked to what extent lay annotators and experts agree in terms of krippendorff's α."", 'on one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of  #TAUTHOR_TAG.', 'on the other hand, we estimated a reliable rating from the crowd ratings using mace  #AUTHOR_TAG and compared it to the experts.', ""table 5 : mean and mace krippendorff's α agreement between ( a ) the crowd and the experts, ( b ) two independent crowd groups and the experts, ( c ) group 1 and the experts, and ( d ) group 2 and the experts."", 'table 5 ( a ) presents the results.', 'for the mean ratings, most α - values are above. 40.', 'this is similar to the study of  #AUTHOR_TAG b ), where a range of. 27 to. 51 is reported, meaning that lay annotators achieve similar agreement to experts.', 'considering the minimum of mean and mace, we observe the highest agreement for overall quality (. 43 ) - analog to  #AUTHOR_TAG b ).', 'also, global sufficiency has the lowest agreement in both cases.', 'in contrast, the experts hardly said "" cannot judge "" at all, whereas the crowd chose it for about 4 % of all ratings ( most often for global sufficiency ), possibly due to a lack of training.', 'still, we conclude that the crowd generally handles the theory - based quality assessment almost as well as the experts.', 'however, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.', 'regarding simplification, the most common practical reasons of  #AUTHOR_TAG a ) imply what to focus on']",5
"['achieving agreement ( van  #AUTHOR_TAG.', ' #TAUTHOR_TAG point out that dialectic']","['achieving agreement ( van  #AUTHOR_TAG.', ' #TAUTHOR_TAG point out that dialectical']","['achieving agreement ( van  #AUTHOR_TAG.', ' #TAUTHOR_TAG point out that dialectic']","['theory discusses logical, rhetorical, and dialectical quality.', 'as few real - life arguments are logically sound, requiring true premises that deductively entail a conclusion, cogency ( as defined in section 1 ) is largely seen as the main logical quality  #AUTHOR_TAG.', ' #AUTHOR_TAG models the general structure of logical arguments, and  #AUTHOR_TAG analyze schemes of fallacies and strong arguments.', 'a fallacy is a kind of error that undermines reasoning  #AUTHOR_TAG.', 'strength may mean cogency but also rhetorical effectiveness ( perelman and olbrechts -  #AUTHOR_TAG.', 'rhetoric has been studied since  #AUTHOR_TAG who developed the notion of the means of persuasion ( logos, ethos, pathos ) and their linguistic delivery in terms of arrangement and style.', 'dialectical quality dimensions resemble those of cogency, but arguments are judged specifically by their reasonableness for achieving agreement ( van  #AUTHOR_TAG.', ' #TAUTHOR_TAG point out that dialectical builds on rhetorical, and rhetorical builds on logical quality.', 'they derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions.', '']",0
['- 3 9 -  #TAUTHOR_TAG'],['9 - 3 9 -  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['( with word2vec  #TAUTHOR_TAG,']","['( with word2vec  #TAUTHOR_TAG,']","['semantic word - sense classification accuracy ( with word2vec  #TAUTHOR_TAG,']","['the internal state of a robotic system is complex : this is performed from multiple heterogeneous sensor inputs and knowledge sources.', 'discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence.', 'as these sequences are used to reason over complex scenarios [ 1 ], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [ 1 ], [ 2 ].', 'such problems need to be addressed to ensure timely and meaningful human - robot interaction.', 'our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset.', 'this is in view of reducing the training size while retaining the majority of the symbolic learning potential.', 'we prove the concept on human - written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [ 3 ].', 'we computed multiple random subsets of sentences from the umbc webbase corpus ( ∼ 17. 13gb ) via a custom implementation using the spark distributed framework.', 'we evaluated the learning informativess of such sets in terms of semantic word - sense classification accuracy ( with word2vec  #TAUTHOR_TAG, and of n - gram perplexity.', 'previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks ( e. g. semantic measures ) [ 5 ].', 'in our semantic tests, on average 85 % of the quality can be obtained by training on a random ∼ 4 % subset of the original corpus ( e. g. as in fig. 1, 5 random million lines yield 64. 14 % instead of 75. 14 % ).', 'our claims are that i ) such evaluation posteriors are normally distributed ( tab. i ), and that ii ) the variance is inversely proportional to the subset size ( tab. ii ).', 'it is therefore possible to select the best random subset for a given size, if an information criterion is known.', 'such metric is currently under investigation.', 'within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human - written instructions is particularly important for non - recursive online training algorithms, such as current symbol - based probabilistic reasoning systems [ 1 ], [ 3 ], [ 6 ]']",5
"['( with word2vec  #TAUTHOR_TAG, and of n - gram perplexity.', 'previous literature inform us']","['( with word2vec  #TAUTHOR_TAG, and of n - gram perplexity.', 'previous literature inform us']","['semantic word - sense classification accuracy ( with word2vec  #TAUTHOR_TAG, and of n - gram perplexity.', 'previous literature inform us']","['computed multiple random subsets of sentences from the umbc webbase corpus ( ∼ 17. 13gb ) via a custom implementation using the spark distributed framework.', 'we evaluated the learning informativess of such sets in terms of semantic word - sense classification accuracy ( with word2vec  #TAUTHOR_TAG, and of n - gram perplexity.', 'previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks ( e. g. semantic measures ) [ 5 ].', 'in our semantic tests, on average 85 % of the quality can be obtained by training on a random ∼ 4 % subset of the original corpus ( e. g. as in fig. 1, 5 random million lines yield 64. 14 % instead of 75. 14 % ).', 'our claims are that i ) such evaluation posteriors are normally distributed ( tab. i ), and that ii ) the variance is inversely proportional to the subset size ( tab. ii ).', 'it is therefore possible to select the best random subset for a given size, if an information criterion is known.', 'such metric is currently under investigation.', 'within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human - written instructions is particularly important for non - recursive online training algorithms, such as current symbol - based probabilistic reasoning systems [ 1 ], [ 3 ], [ 6 ]']",5
"['.,, 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models']","['al.,, 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models']","[', 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models']","['advent of neural networks in natural language processing ( nlp ) has significantly improved state - of - the - art results within the field.', 'while recurrent neural networks ( rnns ) and long short - term memory networks ( lstms ) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so - called transformer models  #AUTHOR_TAG.', 'this latter type of model caused a new revolution in nlp and led to popular language models like gpt - 2  #AUTHOR_TAG ( radford et al.,, 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left - to - right or the other way around.', 'this model was later reimplemented, critically evaluated and improved in the roberta model.', 'these large - scale transformer models provide the advantage of being able to solve nlp tasks by having a common, expensive pre - training phase, followed by a smaller fine - tuning phase.', 'the pretraining happens in an unsupervised way by providing large corpora of text in the desired language.', 'the second phase only needs a relatively small annotated data set for fine - tuning to outperform previous popular approaches in one of a large number of possible language tasks.', 'while language models are usually trained on english data, some multilingual models also exist.', 'these are usually trained on a large quantity of text in different languages.', 'for example, multilingual - bert is trained on a collection of corpora in 104 different languages  #TAUTHOR_TAG, and generalizes language components well across languages  #AUTHOR_TAG.', 'however, models trained on data from one specific language usually improve the performance of multilingual models for this particular language  #AUTHOR_TAG de  #AUTHOR_TAG.', 'training a roberta model on a dutch dataset thus has a lot of potential for increasing performance for many downstream dutch nlp tasks.', 'in this paper, we introduce robbert 1, a dutch robertabased pre - trained language model, and critically test its performance using natural language tasks against other dutch languages models']",0
"['.,, 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models']","['al.,, 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models']","[', 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models']","['advent of neural networks in natural language processing ( nlp ) has significantly improved state - of - the - art results within the field.', 'while recurrent neural networks ( rnns ) and long short - term memory networks ( lstms ) initially dominated the field, recent models started incorporating attention mechanisms and then later dropped the recurrent part and just kept the attention mechanisms in so - called transformer models  #AUTHOR_TAG.', 'this latter type of model caused a new revolution in nlp and led to popular language models like gpt - 2  #AUTHOR_TAG ( radford et al.,, 2019 and elmo  #AUTHOR_TAG.', 'bert  #TAUTHOR_TAG improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left - to - right or the other way around.', 'this model was later reimplemented, critically evaluated and improved in the roberta model.', 'these large - scale transformer models provide the advantage of being able to solve nlp tasks by having a common, expensive pre - training phase, followed by a smaller fine - tuning phase.', 'the pretraining happens in an unsupervised way by providing large corpora of text in the desired language.', 'the second phase only needs a relatively small annotated data set for fine - tuning to outperform previous popular approaches in one of a large number of possible language tasks.', 'while language models are usually trained on english data, some multilingual models also exist.', 'these are usually trained on a large quantity of text in different languages.', 'for example, multilingual - bert is trained on a collection of corpora in 104 different languages  #TAUTHOR_TAG, and generalizes language components well across languages  #AUTHOR_TAG.', 'however, models trained on data from one specific language usually improve the performance of multilingual models for this particular language  #AUTHOR_TAG de  #AUTHOR_TAG.', 'training a roberta model on a dutch dataset thus has a lot of potential for increasing performance for many downstream dutch nlp tasks.', 'in this paper, we introduce robbert 1, a dutch robertabased pre - trained language model, and critically test its performance using natural language tasks against other dutch languages models']",0
['. that  #TAUTHOR_TAG trained a'],['. that  #TAUTHOR_TAG trained a better model'],['. that  #TAUTHOR_TAG trained a better model'],"['attention allows them to better resolve coreferences between words. a typical', 'example for the importance of coreference resolution is "" the trophy doesnt fit in the brown suitcase because its too big. "", where the word "" it ""', 'would refer to the the suitcase instead of the trophy if the last word was changed to "" small ""  #AUTHOR_TAG. being able to resolve these coreferences is for example important for translating to languages', 'with gender, as suitcase and trophy have different genders in french. although bert has been shown to be a useful language model, it has also received some scrutiny on the training', 'and pre - processing of the language model. as mentioned before, bert uses', 'next sentence prediction ( nsp ) as one of its two training tasks. in nsp, the model has to predict whether two sentences follow each other in the training text, or are just randomly selected from the corpora', '. the authors of roberta ) showed', 'that while this task made the model achieve a better performance, it was not due to its intended reason, as it might', 'merely predict relatedness rather than subsequent sentences. that  #TAUTHOR_TAG trained a better model when using nsp than without nsp is likely due', 'to the model learning long - range dependencies in text from its inputs, which are longer than just the single sentence on itself. as such, the roberta model uses only the mlm task,', 'and uses multiple full sentences in every input. other research improved', 'the nsp task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order  #AUTHOR_TAG.  #AUTHOR_TAG', 'also presented a multilingual model ( mbert ) with the same architecture as bert, but trained on wikipedia corpora in 104 languages. unfortunately, the quality of these', 'multilingual embeddings is often considered worse than their monolingual counterparts. ronnqvist et', 'al. ( 2019 ) illustrated this difference in quality for german and english models in a generative setting. the monolingual french camembert model  #AUTHOR_TAG also compared their model to mbert, which performed poorer on all tasks. more recently, de  #AUTHOR_TAG also showed similar results for dutch using their bertje', 'model, outperforming multilingual bert in a wide range of tasks, such as sentiment analysis and part - of - speech tagging. since this work is concurrent', 'with ours, we compare our results with bertje in this paper. this section describes the data and training regime we used to train our dutch roberta -', 'based language model called robbert']",0
['. that  #TAUTHOR_TAG trained a'],['. that  #TAUTHOR_TAG trained a better model'],['. that  #TAUTHOR_TAG trained a better model'],"['attention allows them to better resolve coreferences between words. a typical', 'example for the importance of coreference resolution is "" the trophy doesnt fit in the brown suitcase because its too big. "", where the word "" it ""', 'would refer to the the suitcase instead of the trophy if the last word was changed to "" small ""  #AUTHOR_TAG. being able to resolve these coreferences is for example important for translating to languages', 'with gender, as suitcase and trophy have different genders in french. although bert has been shown to be a useful language model, it has also received some scrutiny on the training', 'and pre - processing of the language model. as mentioned before, bert uses', 'next sentence prediction ( nsp ) as one of its two training tasks. in nsp, the model has to predict whether two sentences follow each other in the training text, or are just randomly selected from the corpora', '. the authors of roberta ) showed', 'that while this task made the model achieve a better performance, it was not due to its intended reason, as it might', 'merely predict relatedness rather than subsequent sentences. that  #TAUTHOR_TAG trained a better model when using nsp than without nsp is likely due', 'to the model learning long - range dependencies in text from its inputs, which are longer than just the single sentence on itself. as such, the roberta model uses only the mlm task,', 'and uses multiple full sentences in every input. other research improved', 'the nsp task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order  #AUTHOR_TAG.  #AUTHOR_TAG', 'also presented a multilingual model ( mbert ) with the same architecture as bert, but trained on wikipedia corpora in 104 languages. unfortunately, the quality of these', 'multilingual embeddings is often considered worse than their monolingual counterparts. ronnqvist et', 'al. ( 2019 ) illustrated this difference in quality for german and english models in a generative setting. the monolingual french camembert model  #AUTHOR_TAG also compared their model to mbert, which performed poorer on all tasks. more recently, de  #AUTHOR_TAG also showed similar results for dutch using their bertje', 'model, outperforming multilingual bert in a wide range of tasks, such as sentiment analysis and part - of - speech tagging. since this work is concurrent', 'with ours, we compare our results with bertje in this paper. this section describes the data and training regime we used to train our dutch roberta -', 'based language model called robbert']",0
['. that  #TAUTHOR_TAG trained a'],['. that  #TAUTHOR_TAG trained a better model'],['. that  #TAUTHOR_TAG trained a better model'],"['attention allows them to better resolve coreferences between words. a typical', 'example for the importance of coreference resolution is "" the trophy doesnt fit in the brown suitcase because its too big. "", where the word "" it ""', 'would refer to the the suitcase instead of the trophy if the last word was changed to "" small ""  #AUTHOR_TAG. being able to resolve these coreferences is for example important for translating to languages', 'with gender, as suitcase and trophy have different genders in french. although bert has been shown to be a useful language model, it has also received some scrutiny on the training', 'and pre - processing of the language model. as mentioned before, bert uses', 'next sentence prediction ( nsp ) as one of its two training tasks. in nsp, the model has to predict whether two sentences follow each other in the training text, or are just randomly selected from the corpora', '. the authors of roberta ) showed', 'that while this task made the model achieve a better performance, it was not due to its intended reason, as it might', 'merely predict relatedness rather than subsequent sentences. that  #TAUTHOR_TAG trained a better model when using nsp than without nsp is likely due', 'to the model learning long - range dependencies in text from its inputs, which are longer than just the single sentence on itself. as such, the roberta model uses only the mlm task,', 'and uses multiple full sentences in every input. other research improved', 'the nsp task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order  #AUTHOR_TAG.  #AUTHOR_TAG', 'also presented a multilingual model ( mbert ) with the same architecture as bert, but trained on wikipedia corpora in 104 languages. unfortunately, the quality of these', 'multilingual embeddings is often considered worse than their monolingual counterparts. ronnqvist et', 'al. ( 2019 ) illustrated this difference in quality for german and english models in a generative setting. the monolingual french camembert model  #AUTHOR_TAG also compared their model to mbert, which performed poorer on all tasks. more recently, de  #AUTHOR_TAG also showed similar results for dutch using their bertje', 'model, outperforming multilingual bert in a wide range of tasks, such as sentiment analysis and part - of - speech tagging. since this work is concurrent', 'with ours, we compare our results with bertje in this paper. this section describes the data and training regime we used to train our dutch roberta -', 'based language model called robbert']",0
['##r ( majority class ) 66. 70 mbert  #TAUTHOR_TAG 90. 21 bertje ( de  #AUTHOR_TAG 94'],"['illustrating the benefits of pre - training on low - resource tasks.', 'zeror ( majority class ) 66. 70 mbert  #TAUTHOR_TAG 90. 21 bertje ( de  #AUTHOR_TAG 94']",['##r ( majority class ) 66. 70 mbert  #TAUTHOR_TAG 90. 21 bertje ( de  #AUTHOR_TAG 94'],"['', 'for the test set for example, this resulted in about 289k masked sentences.', 'we then test two different approaches for solving this task on this dataset.', 'the first approach is making the bert models use their mlm task and guess which word should be filled in this spot, and check if it has more confidence in either "" die "" or "" dat "" ( by checking the first 2, 048 guesses at most, as this seemed sufficiently large ).', 'this allows us to compare the zero - shot bert models, i. e. without any fine - tuning after pre - training, for which the results can be seen in table 2.', 'the second approach uses the same data, but creates two sentences by filling in the mask with both "" die "" and "" dat "", ap - pending both with the [SEP] token and making the model predict which of the two sentences is correct.', 'the fine - tuning was performed using 4 nvidia gtx 1080 ti gpus and evaluated against the same test set of 399k utterances.', 'as before, we fine - tuned the model twice : once with the full training set and once with a subset of 10k utterances from the training set for illustrating the benefits of pre - training on low - resource tasks.', 'zeror ( majority class ) 66. 70 mbert  #TAUTHOR_TAG 90. 21 bertje ( de  #AUTHOR_TAG 94']",0
"['- attention layers with 12 heads  #TAUTHOR_TAG.', 'one difference with']","['original bert model with 12 self - attention layers with 12 heads  #TAUTHOR_TAG.', 'one difference with']","['over bert.', 'the architecture of our language model is thus equal to the original bert model with 12 self - attention layers with 12 heads  #TAUTHOR_TAG.', 'one difference with the original bert is due']","[""##bert shares its architecture with roberta's base model, which itself is a replication and improvement over bert."", 'the architecture of our language model is thus equal to the original bert model with 12 self - attention layers with 12 heads  #TAUTHOR_TAG.', 'one difference with the original bert is due to the different pre - training task specified by roberta, using only the mlm task and not the nsp task.', 'the training thus only uses word masking, where the model has to predict which words were masked in certain positions of a given line of text.', ""the training process uses the adam optimizer  #AUTHOR_TAG with polynomial decay of the learning rate l r = 10 −6 and a ramp - up period of 1000 iterations, with parameters β 1 = 0. 9 ( a common default ) and roberta's default β 2 = 0. 98."", 'additionally, we also used a weight decay of 0. 1 as well as a small dropout of 0. 1 to help prevent the model from overfitting  #AUTHOR_TAG.', 'we used a computing cluster in order to efficiently pre - train our model.', 'more specifically, the pre - training was executed on a computing cluster with 20 nodes with 4 nvidia tesla p100 gpus ( 16 gb vram each ) and 2 nodes with 8 nvidia v100 gpus ( having 32 gb vram each ).', 'this pretraining happened in fixed batches of 8192 sentences by rescaling each gpus batch size depending on the number of gpus available, in order to maximally utilize the cluster without blocking it entirely for other users.', 'the model trained for two epochs, which is over 16k batches in total.', 'with the large batch size of 8192, this equates to 0. 5m updates for a traditional bert model.', 'at this point, the perplexity did not decrease any further']",3
"['strong baselines  #TAUTHOR_TAG, often beat']","['strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this']","['basic machine learning techniques can yield strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.', 'however, there are']","['analysis is among the most popular, simple and useful tasks in natural language processing.', 'it aims at predicting the attitude of text, typically a sentence or a review.', 'for instance, movies or restaurant are often rated with a certain number of stars, which indicate the degree to which the reviewer was satisfied.', 'this task is often considered as one of the simplest in nlp because basic machine learning techniques can yield strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.', 'however, there are several challenges towards achieving the best possible accuracy.', 'it is not obvious how to represent variable length documents beyond simple bag of words approaches that lose word order information.', 'one can use advanced machine learning techniques such as recurrent neural networks and their variations  #AUTHOR_TAG, however it is not clear if these provide any significant gain over simple bag - of - words and bag - of - ngram techniques  #TAUTHOR_TAG.', 'in this work, we compared several different approaches and realized, without much surprise, that model combination performs better than any individual technique.', 'the ensemble best benefits from models that are complementary, thus having diverse set of techniques is desirable.', 'the vast majority of models proposed in the literature are discriminative in nature, as their parameters are tuned for the classification task directly.', 'in this work, we boost the performance of the ensemble by considering a generative language model.', 'to this end, we train two language models, one on the positive reviews and one on the negative ones, and use the likelihood ratio of these two models evaluated on the test data as an additional feature.', 'for example, we assume that a positive review will have higher likelihood to be generated by a model that was trained on a large set of positive reviews, and lower likelihood given the negative model.', 'in this paper, we constrained our work to binary classification where we trained two generative models, positive and negative.', 'one could consider a higher number of classes since this approach scales linearily with the number of models to be train, i. e. one for each class.', 'the large pool of diverse models is a ) simple to implement ( in line with previous work by wang and manning  #TAUTHOR_TAG and b ) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the stanford imdb dataset of reviews.', 'code to reproduce our experiments is available at https : / / github. com / mesnilgr / iclr15']",0
"['strong baselines  #TAUTHOR_TAG, often beat']","['strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this']","['basic machine learning techniques can yield strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.', 'however, there are']","['analysis is among the most popular, simple and useful tasks in natural language processing.', 'it aims at predicting the attitude of text, typically a sentence or a review.', 'for instance, movies or restaurant are often rated with a certain number of stars, which indicate the degree to which the reviewer was satisfied.', 'this task is often considered as one of the simplest in nlp because basic machine learning techniques can yield strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.', 'however, there are several challenges towards achieving the best possible accuracy.', 'it is not obvious how to represent variable length documents beyond simple bag of words approaches that lose word order information.', 'one can use advanced machine learning techniques such as recurrent neural networks and their variations  #AUTHOR_TAG, however it is not clear if these provide any significant gain over simple bag - of - words and bag - of - ngram techniques  #TAUTHOR_TAG.', 'in this work, we compared several different approaches and realized, without much surprise, that model combination performs better than any individual technique.', 'the ensemble best benefits from models that are complementary, thus having diverse set of techniques is desirable.', 'the vast majority of models proposed in the literature are discriminative in nature, as their parameters are tuned for the classification task directly.', 'in this work, we boost the performance of the ensemble by considering a generative language model.', 'to this end, we train two language models, one on the positive reviews and one on the negative ones, and use the likelihood ratio of these two models evaluated on the test data as an additional feature.', 'for example, we assume that a positive review will have higher likelihood to be generated by a model that was trained on a large set of positive reviews, and lower likelihood given the negative model.', 'in this paper, we constrained our work to binary classification where we trained two generative models, positive and negative.', 'one could consider a higher number of classes since this approach scales linearily with the number of models to be train, i. e. one for each class.', 'the large pool of diverse models is a ) simple to implement ( in line with previous work by wang and manning  #TAUTHOR_TAG and b ) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the stanford imdb dataset of reviews.', 'code to reproduce our experiments is available at https : / / github. com / mesnilgr / iclr15']",0
"['strong baselines  #TAUTHOR_TAG, often beat']","['strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this']","['basic machine learning techniques can yield strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.', 'however, there are']","['analysis is among the most popular, simple and useful tasks in natural language processing.', 'it aims at predicting the attitude of text, typically a sentence or a review.', 'for instance, movies or restaurant are often rated with a certain number of stars, which indicate the degree to which the reviewer was satisfied.', 'this task is often considered as one of the simplest in nlp because basic machine learning techniques can yield strong baselines  #TAUTHOR_TAG, often beating much more intricate approaches  #AUTHOR_TAG.', 'in the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.', 'however, there are several challenges towards achieving the best possible accuracy.', 'it is not obvious how to represent variable length documents beyond simple bag of words approaches that lose word order information.', 'one can use advanced machine learning techniques such as recurrent neural networks and their variations  #AUTHOR_TAG, however it is not clear if these provide any significant gain over simple bag - of - words and bag - of - ngram techniques  #TAUTHOR_TAG.', 'in this work, we compared several different approaches and realized, without much surprise, that model combination performs better than any individual technique.', 'the ensemble best benefits from models that are complementary, thus having diverse set of techniques is desirable.', 'the vast majority of models proposed in the literature are discriminative in nature, as their parameters are tuned for the classification task directly.', 'in this work, we boost the performance of the ensemble by considering a generative language model.', 'to this end, we train two language models, one on the positive reviews and one on the negative ones, and use the likelihood ratio of these two models evaluated on the test data as an additional feature.', 'for example, we assume that a positive review will have higher likelihood to be generated by a model that was trained on a large set of positive reviews, and lower likelihood given the negative model.', 'in this paper, we constrained our work to binary classification where we trained two generative models, positive and negative.', 'one could consider a higher number of classes since this approach scales linearily with the number of models to be train, i. e. one for each class.', 'the large pool of diverse models is a ) simple to implement ( in line with previous work by wang and manning  #TAUTHOR_TAG and b ) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the stanford imdb dataset of reviews.', 'code to reproduce our experiments is available at https : / / github. com / mesnilgr / iclr15']",3
"['- svm ) approach  #TAUTHOR_TAG.', 'this']","['machine ( nb - svm ) approach  #TAUTHOR_TAG.', 'this']","['- svm ) approach  #TAUTHOR_TAG.', 'this approach computes a log - ratio vector between the average word counts extracted']","['purely discriminative methods, the most popular choice is a linear classifier on top of a bagof - word representation of the document.', 'the input representation is usually a tf - idf weighted word counts of the document.', 'in order to preserve local ordering of the words, a better representation would consider also the position - independent n - gram counts of the document ( bag - of - n - grams ).', 'in our ensemble, we used a supervised reweighing of the counts as in the naive bayes support vector machine ( nb - svm ) approach  #TAUTHOR_TAG.', 'this approach computes a log - ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents.', 'the input to the logistic regression classifier corresponds to the log - ratio vector multiplied by the binary pattern for each word in the document vector.', 'note that the logictic regression can be replaced by a linear svm.', 'our implementation 1 slightly improved the performance reported in  #TAUTHOR_TAG by adding tri - grams ( improvement of + 0. 6 % ), as shown in table 1']",3
"['- svm ) approach  #TAUTHOR_TAG.', 'this']","['machine ( nb - svm ) approach  #TAUTHOR_TAG.', 'this']","['- svm ) approach  #TAUTHOR_TAG.', 'this approach computes a log - ratio vector between the average word counts extracted']","['purely discriminative methods, the most popular choice is a linear classifier on top of a bagof - word representation of the document.', 'the input representation is usually a tf - idf weighted word counts of the document.', 'in order to preserve local ordering of the words, a better representation would consider also the position - independent n - gram counts of the document ( bag - of - n - grams ).', 'in our ensemble, we used a supervised reweighing of the counts as in the naive bayes support vector machine ( nb - svm ) approach  #TAUTHOR_TAG.', 'this approach computes a log - ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents.', 'the input to the logistic regression classifier corresponds to the log - ratio vector multiplied by the binary pattern for each word in the document vector.', 'note that the logictic regression can be replaced by a linear svm.', 'our implementation 1 slightly improved the performance reported in  #TAUTHOR_TAG by adding tri - grams ( improvement of + 0. 6 % ), as shown in table 1']",5
"['- svm ) approach  #TAUTHOR_TAG.', 'this']","['machine ( nb - svm ) approach  #TAUTHOR_TAG.', 'this']","['- svm ) approach  #TAUTHOR_TAG.', 'this approach computes a log - ratio vector between the average word counts extracted']","['purely discriminative methods, the most popular choice is a linear classifier on top of a bagof - word representation of the document.', 'the input representation is usually a tf - idf weighted word counts of the document.', 'in order to preserve local ordering of the words, a better representation would consider also the position - independent n - gram counts of the document ( bag - of - n - grams ).', 'in our ensemble, we used a supervised reweighing of the counts as in the naive bayes support vector machine ( nb - svm ) approach  #TAUTHOR_TAG.', 'this approach computes a log - ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents.', 'the input to the logistic regression classifier corresponds to the log - ratio vector multiplied by the binary pattern for each word in the document vector.', 'note that the logictic regression can be replaced by a linear svm.', 'our implementation 1 slightly improved the performance reported in  #TAUTHOR_TAG by adding tri - grams ( improvement of + 0. 6 % ), as shown in table 1']",4
"['on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance,']","['on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance,']","['on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear classifier.', '']","['this section we report results on one of the largest publicly available sentiment analysis datasets, the imdb dataset of movie reviews.', 'the dataset consists of 50, 000 movie reviews which are categorized as being either positive or negative.', 'we use 25, 000 reviews for training and the rest for testing, using the same protocol proposed by  #AUTHOR_TAG.', 'all experiments can be reproduced using the code available at https : / / github. com / mesnilgr / iclr15.', 'table 2 reports the results of each individual model.', 'we have found that generative models performed the worst, with rnns slightly better than n - grams.', 'the most competitive method is the method based on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear classifier.', 'finally, table 3 reports the results of combining the previous models into an ensemble.', 'when we interpolate the scores of rnn, sentence vectors and nb - svm, we achieve a new state - of - the - art performance of 92. 57 %, to be compared to 91. 22 % reported by  #TAUTHOR_TAG.', 'notice that our implementation of the sentence vectors method  #AUTHOR_TAG alone yielded only 89. 3 % ( a difference of ≃ 3 % ).', 'in order to measure the contribution of each model to the final ensemble classifier, we remove one model at a time from the ensemble.', 'we observe that the removal of the generative model affects the least the ensemble performance.', 'overall, all three models contribute to the success of the overall ensemble, suggesting that these three models pick up complimentary features useful for discrimination.', 'in table 4, we show test reviews misclassified by single models but classified accurately by the ensemble']",4
"['on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance,']","['on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance,']","['on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear classifier.', '']","['this section we report results on one of the largest publicly available sentiment analysis datasets, the imdb dataset of movie reviews.', 'the dataset consists of 50, 000 movie reviews which are categorized as being either positive or negative.', 'we use 25, 000 reviews for training and the rest for testing, using the same protocol proposed by  #AUTHOR_TAG.', 'all experiments can be reproduced using the code available at https : / / github. com / mesnilgr / iclr15.', 'table 2 reports the results of each individual model.', 'we have found that generative models performed the worst, with rnns slightly better than n - grams.', 'the most competitive method is the method based on reweighed bag - of - words  #TAUTHOR_TAG 2.', 'favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear classifier.', 'finally, table 3 reports the results of combining the previous models into an ensemble.', 'when we interpolate the scores of rnn, sentence vectors and nb - svm, we achieve a new state - of - the - art performance of 92. 57 %, to be compared to 91. 22 % reported by  #TAUTHOR_TAG.', 'notice that our implementation of the sentence vectors method  #AUTHOR_TAG alone yielded only 89. 3 % ( a difference of ≃ 3 % ).', 'in order to measure the contribution of each model to the final ensemble classifier, we remove one model at a time from the ensemble.', 'we observe that the removal of the generative model affects the least the ensemble performance.', 'overall, all three models contribute to the success of the overall ensemble, suggesting that these three models pick up complimentary features useful for discrimination.', 'in table 4, we show test reviews misclassified by single models but classified accurately by the ensemble']",7
"['answering systems to dependency parsing.', ' #TAUTHOR_TAG explored word embeddings and their utility']","['answering systems to dependency parsing.', ' #TAUTHOR_TAG explored word embeddings and their utility']","['- art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing.', ' #TAUTHOR_TAG explored word embeddings and their utility']","['embeddings have been shown to be useful across state - of - the - art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing.', ' #TAUTHOR_TAG explored word embeddings and their utility for modeling language semantics.', 'in particular, they presented an approach to automatically map a standard distributional semantic space onto a set - theoretic model using partial least squares regression.', 'we show in this paper that a simple baseline achieves a + 51 % relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset']",0
"['', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is']","['information word embeddings contain is subsequently of high interest.', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is']","['information word embeddings contain is subsequently of high interest.', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is']","['embeddings are one of the main components in many state - of - the - art systems for natural language processing ( nlp ), such as language modeling  #AUTHOR_TAG, text classification  #AUTHOR_TAG question answering  #AUTHOR_TAG, machine translation  #AUTHOR_TAG, as well as named entity recognition  #AUTHOR_TAG.', 'word embeddings can be pre - trained using large unlabeled datasets typically based on token cooccurrences  #AUTHOR_TAG.', 'they can also be jointly learned with the task.', 'understanding what information word embeddings contain is subsequently of high interest.', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is the center of interest of this paper.', 'specifically, given a feature and a word vector of a concept, they tried to automatically find how often the given concept has the given feature.', 'for example, the concept yam is always a vegetable, the concept cat has a coat most of the time, the concept plug has sometimes 3 prongs, and the concept dog never has wings.', 'the method they used was based on partial least squares regression ( plsr ).', 'we propose a simple baseline that outperforms their model']",0
"['theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['the previous section, we have seen how to convert a concept into a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze whether there exists a transformation from the word embedding of a concept to its model - theoretic vector, the gold standard being the human annotations.', 'the word embeddings are taken from the word embeddings pre - trained with word2vec googlenews - vectors - negative300 1 ( 300 dimensions ), which were trained on part of the google news dataset, consisting of approximately 100 billion words.', 'the transformation used in  #TAUTHOR_TAG is based on partial least squares regression ( plsr ).', 'the plsr is fitted on the training set : the inputs are the word embeddings for each concept, and the outputs are the model - theoretic vectors for each concept.', '']",0
"['theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['the previous section, we have seen how to convert a concept into a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze whether there exists a transformation from the word embedding of a concept to its model - theoretic vector, the gold standard being the human annotations.', 'the word embeddings are taken from the word embeddings pre - trained with word2vec googlenews - vectors - negative300 1 ( 300 dimensions ), which were trained on part of the google news dataset, consisting of approximately 100 billion words.', 'the transformation used in  #TAUTHOR_TAG is based on partial least squares regression ( plsr ).', 'the plsr is fitted on the training set : the inputs are the word embeddings for each concept, and the outputs are the model - theoretic vectors for each concept.', '']",0
"['', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is']","['information word embeddings contain is subsequently of high interest.', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is']","['information word embeddings contain is subsequently of high interest.', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is']","['embeddings are one of the main components in many state - of - the - art systems for natural language processing ( nlp ), such as language modeling  #AUTHOR_TAG, text classification  #AUTHOR_TAG question answering  #AUTHOR_TAG, machine translation  #AUTHOR_TAG, as well as named entity recognition  #AUTHOR_TAG.', 'word embeddings can be pre - trained using large unlabeled datasets typically based on token cooccurrences  #AUTHOR_TAG.', 'they can also be jointly learned with the task.', 'understanding what information word embeddings contain is subsequently of high interest.', ' #TAUTHOR_TAG investigated a method to map word embeddings to formal semantics, which is the center of interest of this paper.', 'specifically, given a feature and a word vector of a concept, they tried to automatically find how often the given concept has the given feature.', 'for example, the concept yam is always a vegetable, the concept cat has a coat most of the time, the concept plug has sometimes 3 prongs, and the concept dog never has wings.', 'the method they used was based on partial least squares regression ( plsr ).', 'we propose a simple baseline that outperforms their model']",5
['the task presented in  #TAUTHOR_TAG'],['the task presented in  #TAUTHOR_TAG'],"['this section, we summarize the task presented in  #TAUTHOR_TAG.', 'the following is an example of a concept along with some of its features, as formatted in']","['this section, we summarize the task presented in  #TAUTHOR_TAG.', 'the following is an example of a concept along with some of its features, as formatted in one of the two datasets used to evaluate the model : yam a vegetable all all all yam eaten by cooking all most most yam grows in the ground all all all yam is edible all most all yam is orange some most most yam like a potato all all all the concept yam has six features ( a vegetable, eaten by cooking, grows in the ground, is edible, is orange, and like a potato ).', 'each feature in this dataset is annotated by three different humans.', 'the annotation is a quantifier that reflects how frequently the concept has a feature.', 'five quantifiers are used : no, few, some, most, and all.', 'in this example, the concept yam has been annotated as some, most and most for the feature is orange.', '']",5
"['theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze']","['the previous section, we have seen how to convert a concept into a model - theoretic vector based on human annotations.', 'the goal of  #TAUTHOR_TAG is to analyze whether there exists a transformation from the word embedding of a concept to its model - theoretic vector, the gold standard being the human annotations.', 'the word embeddings are taken from the word embeddings pre - trained with word2vec googlenews - vectors - negative300 1 ( 300 dimensions ), which were trained on part of the google news dataset, consisting of approximately 100 billion words.', 'the transformation used in  #TAUTHOR_TAG is based on partial least squares regression ( plsr ).', 'the plsr is fitted on the training set : the inputs are the word embeddings for each concept, and the outputs are the model - theoretic vectors for each concept.', '']",5
['compare  #TAUTHOR_TAG against'],['compare  #TAUTHOR_TAG against'],"['compare  #TAUTHOR_TAG against three baselines : random vectors, mode,']","['compare  #TAUTHOR_TAG against three baselines : random vectors, mode, and nearest neighbor.', '• mode : a predictor that outputs, for each feature, the most common feature value ( i. e., the mode ) in the training set.', 'for example, if a feature is annotated as all for most concepts, then the predictor will always output all for this feature.', 'when finding the most common value of a feature, we ignore all the concepts for which the feature is not annotated.', 'the resulting predictor does not take any concept into account when making a prediction.', 'indeed, the predicted values are always the same, regardless of the concept.', '']",5
['compare  #TAUTHOR_TAG against'],['compare  #TAUTHOR_TAG against'],"['compare  #TAUTHOR_TAG against three baselines : random vectors, mode,']","['compare  #TAUTHOR_TAG against three baselines : random vectors, mode, and nearest neighbor.', '• mode : a predictor that outputs, for each feature, the most common feature value ( i. e., the mode ) in the training set.', 'for example, if a feature is annotated as all for most concepts, then the predictor will always output all for this feature.', 'when finding the most common value of a feature, we ignore all the concepts for which the feature is not annotated.', 'the resulting predictor does not take any concept into account when making a prediction.', 'indeed, the predicted values are always the same, regardless of the concept.', '']",5
['in  #TAUTHOR_TAG vs.'],['using plsr and word2vec embeddings ( 0. 346 in  #TAUTHOR_TAG vs. 0. 572 in our experiments ) : this discrepancy'],['in  #TAUTHOR_TAG vs.'],"['results and discussion table 1 presents the results, using the spearman correlation as the performance metric.', 'the experiment was coded in python using scikit - learn  #AUTHOR_TAG and the source as well as the complete result log and the two datasets are available online 3.', 'we could reproduce the results for the qmr dataset using plsr and word2vec embeddings ( 0. 346 in  #TAUTHOR_TAG vs. 0. 572 in our experiments ) : this discrepancy most likely results from the choice of the training set.', ""our experiments'results are averaged over 1000 runs, and for each run the training / test split is randomly chosen, the only constraint being having the same number of training samples as in  #TAUTHOR_TAG."", 'for the ad dataset, our worst run achieved 0. 435, and our best run achieved 0. 713, which emphasizes the lack of robustness of the results with respect to the train / test split.', 'qmr dataset ( min : 0. 244 ; max : 0. 407 ), which is expected since qmr is significantly larger than ad.', 'furthermore, the mode baseline yields results that are good on the ad dataset ( 0. 554, vs. 0. 634 in  #TAUTHOR_TAG, i. e. + 51 % improvement ).', 'to get an intuition of why the mode baseline works well, figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the ad dataset.', '']",5
['compare  #TAUTHOR_TAG against'],['compare  #TAUTHOR_TAG against'],"['compare  #TAUTHOR_TAG against three baselines : random vectors, mode,']","['compare  #TAUTHOR_TAG against three baselines : random vectors, mode, and nearest neighbor.', '• mode : a predictor that outputs, for each feature, the most common feature value ( i. e., the mode ) in the training set.', 'for example, if a feature is annotated as all for most concepts, then the predictor will always output all for this feature.', 'when finding the most common value of a feature, we ignore all the concepts for which the feature is not annotated.', 'the resulting predictor does not take any concept into account when making a prediction.', 'indeed, the predicted values are always the same, regardless of the concept.', '']",4
['in  #TAUTHOR_TAG vs.'],['using plsr and word2vec embeddings ( 0. 346 in  #TAUTHOR_TAG vs. 0. 572 in our experiments ) : this discrepancy'],['in  #TAUTHOR_TAG vs.'],"['results and discussion table 1 presents the results, using the spearman correlation as the performance metric.', 'the experiment was coded in python using scikit - learn  #AUTHOR_TAG and the source as well as the complete result log and the two datasets are available online 3.', 'we could reproduce the results for the qmr dataset using plsr and word2vec embeddings ( 0. 346 in  #TAUTHOR_TAG vs. 0. 572 in our experiments ) : this discrepancy most likely results from the choice of the training set.', ""our experiments'results are averaged over 1000 runs, and for each run the training / test split is randomly chosen, the only constraint being having the same number of training samples as in  #TAUTHOR_TAG."", 'for the ad dataset, our worst run achieved 0. 435, and our best run achieved 0. 713, which emphasizes the lack of robustness of the results with respect to the train / test split.', 'qmr dataset ( min : 0. 244 ; max : 0. 407 ), which is expected since qmr is significantly larger than ad.', 'furthermore, the mode baseline yields results that are good on the ad dataset ( 0. 554, vs. 0. 634 in  #TAUTHOR_TAG, i. e. + 51 % improvement ).', 'to get an intuition of why the mode baseline works well, figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the ad dataset.', '']",4
"["" #TAUTHOR_TAG's model on""]","["" #TAUTHOR_TAG's model on""]","["" #TAUTHOR_TAG's model on the qmr dataset,""]","['this paper we have presented several baselines for mapping distributional to model - theoretic semantic spaces.', ""the mode baseline significantly outperforms  #TAUTHOR_TAG's model on the qmr dataset, and yields competitive results on the ad dataset."", 'this indicates that state - of - the - art models do not efficiently map word embeddings to model - theoretic vectors in these datasets']",4
['compare  #TAUTHOR_TAG against'],['compare  #TAUTHOR_TAG against'],"['compare  #TAUTHOR_TAG against three baselines : random vectors, mode,']","['compare  #TAUTHOR_TAG against three baselines : random vectors, mode, and nearest neighbor.', '• mode : a predictor that outputs, for each feature, the most common feature value ( i. e., the mode ) in the training set.', 'for example, if a feature is annotated as all for most concepts, then the predictor will always output all for this feature.', 'when finding the most common value of a feature, we ignore all the concepts for which the feature is not annotated.', 'the resulting predictor does not take any concept into account when making a prediction.', 'indeed, the predicted values are always the same, regardless of the concept.', '']",3
['in  #TAUTHOR_TAG vs.'],['using plsr and word2vec embeddings ( 0. 346 in  #TAUTHOR_TAG vs. 0. 572 in our experiments ) : this discrepancy'],['in  #TAUTHOR_TAG vs.'],"['results and discussion table 1 presents the results, using the spearman correlation as the performance metric.', 'the experiment was coded in python using scikit - learn  #AUTHOR_TAG and the source as well as the complete result log and the two datasets are available online 3.', 'we could reproduce the results for the qmr dataset using plsr and word2vec embeddings ( 0. 346 in  #TAUTHOR_TAG vs. 0. 572 in our experiments ) : this discrepancy most likely results from the choice of the training set.', ""our experiments'results are averaged over 1000 runs, and for each run the training / test split is randomly chosen, the only constraint being having the same number of training samples as in  #TAUTHOR_TAG."", 'for the ad dataset, our worst run achieved 0. 435, and our best run achieved 0. 713, which emphasizes the lack of robustness of the results with respect to the train / test split.', 'qmr dataset ( min : 0. 244 ; max : 0. 407 ), which is expected since qmr is significantly larger than ad.', 'furthermore, the mode baseline yields results that are good on the ad dataset ( 0. 554, vs. 0. 634 in  #TAUTHOR_TAG, i. e. + 51 % improvement ).', 'to get an intuition of why the mode baseline works well, figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the ad dataset.', '']",3
"['relation examples.', 'similar to  #TAUTHOR_TAG,']","['relation examples.', 'similar to  #TAUTHOR_TAG,']","['relation examples.', 'similar to  #TAUTHOR_TAG,']","['', ' #AUTHOR_TAG proposed a composite kernel that consists of two individual kernels : an entity kernel that allows for entity - related features and a convolution parse tree kernel that models syntactic information of relation examples.', 'however, their method needs to manually tune parameters in composite kernels that are often difficult to determine.', 'this paper describes an expanded convolution parse tree kernel to incorporate entity information into syntactic structure of relation examples.', 'similar to  #TAUTHOR_TAG, we employ a convolution parse tree kernel in order to model syntactic structures.', 'different from their method, we use the convolution parse tree kernel expanded with entity information other than a composite kernel.', 'one of our motivations is to capture syntactic and semantic information in a single parse tree for further graceful refinement, the other is that we can avoid the difficulty with tuning parameters in composite kernels.', 'evaluation on the ace2004 corpus shows that our method slightly outperforms the previous feature - base and kernel - based methods.', 'the rest of the paper is organized as follows.', 'first, we present our expanded convolution tree kernel in section 2.', 'then, section 3 reports the experimental setting and']",3
"['kernel used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution']","['kernel used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution']","['used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution tree kernel counts the number of']","['this section, we describe the expanded convolution parse tree kernel and demonstrate how entity information can be incorporated into the parse tree.', 'figure 1 : different representations of a relation instance in the example sentence "" in many cities, angry crowds roam the streets. "", which is excerpted from the ace2004 corpus, where a relation "" phsy. located "" holds between the first entity "" crowds "" ( per ) and the second entity "" streets "" ( fac ).', 'we employ the same convolution tree kernel used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution tree kernel counts the number of subtrees that have similar productions on every node between two parse trees.', '']",3
"['kernel used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution']","['kernel used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution']","['used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution tree kernel counts the number of']","['this section, we describe the expanded convolution parse tree kernel and demonstrate how entity information can be incorporated into the parse tree.', 'figure 1 : different representations of a relation instance in the example sentence "" in many cities, angry crowds roam the streets. "", which is excerpted from the ace2004 corpus, where a relation "" phsy. located "" holds between the first entity "" crowds "" ( per ) and the second entity "" streets "" ( fac ).', 'we employ the same convolution tree kernel used by  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'this convolution tree kernel counts the number of subtrees that have similar productions on every node between two parse trees.', '']",3
"['systems. compared with the composite kernel  #TAUTHOR_TAG, our', '']","['- reported systems. compared with the composite kernel  #TAUTHOR_TAG, our', '']","['recently best - reported systems. compared with the composite kernel  #TAUTHOR_TAG, our', 'system further prunes the']","['', 'the ace2004 corpus. it shows that our system slightly outperforms recently best - reported systems. compared with the composite kernel  #TAUTHOR_TAG, our', 'system further prunes the parse tree and incorporates entity features into the convolution parse tree kernel. it shows that our system achieves higher precision, lower recall and slightly better f - measure than their method', '. compared with featurebased systems  #AUTHOR_TAG that incorporate many lexical, syntactic and semantic features, our system improves the f - measure by', '1. 8 / 2. 5 units over relation types respectively. this suggests that kernel - based systems can promisingly outperform feature - based systems, although much work like performance enhancement and reduction of training speed still', 'needs to be done to further improve the system']",4
['of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser'],['of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser'],['of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser performance'],"['the success of statistical parsing models on the wall street journal ( wsj ) section of the penn treebank ( ptb )  #AUTHOR_TAG for example ), there has been a change in focus in recent years towards the problem of replicating this success on genres other than american financial news stories.', 'the main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples.', 'a breakthrough has come in the form of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser performance when combined with a two - stage reranking parser model  #AUTHOR_TAG.', 'selftraining is the process of training a parser on its own output, and earlier self - training experiments using generative statistical parsers did not yield encouraging results  #AUTHOR_TAG.', 'mc  #AUTHOR_TAG a ; 2006b ) proceed as follows : sentences * now affiliated to lalic, universite paris 4 la sorbonne.', '']",0
['of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser'],['of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser'],['of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser performance'],"['the success of statistical parsing models on the wall street journal ( wsj ) section of the penn treebank ( ptb )  #AUTHOR_TAG for example ), there has been a change in focus in recent years towards the problem of replicating this success on genres other than american financial news stories.', 'the main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples.', 'a breakthrough has come in the form of research by  #TAUTHOR_TAG ; 2006b ) who show that self - training can be used to improve parser performance when combined with a two - stage reranking parser model  #AUTHOR_TAG.', 'selftraining is the process of training a parser on its own output, and earlier self - training experiments using generative statistical parsers did not yield encouraging results  #AUTHOR_TAG.', 'mc  #AUTHOR_TAG a ; 2006b ) proceed as follows : sentences * now affiliated to lalic, universite paris 4 la sorbonne.', '']",0
['of  #TAUTHOR_TAG that'],['of  #TAUTHOR_TAG that self - training with a reranking'],['the claim of  #TAUTHOR_TAG that'],"['', 'this difference is reasonable since there is greater domain variation between the wsj and the bnc than between the wsj and the brown corpus, and all bnc gold standard sentences contain verbs not attested in wsj sections 2 - 21.', 'we retrain the first - stage generative statistical parser of charniak and johnson using combinations of bnc trees ( parsed using the reranking parser ) and wsj treebank trees.', 'we test the combinations on the bnc gold standard development set and on wsj section 00.', 'table 1 shows that parser accuracy increases with the size of the in - domain selftraining material.', '3 the figures confirm the claim of  #TAUTHOR_TAG that self - training with a reranking parsing model is effective for improving parser accuracy in general, and the claim of  #AUTHOR_TAG that training on in - domain data is effective for parser adaption.', 'they confirm that self - training on in - domain data is effective for parser adaptation.', 'the wsj section 00 results suggest that, in order to maintain performance on the seed training domain, it is necessary to combine bnc parse trees of the self - training combinations with abovebaseline improvements for both development sets, the combination of 1, 000k bnc parse trees and section 2 - 21 of the wsj ( multiplied by ten ) yields the highest improvement for the bnc data, and we present final results with this combination for the bnc gold standard test set and wsj section 23.', 'there is an absolute improvement on the original reranking parser of 1. 7 % on the bnc gold standard test set and 0. 4 % on wsj section 23.', 'the improvement on bnc data is statistically significant for both precision and recall ( p < 0. 0002, p < 0. 0002 ).', 'the improvement on wsj section 23 is statistically significant for precision only ( p < 0. 003 )']",7
['- art  #TAUTHOR_TAG'],['- of - the - art  #TAUTHOR_TAG'],['- art  #TAUTHOR_TAG. the main contributions of the paper are : • novel self - governing neural networks ( sgnns ) for on - device deep'],"['', ', on - the - fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. we evaluate the performance of our sgn', '##ns on dialogue act classification, because ( 1 ) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and ( 2 ) deep learning', 'methods reached state - of - the - art  #TAUTHOR_TAG. the main contributions of the paper are : • novel self - governing neural networks ( sgnns ) for on - device deep learning for short text classification. • compression technique that effectively captures low - dimensional semantic text representation and produces compact models that save on', 'storage and computational cost. • on the', 'fly computation of projection vectors that eliminate the need for large pre - trained word embeddings or vocabulary pruning. • exhaustive', 'experimental evaluation on dialog act datasets, outperforming state - of - theart deep cnn  #TAUTHOR_TAG and rnn variants  #AUTHOR_TAG']",3
['- art  #TAUTHOR_TAG'],['- of - the - art  #TAUTHOR_TAG'],['- art  #TAUTHOR_TAG. the main contributions of the paper are : • novel self - governing neural networks ( sgnns ) for on - device deep'],"['', ', on - the - fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. we evaluate the performance of our sgn', '##ns on dialogue act classification, because ( 1 ) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and ( 2 ) deep learning', 'methods reached state - of - the - art  #TAUTHOR_TAG. the main contributions of the paper are : • novel self - governing neural networks ( sgnns ) for on - device deep learning for short text classification. • compression technique that effectively captures low - dimensional semantic text representation and produces compact models that save on', 'storage and computational cost. • on the', 'fly computation of projection vectors that eliminate the need for large pre - trained word embeddings or vocabulary pruning. • exhaustive', 'experimental evaluation on dialog act datasets, outperforming state - of - theart deep cnn  #TAUTHOR_TAG and rnn variants  #AUTHOR_TAG']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['with cosine annealing decay  #AUTHOR_TAG.', 'unlike prior approaches  #TAUTHOR_TAG']","['both datasets we used the following : 2 - layer sgnn ( p t = 80, d = 14 × fullyconnected 256 × fullyconnected 256 ), mini - batch size of 100, dropout rate of 0. 25, learning rate was initialized to 0. 025 with cosine annealing decay  #AUTHOR_TAG.', 'unlike prior approaches  #TAUTHOR_TAG that rely on pre - trained word embeddings, we learn the projection weights on the fly during training, i. e word embeddings ( or vocabularies ) do not need to be stored.', 'instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors.', 'these values were chosen via a grid search on development sets, we do not perform any other dataset - specific tuning.', 'training is performed through stochastic gradient descent over shuffled mini - batches with nesterov momentum optimizer  #AUTHOR_TAG, run for 1m steps.', 'tables 2 and 3 show results on the swda and mrda dialog act datasets.', 'overall, our sgnn model consistently outperforms the baselines and prior state - of - the - art deep learning models']",4
"['classifier  #TAUTHOR_TAG.', 'our model significantly']","['classifier  #TAUTHOR_TAG.', 'our model significantly']","['and naive bayes classifier  #TAUTHOR_TAG.', 'our model significantly']","['compare our model against a majority class baseline and naive bayes classifier  #TAUTHOR_TAG.', 'our model significantly outperforms both baselines by 12 to 35 % absolute']",4
"[' #TAUTHOR_TAG, rn']","[' #TAUTHOR_TAG, rnn  #AUTHOR_TAG and rnn with gated attention  #AUTHOR_TAG.', 'to the best of our knowledge,  #TAUTHOR_TAG are']","['cnn  #TAUTHOR_TAG, rn']","['also compare our performance against prior work using hmms  #AUTHOR_TAG and recent deep learning methods like cnn  #TAUTHOR_TAG, rnn  #AUTHOR_TAG and rnn with gated attention  #AUTHOR_TAG.', 'to the best of our knowledge,  #TAUTHOR_TAG are the latest approaches in dialog act classification, which also reported on the same data splits.', 'therefore, we compare our research against these works.', 'according to  #AUTHOR_TAG, prior work by  #AUTHOR_TAG achieved promising results on the mrda dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.', '']",4
"['##ing methods  #TAUTHOR_TAG.', '']","['state - of - the - art deep leaning methods  #TAUTHOR_TAG.', '']","['- art deep leaning methods  #TAUTHOR_TAG.', 'we introduced a compression technique']","['proposed self - governing neural networks for on - device short text classification.', 'experiments on multiple dialog act datasets showed that our model outperforms state - of - the - art deep leaning methods  #TAUTHOR_TAG.', 'we introduced a compression technique that effectively captures low - dimensional semantic representation and produces compact models that significantly save on storage and computational cost.', 'our approach does not rely on pre - trained embeddings and efficiently computes the projection vectors on the fly.', 'in the future, we are interested in extending this approach to more natural language tasks.', 'for instance, we built a multilingual sgnn model for customer feedback classification  #AUTHOR_TAG and obtained 73 % on japanese, close to best performing system on the challenge  #AUTHOR_TAG.', 'unlike their method, we did not use any pre - processing, tagging, parsing, pre - trained embeddings or other resources']",4
"['dataset statistics.', 'we use the train, validation and test splits as defined in  #TAUTHOR_TAG']","['dataset statistics.', 'we use the train, validation and test splits as defined in  #TAUTHOR_TAG']","[': icsi meeting recorder dialog act corpus  #AUTHOR_TAG is a dialog corpus of multiparty meetings with 5 tags of dialog acts.', 'table 1 summarizes dataset statistics.', 'we use the train, validation and test splits as defined in  #TAUTHOR_TAG']","['conduct our experimental evaluation on two dialog act benchmark datasets.', '• swda : switchboard dialog act corpus  #AUTHOR_TAG is a popular open domain dialogs corpus between two speakers with 42 dialogs acts.', '• mrda : icsi meeting recorder dialog act corpus  #AUTHOR_TAG is a dialog corpus of multiparty meetings with 5 tags of dialog acts.', 'table 1 summarizes dataset statistics.', 'we use the train, validation and test splits as defined in  #TAUTHOR_TAG']",5
"['classifier  #TAUTHOR_TAG.', 'our model significantly']","['classifier  #TAUTHOR_TAG.', 'our model significantly']","['and naive bayes classifier  #TAUTHOR_TAG.', 'our model significantly']","['compare our model against a majority class baseline and naive bayes classifier  #TAUTHOR_TAG.', 'our model significantly outperforms both baselines by 12 to 35 % absolute']",5
"[' #TAUTHOR_TAG, rn']","[' #TAUTHOR_TAG, rnn  #AUTHOR_TAG and rnn with gated attention  #AUTHOR_TAG.', 'to the best of our knowledge,  #TAUTHOR_TAG are']","['cnn  #TAUTHOR_TAG, rn']","['also compare our performance against prior work using hmms  #AUTHOR_TAG and recent deep learning methods like cnn  #TAUTHOR_TAG, rnn  #AUTHOR_TAG and rnn with gated attention  #AUTHOR_TAG.', 'to the best of our knowledge,  #TAUTHOR_TAG are the latest approaches in dialog act classification, which also reported on the same data splits.', 'therefore, we compare our research against these works.', 'according to  #AUTHOR_TAG, prior work by  #AUTHOR_TAG achieved promising results on the mrda dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.', '']",5
"[' #TAUTHOR_TAG, rn']","[' #TAUTHOR_TAG, rnn  #AUTHOR_TAG and rnn with gated attention  #AUTHOR_TAG.', 'to the best of our knowledge,  #TAUTHOR_TAG are']","['cnn  #TAUTHOR_TAG, rn']","['also compare our performance against prior work using hmms  #AUTHOR_TAG and recent deep learning methods like cnn  #TAUTHOR_TAG, rnn  #AUTHOR_TAG and rnn with gated attention  #AUTHOR_TAG.', 'to the best of our knowledge,  #TAUTHOR_TAG are the latest approaches in dialog act classification, which also reported on the same data splits.', 'therefore, we compare our research against these works.', 'according to  #AUTHOR_TAG, prior work by  #AUTHOR_TAG achieved promising results on the mrda dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.', '']",0
"['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['', 'our state - of - the - art text categorisation system for wikipedia achieved an improvement of up to 5 % f - score over previous', 'approaches. accurate classifications for wikipedia articles are useful for a number of natural language processing ( nlp ) tasks, such as question answering and', 'ner. to produce article classifications for generating ner training data,  #AUTHOR_TAG used a heuristicbased text categorisation system. this involved extracting', ""the first head noun after the copula, head nouns from an article's categories, and"", 'incoming link information. they reported an f - score of 89 % when evaluating on a set of 1, 300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers for categorising wikipedia. they expanded each', '']",0
"['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['', 'our state - of - the - art text categorisation system for wikipedia achieved an improvement of up to 5 % f - score over previous', 'approaches. accurate classifications for wikipedia articles are useful for a number of natural language processing ( nlp ) tasks, such as question answering and', 'ner. to produce article classifications for generating ner training data,  #AUTHOR_TAG used a heuristicbased text categorisation system. this involved extracting', ""the first head noun after the copula, head nouns from an article's categories, and"", 'incoming link information. they reported an f - score of 89 % when evaluating on a set of 1, 300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers for categorising wikipedia. they expanded each', '']",0
"['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers']","['', 'our state - of - the - art text categorisation system for wikipedia achieved an improvement of up to 5 % f - score over previous', 'approaches. accurate classifications for wikipedia articles are useful for a number of natural language processing ( nlp ) tasks, such as question answering and', 'ner. to produce article classifications for generating ner training data,  #AUTHOR_TAG used a heuristicbased text categorisation system. this involved extracting', ""the first head noun after the copula, head nouns from an article's categories, and"", 'incoming link information. they reported an f - score of 89 % when evaluating on a set of 1, 300 hand -', 'labelled articles.  #TAUTHOR_TAG explored the use of nb and svm classifiers for categorising wikipedia. they expanded each', '']",5
"['by  #TAUTHOR_TAG as useful features for a machine learner, an idea stemming from the fact that']","['by  #TAUTHOR_TAG as useful features for a machine learner, an idea stemming from the fact that']","['were identified by  #TAUTHOR_TAG as useful features for a machine learner, an idea stemming from the fact that']","['baseline system used a simple bag - of - words including tokens from the entire article body and the article title.', 'this did not include tokens that appear in templates used in the generation of an article.', 'we then experimented with a number of different feature extraction methods, focusing primarily on the document structure for identifying useful features.', ""tokens in the first paragraph were identified by  #TAUTHOR_TAG as useful features for a machine learner, an idea stemming from the fact that most human annotators will recognise an article's category after reading just the first paragraph."", 'we extended this idea by also marking the first sentence and title tokens as separate from other tokens, as we found that often the first sentence was all that was required for a human annotator to classify an article.', 'we ran experiments limiting the feature space to these smaller portions of the document.', ""wikipedia articles often have a large amount of metadata that helps in identifying an article's category, in particular wikipedia categories and templates."", 'wikipedia categories are informal user defined and applied categories, forming a "" folksonomy "" rather than a strict taxonomy suitable for classification tasks, but the terms in the category names are usually strong indicators of an article\'s class.', '']",6
"['heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['compared our two classifiers against the heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['compared our two classifiers against the heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', 'we also tested a baseline system that used a bag - of - words representation of wikipedia articles with rich metadata excluded.', 'all svm experiments were run using lib - svm  #AUTHOR_TAG using a linear kernel with parameter c = 2.', 'for nb experiments we used the nltk.', 'the text categorisation system developed by  #AUTHOR_TAG was provided to us by the authors, and we evaluated it using our hand - labelled training data.', 'direct comparison with this system was difficult, as it has the ability to mark an article as "" unknown "" or "" conflict "" and defer classification.', 'given that these classifications cannot be considered correct we marked them as classification errors.', 'there were also a number of complications when comparing our system with the system described by  #TAUTHOR_TAG : they used a different, and substantially smaller, hand - labelled data set ; they did not specify how they handled disambiguation pages ; they provided no results for experiments using only hand - labelled data, instead incorporating training data produced via their semi - automated approach into the final results ; and they neglected to report the final size of the training data produced by their semi - automated annotation.', 'however, these two systems provided the closest benchmarks for comparison.', 'we found that across all experiments the nb classifier performed best when using a bag - of - words representation incorporating the first sentence of an article only, along with tokens extracted from categories, templates and infoboxes.', 'conversely, the svm classifier performed best using a bag - of - words representation incorporating the entire body of an article, along with category, template and infobox tokens.', 'all experiment results listed were run with these respective configurations.', 'we evaluated our system on two coarse - grained sets of data : the first containing all articles from our hand - labelled set, and the second containing only those articles that described nes.', 'table 2 lists results from the top scoring configurations for both the nb and svm classifiers.', 'the svm classifier performed significantly better than the nb classifier.', 'limiting the categorisation scheme to ne - only classes improved the classification accuracy for both classifiers, as the difficult non class was excluded.', 'with this exclusion the nb classifier became much more competitive with the svm classifier.', '']",7
"['heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['compared our two classifiers against the heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['compared our two classifiers against the heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', 'we also tested a baseline system that used a bag - of - words representation of wikipedia articles with rich metadata excluded.', 'all svm experiments were run using lib - svm  #AUTHOR_TAG using a linear kernel with parameter c = 2.', 'for nb experiments we used the nltk.', 'the text categorisation system developed by  #AUTHOR_TAG was provided to us by the authors, and we evaluated it using our hand - labelled training data.', 'direct comparison with this system was difficult, as it has the ability to mark an article as "" unknown "" or "" conflict "" and defer classification.', 'given that these classifications cannot be considered correct we marked them as classification errors.', 'there were also a number of complications when comparing our system with the system described by  #TAUTHOR_TAG : they used a different, and substantially smaller, hand - labelled data set ; they did not specify how they handled disambiguation pages ; they provided no results for experiments using only hand - labelled data, instead incorporating training data produced via their semi - automated approach into the final results ; and they neglected to report the final size of the training data produced by their semi - automated annotation.', 'however, these two systems provided the closest benchmarks for comparison.', 'we found that across all experiments the nb classifier performed best when using a bag - of - words representation incorporating the first sentence of an article only, along with tokens extracted from categories, templates and infoboxes.', 'conversely, the svm classifier performed best using a bag - of - words representation incorporating the entire body of an article, along with category, template and infobox tokens.', 'all experiment results listed were run with these respective configurations.', 'we evaluated our system on two coarse - grained sets of data : the first containing all articles from our hand - labelled set, and the second containing only those articles that described nes.', 'table 2 lists results from the top scoring configurations for both the nb and svm classifiers.', 'the svm classifier performed significantly better than the nb classifier.', 'limiting the categorisation scheme to ne - only classes improved the classification accuracy for both classifiers, as the difficult non class was excluded.', 'with this exclusion the nb classifier became much more competitive with the svm classifier.', '']",7
"['heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['compared our two classifiers against the heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', '']","['compared our two classifiers against the heuristic - based system described by  #AUTHOR_TAG and the classifiers described by  #TAUTHOR_TAG.', 'we also tested a baseline system that used a bag - of - words representation of wikipedia articles with rich metadata excluded.', 'all svm experiments were run using lib - svm  #AUTHOR_TAG using a linear kernel with parameter c = 2.', 'for nb experiments we used the nltk.', 'the text categorisation system developed by  #AUTHOR_TAG was provided to us by the authors, and we evaluated it using our hand - labelled training data.', 'direct comparison with this system was difficult, as it has the ability to mark an article as "" unknown "" or "" conflict "" and defer classification.', 'given that these classifications cannot be considered correct we marked them as classification errors.', 'there were also a number of complications when comparing our system with the system described by  #TAUTHOR_TAG : they used a different, and substantially smaller, hand - labelled data set ; they did not specify how they handled disambiguation pages ; they provided no results for experiments using only hand - labelled data, instead incorporating training data produced via their semi - automated approach into the final results ; and they neglected to report the final size of the training data produced by their semi - automated annotation.', 'however, these two systems provided the closest benchmarks for comparison.', 'we found that across all experiments the nb classifier performed best when using a bag - of - words representation incorporating the first sentence of an article only, along with tokens extracted from categories, templates and infoboxes.', 'conversely, the svm classifier performed best using a bag - of - words representation incorporating the entire body of an article, along with category, template and infobox tokens.', 'all experiment results listed were run with these respective configurations.', 'we evaluated our system on two coarse - grained sets of data : the first containing all articles from our hand - labelled set, and the second containing only those articles that described nes.', 'table 2 lists results from the top scoring configurations for both the nb and svm classifiers.', 'the svm classifier performed significantly better than the nb classifier.', 'limiting the categorisation scheme to ne - only classes improved the classification accuracy for both classifiers, as the difficult non class was excluded.', 'with this exclusion the nb classifier became much more competitive with the svm classifier.', '']",4
"['to happen.', ' #TAUTHOR_TAG, where']","['to happen.', ' #TAUTHOR_TAG, where']","['to happen.', ' #TAUTHOR_TAG, where each sentence is labelled as wish or non -']","['are the datasets which we use for our experiments.', '• wish detection oxford dictionary defines the noun wish as, a desire or hope for something to happen.', ' #TAUTHOR_TAG, where each sentence is labelled as wish or non - wish.', ' #TAUTHOR_TAG : 6379 sentences, out of which 34 % are annotated wishes.', 'b.  #TAUTHOR_TAG : 1235 sentences, out of which 12 % are annotated as wishes.', '']",6
"['##tive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall,']","['subjunctive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall,']","['the part of speech and dependency information, we use stanford parser 3. 3. 1  #AUTHOR_TAG.', 'word stemming is not performed.', 'we use the libsvm implementation of svm classifier ( el -  #AUTHOR_TAG.', 'the parameter values of svm classifiers are : svm type = c - svc, kernel function = radial basis function.', 'features are ranked using the info - gain feature selection algorithm  #AUTHOR_TAG.', 'top 1000 features are used in all the experiments ie.', 'the size of feature vector is not more than 1000.', '5 subjunctive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall,']","['', '• frequent dependency relations : these are a set of dependency relations  #AUTHOR_TAG.', 'using the same method as the part of speech tags, we identify 5 most frequent dependency relations which occur in the subjunctive mood dataset.', 'in order to apply the concept of tf / idf, each dependency relation occurring in the corpus is treated as a term.', 'the top 5 relations were : advmod, aux, ccomp, mark and nsubj.', 'goldberg et. al ( 2009 ) templates n / a n / a 0. 47 unigrams, templates n / a n / a 0. 56 we also obtain classification results of the combination of these features with the standard unigram features ( table 2, 3 ).', 'to obtain the part of speech and dependency information, we use stanford parser 3. 3. 1  #AUTHOR_TAG.', 'word stemming is not performed.', 'we use the libsvm implementation of svm classifier ( el -  #AUTHOR_TAG.', 'the parameter values of svm classifiers are : svm type = c - svc, kernel function = radial basis function.', 'features are ranked using the info - gain feature selection algorithm  #AUTHOR_TAG.', 'top 1000 features are used in all the experiments ie.', 'the size of feature vector is not more than 1000.', '5 subjunctive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall, and area under curve ( auc ) for the positive class.', 'auc was also used by  #TAUTHOR_TAG.', 'to the best of our knowledge, statistical classification based approach have not yet been employed to detect suggestions in reviews.', 'our experiment which uses subjunctive features for suggestion detection, is the first in this regard.', 'table 2 compares the auc values obtained with unigrams, subjunctive features, a combination of both, and the results from  #TAUTHOR_TAG for']",3
"['##tive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall,']","['subjunctive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall,']","['the part of speech and dependency information, we use stanford parser 3. 3. 1  #AUTHOR_TAG.', 'word stemming is not performed.', 'we use the libsvm implementation of svm classifier ( el -  #AUTHOR_TAG.', 'the parameter values of svm classifiers are : svm type = c - svc, kernel function = radial basis function.', 'features are ranked using the info - gain feature selection algorithm  #AUTHOR_TAG.', 'top 1000 features are used in all the experiments ie.', 'the size of feature vector is not more than 1000.', '5 subjunctive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall,']","['', '• frequent dependency relations : these are a set of dependency relations  #AUTHOR_TAG.', 'using the same method as the part of speech tags, we identify 5 most frequent dependency relations which occur in the subjunctive mood dataset.', 'in order to apply the concept of tf / idf, each dependency relation occurring in the corpus is treated as a term.', 'the top 5 relations were : advmod, aux, ccomp, mark and nsubj.', 'goldberg et. al ( 2009 ) templates n / a n / a 0. 47 unigrams, templates n / a n / a 0. 56 we also obtain classification results of the combination of these features with the standard unigram features ( table 2, 3 ).', 'to obtain the part of speech and dependency information, we use stanford parser 3. 3. 1  #AUTHOR_TAG.', 'word stemming is not performed.', 'we use the libsvm implementation of svm classifier ( el -  #AUTHOR_TAG.', 'the parameter values of svm classifiers are : svm type = c - svc, kernel function = radial basis function.', 'features are ranked using the info - gain feature selection algorithm  #AUTHOR_TAG.', 'top 1000 features are used in all the experiments ie.', 'the size of feature vector is not more than 1000.', '5 subjunctive feature evaluation  #TAUTHOR_TAG.', 'in order to compare subjunctive features against  #TAUTHOR_TAG ( politics and products ).', 'the evaluation metrics include precision, recall, and area under curve ( auc ) for the positive class.', 'auc was also used by  #TAUTHOR_TAG.', 'to the best of our knowledge, statistical classification based approach have not yet been employed to detect suggestions in reviews.', 'our experiment which uses subjunctive features for suggestion detection, is the first in this regard.', 'table 2 compares the auc values obtained with unigrams, subjunctive features, a combination of both, and the results from  #TAUTHOR_TAG for']",4
"['of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['##tive features over unigrams in the case of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['detection : unigrams vs subjunctive : one probable reason for the better performance of subjunctive features over unigrams in the case of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in the case of  #TAUTHOR_TAG, similar reason ( big dataset ) can be attributed for the better performance of unigrams over subjunctive features.', ' #TAUTHOR_TAG perform better than our subjunctive features for the politics data.', 'however, subjunctive features perform much better with product data as compared to the wish templates ( table 3 ).', 'this may lead to the conclusion that wish templates need larger training corpus, since they failed for the smaller dataset of product reviews ( auc less than 0. 5 ).', 'one additional benefit of subjunctive features could be that subjunctive mood appears in many languages, and thus such features can be easily extended to multi - lingual wish detection']",4
"['of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['##tive features over unigrams in the case of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['detection : unigrams vs subjunctive : one probable reason for the better performance of subjunctive features over unigrams in the case of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in the case of  #TAUTHOR_TAG, similar reason ( big dataset ) can be attributed for the better performance of unigrams over subjunctive features.', ' #TAUTHOR_TAG perform better than our subjunctive features for the politics data.', 'however, subjunctive features perform much better with product data as compared to the wish templates ( table 3 ).', 'this may lead to the conclusion that wish templates need larger training corpus, since they failed for the smaller dataset of product reviews ( auc less than 0. 5 ).', 'one additional benefit of subjunctive features could be that subjunctive mood appears in many languages, and thus such features can be easily extended to multi - lingual wish detection']",4
"['of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['##tive features over unigrams in the case of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in']","['detection : unigrams vs subjunctive : one probable reason for the better performance of subjunctive features over unigrams in the case of  #TAUTHOR_TAG, could be the small size of the dataset.', 'in the case of  #TAUTHOR_TAG, similar reason ( big dataset ) can be attributed for the better performance of unigrams over subjunctive features.', ' #TAUTHOR_TAG perform better than our subjunctive features for the politics data.', 'however, subjunctive features perform much better with product data as compared to the wish templates ( table 3 ).', 'this may lead to the conclusion that wish templates need larger training corpus, since they failed for the smaller dataset of product reviews ( auc less than 0. 5 ).', 'one additional benefit of subjunctive features could be that subjunctive mood appears in many languages, and thus such features can be easily extended to multi - lingual wish detection']",4
"['parser,  #TAUTHOR_TAG and  #AUTHOR_TAG can']","['parser,  #TAUTHOR_TAG and  #AUTHOR_TAG can']","['parser,  #TAUTHOR_TAG and  #AUTHOR_TAG can learn']","['from partial parses has been dealt in different ways in the literature.', "" #AUTHOR_TAG used post - projection completion / transformation rules to get full parse trees from the projections and train collin's parser  #AUTHOR_TAG on them."", ' #AUTHOR_TAG handle partial projected parses by avoiding committing to entire projected tree during training.', 'the posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation.', ' #AUTHOR_TAG refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees.', 'they deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees.', ' #AUTHOR_TAG requires full projected parses to train their parser,  #TAUTHOR_TAG and  #AUTHOR_TAG can learn from partially projected trees.', ""however, the discriminative training in  #TAUTHOR_TAG doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse."", 'by treating each relation in the projected dependency data independently as a classification instance for parsing,  #AUTHOR_TAG sacrifice the context of the relations such as global structural context, neighboring relations that are crucial for dependency analysis.', 'due to this, they report that the parser suffers from local optimization during training.', 'the parser proposed in this work ( section 3 ) learns from partial trees by using the available structural information in it and also in neighboring partial parses.', 'we evaluated our system ( section 5 ) on bulgarian and spanish projected dependency data used in  #TAUTHOR_TAG for comparison.', 'the same could not be carried out for chinese ( which was the language  #AUTHOR_TAG worked on ) due to the unavailability of projected data used in their work.', 'comparison with the traditional dependency parsers ( mc  #AUTHOR_TAG which train on complete dependency parsers is out of the scope of this work']",6
['of  #TAUTHOR_TAG. these test'],"['of  #TAUTHOR_TAG. these test datasets had sentences from the training section of the con', '##ll shared task  #AUTHOR_TAG that had lengths less than or equal']",['of  #TAUTHOR_TAG. these test'],"['', 'was released as part of the icon - 2010 shared task  #AUTHOR_TAG was used for evaluation. for bulgarian and spanish,', 'we used the same test data that was used in the work of  #TAUTHOR_TAG. these test datasets had sentences from the training section of the con', '##ll shared task  #AUTHOR_TAG that had lengths less than or equal to 10. all the test datasets have gold pos', 'tags. a baseline parser was built to compare learning from partial parses with learning from fully connected parses. full parses are constructed from partial parses in the projected data by randomly assigning parents to unconnected parents, similar to the work in  #AUTHOR_TAG. the unconnected words in the parse are selected randomly one by one and are assigned parents randomly to complete the parse. this process is', 'repeated for all the sentences in the three language datasets. the parser is then', 'trained with the gnppa algorithm on these fully connected parses to be used as the baseline. table 3 lists the accuracies of', 'the baseline, gnppa and e - gnppa parsers. the', 'accuracies are unlabeled attachment scores ( uas ) : the percentage of words with the correct head. table 4 compares our accuracies with those reported in  #TAUTHOR_TAG for', 'bulgarian and spanish']",5
['of  #TAUTHOR_TAG. these test'],"['of  #TAUTHOR_TAG. these test datasets had sentences from the training section of the con', '##ll shared task  #AUTHOR_TAG that had lengths less than or equal']",['of  #TAUTHOR_TAG. these test'],"['', 'was released as part of the icon - 2010 shared task  #AUTHOR_TAG was used for evaluation. for bulgarian and spanish,', 'we used the same test data that was used in the work of  #TAUTHOR_TAG. these test datasets had sentences from the training section of the con', '##ll shared task  #AUTHOR_TAG that had lengths less than or equal to 10. all the test datasets have gold pos', 'tags. a baseline parser was built to compare learning from partial parses with learning from fully connected parses. full parses are constructed from partial parses in the projected data by randomly assigning parents to unconnected parents, similar to the work in  #AUTHOR_TAG. the unconnected words in the parse are selected randomly one by one and are assigned parents randomly to complete the parse. this process is', 'repeated for all the sentences in the three language datasets. the parser is then', 'trained with the gnppa algorithm on these fully connected parses to be used as the baseline. table 3 lists the accuracies of', 'the baseline, gnppa and e - gnppa parsers. the', 'accuracies are unlabeled attachment scores ( uas ) : the percentage of words with the correct head. table 4 compares our accuracies with those reported in  #TAUTHOR_TAG for', 'bulgarian and spanish']",5
['of  #TAUTHOR_TAG. these test'],"['of  #TAUTHOR_TAG. these test datasets had sentences from the training section of the con', '##ll shared task  #AUTHOR_TAG that had lengths less than or equal']",['of  #TAUTHOR_TAG. these test'],"['', 'was released as part of the icon - 2010 shared task  #AUTHOR_TAG was used for evaluation. for bulgarian and spanish,', 'we used the same test data that was used in the work of  #TAUTHOR_TAG. these test datasets had sentences from the training section of the con', '##ll shared task  #AUTHOR_TAG that had lengths less than or equal to 10. all the test datasets have gold pos', 'tags. a baseline parser was built to compare learning from partial parses with learning from fully connected parses. full parses are constructed from partial parses in the projected data by randomly assigning parents to unconnected parents, similar to the work in  #AUTHOR_TAG. the unconnected words in the parse are selected randomly one by one and are assigned parents randomly to complete the parse. this process is', 'repeated for all the sentences in the three language datasets. the parser is then', 'trained with the gnppa algorithm on these fully connected parses to be used as the baseline. table 3 lists the accuracies of', 'the baseline, gnppa and e - gnppa parsers. the', 'accuracies are unlabeled attachment scores ( uas ) : the percentage of words with the correct head. table 4 compares our accuracies with those reported in  #TAUTHOR_TAG for', 'bulgarian and spanish']",5
['baseline reported in  #TAUTHOR_TAG significantly'],['baseline reported in  #TAUTHOR_TAG significantly'],['baseline reported in  #TAUTHOR_TAG significantly'],"['baseline reported in  #TAUTHOR_TAG significantly outperforms our baseline ( see table 4 ) due to the different baselines used in both the works.', 'in our work, while creating the data for the baseline by assigning random parents to unconnected words, acyclicity and projectivity con - table 4 : comparison of baseline, gnppa and e - gnppa with baseline and discriminative model from  #TAUTHOR_TAG for bulgarian and spanish.', ""evaluation didn't include punctuation."", 'straints are not enforced.', "" #AUTHOR_TAG's baseline is similar to the first iteration of their discriminative model and hence performs better than ours."", 'our bulgarian e - gnppa parser achieved a 1. 8 % gain over theirs while the spanish results are lower.', 'though their training data size is also 10k, the training data is different in both our works due to the difference in the method of choosing 10k sentences from the large projected treebanks.', 'the gnppa accuracies ( see table 3 ) for all the three languages are significant improvements over the baseline accuracies.', 'this shows that learning from partial parses is effective when compared to imposing the connected constraint on the partially projected dependency parse.', 'even while projecting source dependencies during data creation, it is better to project high confidence relations than look to project more relations and thereby introduce noise.', 'the e - gnppa which also learns from partially contiguous partial parses achieved statistically significant gains for all the three languages.', 'the gains across languages is due to the fact that in the 10k data that was used for training, e - gnppa parser could learn 2 − 5 % more relations over gnppa ( see table 2 ).', 'figure 6 shows the accuracies of baseline and e - gnppa parser for the three languages when training data size is varied.', 'the parsers peak early with less than 1000 sentences and make small gains with the addition of more data']",4
"['and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for']","['and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for']","['to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the']","['iadaatpa 1 project coded as n • 2016 - eu - ia - 0132 that ended in february 2019 is made for building of customized, domain - specific engines for public administrations from eu member states.', 'the consortium of the project decided to use neural machine translation at the beginning of the project.', 'this represented a challenge for all involved, and the positive aspect is that all public administrations engaged in the iadaatpa project were able to try, test and use state - of - the - art neural technology with a high level of satisfaction.', 'one of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #AUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG engine customization the data was cleaned using the bicleaner tool ( sanchez -  #AUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the']",5
"['and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for']","['and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for']","['to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra token was added at target - side and there was no need for apriori domain information.', 'this approach allowed the model to improve the quality for each domain']","['of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #AUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG.', 'engine customization the data was cleaned using the bicleaner tool ( sanchez -  #AUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #TAUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra token was added at target - side and there was no need for apriori domain information.', 'this approach allowed the model to improve the quality for each domain']",5
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['', 'referential translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qe']",0
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['', 'referential translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qe']",0
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['##tial translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qet provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation.', 'rtms are powerful enough to be applicable in different domains and tasks while achieving top performance']",0
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['##tial translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qet provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation.', 'rtms are powerful enough to be applicable in different domains and tasks while achieving top performance']",0
"[') ( bicici, 2013 ;  #TAUTHOR_TAG for']","['sentence and document translation prediction tasks and global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd ) ( bicici, 2013 ;  #TAUTHOR_TAG for']","[') ( bicici, 2013 ;  #TAUTHOR_TAG for']","['present results using support vector regression ( svr ) with rbf ( radial basis functions ) kernel ( smola and scholkopf, 2004 ) for sentence and document translation prediction tasks and global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd ) ( bicici, 2013 ;  #TAUTHOR_TAG for word - level translation performance prediction.', 'we also use these learning models after a feature subset selection ( fs ) with recursive feature elimination ( rfe )  #AUTHOR_TAG or a dimensionality reduction and mapping step using partial least squares ( pls )  #AUTHOR_TAG, or pls after fs ( fs + pls ).', 'glm relies on viterbi decoding, perceptron learning, and flexible feature definitions.', 'glmd extends the glm framework by parallel perceptron training ( mc  #AUTHOR_TAG and dynamic learning with adaptive weight updates in the perceptron learning algorithm :', 'where φ returns a global representation for instance i and the weights are updated by α, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates.', 'the learning rate updates the weight values with weights in the range [ a, b ] using the following function taking error rate as the input :', 'learning rate curve for a = 0. 5 and b = 1. 0 is provided in figure 2']",0
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['', 'referential translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qe']",1
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['##tial translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qet provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation.', 'rtms are powerful enough to be applicable in different domains and tasks while achieving top performance']",1
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['', 'referential translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qe']",4
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['##tial translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qet provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation.', 'rtms are powerful enough to be applicable in different domains and tasks while achieving top performance']",4
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['', 'referential translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qe']",5
"['rtm models  #TAUTHOR_TAG :', '• by using improved par']","['rtm models  #TAUTHOR_TAG :', '• by using improved']","['rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models']","['##tial translation machines are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants.', 'rtms achieve top performance in automatic, accurate, and language independent prediction of machine translation performance and reduce our dependence on any task dependent resource.', 'prediction of translation performance can help in estimating the effort required for correcting the translations during post - editing by human translators.', 'we improve our rtm models  #TAUTHOR_TAG :', '• by using improved parfda instance selection model allowing better language models ( lm ) in which similarity judgments are made to be built with improved optimization and selection of the lm data,', '• by selecting treef features over source and translation data jointly instead of taking their intersection,', '• with extended learning models including bayesian ridge regression  #AUTHOR_TAG, which did not obtain better performance than support vector regression in training results ( section 2. 2 ).', 'we present top results with referential translation machines ( bicici, 2015 ;  #TAUTHOR_TAG at quality estimation task ( qet15 ) in wmt15  #AUTHOR_TAG.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', 'rtms use machine translation performance prediction ( mtpp ) system bicici, 2015 ), which is a state - of - the - art performance predictor of translation even without using the translation by using only the source.', 'we use parfda for selecting the interpretants bicici and  #AUTHOR_TAG and build an mtpp model.', 'mtpp derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.', 'we view that acts of translation are ubiquitously used during communication :', 'every act of communication is an act of translation  #AUTHOR_TAG.', 'figure 1 depicts rtm.', 'our encouraging results in qet provides a greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation.', 'rtms are powerful enough to be applicable in different domains and tasks while achieving top performance']",5
"[') ( bicici, 2013 ;  #TAUTHOR_TAG for']","['sentence and document translation prediction tasks and global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd ) ( bicici, 2013 ;  #TAUTHOR_TAG for']","[') ( bicici, 2013 ;  #TAUTHOR_TAG for']","['present results using support vector regression ( svr ) with rbf ( radial basis functions ) kernel ( smola and scholkopf, 2004 ) for sentence and document translation prediction tasks and global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd ) ( bicici, 2013 ;  #TAUTHOR_TAG for word - level translation performance prediction.', 'we also use these learning models after a feature subset selection ( fs ) with recursive feature elimination ( rfe )  #AUTHOR_TAG or a dimensionality reduction and mapping step using partial least squares ( pls )  #AUTHOR_TAG, or pls after fs ( fs + pls ).', 'glm relies on viterbi decoding, perceptron learning, and flexible feature definitions.', 'glmd extends the glm framework by parallel perceptron training ( mc  #AUTHOR_TAG and dynamic learning with adaptive weight updates in the perceptron learning algorithm :', 'where φ returns a global representation for instance i and the weights are updated by α, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates.', 'the learning rate updates the weight values with weights in the range [ a, b ] using the following function taking error rate as the input :', 'learning rate curve for a = 0. 5 and b = 1. 0 is provided in figure 2']",5
"['subtask and use glmd model ( bicici, 2013 ;  #TAUTHOR_TAG,']","['subtask and use glmd model ( bicici, 2013 ;  #TAUTHOR_TAG,']","['- level translation quality task 2 is about binary classification of word - level quality.', 'we develop individual rtm models for each subtask and use glmd model ( bicici, 2013 ;  #TAUTHOR_TAG,']","['1 : predicting the hter for sentence translations the results on the test set are given in table 4.', 'rank lists the overall ranking in the task out of about 9 submissions.', 'we obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties.', 'rtms with fs followed by pls and learning with svr is able to achieve the top rank in this task.', 'task 2 : prediction of word - level translation quality task 2 is about binary classification of word - level quality.', 'we develop individual rtm models for each subtask and use glmd model ( bicici, 2013 ;  #TAUTHOR_TAG, for predicting the quality at the word - level.', 'the results on the test set are in table 5 where the ranks are out of about 17 submissions.', 'rtms with glmd becomes the second best system this task.', 'task 3 : predicting meteor of document translations task 3 is about predicting me - teor  #AUTHOR_TAG and their ranking.', 'the results on the test set are given in table 4 where the ranks are out of about 6 submissions using wf 1.', 'rtms achieve top rankings in this task.', 'table 5 : rtm - dcu task 2 results on the test set.', 'wf 1 is the average weighted f 1 score']",5
"['##14  #TAUTHOR_TAG, and']","['qet14  #TAUTHOR_TAG, and']","['##15, qet14  #TAUTHOR_TAG, and']","['compare the difficulty of tasks according to mraer levels achieved.', 'in table 6, we list the rtm test results for tasks and subtasks that predict hter or meteor from qet15, qet14  #TAUTHOR_TAG, and qet13 ( bicici, 2013 ).', 'the best results when predicting hter are obtained this year']",5
['##14  #TAUTHOR_TAG and qe'],"['predicting hter or meteor also including results from qet14  #TAUTHOR_TAG and qet13 ( bicici, 2013 )']",['##14  #TAUTHOR_TAG and qe'],"['##tial translation machines achieve top performance in automatic, accurate, and language independent prediction of document -, sentence -, and word - level statistical machine translation ( smt ) performance.', 'rtms remove the need to access any smt system specific information or prior knowledge of the training data or models used when generating the translations.', 'rtms achieve top performance when predicting translation performance.', 'table 6 : test performance of the top individual rtm results when predicting hter or meteor also including results from qet14  #TAUTHOR_TAG and qet13 ( bicici, 2013 )']",7
['up on the span - based parser  #TAUTHOR_TAG ;'],['up on the span - based parser  #TAUTHOR_TAG ;'],"['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ;']","['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ; it employs the strong generalization power of bi - directional lstms, and parses efficiently and robustly with an extremely simple span - based feature set that does not use any tree structure information.', 'we make the following contributions :', '']",4
['up on the span - based parser  #TAUTHOR_TAG ;'],['up on the span - based parser  #TAUTHOR_TAG ;'],"['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ;']","['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ; it employs the strong generalization power of bi - directional lstms, and parses efficiently and robustly with an extremely simple span - based feature set that does not use any tree structure information.', 'we make the following contributions :', '']",4
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"[', the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans']","['mentioned above, the input sequences are substantially longer than ptb parsing, so we choose linear - time parsing, by adapting a popular greedy constituency parser, the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans.', '']",4
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"[', the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans']","['mentioned above, the input sequences are substantially longer than ptb parsing, so we choose linear - time parsing, by adapting a popular greedy constituency parser, the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans.', '']",4
['values suggested by  #TAUTHOR_TAG'],['values suggested by  #TAUTHOR_TAG'],"['of the hyperparameters we settle with the same values suggested by  #TAUTHOR_TAG.', 'to alleviate the overfitting problem']","['', 'we randomly choose 30 documents from the training set as the development set.', 'we tune the hyperparameters of the neural model on the development set.', 'for most of the hyperparameters we settle with the same values suggested by  #TAUTHOR_TAG.', 'to alleviate the overfitting problem for training on the relative small rst treebank, we use a dropout of 0. 5.', 'one particular hyperparameter is that we use a value β to balance the chances between training following the exploration ( i. e., the best action chosen by the neural model ) and following the correct path provided by the dynamic oracle.', 'we find that β = 0. 8, i. e., following the dynamic oracle with asyntactic feats.', 'segmentation structure + nuclearity + relation  #AUTHOR_TAG segmentation only table 2 : f1 scores of end - to - end systems. "" + nuclearity "" indicates scoring of tree structures with nuclearity included. "" + relation "" has both nuclearity and relation included ( e. g., ←elaboration ).', 'syntactic feats structure + nuclearity + relation human annotation  #AUTHOR_TAG table 3 : experiments using gold segmentations.', '']",4
['up on the span - based parser  #TAUTHOR_TAG ;'],['up on the span - based parser  #TAUTHOR_TAG ;'],"['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ;']","['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ; it employs the strong generalization power of bi - directional lstms, and parses efficiently and robustly with an extremely simple span - based feature set that does not use any tree structure information.', 'we make the following contributions :', '']",6
['up on the span - based parser  #TAUTHOR_TAG ;'],['up on the span - based parser  #TAUTHOR_TAG ;'],"['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ;']","['', 'our algorithm builds up on the span - based parser  #TAUTHOR_TAG ; it employs the strong generalization power of bi - directional lstms, and parses efficiently and robustly with an extremely simple span - based feature set that does not use any tree structure information.', 'we make the following contributions :', '']",6
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"[', the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans']","['mentioned above, the input sequences are substantially longer than ptb parsing, so we choose linear - time parsing, by adapting a popular greedy constituency parser, the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans.', '']",6
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"[', the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans']","['mentioned above, the input sequences are substantially longer than ptb parsing, so we choose linear - time parsing, by adapting a popular greedy constituency parser, the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans.', '']",3
['lstm model in  #TAUTHOR_TAG'],['the bi - directional lstm model in  #TAUTHOR_TAG'],['the bi - directional lstm model in  #TAUTHOR_TAG'],"['scoring functions in the deductive system ( figure 4 ) are calculated by an underlying neural model, which is similar to the bi - directional lstm model in  #TAUTHOR_TAG that evaluates based on span boundary features.', 'again, it is important to note that no discourse or syntactic tree structures are represented in the features.', '']",3
['lstm model in  #TAUTHOR_TAG'],['the bi - directional lstm model in  #TAUTHOR_TAG'],['the bi - directional lstm model in  #TAUTHOR_TAG'],"['scoring functions in the deductive system ( figure 4 ) are calculated by an underlying neural model, which is similar to the bi - directional lstm model in  #TAUTHOR_TAG that evaluates based on span boundary features.', 'again, it is important to note that no discourse or syntactic tree structures are represented in the features.', '']",3
['values suggested by  #TAUTHOR_TAG'],['values suggested by  #TAUTHOR_TAG'],"['of the hyperparameters we settle with the same values suggested by  #TAUTHOR_TAG.', 'to alleviate the overfitting problem']","['', 'we randomly choose 30 documents from the training set as the development set.', 'we tune the hyperparameters of the neural model on the development set.', 'for most of the hyperparameters we settle with the same values suggested by  #TAUTHOR_TAG.', 'to alleviate the overfitting problem for training on the relative small rst treebank, we use a dropout of 0. 5.', 'one particular hyperparameter is that we use a value β to balance the chances between training following the exploration ( i. e., the best action chosen by the neural model ) and following the correct path provided by the dynamic oracle.', 'we find that β = 0. 8, i. e., following the dynamic oracle with asyntactic feats.', 'segmentation structure + nuclearity + relation  #AUTHOR_TAG segmentation only table 2 : f1 scores of end - to - end systems. "" + nuclearity "" indicates scoring of tree structures with nuclearity included. "" + relation "" has both nuclearity and relation included ( e. g., ←elaboration ).', 'syntactic feats structure + nuclearity + relation human annotation  #AUTHOR_TAG table 3 : experiments using gold segmentations.', '']",3
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"[', the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans']","['mentioned above, the input sequences are substantially longer than ptb parsing, so we choose linear - time parsing, by adapting a popular greedy constituency parser, the span - based constituency parser of  #TAUTHOR_TAG.', 'as in span - based parsing, at each step, we maintain a a stack of spans.', '']",5
['lstm model in  #TAUTHOR_TAG'],['the bi - directional lstm model in  #TAUTHOR_TAG'],['the bi - directional lstm model in  #TAUTHOR_TAG'],"['scoring functions in the deductive system ( figure 4 ) are calculated by an underlying neural model, which is similar to the bi - directional lstm model in  #TAUTHOR_TAG that evaluates based on span boundary features.', 'again, it is important to note that no discourse or syntactic tree structures are represented in the features.', '']",5
['values suggested by  #TAUTHOR_TAG'],['values suggested by  #TAUTHOR_TAG'],"['of the hyperparameters we settle with the same values suggested by  #TAUTHOR_TAG.', 'to alleviate the overfitting problem']","['', 'we randomly choose 30 documents from the training set as the development set.', 'we tune the hyperparameters of the neural model on the development set.', 'for most of the hyperparameters we settle with the same values suggested by  #TAUTHOR_TAG.', 'to alleviate the overfitting problem for training on the relative small rst treebank, we use a dropout of 0. 5.', 'one particular hyperparameter is that we use a value β to balance the chances between training following the exploration ( i. e., the best action chosen by the neural model ) and following the correct path provided by the dynamic oracle.', 'we find that β = 0. 8, i. e., following the dynamic oracle with asyntactic feats.', 'segmentation structure + nuclearity + relation  #AUTHOR_TAG segmentation only table 2 : f1 scores of end - to - end systems. "" + nuclearity "" indicates scoring of tree structures with nuclearity included. "" + relation "" has both nuclearity and relation included ( e. g., ←elaboration ).', 'syntactic feats structure + nuclearity + relation human annotation  #AUTHOR_TAG table 3 : experiments using gold segmentations.', '']",5
"['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand']","['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand']","['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand']","['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand them to include morphologically complex languages.', 'we learn vector representations for dutch, french, german, and spanish with the word2vec tool, and investigate to what extent inflectional information is preserved across vectors.', 'we observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language']",5
"['order to to validate our methodology, we first replicate the results of  #TAUTHOR_TAG on english syntactic analogies']","['order to to validate our methodology, we first replicate the results of  #TAUTHOR_TAG on english syntactic analogies']","['order to to validate our methodology, we first replicate the results of  #TAUTHOR_TAG on english syntactic analogies']","['order to to validate our methodology, we first replicate the results of  #TAUTHOR_TAG on english syntactic analogies']",5
"['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set']","['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results']","['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results']","['table 1, we report two numbers for each part of speech.', 'the first, labeled as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results match the results reported in their paper, except for the nominal results, which reflect our modifications described in section 2. 2.', '']",5
"['several changes to the construction of syntactic analogy questions.', 'we follow the methodology of  #TAUTHOR_TAG in limiting analogy questions to the 100 most frequent verbs or nouns.', 'the frequencies are']","['several changes to the construction of syntactic analogy questions.', 'we follow the methodology of  #TAUTHOR_TAG in limiting analogy questions to the 100 most frequent verbs or nouns.', 'the frequencies are']","['several changes to the construction of syntactic analogy questions.', 'we follow the methodology of  #TAUTHOR_TAG in limiting analogy questions to the 100 most frequent verbs or nouns.', 'the frequencies are obtained from corpora tagged by treetagger  #AUTHOR_TAG.', 'we identify inflections using manually constructed inflection tables from several sources.', 'spanish']","['order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions.', 'we follow the methodology of  #TAUTHOR_TAG in limiting analogy questions to the 100 most frequent verbs or nouns.', 'the frequencies are obtained from corpora tagged by treetagger  #AUTHOR_TAG.', 'we identify inflections using manually constructed inflection tables from several sources.', 'spanish and german verbal inflections, as well as german nominal inflections, are from a wiktionary data set introduced by durrett and de  #AUTHOR_TAG.', '4 dutch verbal inflections and english verbal and nominal inflections are from the celex database  #AUTHOR_TAG.', 'french verbal inflections are from verbiste, an online french conjugation dictionary.', '5 whereas mikolov et al. create analogies from various inflectional forms, we require the analogies to always include the base dictionary form : the infinitive for verbs, and the nominative singular for nouns.', 'in other words, all analogies are limited to 4 we exclude finnish because of its high morphological complexity and the small size of the corresponding polyglot corpus.', '5 http : / / perso. b2b2c. ca / sarrazip / dev / verbiste. html comparisons between the base form and an inflected form.', 'this is to prevent a combinatorial explosion of the number of analogies in languages that contain dozens of different inflection forms.', 'we also create new english test sets using this methodology, in order to ensure a fair cross - lingual comparison.', 'table 2 shows the number of analogy questions for each language set.', 'note that the languages are ordered according to increasing morphological complexity.', 'following mikolov et al., we ensure that all analogies contain at least one pair of non - syncretic forms.', 'it would make little sense to include analogies such as "" set is to set as put is to? "" because both verbs in question have the same present and past tense form.', 'however, we do allow analogies which involve syncretic forms for one half of the analogy.', 'for example, either taken or took is a correct answer to "" play is to played as take is to? "".', '']",5
"['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand']","['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand']","['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand']","['replicate the syntactic experiments of  #TAUTHOR_TAG on english, and expand them to include morphologically complex languages.', 'we learn vector representations for dutch, french, german, and spanish with the word2vec tool, and investigate to what extent inflectional information is preserved across vectors.', 'we observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language']",6
"['introduction  #TAUTHOR_TAG demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language.', 'they observe that by manipulating vector offsets']","['introduction  #TAUTHOR_TAG demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language.', 'they observe that by manipulating vector offsets']","['introduction  #TAUTHOR_TAG demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language.', 'they observe that by manipulating vector offsets']","['introduction  #TAUTHOR_TAG demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language.', 'they observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen ≈ king − man + woman.', 'similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate ; the result of composing such an offset with the base form cook may turn out to be similar to the vector for cooked ( figure 1 ).', 'they report state - of - the - art results on a set of analogy questions of the form "" a is to b as c is to "", where the variables represent various english word forms.', ""our work is motivated by two observations regarding mikolov et al.'s experiments : first, the syntactic analogies that they test correspond to morphological inflections, and second, the tests only evaluate english, a language with little morphological complexity."", 'in this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than english : dutch, french, german, and spanish']",0
['vectors of  #TAUTHOR_TAG were trained on'],"['vectors of  #TAUTHOR_TAG were trained on 320m tokens of broadcast news data, as described by  #AUTHOR_TAG.', 'since we have no access to this data, we instead train english vectors on a corpus']",['vectors of  #TAUTHOR_TAG were trained on'],"['vectors of  #TAUTHOR_TAG were trained on 320m tokens of broadcast news data, as described by  #AUTHOR_TAG.', 'since we have no access to this data, we instead train english vectors on a corpus from the polyglot project ( al -  #AUTHOR_TAG, which contains tokenized wikipedia dumps intended for the training of vector - space models.', 'for comparison with the results of  #TAUTHOR_TAG, we limit the data to the first 320m lowercased tokens of the corpus.', ' #TAUTHOR_TAG obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed.', 'instead, we take as a point of reference their second - best model, which employs 640 - dimensional vectors produced by a single recursive neural network ( rnn ) language model.', '1 rather than use an rnn model to learn our own vectors, we employ the far simpler skip - gram model.', ' #AUTHOR_TAG a ) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train.', 'the skip - gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words.', 'the word2vec package includes code for learning skip - gram models from very large corpora.', '2 we train 640 - dimensional vectors using the skip - gram model with a hierarchical softmax, a context window of 10, sub - sampling of 1e - 3, and a minimum frequency threshold of 10']",0
['vectors of  #TAUTHOR_TAG were trained on'],"['vectors of  #TAUTHOR_TAG were trained on 320m tokens of broadcast news data, as described by  #AUTHOR_TAG.', 'since we have no access to this data, we instead train english vectors on a corpus']",['vectors of  #TAUTHOR_TAG were trained on'],"['vectors of  #TAUTHOR_TAG were trained on 320m tokens of broadcast news data, as described by  #AUTHOR_TAG.', 'since we have no access to this data, we instead train english vectors on a corpus from the polyglot project ( al -  #AUTHOR_TAG, which contains tokenized wikipedia dumps intended for the training of vector - space models.', 'for comparison with the results of  #TAUTHOR_TAG, we limit the data to the first 320m lowercased tokens of the corpus.', ' #TAUTHOR_TAG obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed.', 'instead, we take as a point of reference their second - best model, which employs 640 - dimensional vectors produced by a single recursive neural network ( rnn ) language model.', '1 rather than use an rnn model to learn our own vectors, we employ the far simpler skip - gram model.', ' #AUTHOR_TAG a ) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train.', 'the skip - gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words.', 'the word2vec package includes code for learning skip - gram models from very large corpora.', '2 we train 640 - dimensional vectors using the skip - gram model with a hierarchical softmax, a context window of 10, sub - sampling of 1e - 3, and a minimum frequency threshold of 10']",1
['vectors of  #TAUTHOR_TAG were trained on'],"['vectors of  #TAUTHOR_TAG were trained on 320m tokens of broadcast news data, as described by  #AUTHOR_TAG.', 'since we have no access to this data, we instead train english vectors on a corpus']",['vectors of  #TAUTHOR_TAG were trained on'],"['vectors of  #TAUTHOR_TAG were trained on 320m tokens of broadcast news data, as described by  #AUTHOR_TAG.', 'since we have no access to this data, we instead train english vectors on a corpus from the polyglot project ( al -  #AUTHOR_TAG, which contains tokenized wikipedia dumps intended for the training of vector - space models.', 'for comparison with the results of  #TAUTHOR_TAG, we limit the data to the first 320m lowercased tokens of the corpus.', ' #TAUTHOR_TAG obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed.', 'instead, we take as a point of reference their second - best model, which employs 640 - dimensional vectors produced by a single recursive neural network ( rnn ) language model.', '1 rather than use an rnn model to learn our own vectors, we employ the far simpler skip - gram model.', ' #AUTHOR_TAG a ) show that higher accuracy can be obtained using vectors derived using this model, which is also far less expensive to train.', 'the skip - gram model eschews a language modeling objective in favor of a logistic regression classifier that predicts surrounding words.', 'the word2vec package includes code for learning skip - gram models from very large corpora.', '2 we train 640 - dimensional vectors using the skip - gram model with a hierarchical softmax, a context window of 10, sub - sampling of 1e - 3, and a minimum frequency threshold of 10']",4
"['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set']","['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results']","['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results']","['table 1, we report two numbers for each part of speech.', 'the first, labeled as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results match the results reported in their paper, except for the nominal results, which reflect our modifications described in section 2. 2.', '']",4
"['test set of  #TAUTHOR_TAG is publicly available 3.', 'they extract their gold standard inflections, as well as frequency counts, from tagged newspaper text.', 'their test set was constructed as follows : after tagging 267m words, the 100']","['test set of  #TAUTHOR_TAG is publicly available 3.', 'they extract their gold standard inflections, as well as frequency counts, from tagged newspaper text.', 'their test set was constructed as follows : after tagging 267m words, the 100']","['test set of  #TAUTHOR_TAG is publicly available 3.', 'they extract their gold standard inflections, as well as frequency counts, from tagged newspaper text.', 'their test set was constructed as follows : after tagging 267m words, the 100 most frequent plural nouns, possessive nouns, comparative adjectives,']","['test set of  #TAUTHOR_TAG is publicly available 3.', 'they extract their gold standard inflections, as well as frequency counts, from tagged newspaper text.', 'their test set was constructed as follows : after tagging 267m words, the 100 most frequent plural nouns, possessive nouns, comparative adjectives, and verbal infinitives were selected.', 'each was paired with 5 randomly - selected words of the same part - of - speech, and analogy questions were constructed for each word pair.', 'for example, for the pair people and city, two questions are created : people : person : : cities : city, and its mirror : person : people : : city : cities.', 'to solve the analogies in this test set, we apply the word - analogy tool that is included with word2vec.', 'for each analogy a : b : : c :?, the tool searches the entire vocabulary for the vector d that is most similar to the vector estimated by performing a linear analogy on the query triplet a, b, c :', 'we calculate accuracy as the percentage of analogies whose answers are correctly predicted, according to an exact match.', 'the analogies involve nouns, adjectives, and verbs.', 'nominal analogies consist of comparisons between singular and plural forms, and possessive and nominative forms.', 'due to the tokenization method used in our training corpus, we are unable to build vectors for english possessives.', 'we therefore modify the nominal test set to only include questions that contain the singular vs. plural distinction.', 'we make no changes to the adjectival and verbal analogy sets.', 'the adjectival set contains analogies between the comparative and the superlative, the comparative and the base, and the superlative and the base.', 'the verbal set includes comparisons between the preterite, the infinitive, and the 3rd person singular present, but not the past and present participles']",7
"['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set']","['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results']","['as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results']","['table 1, we report two numbers for each part of speech.', 'the first, labeled as m13, is the result of applying the vectors of  #TAUTHOR_TAG to their test set.', 'the results match the results reported in their paper, except for the nominal results, which reflect our modifications described in section 2. 2.', '']",3
"['methods to different collections  #TAUTHOR_TAG, and increasing compatibility of']","['methods to different collections  #TAUTHOR_TAG, and increasing compatibility of']","['to different collections  #TAUTHOR_TAG, and increasing compatibility of different annotations']","['researchers to share their interests was rewarded by 34 submissions ( 5 posters and 19 full papers ).', 'of those, 10 were accepted as full papers and 18 as poster presentations.', 'the combined expertise of the program committee allowed for providing three thorough reviews for each paper.', 'the exceptionally high quality manuscripts accepted for presentation cover a wide area of subjects in clinical and biological areas, as well as methodological issues applicable to both sublanguages.', 'named entity recognition ( ner ) continues to be an active area of research.', 'ner research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [ 1 ], protein [ 2 ], chemical names [ 3 ], and clinical entities.', 'overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.', 'this is probably the reason for growing interest in creation of annotated corpora [ 4 ], development of methods for augmenting the existing annotation [ 5 ], speeding up the annotation process [ 5 ], and reducing its cost ; evaluating the comparability of results obtained applying the same methods to different collections  #TAUTHOR_TAG, and increasing compatibility of different annotations [ 7 ].', 'increasingly sophisticated relation extraction methods  #TAUTHOR_TAG 8 ] are being applied to a broader set of iii relations [ 9 ].', 'other steps towards deeper understanding of the text include methods for creation of gene profiles [ 10 ], identification of uncertainty [ 11 ], discourse connectivity [ 12 ], and temporal features of clinical conditions [ 13 ].', 'the applicability of nlp methods to clinical tasks is explored in the work on identification of language impairments [ 14 ] and seriousness of suicidal attempts [ 15 ].', 'finally, application of nlp methods to classic information retrieval problems such as automatic indexing of biomedical literature [ 16 ] and the newer information retrieval problem of image retrieval [ 17']",0
"['methods to different collections  #TAUTHOR_TAG, and increasing compatibility of']","['methods to different collections  #TAUTHOR_TAG, and increasing compatibility of']","['to different collections  #TAUTHOR_TAG, and increasing compatibility of different annotations']","['researchers to share their interests was rewarded by 34 submissions ( 5 posters and 19 full papers ).', 'of those, 10 were accepted as full papers and 18 as poster presentations.', 'the combined expertise of the program committee allowed for providing three thorough reviews for each paper.', 'the exceptionally high quality manuscripts accepted for presentation cover a wide area of subjects in clinical and biological areas, as well as methodological issues applicable to both sublanguages.', 'named entity recognition ( ner ) continues to be an active area of research.', 'ner research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [ 1 ], protein [ 2 ], chemical names [ 3 ], and clinical entities.', 'overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.', 'this is probably the reason for growing interest in creation of annotated corpora [ 4 ], development of methods for augmenting the existing annotation [ 5 ], speeding up the annotation process [ 5 ], and reducing its cost ; evaluating the comparability of results obtained applying the same methods to different collections  #TAUTHOR_TAG, and increasing compatibility of different annotations [ 7 ].', 'increasingly sophisticated relation extraction methods  #TAUTHOR_TAG 8 ] are being applied to a broader set of iii relations [ 9 ].', 'other steps towards deeper understanding of the text include methods for creation of gene profiles [ 10 ], identification of uncertainty [ 11 ], discourse connectivity [ 12 ], and temporal features of clinical conditions [ 13 ].', 'the applicability of nlp methods to clinical tasks is explored in the work on identification of language impairments [ 14 ] and seriousness of suicidal attempts [ 15 ].', 'finally, application of nlp methods to classic information retrieval problems such as automatic indexing of biomedical literature [ 16 ] and the newer information retrieval problem of image retrieval [ 17']",0
"['by  #TAUTHOR_TAG.', 'further']","['lexicalized bilm by  #TAUTHOR_TAG.', 'further']","['by  #TAUTHOR_TAG.', 'further improvements of']","['paper presents a novel approach to improve reordering in phrase - based machine translation by using richer, syntactic representations of units of bilingual language models ( bilms ).', 'our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm.', 'the approach is evaluated in a series of arabicenglish and chinese - english translation experiments.', 'the best models demonstrate significant improvements in bleu and ter over the phrase - based baseline, as well as over the lexicalized bilm by  #TAUTHOR_TAG.', '']",0
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",0
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",0
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",0
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",0
"[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events']","['including both source and target information into the representation of translation events we ob - tain a bilingual lm.', 'the richer representation allows for a finer distinction between reorderings.', 'for example, arabic has a morphological marker of definiteness on both nouns and adjectives.', 'if we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model.', 'this kind of intuition underlies the model of  #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events t 1,..., t n as follows :', 'where e i is the i - th target word and a : e → p ( f ) is an alignment function, e and f referring to target and source sentences, and p ( · ) is the powerset function.', 'in other words, the i - th translation event consists of the i - th target word and all source words aligned to it.', ' #TAUTHOR_TAG refer to the defined translation events t i as bilingual tokens and we adopt this terminology.', 'there are alternative definitions of bilingual language models.', '']",0
"[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events']","['including both source and target information into the representation of translation events we ob - tain a bilingual lm.', 'the richer representation allows for a finer distinction between reorderings.', 'for example, arabic has a morphological marker of definiteness on both nouns and adjectives.', 'if we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model.', 'this kind of intuition underlies the model of  #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events t 1,..., t n as follows :', 'where e i is the i - th target word and a : e → p ( f ) is an alignment function, e and f referring to target and source sentences, and p ( · ) is the powerset function.', 'in other words, the i - th translation event consists of the i - th target word and all source words aligned to it.', ' #TAUTHOR_TAG refer to the defined translation events t i as bilingual tokens and we adopt this terminology.', 'there are alternative definitions of bilingual language models.', '']",0
"['##ed categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of']","['reordering is the syntactic role or context of a word.', 'by using unnecessarily fine - grained categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of']","['##ordering is the syntactic role or context of a word.', 'by using unnecessarily fine - grained categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of the original bilm, where words']","['mentioned in the introduction, lexical information is not very well - suited to capture reordering regularities.', 'consider figure 2. a.', 'the extracted sequence of bilingual tokens is produced by aligning source words with respect to target words ( so that they are in the same order ), as demonstrated by the shaded part of the picture.', 'if we substituted the arabic translation of egyptian for the arabic translation of israeli, the reordering should remain the same. what matters for reordering is the syntactic role or context of a word.', 'by using unnecessarily fine - grained categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of the original bilm, where words are substituted by their pos tags ( figure 2. a, shaded part ).', 'also, however, pos information by itself may be insufficiently expressive to separate cor -, it still is a likely sequence.', 'indeed, the log - probabilities of the two sequences with respect to a 4 - gram bilm model 5 result in a higher probability of −10. 25 for the incorrect reordering than for the correct one ( −10. 39 ).', 'since fully lexicalized bilingual tokens suffer from data sparsity and pos - based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.', '5 section 4 contains details about data and software setup']",0
"['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained']","['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained']","['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained']","['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained and integrated into a phrase - based decoder']",0
['two variants of bilm discussed by  #TAUTHOR_TAG :'],['two variants of bilm discussed by  #TAUTHOR_TAG :'],"['the original bilm model.', 'we consider two variants of bilm discussed by  #TAUTHOR_TAG : the standard one, lex • lex,']","['', 'we consider two variants of bilm discussed by  #TAUTHOR_TAG : the standard one, lex • lex, and the simplest syntactic one, pos • pos.', 'results for the experiments can be found in table 2.', 'in the discussion below we mostly focus on the experimental results for the large, combined test set mt08 + mt09.', '']",0
['original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree'],['original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree'],"['of units of a bilingual language model ( bilm ).', 'we argued that the very limited contextual information used in the original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree']","['this paper, we have introduced a simple, yet effective way to include syntactic information into phrase - based smt.', 'our method consists of enriching the representation of units of a bilingual language model ( bilm ).', 'we argued that the very limited contextual information used in the original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.', 'in a series of translation experiments we performed a thorough comparison between various syntacticallyenriched bilms and competing models.', 'the results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n - gram model can yield statistically significant improvements over the competing systems.', 'a number of additional evaluations provided an indication for better modeling of reordering phenomena.', 'the proposed dependency - based bilms resulted in an increase in 4 - gram precision and provided further significant improvements over all considered metrics in experiments with an increased distortion limit.', 'in this paper, we have focused on rather elementary dependency relations, which we are planning to expand on in future work.', 'our current approach is still strictly tied to the number of target tokens.', 'in particular, we are interested in exploring ways to better capture the notion of syntactic cohesion in translation']",0
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",3
"[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events']","['including both source and target information into the representation of translation events we ob - tain a bilingual lm.', 'the richer representation allows for a finer distinction between reorderings.', 'for example, arabic has a morphological marker of definiteness on both nouns and adjectives.', 'if we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model.', 'this kind of intuition underlies the model of  #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events t 1,..., t n as follows :', 'where e i is the i - th target word and a : e → p ( f ) is an alignment function, e and f referring to target and source sentences, and p ( · ) is the powerset function.', 'in other words, the i - th translation event consists of the i - th target word and all source words aligned to it.', ' #TAUTHOR_TAG refer to the defined translation events t i as bilingual tokens and we adopt this terminology.', 'there are alternative definitions of bilingual language models.', '']",3
"['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained']","['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained']","['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained']","['this section, we introduce our model which combines the bilm from  #TAUTHOR_TAG with source dependency information.', 'we further give details on how the proposed models are trained and integrated into a phrase - based decoder']",3
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",5
"[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events']","['including both source and target information into the representation of translation events we ob - tain a bilingual lm.', 'the richer representation allows for a finer distinction between reorderings.', 'for example, arabic has a morphological marker of definiteness on both nouns and adjectives.', 'if we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model.', 'this kind of intuition underlies the model of  #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events t 1,..., t n as follows :', 'where e i is the i - th target word and a : e → p ( f ) is an alignment function, e and f referring to target and source sentences, and p ( · ) is the powerset function.', 'in other words, the i - th translation event consists of the i - th target word and all source words aligned to it.', ' #TAUTHOR_TAG refer to the defined translation events t i as bilingual tokens and we adopt this terminology.', 'there are alternative definitions of bilingual language models.', '']",5
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",6
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",1
"['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2']","['', 'capture reordering regularities of a language pair by incorporating source syntactic parameters', 'into the units of a bilingual language model? what kind of source syntactic parameters are necessary', 'and sufficient? our contributions can be summarized as follows : we argue that the contextual information used in the original bilingual models  #TAUTHOR_TAG is insufficient and introduce a simple model that exploits source - side', 'syntax to improve reordering ( sections 2 and 3 ). we perform a thorough comparison between different variants of our', 'general model and compare them to the original approach. we carry out translation experiments on multiple test sets, two language pairs ( arabicenglish and chinese - english ), and with respect to two metrics ( bleu and ter ). finally, we', 'present a preliminary analysis of the reorderings resulting from the proposed models ( section 4 )']",1
"['##ed categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of']","['reordering is the syntactic role or context of a word.', 'by using unnecessarily fine - grained categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of']","['##ordering is the syntactic role or context of a word.', 'by using unnecessarily fine - grained categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of the original bilm, where words']","['mentioned in the introduction, lexical information is not very well - suited to capture reordering regularities.', 'consider figure 2. a.', 'the extracted sequence of bilingual tokens is produced by aligning source words with respect to target words ( so that they are in the same order ), as demonstrated by the shaded part of the picture.', 'if we substituted the arabic translation of egyptian for the arabic translation of israeli, the reordering should remain the same. what matters for reordering is the syntactic role or context of a word.', 'by using unnecessarily fine - grained categories we risk running into sparsity issues.', ' #TAUTHOR_TAG also described an alternative variant of the original bilm, where words are substituted by their pos tags ( figure 2. a, shaded part ).', 'also, however, pos information by itself may be insufficiently expressive to separate cor -, it still is a likely sequence.', 'indeed, the log - probabilities of the two sequences with respect to a 4 - gram bilm model 5 result in a higher probability of −10. 25 for the incorrect reordering than for the correct one ( −10. 39 ).', 'since fully lexicalized bilingual tokens suffer from data sparsity and pos - based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.', '5 section 4 contains details about data and software setup']",1
['original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree'],['original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree'],"['of units of a bilingual language model ( bilm ).', 'we argued that the very limited contextual information used in the original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree']","['this paper, we have introduced a simple, yet effective way to include syntactic information into phrase - based smt.', 'our method consists of enriching the representation of units of a bilingual language model ( bilm ).', 'we argued that the very limited contextual information used in the original bilingual models  #TAUTHOR_TAG can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.', 'in a series of translation experiments we performed a thorough comparison between various syntacticallyenriched bilms and competing models.', 'the results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n - gram model can yield statistically significant improvements over the competing systems.', 'a number of additional evaluations provided an indication for better modeling of reordering phenomena.', 'the proposed dependency - based bilms resulted in an increase in 4 - gram precision and provided further significant improvements over all considered metrics in experiments with an increased distortion limit.', 'in this paper, we have focused on rather elementary dependency relations, which we are planning to expand on in future work.', 'our current approach is still strictly tied to the number of target tokens.', 'in particular, we are interested in exploring ways to better capture the notion of syntactic cohesion in translation']",1
"[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary']","[' #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events']","['including both source and target information into the representation of translation events we ob - tain a bilingual lm.', 'the richer representation allows for a finer distinction between reorderings.', 'for example, arabic has a morphological marker of definiteness on both nouns and adjectives.', 'if we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model.', 'this kind of intuition underlies the model of  #TAUTHOR_TAG, a bilingual lm ( bilm ), which defines elementary translation events t 1,..., t n as follows :', 'where e i is the i - th target word and a : e → p ( f ) is an alignment function, e and f referring to target and source sentences, and p ( · ) is the powerset function.', 'in other words, the i - th translation event consists of the i - th target word and all source words aligned to it.', ' #TAUTHOR_TAG refer to the defined translation events t i as bilingual tokens and we adopt this terminology.', 'there are alternative definitions of bilingual language models.', '']",7
"['original bilms from  #TAUTHOR_TAG.', '']","['original bilms from  #TAUTHOR_TAG.', 'word - alignment']","['a baseline augmented with the original bilms from  #TAUTHOR_TAG.', '']","['conduct translation experiments with a baseline pbsmt system with additionally one of the dependency - based bilm feature functions specified in section 3.', 'we compare the translation performance to a baseline pbsmt system and to a baseline augmented with the original bilms from  #TAUTHOR_TAG.', 'word - alignment is produced with giza + +  #AUTHOR_TAG.', 'we use an in - house implementation of a pbsmt system similar to moses  #AUTHOR_TAG.', 'our baseline contains all standard pbsmt features including language model, lexical weighting, and lexicalized reordering.', 'the distortion limit is set to 5.', 'a 5 - gram lm is trained on the english gigaword corpus ( 1. 6b tokens ) using srilm with modified kneyser - ney smoothing and interpolation.', 'the bilms were trained as described in section 3. 3.', 'information about the parallel data used for training the arabic - english 7 and chinese - english systems 8 is 7 the following arabic - english parallel corpora were used : ldc2006e25, ldc2004t18, several gale corpora, ldc2004t17, ldc2005e46, ldc2007t08, ldc2004e13.', '']",7
"['##ume  #TAUTHOR_TAG. in this study, we examine']","['various proposals to reconstruct missing entries, in wals and other databases, from known entries ( daume', 'iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG. in this study, we examine']","['##ume  #TAUTHOR_TAG. in this study, we examine whether we can tackle the problem']","[""practical fruit within various subfields of nlp, particularly on problems involving lower - resource languages.  #AUTHOR_TAG, has proven useful in many nlp tasks ( o' #AUTHOR_TAG,"", 'such as multilingual dependency parsing  #AUTHOR_TAG, generative parsing in low - resource settings  #AUTHOR_TAG tackstrom et al., 2013 ), phonological language modeling and loanword prediction  #AUTHOR_TAG, postagging  #AUTHOR_TAG, and machine translation  #AUTHOR_TAG.', 'however, the needs of nlp tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design. 2 in nlp, on the other hand, what is important is having a relatively full set of features', 'for the particular group of languages you are working on. this mismatch of needs has motivated various proposals to reconstruct missing entries, in wals and other databases, from known entries ( daume', 'iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG. in this study, we examine whether we can tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a', '']",0
"[' #TAUTHOR_TAG, which is a collection of binary features']","[' #TAUTHOR_TAG, which is a collection of binary features']","['our analysis, we use the uriel language typology database  #TAUTHOR_TAG, which is a collection of binary features']","['##ology database : to perform our analysis, we use the uriel language typology database  #TAUTHOR_TAG, which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as wals ( world atlas of language structures )  #AUTHOR_TAG, phoible  #AUTHOR_TAG, ethnologue  #AUTHOR_TAG, and glottolog ( hammarstrom et al., 2015 ).', 'these features are divided into separate classes regarding syntax ( e. g. whether a language has prepositions or postpositions ), phonology ( e. g. whether a language has complex syllabic onset clusters ), and phonetic inventory ( e. g. whether a language has interdental fricatives ).', 'there are 103 syntactical features, 28 phonology features and 158 phonetic inventory features in the database.', 'baseline feature vectors : several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language ( daume iii and  #AUTHOR_TAG.', ""as an alternative that does not necessarily require pre - existing knowledge of the typological features in the language at hand,  #TAUTHOR_TAG have proposed a method for inferring typological features directly from the language's k nearest neighbors ( k - nn ) according to geodesic distance ( distance on the earth's surface ) and genetic distance ( distance according to a phylogenetic family tree )."", 'in our experiments, our baseline uses this method by taking the 3 - nn for each language according to normalized geodesic + genetic distance, and calculating an average feature vector of these three neighbors.', 'typology prediction : to perform prediction, we trained a logistic regression classifier 3 with the baseline k - nn feature vectors described above and the proposed nmt feature vectors described in the next section.', '']",0
"['##ume  #TAUTHOR_TAG. in this study, we examine']","['various proposals to reconstruct missing entries, in wals and other databases, from known entries ( daume', 'iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG. in this study, we examine']","['##ume  #TAUTHOR_TAG. in this study, we examine whether we can tackle the problem']","[""practical fruit within various subfields of nlp, particularly on problems involving lower - resource languages.  #AUTHOR_TAG, has proven useful in many nlp tasks ( o' #AUTHOR_TAG,"", 'such as multilingual dependency parsing  #AUTHOR_TAG, generative parsing in low - resource settings  #AUTHOR_TAG tackstrom et al., 2013 ), phonological language modeling and loanword prediction  #AUTHOR_TAG, postagging  #AUTHOR_TAG, and machine translation  #AUTHOR_TAG.', 'however, the needs of nlp tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design. 2 in nlp, on the other hand, what is important is having a relatively full set of features', 'for the particular group of languages you are working on. this mismatch of needs has motivated various proposals to reconstruct missing entries, in wals and other databases, from known entries ( daume', 'iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG. in this study, we examine whether we can tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a', '']",5
"[' #TAUTHOR_TAG, which is a collection of binary features']","[' #TAUTHOR_TAG, which is a collection of binary features']","['our analysis, we use the uriel language typology database  #TAUTHOR_TAG, which is a collection of binary features']","['##ology database : to perform our analysis, we use the uriel language typology database  #TAUTHOR_TAG, which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as wals ( world atlas of language structures )  #AUTHOR_TAG, phoible  #AUTHOR_TAG, ethnologue  #AUTHOR_TAG, and glottolog ( hammarstrom et al., 2015 ).', 'these features are divided into separate classes regarding syntax ( e. g. whether a language has prepositions or postpositions ), phonology ( e. g. whether a language has complex syllabic onset clusters ), and phonetic inventory ( e. g. whether a language has interdental fricatives ).', 'there are 103 syntactical features, 28 phonology features and 158 phonetic inventory features in the database.', 'baseline feature vectors : several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language ( daume iii and  #AUTHOR_TAG.', ""as an alternative that does not necessarily require pre - existing knowledge of the typological features in the language at hand,  #TAUTHOR_TAG have proposed a method for inferring typological features directly from the language's k nearest neighbors ( k - nn ) according to geodesic distance ( distance on the earth's surface ) and genetic distance ( distance according to a phylogenetic family tree )."", 'in our experiments, our baseline uses this method by taking the 3 - nn for each language according to normalized geodesic + genetic distance, and calculating an average feature vector of these three neighbors.', 'typology prediction : to perform prediction, we trained a logistic regression classifier 3 with the baseline k - nn feature vectors described above and the proposed nmt feature vectors described in the next section.', '']",5
"[' #TAUTHOR_TAG, which is a collection of binary features']","[' #TAUTHOR_TAG, which is a collection of binary features']","['our analysis, we use the uriel language typology database  #TAUTHOR_TAG, which is a collection of binary features']","['##ology database : to perform our analysis, we use the uriel language typology database  #TAUTHOR_TAG, which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as wals ( world atlas of language structures )  #AUTHOR_TAG, phoible  #AUTHOR_TAG, ethnologue  #AUTHOR_TAG, and glottolog ( hammarstrom et al., 2015 ).', 'these features are divided into separate classes regarding syntax ( e. g. whether a language has prepositions or postpositions ), phonology ( e. g. whether a language has complex syllabic onset clusters ), and phonetic inventory ( e. g. whether a language has interdental fricatives ).', 'there are 103 syntactical features, 28 phonology features and 158 phonetic inventory features in the database.', 'baseline feature vectors : several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language ( daume iii and  #AUTHOR_TAG.', ""as an alternative that does not necessarily require pre - existing knowledge of the typological features in the language at hand,  #TAUTHOR_TAG have proposed a method for inferring typological features directly from the language's k nearest neighbors ( k - nn ) according to geodesic distance ( distance on the earth's surface ) and genetic distance ( distance according to a phylogenetic family tree )."", 'in our experiments, our baseline uses this method by taking the 3 - nn for each language according to normalized geodesic + genetic distance, and calculating an average feature vector of these three neighbors.', 'typology prediction : to perform prediction, we trained a logistic regression classifier 3 with the baseline k - nn feature vectors described above and the proposed nmt feature vectors described in the next section.', '']",5
"[' #TAUTHOR_TAG, we focused']","[' #TAUTHOR_TAG, we focused']","[' #TAUTHOR_TAG, we focused']",[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG demonstrates that only text - based revisions'],[' #TAUTHOR_TAG demonstrates that only text - based revisions'],"['a specific color.', 'our prior work  #TAUTHOR_TAG demonstrates that only text - based revisions']","['rewriting assistant interface is designed with several principles in mind.', ""1 ) because the revision classification taxonomy goes beyond the binary textual versus surface distinction, we want to make sure that users don't get lost distinguishing different categories ; 2 ) we want to encourage users to think about their revisions holistically, not always just focusing on low - level details ; 3 ) we want to encourage users to continuously re - evaluate whether they succeeded in making changes between drafts ( rather than focusing on generating new contents )."", 'thus, we have designed an interface that offers multiple views of the revision changes.', ""as demonstrated in figure 2, the interface includes a revision overview interface for the overview of the authors'revisions and a revision detail interface that allows the author to access the details of their essays and revisions."", 'inspired by works on learning analytics  #AUTHOR_TAG, we design the revision overview interface which displays the statistics of the revisions.', 'following design principle # 1, the revision purposes are color coded and each purpose corresponds to a specific color.', 'our prior work  #TAUTHOR_TAG demonstrates that only text - based revisions are significantly correlated with the writing improvement.', 'to inspire the writers to focus more on the important text - based revisions, cold colors are chosen for the surface revisions and warm colors are chosen for the text - based revisions.', '']",0
"[' #TAUTHOR_TAG, we focused']","[' #TAUTHOR_TAG, we focused']","[' #TAUTHOR_TAG, we focused']",[' #TAUTHOR_TAG'],6
"[' #TAUTHOR_TAG, revisions are first']","[' #TAUTHOR_TAG, revisions are first']","['', 'revision classification.', 'following the argumentative revision definition in our prior work  #TAUTHOR_TAG, revisions are first']","['', 'after receiving the analysis feedback, the student can choose to continue with the cycle of essay revising until the revisions are satisfactory.', 'aligned pairs where the sentences in the pair are not identical are extracted as revisions.', 'we first use the stanford parser  #AUTHOR_TAG to break the original text into sentences and then align the sentences using the algorithm in our prior work  #AUTHOR_TAG which considers both sentence similarity ( calculated using tf * idf score ) and the global context of sentences.', 'revision classification.', 'following the argumentative revision definition in our prior work  #TAUTHOR_TAG, revisions are first categorized to content ( text - based ) and surface 3 according to whether the revision changed the meaning of the essay or not.', 'the text - based revisions include thesis / ideas ( claim ), rebuttal, reasoning ( warrant ), evidence, and other content changes ( general content ).', 'the surface revisions include fluency ( wordusage / clarity ), reordering ( organization ) and errors ( conventions / grammar / spelling ).', 'on the']",5
"[') systems ( reiter and  #TAUTHOR_TAG.', 'the task of']","['( nlg ) systems ( reiter and  #TAUTHOR_TAG.', 'the task of']","[') systems ( reiter and  #TAUTHOR_TAG.', 'the task of any gre algorithm is to']","['of referring expression ( gre ) is an important task in the field of natural language generation ( nlg ) systems ( reiter and  #TAUTHOR_TAG.', 'the task of any gre algorithm is to find a combination of properties that allow the audience to identify an object ( target object ) from a set of objects ( domain or environment ).', 'the properties should satisfy the target object and dissatisfy all other objects in the domain.', 'we sometimes call it distinguishing description because it helps us to distinguish the target from potential distractors, called contrast set.', 'when we generate any natural language text in a particular domain, it has been observed that the text is centered on certain objects for that domain.', 'when we give introductory description of any object, we generally give full length description ( e. g. "" the large black hairy dog "" ). but the later references to that object tend to be shorter and only support referential communication goal of distinguishing the target from other objects.', 'for example the expression "" the black dog "" suffices if the other dogs in the environment are all non black.', 'grice, an eminent philosopher of language, has stressed on brevity of referential communication to avoid conversational implicature.', ' #AUTHOR_TAG developed full brevity algorithm based on this observation.', 'it always generates shortest possible referring description to identify an object. but reiter and  #TAUTHOR_TAG later proved that full brevity requirement is an np - hard task, thus computationally intractable and offered an alternative polynomial time incremental algorithm.', 'this algorithm adds properties in a predetermined order, based on the observation that human speakers and audiences prefer certain kinds of properties when describing an object in a domain ( krahmer et al. 2003 ).', 'the incremental algorithm is accepted as state of the art algorithm in nlg domain.', 'later many refinements ( like boolean description and set representation ( deemter 2002 ), context sensitivity ( krahmer et al 2002 ) etc ) have been incorporated into this algorithm.', 'several approaches have also been made to propose an alternative algorithmic framework to this problem like graph - based ( krahmer et al. 2003 ), conceptual graph based ( croitoru and deemter 2007 ) etc that also handle the above refinements.', 'in this paper we propose a new prefix tree ( trie ) based framework for modeling gre problems.', '']",0
"[') systems ( reiter and  #TAUTHOR_TAG.', 'the task of']","['( nlg ) systems ( reiter and  #TAUTHOR_TAG.', 'the task of']","[') systems ( reiter and  #TAUTHOR_TAG.', 'the task of any gre algorithm is to']","['of referring expression ( gre ) is an important task in the field of natural language generation ( nlg ) systems ( reiter and  #TAUTHOR_TAG.', 'the task of any gre algorithm is to find a combination of properties that allow the audience to identify an object ( target object ) from a set of objects ( domain or environment ).', 'the properties should satisfy the target object and dissatisfy all other objects in the domain.', 'we sometimes call it distinguishing description because it helps us to distinguish the target from potential distractors, called contrast set.', 'when we generate any natural language text in a particular domain, it has been observed that the text is centered on certain objects for that domain.', 'when we give introductory description of any object, we generally give full length description ( e. g. "" the large black hairy dog "" ). but the later references to that object tend to be shorter and only support referential communication goal of distinguishing the target from other objects.', 'for example the expression "" the black dog "" suffices if the other dogs in the environment are all non black.', 'grice, an eminent philosopher of language, has stressed on brevity of referential communication to avoid conversational implicature.', ' #AUTHOR_TAG developed full brevity algorithm based on this observation.', 'it always generates shortest possible referring description to identify an object. but reiter and  #TAUTHOR_TAG later proved that full brevity requirement is an np - hard task, thus computationally intractable and offered an alternative polynomial time incremental algorithm.', 'this algorithm adds properties in a predetermined order, based on the observation that human speakers and audiences prefer certain kinds of properties when describing an object in a domain ( krahmer et al. 2003 ).', 'the incremental algorithm is accepted as state of the art algorithm in nlg domain.', 'later many refinements ( like boolean description and set representation ( deemter 2002 ), context sensitivity ( krahmer et al 2002 ) etc ) have been incorporated into this algorithm.', 'several approaches have also been made to propose an alternative algorithmic framework to this problem like graph - based ( krahmer et al. 2003 ), conceptual graph based ( croitoru and deemter 2007 ) etc that also handle the above refinements.', 'in this paper we propose a new prefix tree ( trie ) based framework for modeling gre problems.', '']",0
"['scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later']","['scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later']","['', 'the scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later it is extended to take care of different refinements ( like relational, boolean description etc )']","['this section, it is shown how a scene can be represented using a trie data structure.', 'the scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later it is extended to take care of different refinements ( like relational, boolean description etc ) that could not be handled by incremental algorithm.', ""reiter and  #TAUTHOR_TAG pointed out the notion of'preferredattributes'( e. g. type, size, color etc ) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set."", 'we assume that the initial description of an entity is following this sequence ( e. g. "" the large black dog "" ) then the later references will be some subset of initial description ( like "" the dog "" or "" the large dog "" ) which is defined as the prefix of the initial description.', 'so, we have to search for a prefix of the initial full length description so that it is adequate to distinguish the target object.', '']",5
"['scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later']","['scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later']","['', 'the scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later it is extended to take care of different refinements ( like relational, boolean description etc )']","['this section, it is shown how a scene can be represented using a trie data structure.', 'the scheme is based on incremental algorithm  #TAUTHOR_TAG and incorporates the attractive properties ( e. g. speed, simplicity etc ) of that algorithm.', 'later it is extended to take care of different refinements ( like relational, boolean description etc ) that could not be handled by incremental algorithm.', ""reiter and  #TAUTHOR_TAG pointed out the notion of'preferredattributes'( e. g. type, size, color etc ) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set."", 'we assume that the initial description of an entity is following this sequence ( e. g. "" the large black dog "" ) then the later references will be some subset of initial description ( like "" the dog "" or "" the large dog "" ) which is defined as the prefix of the initial description.', 'so, we have to search for a prefix of the initial full length description so that it is adequate to distinguish the target object.', '']",5
"['representations  #TAUTHOR_TAG.', 'however, there exists no clear']","['representations  #TAUTHOR_TAG.', 'however, there exists no clear']","['', 'some recent work has addressed this by learning general - purpose sentence representations  #TAUTHOR_TAG.', 'however, there exists no clear consensus yet']","['learning has driven a number of recent successes in computer vision and nlp.', 'computer vision tasks like image captioning  #AUTHOR_TAG and visual question answering typically use cnns pretrained on imagenet  #AUTHOR_TAG to extract representations of the image, while several natural language tasks such as reading comprehension and sequence labeling  #AUTHOR_TAG have benefited from pretrained word embeddings  #AUTHOR_TAG that are either fine - tuned for a specific task or held fixed.', 'many neural nlp systems are initialized with pretrained word embeddings but learn their representations of words in context from scratch, in a task - specific manner from supervised learning signals.', 'however, learning these representations reliably from scratch is not always feasible, especially in low - resource settings, where we believe that using general purpose sentence representations will be beneficial.', 'some recent work has addressed this by learning general - purpose sentence representations  #TAUTHOR_TAG.', 'however, there exists no clear consensus yet on what training objective or methodology is best suited to this goal.', 'understanding the inductive biases of distinct neural models is important for guiding progress in representation learning.', ' #AUTHOR_TAG and  #AUTHOR_TAG demonstrate that neural ma - chine translation ( nmt ) systems appear to capture morphology and some syntactic properties.', '']",0
"['', 'more recently,  #TAUTHOR_TAG show that']","['skip - thoughts.', 'more recently,  #TAUTHOR_TAG show that']","['', 'more recently,  #TAUTHOR_TAG show that a completely supervised approach to']","['problem of learning distributed representations of phrases and sentences dates back over a decade.', 'for example,  #AUTHOR_TAG present an additive and multiplicative linear composition function of the distributed representations of individual words.', ' #AUTHOR_TAG combine symbolic and distributed representations of words using tensor products.', 'advances in learning better distributed representations of words  #AUTHOR_TAG combined with deep learning have made it possible to learn complex non - linear composition functions of an arbitrary number of word embeddings using convolutional or recurrent neural networks ( rnns ).', 'a network\'s representation of the last element in a sequence, which is a non - linear composition of all inputs, is typically assumed to contain a squashed "" summary "" of the sentence.', 'most work in supervised learning for nlp builds task - specific representations of sentences rather than general - purpose ones.', 'notably, skip - thought vectors, an extension of the skip - gram model for word embeddings  #AUTHOR_TAG to sentences, learn re - usable sentence representations from weakly labeled data.', 'unfortunately, these models take weeks or often months to train.', ' #AUTHOR_TAG address this by considering faster alternatives such as sequential denoising autoencoders and shallow log - linear models.', ' #AUTHOR_TAG, however, demonstrate that simple word embedding averages are comparable to more complicated models like skip - thoughts.', 'more recently,  #TAUTHOR_TAG show that a completely supervised approach to learning sentence representations from natural language inference data outperforms all previous approaches on transfer learning benchmarks.', '']",0
['hidden state while  #TAUTHOR_TAG perform max - pooling across'],['hidden state while  #TAUTHOR_TAG perform max - pooling across'],"['', 'for example, use the last hidden state while  #TAUTHOR_TAG perform max - pooling across']","['addition to performing 10 - fold cross - validation to determine the l2 regularization penalty on the logistic regression models, we also tune the way in which our sentence representations are generated from the hidden states corresponding to words in a sentence.', 'for example, use the last hidden state while  #TAUTHOR_TAG perform max - pooling across all of the hidden states.', 'we consider both of these approaches and pick the one with better performance on the validation set.', 'we note that max - pooling works best on sentiment tasks such as mr, cr, subj and mpqa, while the last hidden state works better on all other tasks.', ""we also employ vocabulary expansion on all tasks as in by training a linear regression to map from the space of pre - trained word embeddings ( glove ) to our model's word embeddings""]",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'we train on a collection of about 1 million sentence pairs']",[' #TAUTHOR_TAG'],5
['presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use'],['presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use'],['presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use'],"['follow a similar evaluation protocol to those presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use our learned representations as features for a low complexity classifier ( typically linear ) on a novel supervised task / domain unseen during training without updating the parameters of our sentence representation model.', 'we also consider such a transfer learning evaluation in an artificially constructed low - resource setting.', '']",5
[' #TAUTHOR_TAG and our models'],[' #TAUTHOR_TAG and our models'],"['', ' #AUTHOR_TAG and the last 4 rows are our experiments using infer', '##sent  #TAUTHOR_TAG and our models']","['', '. cosine similarities correlates reasonably well with their relatedness on semantic textual similarity benchmarks ( appendix table 7 ). we also', 'present qualitative analysis of our learned representations by visualizations using dimensionality reduction techniques ( figure 1 ) and nearest neighbor exploration ( appendix table 8 ). figure 1', 'shows t - sne plots of our sentence representations on three', 'different datasets - subj, trec and dbpedia. dbpedia is a large corpus of sentences from wikipedia labeled by category and used by  #AUTHOR_TAG. sentences appear to cluster reasonably well according to', 'their labels. the clustering also appears better than that demonstrated in figure', '2 of on trec and subj. appendix table 8 contains sentences from the bookcorpus and their nearest neighbors. sentences with some lexical overlap', 'and similar discourse structure appear to be clustered together. mrksic et al. ( 2017 )', 'respectively. we also report qvec benchmarks  #AUTHOR_TAG model accuracy 1k 5k 10k 25k all ( 400k ) table 4 : supervised & low', '- resource classification accuracies on the quora duplicate question dataset. accuracies are reported corresponding to the number of training examples used. the', 'first 6 rows are taken from, the next 4 are from  #AUTHOR_TAG, the next 5 from', ' #AUTHOR_TAG and the last 4 rows are our experiments using infer', '##sent  #TAUTHOR_TAG and our models']",5
['product ( as described in  #TAUTHOR_TAG'],['product ( as described in  #TAUTHOR_TAG'],['##rd product ( as described in  #TAUTHOR_TAG are given to'],"['present some architectural specifics and training details of our multi - task framework.', 'our shared encoder uses a common word embedding lookup table and gru.', 'we experiment with unidirectional, bidirectional and 2 layer bidirectional grus ( details in appendix section 9 ).', 'for each task, every decoder has its separate word embedding lookups, conditional grus and fully connected layers that project the gru hidden states to the target vocabularies.', 'the last hidden state of the encoder is used as the initial hidden state of the decoder and is also presented as input to all the gates of the gru at every time step.', 'for natural language inference, the same encoder is used to encode both the premise and hypothesis and a concatenation of their representations along with the absolute difference and hadamard product ( as described in  #TAUTHOR_TAG are given to a single layer mlp with a dropout  #AUTHOR_TAG rate of 0. 3.', 'all models use word embeddings of 512 dimensions and grus with either 1500 or 2048 hidden units.', 'we used minibatches of 48 examples and the  #AUTHOR_TAG optimizer with a learning rate of 0. 002.', 'models were trained for 7 days on an nvidia tesla p100 - sxm2 - 16gb gpu.', 'while report close to a month of training, we only train for 7 days, made possible by advancements in gpu hardware and software ( cudnn rnns ).', 'we did not tune any of the architectural details and hyperparameters owing to the fact that we were unable to identify any clear criterion on which to tune them.', 'gains in performance on a specific task do not often translate to better transfer performance']",5
['hidden state while  #TAUTHOR_TAG perform max - pooling across'],['hidden state while  #TAUTHOR_TAG perform max - pooling across'],"['', 'for example, use the last hidden state while  #TAUTHOR_TAG perform max - pooling across']","['addition to performing 10 - fold cross - validation to determine the l2 regularization penalty on the logistic regression models, we also tune the way in which our sentence representations are generated from the hidden states corresponding to words in a sentence.', 'for example, use the last hidden state while  #TAUTHOR_TAG perform max - pooling across all of the hidden states.', 'we consider both of these approaches and pick the one with better performance on the validation set.', 'we note that max - pooling works best on sentiment tasks such as mr, cr, subj and mpqa, while the last hidden state works better on all other tasks.', ""we also employ vocabulary expansion on all tasks as in by training a linear regression to map from the space of pre - trained word embeddings ( glove ) to our model's word embeddings""]",5
"['with 1500 - dimensional hidden vectors trained without nli.', 'in tables 3 and 5 we do not concatenate the representations of multiple models. and  #TAUTHOR_TAG provide a detailed description of tasks that are']","['bidirectional gru with 1500 - dimensional hidden vectors trained without nli.', 'in tables 3 and 5 we do not concatenate the representations of multiple models. and  #TAUTHOR_TAG provide a detailed description of tasks that are']","['with 1500 - dimensional hidden vectors trained without nli.', 'in tables 3 and 5 we do not concatenate the representations of multiple models. and  #TAUTHOR_TAG provide a detailed description of tasks that are typically used to evaluate sentence representations.', 'we provide a condensed summary and refer readers to their']","['', 'we refer to skip - thought next as stn, french and german nmt as fr and de, natural language inference as nli, skip - thought previous as stp and parsing as par.', '+ stn + fr + de : the sentence representation h x is the concatenation of the final hidden vectors from a forward gru with 1500 - dimensional hidden vectors and a bidirectional gru, also with 1500 - dimensional hidden vectors.', '+ stn + fr + de + nli : the sentence representation h x is the concatenation of the final hidden vectors from a bidirectional gru with 1500 - dimensional hidden vectors and another bidirectional gru with 1500 - dimensional hidden vectors trained without nli.', 'in tables 3 and 5 we do not concatenate the representations of multiple models. and  #TAUTHOR_TAG provide a detailed description of tasks that are typically used to evaluate sentence representations.', 'we provide a condensed summary and refer readers to their work for a more thorough description']",5
['from the evaluation suite provided by'],['from the evaluation suite provided by'],"['7 : evaluation of sentence representations on the semantic textual similarity benchmarks.', 'numbers reported are pearson correlations x100.', 'skipthought, glove average, glove tf - idf, glove + wr ( u ) and all supervised numbers were taken from  #AUTHOR_TAG and  #AUTHOR_TAG and charagram - phrase numbers were taken from  #AUTHOR_TAG.', 'other numbers were obtained from the evaluation suite provided by']","['', 'we use a random subset of the 1 - billion - word dataset for these experiments that were not used to train our multi - task representations.', ""the syntactic properties tasks are setup in the same way as described in  #AUTHOR_TAG. the passive and tense tasks are characterized as binary classification problems given a sentence's representation."", ""the former's objective is to determine if a sentence is written in active / passive voice while the latter's objective is to determine if the sentence is in the past tense or not."", 'the top syntactic sequence ( tss ) is a 20 - way classification problem with 19 most frequent top syntactic sequences and 1 miscellaneous class.', 'we use the same dataset as the authors but different training, validation and test splits.', 'table 7 : evaluation of sentence representations on the semantic textual similarity benchmarks.', 'numbers reported are pearson correlations x100.', 'skipthought, glove average, glove tf - idf, glove + wr ( u ) and all supervised numbers were taken from  #AUTHOR_TAG and  #AUTHOR_TAG and charagram - phrase numbers were taken from  #AUTHOR_TAG.', 'other numbers were obtained from the evaluation suite provided by']",5
['presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use'],['presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use'],['presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use'],"['follow a similar evaluation protocol to those presented in ;  #AUTHOR_TAG ;  #TAUTHOR_TAG which is to use our learned representations as features for a low complexity classifier ( typically linear ) on a novel supervised task / domain unseen during training without updating the parameters of our sentence representation model.', 'we also consider such a transfer learning evaluation in an artificially constructed low - resource setting.', '']",3
[' #TAUTHOR_TAG and our models'],[' #TAUTHOR_TAG and our models'],"['', ' #AUTHOR_TAG and the last 4 rows are our experiments using infer', '##sent  #TAUTHOR_TAG and our models']","['', '. cosine similarities correlates reasonably well with their relatedness on semantic textual similarity benchmarks ( appendix table 7 ). we also', 'present qualitative analysis of our learned representations by visualizations using dimensionality reduction techniques ( figure 1 ) and nearest neighbor exploration ( appendix table 8 ). figure 1', 'shows t - sne plots of our sentence representations on three', 'different datasets - subj, trec and dbpedia. dbpedia is a large corpus of sentences from wikipedia labeled by category and used by  #AUTHOR_TAG. sentences appear to cluster reasonably well according to', 'their labels. the clustering also appears better than that demonstrated in figure', '2 of on trec and subj. appendix table 8 contains sentences from the bookcorpus and their nearest neighbors. sentences with some lexical overlap', 'and similar discourse structure appear to be clustered together. mrksic et al. ( 2017 )', 'respectively. we also report qvec benchmarks  #AUTHOR_TAG model accuracy 1k 5k 10k 25k all ( 400k ) table 4 : supervised & low', '- resource classification accuracies on the quora duplicate question dataset. accuracies are reported corresponding to the number of training examples used. the', 'first 6 rows are taken from, the next 4 are from  #AUTHOR_TAG, the next 5 from', ' #AUTHOR_TAG and the last 4 rows are our experiments using infer', '##sent  #TAUTHOR_TAG and our models']",4
"['by  #TAUTHOR_TAG, we']","['by  #TAUTHOR_TAG, we']","['addition to the above tasks which were considered by  #TAUTHOR_TAG, we']","['addition to the above tasks which were considered by  #TAUTHOR_TAG, we also evaluate on the recently published quora duplicate question dataset 6 since it is an order of magnitude larger than the others ( approximately 400, 000 question pairs ).', 'the task is to correctly identify question pairs that are duplicates of one another, which we formulate as a binary classification problem.', 'we use the same data splits as in.', 'given the size of this data, we consider a more expressive classifier on top of the representations of both questions.', 'specifically, we train a 4 layer mlp with 1024 hidden units, with a dropout rate of 0. 5 after every hidden layer.', 'the evaluation criterion is classification accuracy.', 'we also artificially create a low - resource setting by reducing the number of training examples between 1, 000 and 25, 000 using the same splits as  #AUTHOR_TAG']",6
,,,,0
,,,,0
,,,,0
,,,,0
,,,,5
,,,,5
,,,,5
,,,,5
"[', or as.', 'similar to  #TAUTHOR_TAG, we only']","['serves many different purposes.', 'according to the chinese treebank tagging guidelines  #AUTHOR_TAG, the character can be tagged as dec, deg, dev, sp, der, or as.', 'similar to  #TAUTHOR_TAG, we only']","['or as.', 'similar to  #TAUTHOR_TAG, we only']","['chinese character de serves many different purposes.', 'according to the chinese treebank tagging guidelines  #AUTHOR_TAG, the character can be tagged as dec, deg, dev, sp, der, or as.', 'similar to  #TAUTHOR_TAG, we only consider the majority case when the phrase with ( de ) is a noun phrase modifier.', 'the des in nps have a part - of - speech tag of dec ( a complementizer or a nominalizer ) or deg ( a genitive marker or an associative marker )']",3
"['translated into english.', 'this is implicitly done in the work of  #TAUTHOR_TAG where they use rules to']","['translated into english.', 'this is implicitly done in the work of  #TAUTHOR_TAG where they use rules to']","['translated into english.', 'this is implicitly done in the work of  #TAUTHOR_TAG where they use rules to']","['way we categorize the des is based on their behavior when translated into english.', 'this is implicitly done in the work of  #TAUTHOR_TAG where they use rules to decide if a certain de and the words next to it will need to be reordered.', ""some nps are translated into a hybrid of these categories, or just don't fit into one of the five categories, for instance, involving an adjectival premodifier and a relative clause."", 'in those cases, they are put into an "" other "" category.', '']",3
"['3 are inspired by the rules in  #TAUTHOR_TAG, and']","['are inspired by the rules in  #TAUTHOR_TAG, and']","['preceded by a advp.', 'features 1 - 3 are inspired by the rules in  #TAUTHOR_TAG, and']","['', 'features 1 - 3 are inspired by the rules in  #TAUTHOR_TAG, and the fourth rule is based on the observation that even though the predicative adjective va acts as a verb, it actually corresponds to adjectives in english as described in  #AUTHOR_TAG.', '3 we call these four features a - pattern.', 'our example np in figure 2 will have the fourth feature "" a ends with va "" in table 3, but not the other three features.', 'in table 2 we can see that after adding a - pattern, the 2 - class accuracy is already much higher than the baseline.', 'we attribute this to the fourth rule and also to the fact that the classifier can learn weights for each feature.', '']",3
"[' #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']","[' #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']","[' #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']","['approach de - annotated reorders the chinese sentence, which is similar to the approach proposed by  #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']",3
"['baseline "" is the heuristic rules in  #TAUTHOR_TAG.', 'others are various features added to the log - linear']","['2 - class classification accuracy.', '"" baseline "" is the heuristic rules in  #TAUTHOR_TAG.', 'others are various features added to the log - linear classifier.', 'chinese sentences with the de annotation and extract parse - related features from there']","['baseline "" is the heuristic rules in  #TAUTHOR_TAG.', 'others are various features added to the log - linear classifier.', 'chinese sentences with the de annotation and extract parse - related features from there']","['order to train a classifier and test its performance, we use the chinese treebank 6. 0 ( ldc2007t36 ) and the english chinese translation treebank 1. 0 ( ldc2007t02 table 2 : 5 - class and 2 - class classification accuracy.', '"" baseline "" is the heuristic rules in  #TAUTHOR_TAG.', 'others are various features added to the log - linear classifier.', 'chinese sentences with the de annotation and extract parse - related features from there']",7
"['in  #TAUTHOR_TAG, which is reasonable']","['in  #TAUTHOR_TAG, which is reasonable']","[').', 'the 2 - class accuracy is still lower than using the heuristic rules in  #TAUTHOR_TAG, which is reasonable']","['the part - of - speech tag of de indicates its syntactic function, it is the first obvious feature to add.', 'the np in figure 2 will have the feature "" dec "".', 'this basic feature will be referred to as depos.', 'note that since we are only classifying des in nps, ideally the part - of - speech tag of de will either be dec or deg as described in section 2.', 'however, since we are using automatic parses instead of gold - standard ones, the depos feature might have other values than just dec and deg.', 'from table 2, we can see that with this simple feature, the 5 - class accuracy is low but at least better than simply guessing the majority class ( 47. 92 % ).', 'the 2 - class accuracy is still lower than using the heuristic rules in  #TAUTHOR_TAG, which is reasonable because their rules encode more information than just the pos tags of des.', 'a - pattern : chinese syntactic patterns appearing before', 'secondly, we want to incorporate the rules in  #TAUTHOR_TAG as features in the log - linear classifier.', 'we added features for certain indicative patterns in the parse tree ( listed in table 3 )']",7
"['to', 'the  #TAUTHOR_TAG baseline, we have a 10. 9']","['accuracy to', 'the  #TAUTHOR_TAG baseline, we have a 10. 9']","['', 'the  #TAUTHOR_TAG baseline, we have a 10. 9']","['', 'the  #TAUTHOR_TAG baseline, we have a 10. 9 % absolute improvement. the 5 - class accuracy and confusion matrix is listed in table 4. "" a preposition b "" is a small category and', 'is the most confusing. "" a\'s b "" also', 'has lower accuracy, and is mostly confused with "" b prep', '##osition a "". this could be due to the fact that', 'there are some cases where the translation is correct both ways, but also could be because the features we', 'added have not captured the difference well enough']",7
"['in  #TAUTHOR_TAG, which is reasonable']","['in  #TAUTHOR_TAG, which is reasonable']","[').', 'the 2 - class accuracy is still lower than using the heuristic rules in  #TAUTHOR_TAG, which is reasonable']","['the part - of - speech tag of de indicates its syntactic function, it is the first obvious feature to add.', 'the np in figure 2 will have the feature "" dec "".', 'this basic feature will be referred to as depos.', 'note that since we are only classifying des in nps, ideally the part - of - speech tag of de will either be dec or deg as described in section 2.', 'however, since we are using automatic parses instead of gold - standard ones, the depos feature might have other values than just dec and deg.', 'from table 2, we can see that with this simple feature, the 5 - class accuracy is low but at least better than simply guessing the majority class ( 47. 92 % ).', 'the 2 - class accuracy is still lower than using the heuristic rules in  #TAUTHOR_TAG, which is reasonable because their rules encode more information than just the pos tags of des.', 'a - pattern : chinese syntactic patterns appearing before', 'secondly, we want to incorporate the rules in  #TAUTHOR_TAG as features in the log - linear classifier.', 'we added features for certain indicative patterns in the parse tree ( listed in table 3 )']",5
"['3 are inspired by the rules in  #TAUTHOR_TAG, and']","['are inspired by the rules in  #TAUTHOR_TAG, and']","['preceded by a advp.', 'features 1 - 3 are inspired by the rules in  #TAUTHOR_TAG, and']","['', 'features 1 - 3 are inspired by the rules in  #TAUTHOR_TAG, and the fourth rule is based on the observation that even though the predicative adjective va acts as a verb, it actually corresponds to adjectives in english as described in  #AUTHOR_TAG.', '3 we call these four features a - pattern.', 'our example np in figure 2 will have the fourth feature "" a ends with va "" in table 3, but not the other three features.', 'in table 2 we can see that after adding a - pattern, the 2 - class accuracy is already much higher than the baseline.', 'we attribute this to the fourth rule and also to the fact that the classifier can learn weights for each feature.', '']",5
"['the training data, the tuning and the test sets with the np rules in  #TAUTHOR_TAG and compare our']","['the training data, the tuning and the test sets with the np rules in  #TAUTHOR_TAG and compare our']","['the training data, the tuning and the test sets with the np rules in  #TAUTHOR_TAG and compare our']","['have two different settings as baseline experiments.', 'the first is without reordering or de annotation on the chinese side ; we simply align the parallel texts, extract phrases and tune parameters.', 'this experiment is referred to as baseline.', 'also, we reorder the training data, the tuning and the test sets with the np rules in  #TAUTHOR_TAG and compare our results with this second baseline ( wang - np ).', 'the np reordering preprocessing ( wang - np ) showed consistent improvement in table 5 on all test sets, with bleu point gains ranging from 0. 15 to 0. 40.', 'this confirms that having reordering around des in np helps chinese - english mt']",5
"[' #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']","[' #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']","[' #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']","['approach de - annotated reorders the chinese sentence, which is similar to the approach proposed by  #TAUTHOR_TAG ( wang - np ).', 'however, our focus is on the annotation on des and how this can improve translation quality.', '']",4
"['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as a 4800 by 186027 dimension basic question ( bq ) matrix, and then solve the', 'lasso optimization problem ( huang, alfadly, and ghanem 2017 ), with mq, to find the top 3 similar bq of mq. these bq are the output of module 1. moreover, we take the direct concatenation of mq and bq and the given image as the input of', '']",1
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",1
"['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as a 4800 by 186027 dimension basic question ( bq ) matrix, and then solve the', 'lasso optimization problem ( huang, alfadly, and ghanem 2017 ), with mq, to find the top 3 similar bq of mq. these bq are the output of module 1. moreover, we take the direct concatenation of mq and bq and the given image as the input of', '']",5
"['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as']","['', 'of the questions, also by skip - thought vectors, from the training and validation sets of  #TAUTHOR_TAG dataset as a 4800 by 186027 dimension basic question ( bq ) matrix, and then solve the', 'lasso optimization problem ( huang, alfadly, and ghanem 2017 ), with mq, to find the top 3 similar bq of mq. these bq are the output of module 1. moreover, we take the direct concatenation of mq and bq and the given image as the input of', '']",5
"['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take all of the training and validation questions from the  #TAUTHOR_TAG to be our basic question candidates.', 'then, we take all of the testing questions from the  #TAUTHOR_TAG to be our main question candidates.', 'that is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of  #TAUTHOR_TAG.', 'because we model the basic question generation problem by lasso, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate.', 'the above step guarantees that our lasso can work well']",5
"['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take all of the training and validation questions from the  #TAUTHOR_TAG to be our basic question candidates.', 'then, we take all of the testing questions from the  #TAUTHOR_TAG to be our main question candidates.', 'that is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of  #TAUTHOR_TAG.', 'because we model the basic question generation problem by lasso, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate.', 'the above step guarantees that our lasso can work well']",5
"['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take all of the training and validation questions from the  #TAUTHOR_TAG to be our basic question candidates.', 'then, we take all of the testing questions from the  #TAUTHOR_TAG to be our main question candidates.', 'that is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of  #TAUTHOR_TAG.', 'because we model the basic question generation problem by lasso, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate.', 'the above step guarantees that our lasso can work well']",5
"['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take all of the training and validation questions from the  #TAUTHOR_TAG to be our basic question candidates.', 'then, we take all of the testing questions from the  #TAUTHOR_TAG to be our main question candidates.', 'that is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of  #TAUTHOR_TAG.', 'because we model the basic question generation problem by lasso, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate.', 'the above step guarantees that our lasso can work well']",5
"['sets of  #TAUTHOR_TAG by skip - thought vectors, and then']","['sets of  #TAUTHOR_TAG by skip - thought vectors, and then']","['', 'according to the above subsections, question encoding and problem formulation, we can encode all basic question candidates from the training and validation question sets of  #TAUTHOR_TAG by skip - thought vectors, and then we have a matrix of basic question candidates.', 'each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186']","['now describe how to generate the bq of a query question, illustrated in figure 1.', 'according to the above subsections, question encoding and problem formulation, we can encode all basic question candidates from the training and validation question sets of  #TAUTHOR_TAG by skip - thought vectors, and then we have a matrix of basic question candidates.', 'each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns.', 'that is, the dimension of bq matrix, called a, is 4800 by 186027.', 'also, we encode the given query question as a column vector, 4800 by 1 dimensions, by skip - thought vectors, called b. regarding the selection of the parameter, λ, we will discuss this in section 4. now, we can solve the lasso optimization problem, mentioned in the above subsection of problem formulation, to get the solution, x. here, we consider the elements of the solution vector, x, as the similarity score of the corresponding bq in the bq matrix, a. the first element of x corresponds to the first column, i. e. the first bq, of a. then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding bq to be the ranked bq of the given query question.', '']",5
"['sets of  #TAUTHOR_TAG by skip - thought vectors, and then']","['sets of  #TAUTHOR_TAG by skip - thought vectors, and then']","['', 'according to the above subsections, question encoding and problem formulation, we can encode all basic question candidates from the training and validation question sets of  #TAUTHOR_TAG by skip - thought vectors, and then we have a matrix of basic question candidates.', 'each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186']","['now describe how to generate the bq of a query question, illustrated in figure 1.', 'according to the above subsections, question encoding and problem formulation, we can encode all basic question candidates from the training and validation question sets of  #TAUTHOR_TAG by skip - thought vectors, and then we have a matrix of basic question candidates.', 'each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns.', 'that is, the dimension of bq matrix, called a, is 4800 by 186027.', 'also, we encode the given query question as a column vector, 4800 by 1 dimensions, by skip - thought vectors, called b. regarding the selection of the parameter, λ, we will discuss this in section 4. now, we can solve the lasso optimization problem, mentioned in the above subsection of problem formulation, to get the solution, x. here, we consider the elements of the solution vector, x, as the similarity score of the corresponding bq in the bq matrix, a. the first element of x corresponds to the first column, i. e. the first bq, of a. then, we rank all of the similarity scores in x and pick up the top 21 large weights with corresponding bq to be the ranked bq of the given query question.', '']",5
"['a novel large scale dataset, called basic  #TAUTHOR_TAG']","['a novel large scale dataset, called basic  #TAUTHOR_TAG']","['propose a novel large scale dataset, called basic  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to']","['propose a novel large scale dataset, called basic  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', ' #TAUTHOR_TAG.', 'in bqd, we have 81434 images, 244302 mq and 5130342 ( bq + corresponding similarity score ).', 'furthermore, we exploit the bqd to do robustness analysis of the 6 available pretrained state - of - the - art vqa models  #TAUTHOR_TAG ; lu et al. 2016 ; ben - younes et al. 2017 ; fukui et al. 2016 ; kim et al. 2017 ) in the next subsection']",5
"['a novel large scale dataset, called basic  #TAUTHOR_TAG']","['a novel large scale dataset, called basic  #TAUTHOR_TAG']","['propose a novel large scale dataset, called basic  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to']","['propose a novel large scale dataset, called basic  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', ' #TAUTHOR_TAG.', 'in bqd, we have 81434 images, 244302 mq and 5130342 ( bq + corresponding similarity score ).', 'furthermore, we exploit the bqd to do robustness analysis of the 6 available pretrained state - of - the - art vqa models  #TAUTHOR_TAG ; lu et al. 2016 ; ben - younes et al. 2017 ; fukui et al. 2016 ; kim et al. 2017 ) in the next subsection']",5
"['a novel large scale dataset, called basic  #TAUTHOR_TAG']","['a novel large scale dataset, called basic  #TAUTHOR_TAG']","['propose a novel large scale dataset, called basic  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to']","['propose a novel large scale dataset, called basic  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', ' #TAUTHOR_TAG.', 'in bqd, we have 81434 images, 244302 mq and 5130342 ( bq + corresponding similarity score ).', 'furthermore, we exploit the bqd to do robustness analysis of the 6 available pretrained state - of - the - art vqa models  #TAUTHOR_TAG ; lu et al. 2016 ; ben - younes et al. 2017 ; fukui et al. 2016 ; kim et al. 2017 ) in the next subsection']",5
"['and  #TAUTHOR_TAG.', 'in']","['on bqd and  #TAUTHOR_TAG.', 'in addition, dif f']","['and  #TAUTHOR_TAG.', 'in']","['measure the robustness of any model, we should evaluate it on clean and noisy input and compare the performance.', 'the noise can be completely random, have a specific structure and / or be semantically relevant to the final task.', 'for vqa the input is an image question pair and therefore the noise should be introduced to both.', ""the noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative."", 'for the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models ( fawzi, moosavi dezfooli, and frossard 2017 ; table 3 : mutan without attention model evaluation results on bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'carlini and wagner 2017 ; xu, caramanis, and mannor 2009 ).', ""however, for the question part, we couldn't find any acceptable method to measure the robustness of visual question answering algorithms after extensive literature review."", 'here we propose a novel robustness measure for vqa by introducing semantically relevant noise to the questions where we can control the strength of noisiness.', 'first, we measure the accuracy of the model on the clean  #TAUTHOR_TAG and we call it acc vqa.', 'then, we append the top ranked k bqs to each of the mqs in the clean dataset and recompute the accuracy of the model on this noisy input and we call it acc bqd.', '']",5
"['and  #TAUTHOR_TAG.', 'in']","['on bqd and  #TAUTHOR_TAG.', 'in addition, dif f']","['and  #TAUTHOR_TAG.', 'in']","['measure the robustness of any model, we should evaluate it on clean and noisy input and compare the performance.', 'the noise can be completely random, have a specific structure and / or be semantically relevant to the final task.', 'for vqa the input is an image question pair and therefore the noise should be introduced to both.', ""the noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative."", 'for the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models ( fawzi, moosavi dezfooli, and frossard 2017 ; table 3 : mutan without attention model evaluation results on bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'carlini and wagner 2017 ; xu, caramanis, and mannor 2009 ).', ""however, for the question part, we couldn't find any acceptable method to measure the robustness of visual question answering algorithms after extensive literature review."", 'here we propose a novel robustness measure for vqa by introducing semantically relevant noise to the questions where we can control the strength of noisiness.', 'first, we measure the accuracy of the model on the clean  #TAUTHOR_TAG and we call it acc vqa.', 'then, we append the top ranked k bqs to each of the mqs in the clean dataset and recompute the accuracy of the model on this noisy input and we call it acc bqd.', '']",5
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",5
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",5
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",5
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",5
"['provides multiple - choice and open - ended task for evaluation.', '']","['provides multiple - choice and open - ended task for evaluation.', '']","['provides multiple - choice and open - ended task for evaluation.', 'regarding open - ended task, the answer can be any phrase or word.', 'however, in the multiple - choice task, an answer should be chosen from 18 candidate answers.', 'for both cases, answers']","['provides multiple - choice and open - ended task for evaluation.', 'regarding open - ended task, the answer can be any phrase or word.', 'however, in the multiple - choice task, an answer should be chosen from 18 candidate answers.', 'for both cases, answers are evaluated by accuracy which can reflect human consensus.', 'the accuracy is given by the following : table 5 : mlb with attention model evaluation results on bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', ', where n is the total number of examples, i [ · ] denotes an indicator function, a i is the predicted answer and t i is an answer set of the i th example.', 'that is, a predicted answer is considered as a correct one if at least 3 annotators agree with it and the score depends on the total number of agreements when the predicted answer is not correct']",5
"['provides multiple - choice and open - ended task for evaluation.', '']","['provides multiple - choice and open - ended task for evaluation.', '']","['provides multiple - choice and open - ended task for evaluation.', 'regarding open - ended task, the answer can be any phrase or word.', 'however, in the multiple - choice task, an answer should be chosen from 18 candidate answers.', 'for both cases, answers']","['provides multiple - choice and open - ended task for evaluation.', 'regarding open - ended task, the answer can be any phrase or word.', 'however, in the multiple - choice task, an answer should be chosen from 18 candidate answers.', 'for both cases, answers are evaluated by accuracy which can reflect human consensus.', 'the accuracy is given by the following : table 5 : mlb with attention model evaluation results on bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', ', where n is the total number of examples, i [ · ] denotes an indicator function, a i is the predicted answer and t i is an answer set of the i th example.', 'that is, a predicted answer is considered as a correct one if at least 3 annotators agree with it and the score depends on the total number of agreements when the predicted answer is not correct']",5
['from  #TAUTHOR_TAG cannot'],"['from  #TAUTHOR_TAG cannot find', 'the']","['from  #TAUTHOR_TAG cannot find', 'the proper basic questions']",[' #TAUTHOR_TAG'],5
['from  #TAUTHOR_TAG cannot'],"['from  #TAUTHOR_TAG cannot find', 'the']","['from  #TAUTHOR_TAG cannot find', 'the proper basic questions']",[' #TAUTHOR_TAG'],5
"[', r score and  #TAUTHOR_TAG to measure the robustness of vqa models']","['use the proposed bqd, r score and  #TAUTHOR_TAG to measure the robustness of vqa models']","[', r score and  #TAUTHOR_TAG to measure the robustness of vqa models']","['', 'furthermore, we can use the proposed bqd, r score and  #TAUTHOR_TAG to measure the robustness of vqa models']",5
"['are many papers  #TAUTHOR_TAG ; shih, singh, and hoiem 2016 ; chen et']","['are many papers  #TAUTHOR_TAG ; shih, singh, and hoiem 2016 ; chen et']","['are many papers  #TAUTHOR_TAG ; shih, singh, and hoiem 2016 ; chen et']","[', there are many papers  #TAUTHOR_TAG ; shih, singh, and hoiem 2016 ; chen et al. 2016 ; kafle and kanan 2016 ; ma, lu, and li 2016 ; ren, kiros, and zemel 2015 ; zhu et al. 2016 ; wu et al. 2016 ; lu et al. 2016 ; ben - younes et al. 2017 ; fukui et al. 2016 ; kim et al. 2017 ) have proposed methods to solve the challenging vqa task.', 'our vqabq method involves in different areas in machine learning, natural language processing ( nlp ) and computer vision.', 'the following, we discuss recent works related to our approach.', 'sequence modeling by recurrent neural networks.', 'recurrent neural networks ( rnn ) can handle the sequences of flexible length.', 'long short term memory ( lstm ) ( hochreiter and schmidhuber 1997 ) is a particular variant of rnn and in natural language tasks, such as machine translation ( sutskever, vinyals, and le 2014 ;, lstm is a successful application.', '']",0
"['same time, we only want the minimum number of bq to represent the mq, so modeling our problem as lasso optimization problem is an appropriate way :  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where']","['same time, we only want the minimum number of bq to represent the mq, so modeling our problem as lasso optimization problem is an appropriate way :  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where']","['idea is the bq generation for mq and, at the same time, we only want the minimum number of bq to represent the mq, so modeling our problem as lasso optimization problem is an appropriate way :  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where']","['idea is the bq generation for mq and, at the same time, we only want the minimum number of bq to represent the mq, so modeling our problem as lasso optimization problem is an appropriate way :  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', ', where a is the matrix of encoded bq, b is the encode mq and λ is a parameter of the regularization term']",0
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",0
"['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results']","['and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '']","['conduct our experiments on bqd and  #TAUTHOR_TAG dataset.', ' #TAUTHOR_TAG.', '"" - "" indicates the results are not available, "" - std "" means the accuracy of vqa model evaluated on the complete testing set of bqd and  #TAUTHOR_TAG.', 'in addition, dif f = original dev all − x dev all, where x is equal to "" first "", "" second "",..., "" seventh "".', 'dataset ( lin et al. 2014 ) and it contains a large number of questions.', '']",0
"['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take']","['take the most popular  #TAUTHOR_TAG to develop our bqd.', 'at the beginning, we take all of the training and validation questions from the  #TAUTHOR_TAG to be our basic question candidates.', 'then, we take all of the testing questions from the  #TAUTHOR_TAG to be our main question candidates.', 'that is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of  #TAUTHOR_TAG.', 'because we model the basic question generation problem by lasso, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate.', 'the above step guarantees that our lasso can work well']",3
['from  #TAUTHOR_TAG cannot'],"['from  #TAUTHOR_TAG cannot find', 'the']","['from  #TAUTHOR_TAG cannot find', 'the proper basic questions']",[' #TAUTHOR_TAG'],7
[' #TAUTHOR_TAG have been proposed'],[' #TAUTHOR_TAG have been proposed'],"['', 'most recently, several approaches based on cross - lingual entity embeddings  #AUTHOR_TAG or graph neural networks  #TAUTHOR_TAG have been proposed']","['', 'fore, the cross - lingual kg alignment task, which automatically matches entities between multilingual kgs, is proposed to address this problem.', 'most recently, several approaches based on cross - lingual entity embeddings  #AUTHOR_TAG or graph neural networks  #TAUTHOR_TAG have been proposed for this task.', 'in particular,  #TAUTHOR_TAG introduces the topic entity graph to capture the local context information of an entity within the kg, and further tackles this task as a graph matching problem by proposing a graph matching network.', '']",0
[' #TAUTHOR_TAG have been proposed'],[' #TAUTHOR_TAG have been proposed'],"['', 'most recently, several approaches based on cross - lingual entity embeddings  #AUTHOR_TAG or graph neural networks  #TAUTHOR_TAG have been proposed']","['', 'fore, the cross - lingual kg alignment task, which automatically matches entities between multilingual kgs, is proposed to address this problem.', 'most recently, several approaches based on cross - lingual entity embeddings  #AUTHOR_TAG or graph neural networks  #TAUTHOR_TAG have been proposed for this task.', 'in particular,  #TAUTHOR_TAG introduces the topic entity graph to capture the local context information of an entity within the kg, and further tackles this task as a graph matching problem by proposing a graph matching network.', '']",0
"['alignments,  #TAUTHOR_TAG']","['alignments,  #TAUTHOR_TAG']","['alignments,  #TAUTHOR_TAG views']",[' #TAUTHOR_TAG'],0
"['.', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['layer.', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['', 'our model.', 'in contrast to  #TAUTHOR_TAG that only takes two topic graphs as input, we can utilize additional information such as easy assignments found in previous decoding steps to resolve hard assignments.', 'in particular, we introduce two ways to enhance this baseline model by explicitly integrating the easy assignment information into two layers of  #TAUTHOR_TAG :', '']",0
[' #TAUTHOR_TAG have been proposed'],[' #TAUTHOR_TAG have been proposed'],"['', 'most recently, several approaches based on cross - lingual entity embeddings  #AUTHOR_TAG or graph neural networks  #TAUTHOR_TAG have been proposed']","['', 'fore, the cross - lingual kg alignment task, which automatically matches entities between multilingual kgs, is proposed to address this problem.', 'most recently, several approaches based on cross - lingual entity embeddings  #AUTHOR_TAG or graph neural networks  #TAUTHOR_TAG have been proposed for this task.', 'in particular,  #TAUTHOR_TAG introduces the topic entity graph to capture the local context information of an entity within the kg, and further tackles this task as a graph matching problem by proposing a graph matching network.', '']",5
"['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['for those alignments with normalized probabilities over 0. 9. this result is coherent with our expectation since a higher probability typically suggests that', 'the model is more confident about the prediction and also indicates that this alignment is easier for the model to resolve. therefore, we apply the following steps to', 'decode the test set iteratively. step description 1 employ an alignment model to predict alignments for all source entities in the test set. 2 use a predefined probability threshold α to refine those alignments. in particular, assignments with probabilities higher than α are regarded as easy alignments while the others are viewed as hard alignments. 3 if more than k easy alignments are', 'found in step 2, take these easy alignments as additional knowledge and incorporate them into the alignment model to establish alignments for the remaining entities ( go to step 1 ) ; otherwise, return all alignments. after establishing easy assignments in each decoding step, we need to incorporate them', 'as additional knowledge into the alignment model for the next round decoding. this design heavily depends on alignment model architecture. in this paper, we use the state - of - the - art alignment model  #TAUTHOR_TAG as our baseline method', 'and propose two ways to enhance this model by incorporating easy assignment information. alignment model baseline.  #AUTHOR_TAG utilized a graph ( namely topic graph ) to', 'capture the context information of an entity ( namely topic entity ) within the kg. for instance, figure 2 gives the topic graphs of george bush in both the chinese and english kg. the entity alignment task is then viewed as a graph matching problem, whose goal is to calculate the similarity of these', 'two topic graphs, say g 1', 'and g 2. to achieve this, they further propose a neural graph matching model that includes the', 'following four layers : • input representation layer. the goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using', 'a graph convolution neural network ( gcn )  #AUTHOR_TAG. • node - level matching layer. this layer is', 'designed to capture local matching information by comparing each entity embedding of one topic entity graph against all entity embeddings of the other graph in both ways ( from', 'g 1 to g 2 and g 2 to g 1 ). • graph - level matching layer. in this layer, the model applies another gcn to propagate the local matching information', 'throughout the graph. the motivation behind it is that this gcn layer', 'can encode the global matching state between the pairs of whole graphs. the model then feeds these matching', 'representations', 'to a fully - connected neural network and applies the element', '- wise max and mean pooling method to generate a fixed - length graph matching representation. yale']",5
"['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['for those alignments with normalized probabilities over 0. 9. this result is coherent with our expectation since a higher probability typically suggests that', 'the model is more confident about the prediction and also indicates that this alignment is easier for the model to resolve. therefore, we apply the following steps to', 'decode the test set iteratively. step description 1 employ an alignment model to predict alignments for all source entities in the test set. 2 use a predefined probability threshold α to refine those alignments. in particular, assignments with probabilities higher than α are regarded as easy alignments while the others are viewed as hard alignments. 3 if more than k easy alignments are', 'found in step 2, take these easy alignments as additional knowledge and incorporate them into the alignment model to establish alignments for the remaining entities ( go to step 1 ) ; otherwise, return all alignments. after establishing easy assignments in each decoding step, we need to incorporate them', 'as additional knowledge into the alignment model for the next round decoding. this design heavily depends on alignment model architecture. in this paper, we use the state - of - the - art alignment model  #TAUTHOR_TAG as our baseline method', 'and propose two ways to enhance this model by incorporating easy assignment information. alignment model baseline.  #AUTHOR_TAG utilized a graph ( namely topic graph ) to', 'capture the context information of an entity ( namely topic entity ) within the kg. for instance, figure 2 gives the topic graphs of george bush in both the chinese and english kg. the entity alignment task is then viewed as a graph matching problem, whose goal is to calculate the similarity of these', 'two topic graphs, say g 1', 'and g 2. to achieve this, they further propose a neural graph matching model that includes the', 'following four layers : • input representation layer. the goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using', 'a graph convolution neural network ( gcn )  #AUTHOR_TAG. • node - level matching layer. this layer is', 'designed to capture local matching information by comparing each entity embedding of one topic entity graph against all entity embeddings of the other graph in both ways ( from', 'g 1 to g 2 and g 2 to g 1 ). • graph - level matching layer. in this layer, the model applies another gcn to propagate the local matching information', 'throughout the graph. the motivation behind it is that this gcn layer', 'can encode the global matching state between the pairs of whole graphs. the model then feeds these matching', 'representations', 'to a fully - connected neural network and applies the element', '- wise max and mean pooling method to generate a fixed - length graph matching representation. yale']",5
"[', gm  #TAUTHOR_TAG and rd']","['bootea  #AUTHOR_TAG, gcn, gm  #TAUTHOR_TAG and rdgcn  #AUTHOR_TAG.', 'model variants.', 'to evaluate different reasoning methods, we provide']","[', gm  #TAUTHOR_TAG and rd']","['', 'comparison models.', 'we compare our approach against existing alignment methods : je  #AUTHOR_TAG, mtranse  #AUTHOR_TAG, jape  #AUTHOR_TAG, iptranse  #AUTHOR_TAG, bootea  #AUTHOR_TAG, gcn, gm  #TAUTHOR_TAG and rdgcn  #AUTHOR_TAG.', 'model variants.', 'to evaluate different reasoning methods, we provide three implementation variants of our model for ablation studies, including ( 1 ) x - ehd : the baseline model x that only uses our proposed easy - to - hard decoding strategy ;', '']",5
"[', gm  #TAUTHOR_TAG and rd']","['bootea  #AUTHOR_TAG, gcn, gm  #TAUTHOR_TAG and rdgcn  #AUTHOR_TAG.', 'model variants.', 'to evaluate different reasoning methods, we provide']","[', gm  #TAUTHOR_TAG and rd']","['', 'comparison models.', 'we compare our approach against existing alignment methods : je  #AUTHOR_TAG, mtranse  #AUTHOR_TAG, jape  #AUTHOR_TAG, iptranse  #AUTHOR_TAG, bootea  #AUTHOR_TAG, gcn, gm  #TAUTHOR_TAG and rdgcn  #AUTHOR_TAG.', 'model variants.', 'to evaluate different reasoning methods, we provide three implementation variants of our model for ablation studies, including ( 1 ) x - ehd : the baseline model x that only uses our proposed easy - to - hard decoding strategy ;', '']",5
"['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['alignment model  #TAUTHOR_TAG as our baseline method', 'and']","['for those alignments with normalized probabilities over 0. 9. this result is coherent with our expectation since a higher probability typically suggests that', 'the model is more confident about the prediction and also indicates that this alignment is easier for the model to resolve. therefore, we apply the following steps to', 'decode the test set iteratively. step description 1 employ an alignment model to predict alignments for all source entities in the test set. 2 use a predefined probability threshold α to refine those alignments. in particular, assignments with probabilities higher than α are regarded as easy alignments while the others are viewed as hard alignments. 3 if more than k easy alignments are', 'found in step 2, take these easy alignments as additional knowledge and incorporate them into the alignment model to establish alignments for the remaining entities ( go to step 1 ) ; otherwise, return all alignments. after establishing easy assignments in each decoding step, we need to incorporate them', 'as additional knowledge into the alignment model for the next round decoding. this design heavily depends on alignment model architecture. in this paper, we use the state - of - the - art alignment model  #TAUTHOR_TAG as our baseline method', 'and propose two ways to enhance this model by incorporating easy assignment information. alignment model baseline.  #AUTHOR_TAG utilized a graph ( namely topic graph ) to', 'capture the context information of an entity ( namely topic entity ) within the kg. for instance, figure 2 gives the topic graphs of george bush in both the chinese and english kg. the entity alignment task is then viewed as a graph matching problem, whose goal is to calculate the similarity of these', 'two topic graphs, say g 1', 'and g 2. to achieve this, they further propose a neural graph matching model that includes the', 'following four layers : • input representation layer. the goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using', 'a graph convolution neural network ( gcn )  #AUTHOR_TAG. • node - level matching layer. this layer is', 'designed to capture local matching information by comparing each entity embedding of one topic entity graph against all entity embeddings of the other graph in both ways ( from', 'g 1 to g 2 and g 2 to g 1 ). • graph - level matching layer. in this layer, the model applies another gcn to propagate the local matching information', 'throughout the graph. the motivation behind it is that this gcn layer', 'can encode the global matching state between the pairs of whole graphs. the model then feeds these matching', 'representations', 'to a fully - connected neural network and applies the element', '- wise max and mean pooling method to generate a fixed - length graph matching representation. yale']",6
"['.', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['layer.', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['', 'our model.', 'in contrast to  #TAUTHOR_TAG that only takes two topic graphs as input, we can utilize additional information such as easy assignments found in previous decoding steps to resolve hard assignments.', 'in particular, we introduce two ways to enhance this baseline model by explicitly integrating the easy assignment information into two layers of  #TAUTHOR_TAG :', '']",6
"['.', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['layer.', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['', 'our model.', 'in contrast to  #TAUTHOR_TAG']","['', 'our model.', 'in contrast to  #TAUTHOR_TAG that only takes two topic graphs as input, we can utilize additional information such as easy assignments found in previous decoding steps to resolve hard assignments.', 'in particular, we introduce two ways to enhance this baseline model by explicitly integrating the easy assignment information into two layers of  #TAUTHOR_TAG :', '']",4
"['17,  #TAUTHOR_TAG, 19, 20 ]']","['models [ 17,  #TAUTHOR_TAG, 19, 20 ]']","['17,  #TAUTHOR_TAG, 19, 20 ].', 'recently, henaff et.', '']",[' #TAUTHOR_TAG'],0
"['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in the corresponding memory slots : where prelu is a parametric rectified linear', 'unit [ 21 ], and u, v and w are k × k parameter matrices. now we augment the model with additional relational memory cells. intuitively, the entity memory allows modeling of entities and information about the entities in isolation. this can be', 'insufficient in scenarios where a particular entity participates in may relations with other', 'entities across the document. thus, in order to succeed at relational reasoning the model needs to be able to compare each pair of the entity memories. the relational memories will allow modeling of these relations and provide an inherent inductive bias towards a more structured representation of the participating entities in the text,', '']",0
"['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in the corresponding memory slots : where prelu is a parametric rectified linear', 'unit [ 21 ], and u, v and w are k × k parameter matrices. now we augment the model with additional relational memory cells. intuitively, the entity memory allows modeling of entities and information about the entities in isolation. this can be', 'insufficient in scenarios where a particular entity participates in may relations with other', 'entities across the document. thus, in order to succeed at relational reasoning the model needs to be able to compare each pair of the entity memories. the relational memories will allow modeling of these relations and provide an inherent inductive bias towards a more structured representation of the participating entities in the text,', '']",0
"['[ 24, 19, 25, 20,  #TAUTHOR_TAG ]']","['[ 24, 19, 25, 20,  #TAUTHOR_TAG ]']","['[ 24, 19, 25, 20,  #TAUTHOR_TAG ].', 'our model is also a memory network based model']","['is a long line of work in textual question - answering systems [ 22, 23 ].', 'recent successful approaches use memory based neural networks for question answering [ 24, 19, 25, 20,  #TAUTHOR_TAG ].', 'our model is also a memory network based model and is also related to the neural turing machine [ 26 ].', 'as described previously, the model is closely related to the  #TAUTHOR_TAG ] which describes an end - to - end approach to model entities in text but does not directly model relations.', 'other approaches to question answering use external knowledge, for instance external knowledge bases [ 27, 12, 28, 29, 10 ] or external text like wikipedia [ 30, 31 ].', 'very recently, and in parallel to this work, a method for relational reasoning called relation networks [ 32 ] was proposed.', 'they demonstrated that simple neural network modules are not as effective at relational reasoning and their proposed module is similar to our model.', 'however, relation network is not a memory - based model and there is no mechanism to read and write relevant information for each pair.', '']",0
"['[ 24, 19, 25, 20,  #TAUTHOR_TAG ]']","['[ 24, 19, 25, 20,  #TAUTHOR_TAG ]']","['[ 24, 19, 25, 20,  #TAUTHOR_TAG ].', 'our model is also a memory network based model']","['is a long line of work in textual question - answering systems [ 22, 23 ].', 'recent successful approaches use memory based neural networks for question answering [ 24, 19, 25, 20,  #TAUTHOR_TAG ].', 'our model is also a memory network based model and is also related to the neural turing machine [ 26 ].', 'as described previously, the model is closely related to the  #TAUTHOR_TAG ] which describes an end - to - end approach to model entities in text but does not directly model relations.', 'other approaches to question answering use external knowledge, for instance external knowledge bases [ 27, 12, 28, 29, 10 ] or external text like wikipedia [ 30, 31 ].', 'very recently, and in parallel to this work, a method for relational reasoning called relation networks [ 32 ] was proposed.', 'they demonstrated that simple neural network modules are not as effective at relational reasoning and their proposed module is similar to our model.', 'however, relation network is not a memory - based model and there is no mechanism to read and write relevant information for each pair.', '']",0
"['17,  #TAUTHOR_TAG, 19, 20 ]']","['models [ 17,  #TAUTHOR_TAG, 19, 20 ]']","['17,  #TAUTHOR_TAG, 19, 20 ].', 'recently, henaff et.', '']",[' #TAUTHOR_TAG'],4
"['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search for the learning rate in { 0. 01, 0. 005, 0. 001 } and choose a fixed learning rate of 0. 005 based on performance on the validation set, and clip the gradient norm at 2.', 'we keep all other details similar to  #TAUTHOR_TAG for a fair comparison.', 'embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16.', 'the document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130.', 'the relnet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model.', 'the baseline  #TAUTHOR_TAG.', 'the results are shown in table 1.', 'the relnet model achieves a mean error of 0. 285 % across tasks which is better than the results of the  #TAUTHOR_TAG.', 'the relnet model is able to achieve 0 % test error on 11 of the tasks, whereas the  #TAUTHOR_TAG model achieves 0 % error on 7 of the tasks']",4
"['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search for the learning rate in { 0. 01, 0. 005, 0. 001 } and choose a fixed learning rate of 0. 005 based on performance on the validation set, and clip the gradient norm at 2.', 'we keep all other details similar to  #TAUTHOR_TAG for a fair comparison.', 'embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16.', 'the document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130.', 'the relnet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model.', 'the baseline  #TAUTHOR_TAG.', 'the results are shown in table 1.', 'the relnet model achieves a mean error of 0. 285 % across tasks which is better than the results of the  #TAUTHOR_TAG.', 'the relnet model is able to achieve 0 % test error on 11 of the tasks, whereas the  #TAUTHOR_TAG model achieves 0 % error on 7 of the tasks']",4
['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],"['then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip']","['describe the relnet model in this section.', 'the model is sequential in nature, consisting of the following steps : read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip it with an additional relational memory.', '']",3
['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],"['then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip']","['describe the relnet model in this section.', 'the model is sequential in nature, consisting of the following steps : read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip it with an additional relational memory.', '']",3
"['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in the corresponding memory slots : where prelu is a parametric rectified linear', 'unit [ 21 ], and u, v and w are k × k parameter matrices. now we augment the model with additional relational memory cells. intuitively, the entity memory allows modeling of entities and information about the entities in isolation. this can be', 'insufficient in scenarios where a particular entity participates in may relations with other', 'entities across the document. thus, in order to succeed at relational reasoning the model needs to be able to compare each pair of the entity memories. the relational memories will allow modeling of these relations and provide an inherent inductive bias towards a more structured representation of the participating entities in the text,', '']",3
"['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in the corresponding memory slots : where prelu is a parametric rectified linear', 'unit [ 21 ], and u, v and w are k × k parameter matrices. now we augment the model with additional relational memory cells. intuitively, the entity memory allows modeling of entities and information about the entities in isolation. this can be', 'insufficient in scenarios where a particular entity participates in may relations with other', 'entities across the document. thus, in order to succeed at relational reasoning the model needs to be able to compare each pair of the entity memories. the relational memories will allow modeling of these relations and provide an inherent inductive bias towards a more structured representation of the participating entities in the text,', '']",3
"['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search for the learning rate in { 0. 01, 0. 005, 0. 001 } and choose a fixed learning rate of 0. 005 based on performance on the validation set, and clip the gradient norm at 2.', 'we keep all other details similar to  #TAUTHOR_TAG for a fair comparison.', 'embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16.', 'the document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130.', 'the relnet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model.', 'the baseline  #TAUTHOR_TAG.', 'the results are shown in table 1.', 'the relnet model achieves a mean error of 0. 285 % across tasks which is better than the results of the  #TAUTHOR_TAG.', 'the relnet model is able to achieve 0 % test error on 11 of the tasks, whereas the  #TAUTHOR_TAG model achieves 0 % error on 7 of the tasks']",3
"['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search for the learning rate in { 0. 01, 0. 005, 0. 001 } and choose a fixed learning rate of 0. 005 based on performance on the validation set, and clip the gradient norm at 2.', 'we keep all other details similar to  #TAUTHOR_TAG for a fair comparison.', 'embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16.', 'the document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130.', 'the relnet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model.', 'the baseline  #TAUTHOR_TAG.', 'the results are shown in table 1.', 'the relnet model achieves a mean error of 0. 285 % across tasks which is better than the results of the  #TAUTHOR_TAG.', 'the relnet model is able to achieve 0 % test error on 11 of the tasks, whereas the  #TAUTHOR_TAG model achieves 0 % error on 7 of the tasks']",3
['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],"['then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip']","['describe the relnet model in this section.', 'the model is sequential in nature, consisting of the following steps : read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip it with an additional relational memory.', '']",6
['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],['in a fashion similar to  #TAUTHOR_TAG ] and then equip'],"['then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip']","['describe the relnet model in this section.', 'the model is sequential in nature, consisting of the following steps : read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer.', 'we model the dynamic memory in a fashion similar to  #TAUTHOR_TAG ] and then equip it with an additional relational memory.', '']",6
"['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in']","['a sentence hence a sigmoid operation is more suitable, and it is also more scalable  #TAUTHOR_TAG. after selecting the set of memories, there is an update step which stores', 'information in the corresponding memory slots : where prelu is a parametric rectified linear', 'unit [ 21 ], and u, v and w are k × k parameter matrices. now we augment the model with additional relational memory cells. intuitively, the entity memory allows modeling of entities and information about the entities in isolation. this can be', 'insufficient in scenarios where a particular entity participates in may relations with other', 'entities across the document. thus, in order to succeed at relational reasoning the model needs to be able to compare each pair of the entity memories. the relational memories will allow modeling of these relations and provide an inherent inductive bias towards a more structured representation of the participating entities in the text,', '']",5
"['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","['tasks which have become a benchmark for evaluating memory - augmented neural networks.', 'we task  #TAUTHOR_TAG.', 'performance is']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search']","[""evaluate the model's performance on the babi tasks [ 19 ], a collection of 20 question answering tasks which have become a benchmark for evaluating memory - augmented neural networks."", 'we task  #TAUTHOR_TAG.', 'performance is measured in terms of mean percentage error on the tasks.', 'training details : we used adam and did a grid search for the learning rate in { 0. 01, 0. 005, 0. 001 } and choose a fixed learning rate of 0. 005 based on performance on the validation set, and clip the gradient norm at 2.', 'we keep all other details similar to  #TAUTHOR_TAG for a fair comparison.', 'embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16.', 'the document sizes were limited to most recent 70 sentences for all tasks, except for task 3 for which it was limited to 130.', 'the relnet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model.', 'the baseline  #TAUTHOR_TAG.', 'the results are shown in table 1.', 'the relnet model achieves a mean error of 0. 285 % across tasks which is better than the results of the  #TAUTHOR_TAG.', 'the relnet model is able to achieve 0 % test error on 11 of the tasks, whereas the  #TAUTHOR_TAG model achieves 0 % error on 7 of the tasks']",7
"['original model released by  #TAUTHOR_TAG.', 'both intrinsic']","['original model released by  #TAUTHOR_TAG.', 'both intrinsic']","['with the original model released by  #TAUTHOR_TAG.', 'both intrinsic']","['##2vec is a prominent tool for natural language processing ( nlp ) tasks.', 'similar inspiration is found in distributed embeddings for state - of - the - art ( sota ) deep neural networks.', 'however, wrong combination of hyper - parameters can produce poor quality vectors.', 'the objective of this work is to show optimal combination of hyper - parameters exists and evaluate various combinations.', 'we compare them with the original model released by  #TAUTHOR_TAG.', '']",5
"['tasks, the default pytorch embedding was tested before being replaced by pre - trained embeddings released by  #TAUTHOR_TAG and ours.', 'in']","['tasks, the default pytorch embedding was tested before being replaced by pre - trained embeddings released by  #TAUTHOR_TAG and ours.', 'in']","['tasks, the default pytorch embedding was tested before being replaced by pre - trained embeddings released by  #TAUTHOR_TAG and ours.', 'in each case, the dataset was shuffled before training and split in the ratio 70 : 15 : 15 for training']","['models were generated in a shared cluster running ubuntu 16 with 32 cpus of 32x intel xeon 4110 at 2. 1ghz.', 'gensim ( rehurek and  #AUTHOR_TAG ) python library implementation of word2vec was used with parallelization to utilize all 32 cpus.', 'the downstream experi - ments were run on a tesla gpu on a shared dgx cluster running ubuntu 18.', 'pytorch deep learning framework was used.', 'gensim was chosen because of its relative stability, popular support and to minimize the time required in writing and testing a new implementation in python from scratch.', '1', 'to form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit ( nltk ) (  #AUTHOR_TAG ) and data pre - processing carried out.', ' #AUTHOR_TAG were chosen for intrinsic evaluations.', 'they measure the quality of word vectors.', 'the analogy scores are averages of both semantic and syntactic tests.', 'ner and sa were chosen for extrinsic evaluations.', 'the gmb dataset for ner was trained in an lstm network, which had an embedding layer for input.', 'the network diagram is shown in fig. 1.', 'the imdb dataset for sa was trained in a bilstm network, which also used an embedding layer for input.', 'its network diagram is given in fig. 2.', 'it includes an additional hidden linear layer.', 'hyper - parameter details of the two networks for the downstream tasks are given in table 2.', 'the metrics for extrinsic evaluation include f1, precision, recall and accuracy scores.', 'in both tasks, the default pytorch embedding was tested before being replaced by pre - trained embeddings released by  #TAUTHOR_TAG and ours.', 'in each case, the dataset was shuffled before training and split in the ratio 70 : 15 : 15 for training, validation ( dev ) and test sets.', 'batch size of 64 was used.', 'for each task, experiments for each embedding was conducted four times and an average value calculated and reported in the next section 1.', 'it should be noted, however, that gensim multithreading for 30 and 40 epochs seemed unstable and crashed, preventing any related experiments.', 'table 3 summarizes key results from the intrinsic evaluations for 300 dimensions.', 'table 4 reveals the training time ( in hours ) and average embedding loading time ( in seconds ) representative of the various models used.', 'tables 5 and 6 summarize key results for the extrinsic evaluations.', 'figures 3, 4, 5, 6 and 7 present line graph of the eight combinations for different dimension sizes for simple wiki, trend of simple wiki and']",5
"['original model released by  #TAUTHOR_TAG.', 'both intrinsic']","['original model released by  #TAUTHOR_TAG.', 'both intrinsic']","['with the original model released by  #TAUTHOR_TAG.', 'both intrinsic']","['##2vec is a prominent tool for natural language processing ( nlp ) tasks.', 'similar inspiration is found in distributed embeddings for state - of - the - art ( sota ) deep neural networks.', 'however, wrong combination of hyper - parameters can produce poor quality vectors.', 'the objective of this work is to show optimal combination of hyper - parameters exists and evaluate various combinations.', 'we compare them with the original model released by  #TAUTHOR_TAG.', '']",4
"[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","['##0 maintains its position as the best in terms of wordsim and spearman correlation', '. besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we', ""consider the amount of time and energy spent, since more training time results in more energy consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's interesting to note that"", 'their presumed best model, which was released is also s1h0. its analogy score, which we tested and report, is confirmed in their paper. it beats our best models in only analogy score ( even for simple wiki ), performing worse in others. this is inspite of using a much bigger corpus of 3, 000, 000 vocabulary size', 'and 100 billion words while simple wiki had vocabulary size of 367, 811 and is 711mb. it is', 'very likely our analogy scores will improve when we use a much larger corpus, as', 'can be observed from table 3, which involves just one billion words. although the two best combinations in analogy (', 'w8s0h0 & w4s0h0 ) for sw, as shown in fig. 3, decreased only slightly compared to others with increasing dimensions, the increased training time and much', 'larger serialized model size render any possible minimal score advantage over higher dimensions undesirable. as can be observed in fig. 4, from 100 dimensions, scores improve but', 'start to drop after over 300 dimensions for sw and after over 400 dimensions for bw. more becomes worse! this trend is true for all combinations for all tests. polynomial interpolation may be used to determine the optimal dimension in both corpora. our models are available for confirmation and source codes are available on github. 2 with', 'regards to ner, most pretrained embeddings outperformed the default pytorch embedding, with our bw w4s1h0 model ( which is best in bw', 'analogy score ) performing', 'best in f1 score and closely followed by  #TAUTHOR_TAG model', '. on the other hand, with regards to sa, pytorch', 'embedding outperformed the pretrained embeddings but was closely followed by our sw w8', '##s0h0 model ( which also had the best sw analogy score ).  #TAUTHOR_TAG performed second worst of all, despite originating from a very huge corpus. the combinations w8s0h0 & w4s0h0 of sw performed reasonably well', 'in both extrinsic tasks, just as the default pytorch embedding did']",4
"[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","['##0 maintains its position as the best in terms of wordsim and spearman correlation', '. besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we', ""consider the amount of time and energy spent, since more training time results in more energy consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's interesting to note that"", 'their presumed best model, which was released is also s1h0. its analogy score, which we tested and report, is confirmed in their paper. it beats our best models in only analogy score ( even for simple wiki ), performing worse in others. this is inspite of using a much bigger corpus of 3, 000, 000 vocabulary size', 'and 100 billion words while simple wiki had vocabulary size of 367, 811 and is 711mb. it is', 'very likely our analogy scores will improve when we use a much larger corpus, as', 'can be observed from table 3, which involves just one billion words. although the two best combinations in analogy (', 'w8s0h0 & w4s0h0 ) for sw, as shown in fig. 3, decreased only slightly compared to others with increasing dimensions, the increased training time and much', 'larger serialized model size render any possible minimal score advantage over higher dimensions undesirable. as can be observed in fig. 4, from 100 dimensions, scores improve but', 'start to drop after over 300 dimensions for sw and after over 400 dimensions for bw. more becomes worse! this trend is true for all combinations for all tests. polynomial interpolation may be used to determine the optimal dimension in both corpora. our models are available for confirmation and source codes are available on github. 2 with', 'regards to ner, most pretrained embeddings outperformed the default pytorch embedding, with our bw w4s1h0 model ( which is best in bw', 'analogy score ) performing', 'best in f1 score and closely followed by  #TAUTHOR_TAG model', '. on the other hand, with regards to sa, pytorch', 'embedding outperformed the pretrained embeddings but was closely followed by our sw w8', '##s0h0 model ( which also had the best sw analogy score ).  #TAUTHOR_TAG performed second worst of all, despite originating from a very huge corpus. the combinations w8s0h0 & w4s0h0 of sw performed reasonably well', 'in both extrinsic tasks, just as the default pytorch embedding did']",4
"[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","[""consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's""]","['##0 maintains its position as the best in terms of wordsim and spearman correlation', '. besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we', ""consider the amount of time and energy spent, since more training time results in more energy consumption information on the length of training time for the released  #TAUTHOR_TAG model is not readily available. however, it's interesting to note that"", 'their presumed best model, which was released is also s1h0. its analogy score, which we tested and report, is confirmed in their paper. it beats our best models in only analogy score ( even for simple wiki ), performing worse in others. this is inspite of using a much bigger corpus of 3, 000, 000 vocabulary size', 'and 100 billion words while simple wiki had vocabulary size of 367, 811 and is 711mb. it is', 'very likely our analogy scores will improve when we use a much larger corpus, as', 'can be observed from table 3, which involves just one billion words. although the two best combinations in analogy (', 'w8s0h0 & w4s0h0 ) for sw, as shown in fig. 3, decreased only slightly compared to others with increasing dimensions, the increased training time and much', 'larger serialized model size render any possible minimal score advantage over higher dimensions undesirable. as can be observed in fig. 4, from 100 dimensions, scores improve but', 'start to drop after over 300 dimensions for sw and after over 400 dimensions for bw. more becomes worse! this trend is true for all combinations for all tests. polynomial interpolation may be used to determine the optimal dimension in both corpora. our models are available for confirmation and source codes are available on github. 2 with', 'regards to ner, most pretrained embeddings outperformed the default pytorch embedding, with our bw w4s1h0 model ( which is best in bw', 'analogy score ) performing', 'best in f1 score and closely followed by  #TAUTHOR_TAG model', '. on the other hand, with regards to sa, pytorch', 'embedding outperformed the pretrained embeddings but was closely followed by our sw w8', '##s0h0 model ( which also had the best sw analogy score ).  #TAUTHOR_TAG performed second worst of all, despite originating from a very huge corpus. the combinations w8s0h0 & w4s0h0 of sw performed reasonably well', 'in both extrinsic tasks, just as the default pytorch embedding did']",4
"['continuous bag of words ( cbow )  #TAUTHOR_TAG.', 'similar distributed models of word']","['continuous bag of words ( cbow )  #TAUTHOR_TAG.', 'similar distributed models of word']","['continuous bag of words ( cbow )  #TAUTHOR_TAG.', 'similar distributed models of word']","['have been many implementations of the word2vec model in either of the two architectures it provides : continuous skipgram and continuous bag of words ( cbow )  #TAUTHOR_TAG.', 'similar distributed models of word or subword embeddings ( or vector representations ) find usage in sota, deep neural networks like bidirectional encoder representations from transformers ( bert ) and its successors (  #AUTHOR_TAG ; ;  #AUTHOR_TAG ).', 'these deep networks generate contextual representations of words after been trained for extended periods on large corpora, unsupervised, using the attention mechanisms (  #AUTHOR_TAG ).', 'it has been observed that various hyper - parameter combinations have been used in different research involving word2vec with the possibility of many of them being sub - optimal (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ).', 'therefore, the authors seek to address the research question : what is the optimal combination of word2vec hyperparameters for intrinsic and extrinsic nlp purposes?', 'there are astronomically high numbers of combinations of hyper - parameters possible for neural networks, even with just a c 2000 * corresponding author.', 'license : cc - by 4. 0, see https : / / creativecommons. org / licenses / by / 4. 0 /. attribution requirements are provided at http : / / jmlr. org / papers / v1 / meila00a. html.', 'few layers.', 'hence, the scope of our extensive work over three corpora is on dimension size, training epochs, window size and vocabulary size for the training algorithms ( hierarchical softmax and negative sampling ) of both skipgram and cbow. the corpora used for word embeddings are english wiki news abstract by  #AUTHOR_TAG a ) of about 15mb, english wiki simple ( sw ) articles by  #AUTHOR_TAG b ) of about 711mb and the billion word ( bw ) of 3. 9gb by  #AUTHOR_TAG.', 'the corpus used for sentiment analysis is the internet movie databse ( imdb ) dataset of movie reviews by  #AUTHOR_TAG while that for ner is groningen meaning bank ( gmb ) by  #AUTHOR_TAG, containing 47, 959 sentence samples.', 'the imdb dataset used has a total of 25, 000 sentences with half being positive sentiments and the other half being negative sentiments.', 'the groningen meaning bank ( gmb ) dataset has 17 labels, with 9 main labels and 2 context tags.', ""it is however unbalanced due to the high percentage of tokens with the label'o '."", 'this skew in the gmb dataset is typical with ner datasets.', 'the objective of this work is to determine the optimal combinations of word2vec hyperparameters for intrinsic evaluation ( semantic and']",1
"['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['and subsample frequent words (  #AUTHOR_TAG b ) ). being a neural network language model ( nnlm ), word2vec assigns probabilities to words in a sequence, like other nnlms such as feedforward', 'networks or recurrent neural networks (  #AUTHOR_TAG ). earlier models like latent dirichlet allocation ( lda ) and latent semantic analysis ( lsa ) exist and effectively achieve low dimensional vectors by matrix factorization', ""(  #AUTHOR_TAG ;  #AUTHOR_TAG ). it's been shown that word vectors are beneficial for nlp tasks (  #AUTHOR_TAG ), such"", 'as sentiment analysis and named entity recognition. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words can be evaluated, expressing the quality of vectors produced from the model', '. the famous, semantic example : vector ( "" king "" ) - vector ( "" man "" ) + vector ( "" woman "" ) ≈ vector ( "" queen "" )', 'can be verified using cosine distance. another type of semantic meaning is the relationship between a capital city and its corresponding country', '. syntactic relationship examples include plural verbs and past tense, among others. combination of both syntactic and semantic analyses is possible and', 'provided ( totaling over 19, 000 questions ) as google analogy test set by  #TAUTHOR_TAG. wordsimilarity - 353 test set is another analysis tool for word', 'vectors (  #AUTHOR_TAG ). unlike google analogy score, which is based on vector space algebra, wordsimilarity is', 'based on human expert - assigned semantic similarity on two sets of english word pairs. both tools', 'rank from 0 ( totally dissimilar ) to 1 ( very much similar or exact, in google analogy case ). a typical artificial neural', 'network ( ann ) has very many hyper - parameters which', 'may be tuned. hyper - parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate', '(  #AUTHOR_TAG ).  #TAUTHOR_TAG model, ranging from 50 to 1, 000 dimensions, 30, 000 to 3, 000, 000 vocabulary sizes, 1 to 3 epochs, among others. in our work', ', we extended research to 3, 000 dimensions. different observations were noted from the many trials. they observed diminishing returns after a certain point, despite additional dimensions or larger', ', unstructured training data. however, quality increased when both dimensions and data size were increased together.  #AUTHOR_TAG b ) pointed out that choice of', 'optimal hyper - parameter configurations depends on the nlp problem at hand, they identified the most important factors are architecture, dimension size,', 'subsampling rate, and the window size. in addition, it has been observed that variables like size of datasets improve the quality of word vectors and, potentially']",0
"['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['and subsample frequent words (  #AUTHOR_TAG b ) ). being a neural network language model ( nnlm ), word2vec assigns probabilities to words in a sequence, like other nnlms such as feedforward', 'networks or recurrent neural networks (  #AUTHOR_TAG ). earlier models like latent dirichlet allocation ( lda ) and latent semantic analysis ( lsa ) exist and effectively achieve low dimensional vectors by matrix factorization', ""(  #AUTHOR_TAG ;  #AUTHOR_TAG ). it's been shown that word vectors are beneficial for nlp tasks (  #AUTHOR_TAG ), such"", 'as sentiment analysis and named entity recognition. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words can be evaluated, expressing the quality of vectors produced from the model', '. the famous, semantic example : vector ( "" king "" ) - vector ( "" man "" ) + vector ( "" woman "" ) ≈ vector ( "" queen "" )', 'can be verified using cosine distance. another type of semantic meaning is the relationship between a capital city and its corresponding country', '. syntactic relationship examples include plural verbs and past tense, among others. combination of both syntactic and semantic analyses is possible and', 'provided ( totaling over 19, 000 questions ) as google analogy test set by  #TAUTHOR_TAG. wordsimilarity - 353 test set is another analysis tool for word', 'vectors (  #AUTHOR_TAG ). unlike google analogy score, which is based on vector space algebra, wordsimilarity is', 'based on human expert - assigned semantic similarity on two sets of english word pairs. both tools', 'rank from 0 ( totally dissimilar ) to 1 ( very much similar or exact, in google analogy case ). a typical artificial neural', 'network ( ann ) has very many hyper - parameters which', 'may be tuned. hyper - parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate', '(  #AUTHOR_TAG ).  #TAUTHOR_TAG model, ranging from 50 to 1, 000 dimensions, 30, 000 to 3, 000, 000 vocabulary sizes, 1 to 3 epochs, among others. in our work', ', we extended research to 3, 000 dimensions. different observations were noted from the many trials. they observed diminishing returns after a certain point, despite additional dimensions or larger', ', unstructured training data. however, quality increased when both dimensions and data size were increased together.  #AUTHOR_TAG b ) pointed out that choice of', 'optimal hyper - parameter configurations depends on the nlp problem at hand, they identified the most important factors are architecture, dimension size,', 'subsampling rate, and the window size. in addition, it has been observed that variables like size of datasets improve the quality of word vectors and, potentially']",0
"['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['and subsample frequent words (  #AUTHOR_TAG b ) ). being a neural network language model ( nnlm ), word2vec assigns probabilities to words in a sequence, like other nnlms such as feedforward', 'networks or recurrent neural networks (  #AUTHOR_TAG ). earlier models like latent dirichlet allocation ( lda ) and latent semantic analysis ( lsa ) exist and effectively achieve low dimensional vectors by matrix factorization', ""(  #AUTHOR_TAG ;  #AUTHOR_TAG ). it's been shown that word vectors are beneficial for nlp tasks (  #AUTHOR_TAG ), such"", 'as sentiment analysis and named entity recognition. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words can be evaluated, expressing the quality of vectors produced from the model', '. the famous, semantic example : vector ( "" king "" ) - vector ( "" man "" ) + vector ( "" woman "" ) ≈ vector ( "" queen "" )', 'can be verified using cosine distance. another type of semantic meaning is the relationship between a capital city and its corresponding country', '. syntactic relationship examples include plural verbs and past tense, among others. combination of both syntactic and semantic analyses is possible and', 'provided ( totaling over 19, 000 questions ) as google analogy test set by  #TAUTHOR_TAG. wordsimilarity - 353 test set is another analysis tool for word', 'vectors (  #AUTHOR_TAG ). unlike google analogy score, which is based on vector space algebra, wordsimilarity is', 'based on human expert - assigned semantic similarity on two sets of english word pairs. both tools', 'rank from 0 ( totally dissimilar ) to 1 ( very much similar or exact, in google analogy case ). a typical artificial neural', 'network ( ann ) has very many hyper - parameters which', 'may be tuned. hyper - parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate', '(  #AUTHOR_TAG ).  #TAUTHOR_TAG model, ranging from 50 to 1, 000 dimensions, 30, 000 to 3, 000, 000 vocabulary sizes, 1 to 3 epochs, among others. in our work', ', we extended research to 3, 000 dimensions. different observations were noted from the many trials. they observed diminishing returns after a certain point, despite additional dimensions or larger', ', unstructured training data. however, quality increased when both dimensions and data size were increased together.  #AUTHOR_TAG b ) pointed out that choice of', 'optimal hyper - parameter configurations depends on the nlp problem at hand, they identified the most important factors are architecture, dimension size,', 'subsampling rate, and the window size. in addition, it has been observed that variables like size of datasets improve the quality of word vectors and, potentially']",0
"['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['and subsample frequent words (  #AUTHOR_TAG b ) ). being a neural network language model ( nnlm ), word2vec assigns probabilities to words in a sequence, like other nnlms such as feedforward', 'networks or recurrent neural networks (  #AUTHOR_TAG ). earlier models like latent dirichlet allocation ( lda ) and latent semantic analysis ( lsa ) exist and effectively achieve low dimensional vectors by matrix factorization', ""(  #AUTHOR_TAG ;  #AUTHOR_TAG ). it's been shown that word vectors are beneficial for nlp tasks (  #AUTHOR_TAG ), such"", 'as sentiment analysis and named entity recognition. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words can be evaluated, expressing the quality of vectors produced from the model', '. the famous, semantic example : vector ( "" king "" ) - vector ( "" man "" ) + vector ( "" woman "" ) ≈ vector ( "" queen "" )', 'can be verified using cosine distance. another type of semantic meaning is the relationship between a capital city and its corresponding country', '. syntactic relationship examples include plural verbs and past tense, among others. combination of both syntactic and semantic analyses is possible and', 'provided ( totaling over 19, 000 questions ) as google analogy test set by  #TAUTHOR_TAG. wordsimilarity - 353 test set is another analysis tool for word', 'vectors (  #AUTHOR_TAG ). unlike google analogy score, which is based on vector space algebra, wordsimilarity is', 'based on human expert - assigned semantic similarity on two sets of english word pairs. both tools', 'rank from 0 ( totally dissimilar ) to 1 ( very much similar or exact, in google analogy case ). a typical artificial neural', 'network ( ann ) has very many hyper - parameters which', 'may be tuned. hyper - parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate', '(  #AUTHOR_TAG ).  #TAUTHOR_TAG model, ranging from 50 to 1, 000 dimensions, 30, 000 to 3, 000, 000 vocabulary sizes, 1 to 3 epochs, among others. in our work', ', we extended research to 3, 000 dimensions. different observations were noted from the many trials. they observed diminishing returns after a certain point, despite additional dimensions or larger', ', unstructured training data. however, quality increased when both dimensions and data size were increased together.  #AUTHOR_TAG b ) pointed out that choice of', 'optimal hyper - parameter configurations depends on the nlp problem at hand, they identified the most important factors are architecture, dimension size,', 'subsampling rate, and the window size. in addition, it has been observed that variables like size of datasets improve the quality of word vectors and, potentially']",0
"['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words']","['and subsample frequent words (  #AUTHOR_TAG b ) ). being a neural network language model ( nnlm ), word2vec assigns probabilities to words in a sequence, like other nnlms such as feedforward', 'networks or recurrent neural networks (  #AUTHOR_TAG ). earlier models like latent dirichlet allocation ( lda ) and latent semantic analysis ( lsa ) exist and effectively achieve low dimensional vectors by matrix factorization', ""(  #AUTHOR_TAG ;  #AUTHOR_TAG ). it's been shown that word vectors are beneficial for nlp tasks (  #AUTHOR_TAG ), such"", 'as sentiment analysis and named entity recognition. besides,  #TAUTHOR_TAG showed with vector space algebra that relationships among', 'words can be evaluated, expressing the quality of vectors produced from the model', '. the famous, semantic example : vector ( "" king "" ) - vector ( "" man "" ) + vector ( "" woman "" ) ≈ vector ( "" queen "" )', 'can be verified using cosine distance. another type of semantic meaning is the relationship between a capital city and its corresponding country', '. syntactic relationship examples include plural verbs and past tense, among others. combination of both syntactic and semantic analyses is possible and', 'provided ( totaling over 19, 000 questions ) as google analogy test set by  #TAUTHOR_TAG. wordsimilarity - 353 test set is another analysis tool for word', 'vectors (  #AUTHOR_TAG ). unlike google analogy score, which is based on vector space algebra, wordsimilarity is', 'based on human expert - assigned semantic similarity on two sets of english word pairs. both tools', 'rank from 0 ( totally dissimilar ) to 1 ( very much similar or exact, in google analogy case ). a typical artificial neural', 'network ( ann ) has very many hyper - parameters which', 'may be tuned. hyper - parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate', '(  #AUTHOR_TAG ).  #TAUTHOR_TAG model, ranging from 50 to 1, 000 dimensions, 30, 000 to 3, 000, 000 vocabulary sizes, 1 to 3 epochs, among others. in our work', ', we extended research to 3, 000 dimensions. different observations were noted from the many trials. they observed diminishing returns after a certain point, despite additional dimensions or larger', ', unstructured training data. however, quality increased when both dimensions and data size were increased together.  #AUTHOR_TAG b ) pointed out that choice of', 'optimal hyper - parameter configurations depends on the nlp problem at hand, they identified the most important factors are architecture, dimension size,', 'subsampling rate, and the window size. in addition, it has been observed that variables like size of datasets improve the quality of word vectors and, potentially']",0
"['', 'similarly to  #TAUTHOR_TAG,']","['', 'similarly to  #TAUTHOR_TAG,']","['question parsing using a sequence - to - sequence model within an architecture dubbed neural sparql machine, previously introduced in  #AUTHOR_TAG.', 'similarly to  #TAUTHOR_TAG,']","['', 'using complex surface forms leads to more graph patterns.', 'we aim at learning these compositions.', 'tackle the kbqa problem  #AUTHOR_TAG.', 'we study the application of the neural machine translation paradigm for question parsing using a sequence - to - sequence model within an architecture dubbed neural sparql machine, previously introduced in  #AUTHOR_TAG.', 'similarly to  #TAUTHOR_TAG, we employ a sequence - to - sequence model to learn query expressions and their compositions.', 'instead of inducing the programs through question - answer pairs, we expect a semi - supervised approach, where alignments between questions and queries are built through templates.', 'although query induction can save a considerable amount of supervision effort  #TAUTHOR_TAG, a pseudo - gold program is not guaranteed to be correct when the same answer can be found with more than one query ( e. g., as the capital is often the largest city of a country, predicates might be confused ).', 'on the contrary, our proposed solution relies on manual annotation and a weakly - supervised expansion of question - query templates']",3
"['', 'similarly to  #TAUTHOR_TAG,']","['', 'similarly to  #TAUTHOR_TAG,']","['question parsing using a sequence - to - sequence model within an architecture dubbed neural sparql machine, previously introduced in  #AUTHOR_TAG.', 'similarly to  #TAUTHOR_TAG,']","['', 'using complex surface forms leads to more graph patterns.', 'we aim at learning these compositions.', 'tackle the kbqa problem  #AUTHOR_TAG.', 'we study the application of the neural machine translation paradigm for question parsing using a sequence - to - sequence model within an architecture dubbed neural sparql machine, previously introduced in  #AUTHOR_TAG.', 'similarly to  #TAUTHOR_TAG, we employ a sequence - to - sequence model to learn query expressions and their compositions.', 'instead of inducing the programs through question - answer pairs, we expect a semi - supervised approach, where alignments between questions and queries are built through templates.', 'although query induction can save a considerable amount of supervision effort  #TAUTHOR_TAG, a pseudo - gold program is not guaranteed to be correct when the same answer can be found with more than one query ( e. g., as the capital is often the largest city of a country, predicates might be confused ).', 'on the contrary, our proposed solution relies on manual annotation and a weakly - supervised expansion of question - query templates']",5
"['', 'similarly to  #TAUTHOR_TAG,']","['', 'similarly to  #TAUTHOR_TAG,']","['question parsing using a sequence - to - sequence model within an architecture dubbed neural sparql machine, previously introduced in  #AUTHOR_TAG.', 'similarly to  #TAUTHOR_TAG,']","['', 'using complex surface forms leads to more graph patterns.', 'we aim at learning these compositions.', 'tackle the kbqa problem  #AUTHOR_TAG.', 'we study the application of the neural machine translation paradigm for question parsing using a sequence - to - sequence model within an architecture dubbed neural sparql machine, previously introduced in  #AUTHOR_TAG.', 'similarly to  #TAUTHOR_TAG, we employ a sequence - to - sequence model to learn query expressions and their compositions.', 'instead of inducing the programs through question - answer pairs, we expect a semi - supervised approach, where alignments between questions and queries are built through templates.', 'although query induction can save a considerable amount of supervision effort  #TAUTHOR_TAG, a pseudo - gold program is not guaranteed to be correct when the same answer can be found with more than one query ( e. g., as the capital is often the largest city of a country, predicates might be confused ).', 'on the contrary, our proposed solution relies on manual annotation and a weakly - supervised expansion of question - query templates']",0
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"['##ql operator composition, in a similar fashion of  #TAUTHOR_TAG.', 'we argue that the coverage of language utterances can be expanded using techniques']","['', 'note that a sequence can be represented by any lisp s - expression ; therefore, alternatively, sentence dependency trees can be used to encode questions and arq algebra  #AUTHOR_TAG can be used to encode sparql queries.', 'neural sparql machines do not rely on entity linking methods, since entities and relations are detected within the query construction phase.', 'external pre - trained word embeddings help deal with vocabulary mismatch.', 'knowledge graph jointly embedded with sparql operators  #AUTHOR_TAG can be utilized in the target space.', 'a curriculum learning  #AUTHOR_TAG paradigm can learn graph pattern and sparql operator composition, in a similar fashion of  #TAUTHOR_TAG.', 'we argue that the coverage of language utterances can be expanded using techniques such as question  #AUTHOR_TAG and query generation  #AUTHOR_TAG as well as universal sentence encoders  #AUTHOR_TAG.', 'another problem is the disambiguation between entities having the same surface forms.', 'building on top of the dbtrends approach  #AUTHOR_TAG, we force the number of occurrences of a given entity in the training set to be inversely proportional to the entity ranking.', 'following this strategy, we expect the rnn to associate the word berlin with the german capital and not with berlin, new hampshire']",0
[' #TAUTHOR_TAG. this corpus was also useful'],"['also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check']","['to earlier efforts, we also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check']","['each with a weakly connected dag discourse structure ), 12, 588 "" linguistic "" dus, 31, 811 "" non - linguistic "" dus and 31, 251', ""semantic relations. a dialogue begins at the beginning of a player's turn, and ends at the end of that player's turn. in the interim, players can bargain with each other or make spontaneous conversation. these player utterances"", 'are the "" linguistic "" turns. in addition the corpus contains information given visually in the game interface but transcribed in the corpus into', 'server or interface messages, "" nonlinguistic "" turns  #AUTHOR_TAG. all turns', 'are segmented into dus, and these units are', 'then connected by semantic relations. each dialogue represents a complete conversation. there are typically many such conversations, each beginning with a non - linguistic turn in which a player', 'is designated to begin negotiations ( see figure 1 ). the dialogues end when this player', 'performs a non - linguistic action that', 'signals the end of their turn. the dialogues are the units on which we build a complete discourse structure. the stac multimodal corpus is divided into', 'a development, train and test set. the development and test sets are each 10 % of the total size', 'of the corpus. to compare our approach to earlier efforts, we also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check for over fitting of', 'our generative model developed on the multi - modal data. the corpus from  #TAUTHOR_TAG is an', 'early version of a "" linguistic only "" version of the stac corpus. it', 'contains no nonlinguistic dus, unlike the stac multimodal corpus. 3 it also contains quite a', 'few errors ; for example, about 60 stories in the  #TAUTHOR_TAG dataset have no discourse structure in them at all', 'and consist of only one du. we eliminated these from the perret 2016 data set', 'that we used in our comparative experiments below, as these sto - 3 there is also on the stac website an updated linguistic only version of the stac corpus. it', 'has 1, 091 dialogues, 11, 961 linguistic only dus and 10, 191 semantic relations. we have not reported results on that data set here. the dataset from  #TAUTHOR_TAG is similar to our linguistic', 'only stac corpus but is still substantially different and degraded in quality. report significant error rates in annotation on the earlier versions', 'of the stac corpus and that the current linguistic only', 'corpus of stac offers an improvement over the  #TAUTHOR_TAG corpus. ries were obviously not a correct representation of what was going', 'on in the game at the relevant point']",4
[' #TAUTHOR_TAG. this corpus was also useful'],"['also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check']","['to earlier efforts, we also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check']","['each with a weakly connected dag discourse structure ), 12, 588 "" linguistic "" dus, 31, 811 "" non - linguistic "" dus and 31, 251', ""semantic relations. a dialogue begins at the beginning of a player's turn, and ends at the end of that player's turn. in the interim, players can bargain with each other or make spontaneous conversation. these player utterances"", 'are the "" linguistic "" turns. in addition the corpus contains information given visually in the game interface but transcribed in the corpus into', 'server or interface messages, "" nonlinguistic "" turns  #AUTHOR_TAG. all turns', 'are segmented into dus, and these units are', 'then connected by semantic relations. each dialogue represents a complete conversation. there are typically many such conversations, each beginning with a non - linguistic turn in which a player', 'is designated to begin negotiations ( see figure 1 ). the dialogues end when this player', 'performs a non - linguistic action that', 'signals the end of their turn. the dialogues are the units on which we build a complete discourse structure. the stac multimodal corpus is divided into', 'a development, train and test set. the development and test sets are each 10 % of the total size', 'of the corpus. to compare our approach to earlier efforts, we also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check for over fitting of', 'our generative model developed on the multi - modal data. the corpus from  #TAUTHOR_TAG is an', 'early version of a "" linguistic only "" version of the stac corpus. it', 'contains no nonlinguistic dus, unlike the stac multimodal corpus. 3 it also contains quite a', 'few errors ; for example, about 60 stories in the  #TAUTHOR_TAG dataset have no discourse structure in them at all', 'and consist of only one du. we eliminated these from the perret 2016 data set', 'that we used in our comparative experiments below, as these sto - 3 there is also on the stac website an updated linguistic only version of the stac corpus. it', 'has 1, 091 dialogues, 11, 961 linguistic only dus and 10, 191 semantic relations. we have not reported results on that data set here. the dataset from  #TAUTHOR_TAG is similar to our linguistic', 'only stac corpus but is still substantially different and degraded in quality. report significant error rates in annotation on the earlier versions', 'of the stac corpus and that the current linguistic only', 'corpus of stac offers an improvement over the  #TAUTHOR_TAG corpus. ries were obviously not a correct representation of what was going', 'on in the game at the relevant point']",4
"['##ed on our version of the ( perret  #TAUTHOR_TAG, lo - greg *']","[""##ed on our version of the ( perret  #TAUTHOR_TAG, lo - greg *'s global model significantly""]","[""##ed on our version of the ( perret  #TAUTHOR_TAG, lo - greg *'s global model significantly""]","['', 'link variant. gen + "" mst - short "" has the highest attachment score of all approaches to the problem of attachment in the literature  #AUTHOR_TAG, though we are cautious in', 'comparing scores for systems applied to different corpora. finally, we', ""wanted to see how gen and our other models fared on our version of the ( perret  #TAUTHOR_TAG, lo - greg *'s global model significantly improves over the gen's. we see a 6 point loss in f1 score on gen's"", ""global model relative to logreg *'s, even though both used"", ""identical mst decoding mechanisms. this is what one would expect from a snorkel based architecture, although it '"", 's not the rule that we observed for gen. the', 'only reason gen did not beat logreg * is that it did not get a sufficient boost from decoding. we think that this', 'happened because our lfs already contain a lot of global information about the discourse structure, which meant that mst had less of an effect. note, however, that even on the  #TAUTHOR_TAG', '. this 12 % boost is the upper limit for boosts with mst that we were able to reproduce. this could be a result of our eliminating the degraded one edu stories from the data set']",4
[' #TAUTHOR_TAG. this corpus was also useful'],"['also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check']","['to earlier efforts, we also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check']","['each with a weakly connected dag discourse structure ), 12, 588 "" linguistic "" dus, 31, 811 "" non - linguistic "" dus and 31, 251', ""semantic relations. a dialogue begins at the beginning of a player's turn, and ends at the end of that player's turn. in the interim, players can bargain with each other or make spontaneous conversation. these player utterances"", 'are the "" linguistic "" turns. in addition the corpus contains information given visually in the game interface but transcribed in the corpus into', 'server or interface messages, "" nonlinguistic "" turns  #AUTHOR_TAG. all turns', 'are segmented into dus, and these units are', 'then connected by semantic relations. each dialogue represents a complete conversation. there are typically many such conversations, each beginning with a non - linguistic turn in which a player', 'is designated to begin negotiations ( see figure 1 ). the dialogues end when this player', 'performs a non - linguistic action that', 'signals the end of their turn. the dialogues are the units on which we build a complete discourse structure. the stac multimodal corpus is divided into', 'a development, train and test set. the development and test sets are each 10 % of the total size', 'of the corpus. to compare our approach to earlier efforts, we also used', 'the corpus from  #TAUTHOR_TAG. this corpus was also useful to check for over fitting of', 'our generative model developed on the multi - modal data. the corpus from  #TAUTHOR_TAG is an', 'early version of a "" linguistic only "" version of the stac corpus. it', 'contains no nonlinguistic dus, unlike the stac multimodal corpus. 3 it also contains quite a', 'few errors ; for example, about 60 stories in the  #TAUTHOR_TAG dataset have no discourse structure in them at all', 'and consist of only one du. we eliminated these from the perret 2016 data set', 'that we used in our comparative experiments below, as these sto - 3 there is also on the stac website an updated linguistic only version of the stac corpus. it', 'has 1, 091 dialogues, 11, 961 linguistic only dus and 10, 191 semantic relations. we have not reported results on that data set here. the dataset from  #TAUTHOR_TAG is similar to our linguistic', 'only stac corpus but is still substantially different and degraded in quality. report significant error rates in annotation on the earlier versions', 'of the stac corpus and that the current linguistic only', 'corpus of stac offers an improvement over the  #TAUTHOR_TAG corpus. ries were obviously not a correct representation of what was going', 'on in the game at the relevant point']",3
"[""exploit information about the dus'linguistic or non - linguistic status, the dialogue acts they"", 'express, their lexical content, grammatical category and speaker, and the distance between them - features also used in supervised learning methods  #TAUTHOR_TAG']","[""attachment prediction with the result relation in mind. lfs also exploit information about the dus'linguistic or non - linguistic status, the dialogue acts they"", 'express, their lexical content, grammatical category and speaker, and the distance between them - features also used in supervised learning methods  #TAUTHOR_TAG']","[""exploit information about the dus'linguistic or non - linguistic status, the dialogue acts they"", 'express, their lexical content, grammatical category and speaker, and the distance between them - features also used in supervised learning methods  #TAUTHOR_TAG. finally, we fix', '']","['', '1, i. e. before d 2 was asked. that is, we include ( d 1, d 2 ) in our candidates but rule out ( d 2, d 1 ). lfs are expert - composed functions that make an attachment prediction for a given candidate : each lf returns a 1,', 'a 0 or a - 1 ( "" attached "" / "" do not know "" / "" not attached "" ) for each candidate. however, each of our lfs is written and evaluated with a specific relation type result, question - answerpair ( qap ), continuation, sequence, acknowledgement, conditional, contrast, elaboration and comment in mind.', 'in this way, lfs leverage a kind of type - related information, which makes sense from an empirical perspective as well as an', 'epistemological one. an attachment decision concerning two dus is tightly linked to the type of relation relating the dus : when an annotator decides that two dus are attached, he or she does so with some knowledge of what type of relation', ""attaches them. figure 2 shows a sample lfs used for attachment prediction with the result relation in mind. lfs also exploit information about the dus'linguistic or non - linguistic status, the dialogue acts they"", 'express, their lexical content, grammatical category and speaker, and the distance between them - features also used in supervised learning methods  #TAUTHOR_TAG. finally, we fix', 'the order in which each lf "" sees "" the candidates such that it considers adjacent dus before distant dus. this allows lfs to exploit information about previously predicted attachments and dialogue history in new predictions. our rule set and their description are', 'available here : https : / / tizirinagh. github. io /', 'acl2019 /. figure 2 gives an example of a labeling function that we used']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we use']","['set of highly accurate predictions for individual candidates does not necessarily lead to accurate discourse structures ; for instance, without global structural constraints, gen and local models may not yield the directed acyclic graphs ( dags ), required by sdrt.', 'as in previous work  #TAUTHOR_TAG, we use the maximum spanning tree ( mst ) algorithm, and a variation thereof, to ensure that the dialogue structures predicted conform to some more general structural principle.', 'we implemented the chu - liu - edmonds algorithm  #AUTHOR_TAG, an efficient method of finding the highest - scoring non - projective tree in a directed graph, as described in jurafsky and martin 5.', '']",6
"[', by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to']","['that followed improved upon this strategy, by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to']","[""jane smith were also coreferent regardless of the classifier's evaluation of that pair."", 'much work that followed improved upon this strategy, by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to']","['recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on  #AUTHOR_TAG.', 'they built a decision tree classifier to label pairs of mentions as coreferent or not.', 'using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed.', 'transitive closure in this model was done implicitly.', ""if john smith was labeled coreferent with smith, and smith with jane smith, then john smith and jane smith were also coreferent regardless of the classifier's evaluation of that pair."", 'much work that followed improved upon this strategy, by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent  #AUTHOR_TAG b ).', '']",0
"[', by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to']","['that followed improved upon this strategy, by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to']","[""jane smith were also coreferent regardless of the classifier's evaluation of that pair."", 'much work that followed improved upon this strategy, by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to']","['recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on  #AUTHOR_TAG.', 'they built a decision tree classifier to label pairs of mentions as coreferent or not.', 'using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed.', 'transitive closure in this model was done implicitly.', ""if john smith was labeled coreferent with smith, and smith with jane smith, then john smith and jane smith were also coreferent regardless of the classifier's evaluation of that pair."", 'much work that followed improved upon this strategy, by improving the features  #AUTHOR_TAG b ), the type of classifier  #TAUTHOR_TAG, and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent  #AUTHOR_TAG b ).', '']",0
"['will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for']","['will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for']","['it assigns probability mass to nonsensical clusterings.', 'specifically, it will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for each mention, work backwards']","['baseline systems are based on a logistic classifier over pairs of mentions.', 'the probability of a pair of mentions takes the standard logistic form :', '( 1 ) where m i and m j correspond to mentions i and j respectively ; f ( m i, m j ) is a feature function over a pair of mentions ; θ are the feature weights we wish to learn ; and x i, j is a boolean variable which takes value 1 if m i and m j are coreferent, and 0 if they are not.', 'the log likelihood of a document is the sum of the log likelihoods of all pairs of mentions :', '( 2 ) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x i, j. note that this model is degenerate, because it assigns probability mass to nonsensical clusterings.', 'specifically, it will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention.', '']",0
"['will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for']","['will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for']","['it assigns probability mass to nonsensical clusterings.', 'specifically, it will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for each mention, work backwards']","['baseline systems are based on a logistic classifier over pairs of mentions.', 'the probability of a pair of mentions takes the standard logistic form :', '( 1 ) where m i and m j correspond to mentions i and j respectively ; f ( m i, m j ) is a feature function over a pair of mentions ; θ are the feature weights we wish to learn ; and x i, j is a boolean variable which takes value 1 if m i and m j are coreferent, and 0 if they are not.', 'the log likelihood of a document is the sum of the log likelihoods of all pairs of mentions :', '( 2 ) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x i, j. note that this model is degenerate, because it assigns probability mass to nonsensical clusterings.', 'specifically, it will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention.', '']",0
"['same entity.', 'when describing our model, we build upon the notation used by  #TAUTHOR_TAG']","['same entity.', 'when describing our model, we build upon the notation used by  #TAUTHOR_TAG']","['this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity.', 'when describing our model, we build upon the notation used by  #TAUTHOR_TAG']","['this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity.', 'when describing our model, we build upon the notation used by  #TAUTHOR_TAG']",5
"['will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for']","['will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for']","['it assigns probability mass to nonsensical clusterings.', 'specifically, it will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for each mention, work backwards']","['baseline systems are based on a logistic classifier over pairs of mentions.', 'the probability of a pair of mentions takes the standard logistic form :', '( 1 ) where m i and m j correspond to mentions i and j respectively ; f ( m i, m j ) is a feature function over a pair of mentions ; θ are the feature weights we wish to learn ; and x i, j is a boolean variable which takes value 1 if m i and m j are coreferent, and 0 if they are not.', 'the log likelihood of a document is the sum of the log likelihoods of all pairs of mentions :', '( 2 ) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x i, j. note that this model is degenerate, because it assigns probability mass to nonsensical clusterings.', 'specifically, it will allow', 'prior work  #TAUTHOR_TAG has generated training data for pairwise classifiers in the following manner.', 'for each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention.', '']",5
"['ilp system of  #TAUTHOR_TAG, which was']","['coref - ilp system of  #TAUTHOR_TAG, which was']","['the coref - ilp system of  #TAUTHOR_TAG, which was also based on a naive pairwise classifier.', 'they used an ilp solver to']","['results are summarized in table 1.', 'we show performance for both baseline classifiers, as well as our ilp - based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions.', 'for comparison, we also give the results of the coref - ilp system of  #TAUTHOR_TAG, which was also based on a naive pairwise classifier.', 'they used an ilp solver to find an assignment for the variables, but as they note at the end of section 5. 1, it is equivalent to taking all links for which the classifier returns a probability ≥ 0. 5, and so the ilp solver is not really necessary.', 'we also include their joint - ilp numbers, however that system makes use of an additional anaphoricity classifier.', 'for all three corpora, the ilp model beat both baselines for the cluster f - score, rand index, and variation of information metrics.', 'using the b 3 metric, the ilp system and the d & b - style baseline performed about the same on the muc - 6 corpus, though for both ace corpora, the ilp system was the clear winner.', 'when using the muc scorer, the ilp system always did worse than the d & b - style baseline.', '']",5
"['models trained in  #TAUTHOR_TAG using the corpus of historical american english  #AUTHOR_TAG.', '']","['validate our model, we compare our results to those produced via the decade - by - decade models trained in  #TAUTHOR_TAG using the corpus of historical american english  #AUTHOR_TAG.', '']","['validate our model, we compare our results to those produced via the decade - by - decade models trained in  #TAUTHOR_TAG using the corpus of historical american english  #AUTHOR_TAG.', 'we use the']","['validate our model, we compare our results to those produced via the decade - by - decade models trained in  #TAUTHOR_TAG using the corpus of historical american english  #AUTHOR_TAG.', 'we use the same metric and word lists as the authors to compute bias scores.', 'in particular, we compute linguistic bias scores for two analyses presented in  #TAUTHOR_TAG : the extent to which female versus male words are semantically similar to occupation - related words, and the extent to which asian vs. white last names are semantically similar to the same, from 1910 through 1990.', '']",4
['decade - specific model in  #TAUTHOR_TAG'],['decade - specific model in  #TAUTHOR_TAG'],"['in the talk radio corpus is 4 million - over 5x fewer than a median of 22 million words per decade used to train each decade - specific model in  #TAUTHOR_TAG.', 'these results suggest that using our dynamic embedding approach is particularly valuable']","['', 'computing the correlation between daily bias scores and 5 for historical coverage of different refugeerelated news events, please see https : / / www. nytimes. com / topic / subject / refugees - and - displaced - people.', 'the number of mentions of the keyword "" refugee "" across stations yields r = 0. 56, p < 0. 001, suggesting that additional discourse about refugees tends to be biased against them.', 'as a comparison, we also compute bias scores by training one word2vec model per day and projecting all day - by - day models into the same vector space using orthogonal procrustes alignment 6 similar to  #AUTHOR_TAG.', 'the resulting scores from this non - dynamic model are depicted in 3 ( a ).', 'from qualitative inspection, the day - byday scores produced by the non - dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee - related news event.', 'one possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million - over 5x fewer than a median of 22 million words per decade used to train each decade - specific model in  #TAUTHOR_TAG.', 'these results suggest that using our dynamic embedding approach is particularly valuable when data is sparse for any given attribute']",4
"['sentiment classifier  #TAUTHOR_TAG to assign sentiment labels.', 'to the best of our knowledge, this is the first study to measure the']","['to arabic sa on social media, using off - the - shelf machine translation systems to translate arabic tweets into english and then use a state - of - the - art sentiment classifier  #TAUTHOR_TAG to assign sentiment labels.', 'to the best of our knowledge, this is the first study to measure the']","['- shelf machine translation systems to translate arabic tweets into english and then use a state - of - the - art sentiment classifier  #TAUTHOR_TAG to assign sentiment labels.', 'to the best of our knowledge, this is the first study to measure the impact of automatically translated data on']","['the past decade, there has been a growing interest in collecting, processing and analysing usergenerated text from social media using sentiment analysis ( sa ).', 'sa determines the polarity of a given text, i. e. whether its overall sentiment is negative or positive.', 'while previous work on sa for english tweets reports an overall accuracy of 65 - 71 % on average  #AUTHOR_TAG, recent studies investigating arabic tweets only report accuracy scores ranging between 49 - 65 %  #AUTHOR_TAG b ).', 'arabic sa faces a number of challenges : first, arabic used in social media is usually a mixture of modern standard arabic ( msa ) and one or more of its dialects ( das ).', 'standard toolkits for natural language processing ( nlp ) mainly cover the former and perform poorly on the latter 1.', 'these tools are vital for the performance of machine learning ( ml ) approaches to arabic sa : traditionally, ml approaches use a "" bag of words "" ( bow ) model ( e. g.  #AUTHOR_TAG ).', 'however, for morphologically rich languages, such as arabic, a mixture of stemmed tokens and morphological features have shown to outperform bow approaches ( abdul -  #AUTHOR_TAG, accounting for the fact that arabic contains a very large number of inflected words.', 'in addition ( or maybe as a result ), there is much less interest from the research community in tackling the challenge of arabic sa for social media.', 'as such, there are much fewer open resources available, such as annotated data sets or sentiment lexica.', 'we therefore explore an alternative approach to arabic sa on social media, using off - the - shelf machine translation systems to translate arabic tweets into english and then use a state - of - the - art sentiment classifier  #TAUTHOR_TAG to assign sentiment labels.', 'to the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of arabic tweets.', 'in particular, we address the following research questions :', '1. how does off - the - shelf mt on arabic social data influence sa performance?', '2. can mt - based approaches be a viable alternative to improve sentiment classification performance on arabic tweets? 3.', 'given the linguistic resources currently available for arabic and its dialects, is it more effective to adapt an mt - based approach instead of building a new system from scratch']",5
"['sa classifier for english  #TAUTHOR_TAG. most importantly, we empirically benchmark its']","['sa classifier for english  #TAUTHOR_TAG. most importantly, we empirically benchmark its performance towards', 'previous sa approaches, including lexicon - based, fully supervised']","['- art sa classifier for english  #TAUTHOR_TAG. most importantly, we empirically benchmark its']","['', 'into chinese. using a held - out test set, the best reported accuracy score was at 81. 3 % with svm on binary classification task : positive vs negative. our', 'approach differs from the ones described, in that we use automatic mt to translate arabic tweets into english and', 'then perform sa using a stateof - the - art sa classifier for english  #TAUTHOR_TAG. most importantly, we empirically benchmark its performance towards', 'previous sa approaches, including lexicon - based, fully supervised and distant supervision sa. tweets from', 'the twitter public stream. we restrict the language of all retrieved tweets to arabic by setting the language parameter to ar. the data - set was manually labeled with gold - standard', 'sentiment orientation by two native speakers of arabic, obtaining a kappa score of 0. 81, which indicates', 'highly reliable annotations. table 1 summarises the data set and its distribution of labels. for sa, we perform binary classification using positive and negative tweets. we apply a number of common', 'preprocessing steps following  #AUTHOR_TAG and  #AUTHOR_TAG to account for noise introduced by twitter. the data set will be released', 'as part of this submission']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['order to obtain the english translation of our twitter data - set, we employ two common and freelyavailable mt systems : google translate and microsoft translator service.', 'we then use the stanford sentiment classifier ( ssc ) developed by  #TAUTHOR_TAG to automatically assign sentiment labels ( positive, negative ) to translated tweets.', 'the classifier is based on a deep learning ( dl ) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments.', '']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['order to obtain the english translation of our twitter data - set, we employ two common and freelyavailable mt systems : google translate and microsoft translator service.', 'we then use the stanford sentiment classifier ( ssc ) developed by  #TAUTHOR_TAG to automatically assign sentiment labels ( positive, negative ) to translated tweets.', 'the classifier is based on a deep learning ( dl ) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments.', '']",5
['classifier  #TAUTHOR_TAG'],['classifier  #TAUTHOR_TAG'],['then use the stanford sentiment classifier  #TAUTHOR_TAG'],"['paper is the first to investigate and empirically evaluate the performance of machine translation ( mt ) - based sentiment analysis ( sa ) for arabic tweets.', 'in particular, we make use of off - theshelf mt tools, such as google and microsoft mt, to translate arabic tweets into english.', 'we then use the stanford sentiment classifier  #TAUTHOR_TAG to automatically assign sentiment labels ( positive, negative ) to translated tweets.', 'in contrast to previous work, we benchmark this approach on a gold - standard test set of 937 manually annotated tweets and compare its performance to standard sa approaches, including lexicon - based, supervised and distant supervision approaches.', '']",5
"['to arabic sa.', 'only the lexiconapproach is balanced between the positive and negative class.', 'note that our ml baseline systems as well as the english sa classifier by  #TAUTHOR_TAG are trained on balanced data sets, i. e. we can assume no prior bias towards one class']","['to arabic sa.', 'only the lexiconapproach is balanced between the positive and negative class.', 'note that our ml baseline systems as well as the english sa classifier by  #TAUTHOR_TAG are trained on balanced data sets, i. e. we can assume no prior bias towards one class']","['negative tweets.', 'this is in line with results reported by  #AUTHOR_TAG b ) which evaluate ds approaches to arabic sa.', 'only the lexiconapproach is balanced between the positive and negative class.', 'note that our ml baseline systems as well as the english sa classifier by  #TAUTHOR_TAG are trained on balanced data sets, i. e. we can assume no prior bias towards one class']","['', 'a possible explanation for that is the timechanging nature of twitter resulting in issues like topic - shift resulting in word token - based features being less effective in such a medium  #AUTHOR_TAG c ).', '• mt - based sa approaches in general have a problem of identifying positive tweets ( low recall and precision ), often misclassifying them as negative.', 'the reverse it true for the ds and fully supervised baselines, which find it hard to identify negative tweets.', 'this is in line with results reported by  #AUTHOR_TAG b ) which evaluate ds approaches to arabic sa.', 'only the lexiconapproach is balanced between the positive and negative class.', 'note that our ml baseline systems as well as the english sa classifier by  #TAUTHOR_TAG are trained on balanced data sets, i. e. we can assume no prior bias towards one class']",3
['ordering  #TAUTHOR_TAG. in'],"['deep "" architecture that adds to the model the feature of being sensitive to word ordering  #TAUTHOR_TAG. in']","['deep "" architecture that adds to the model the feature of being sensitive to word ordering  #TAUTHOR_TAG. in future work, we will verify this']",[' #TAUTHOR_TAG'],2
"['providers  #TAUTHOR_TAG.', 'new']","['providers  #TAUTHOR_TAG.', 'new']","['', 'neural models have consistently shown top performance in shared evaluation tasks  #AUTHOR_TAG and are becoming the technology of choice for commercial mt service providers  #TAUTHOR_TAG.', 'new work from']","['the relatively short time since its introduction, neural machine translation has risen to prominence in both academia and industry.', 'neural models have consistently shown top performance in shared evaluation tasks  #AUTHOR_TAG and are becoming the technology of choice for commercial mt service providers  #TAUTHOR_TAG.', 'new work from the research community regularly introduces model extensions and algorithms that show significant gains over baseline nmt.', '']",0
"['0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to']","['0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to']","['0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to the top 50k source words and 50k target words by frequency, with all others mapped to an unk token.', 'a post - processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word ( highest attention score ) and backing off to copying the source word directly  #AUTHOR_TAG.', 'experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm.', 'evaluation is conducted by average bleu score over multiple independent training runs  #AUTHOR_TAG']","['starting point for experimentation is a standard baseline neural machine translation system implemented using the lamtram 1 and dynet 2 toolkits  #AUTHOR_TAG.', 'this system uses the attentional encoder - decoder architecture described by  #AUTHOR_TAG, building on work by  #AUTHOR_TAG.', 'the translation model uses a bi - directional encoder with a single lstm layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512.', 'translation models are trained until perplexity convergence on held - out data using the adam algorithm with a maximum step size of 0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to the top 50k source words and 50k target words by frequency, with all others mapped to an unk token.', 'a post - processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word ( highest attention score ) and backing off to copying the source word directly  #AUTHOR_TAG.', 'experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm.', 'evaluation is conducted by average bleu score over multiple independent training runs  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', ""however, adam's speed and reputation of generally being""]","['annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough "" have made']","['annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough']","['first neural translation models were optimized with stochastic gradient descent  #AUTHOR_TAG.', 'after training for several epochs with a fixed learning rate, the rate is halved at prespecified intervals.', 'this widely used rate "" annealing "" technique takes large steps to move parameters from their initial point to a promising part of the search space followed by increasingly smaller steps to explore that part of the space for a good local optimum.', 'while effective, this approach can be time consuming and relies on hand - crafted learning schedules that may not generalize to different models and data sets.', 'to eliminate the need for schedules, subsequent nmt work trained models using the adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training  #AUTHOR_TAG.', 'model performance is reported to be equivalent to sgd with annealing, though training still takes a considerable amount of time  #AUTHOR_TAG b ).', 'more recent work seeks to accelerate training with the adam algorithm, which applies momentum on a per - parameter basis and automatically adapts step size subject to a user - specified maximum  #AUTHOR_TAG.', 'while this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough "" have made it a popular choice for researchers and nmt toolkit authors 6  #AUTHOR_TAG.', 'while differences in automatic metric scores between sgd and adam - trained systems may be relatively small, they raise the more general question of training effectiveness.', 'in the following section, we explore the relative quality of the optima found by these training algorithms']",0
"['0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to']","['0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to']","['0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to the top 50k source words and 50k target words by frequency, with all others mapped to an unk token.', 'a post - processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word ( highest attention score ) and backing off to copying the source word directly  #AUTHOR_TAG.', 'experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm.', 'evaluation is conducted by average bleu score over multiple independent training runs  #AUTHOR_TAG']","['starting point for experimentation is a standard baseline neural machine translation system implemented using the lamtram 1 and dynet 2 toolkits  #AUTHOR_TAG.', 'this system uses the attentional encoder - decoder architecture described by  #AUTHOR_TAG, building on work by  #AUTHOR_TAG.', 'the translation model uses a bi - directional encoder with a single lstm layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512.', 'translation models are trained until perplexity convergence on held - out data using the adam algorithm with a maximum step size of 0. 0002  #TAUTHOR_TAG.', 'maximum training sentence length is set to 100 words.', 'model vocabulary is limited to the top 50k source words and 50k target words by frequency, with all others mapped to an unk token.', 'a post - processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word ( highest attention score ) and backing off to copying the source word directly  #AUTHOR_TAG.', 'experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm.', 'evaluation is conducted by average bleu score over multiple independent training runs  #AUTHOR_TAG']",5
"['0002 for adam or very similar are shown to work well in nmt implementations including gnmt  #TAUTHOR_TAG, nematus, marian, and opennmt ( http : / / opennmt']","['for sgd and 0. 0002 for adam or very similar are shown to work well in nmt implementations including gnmt  #TAUTHOR_TAG, nematus, marian, and opennmt ( http : / / opennmt. net ).', '8']","['0002 for adam or very similar are shown to work well in nmt implementations including gnmt  #TAUTHOR_TAG, nematus, marian, and opennmt ( http : / / opennmt']","['', 'for all training, we use a mini - batch size of 512 words.', '8 for wmt systems, we evaluate dev 6 adam is the default optimizer for the lamtram, nematus ( https : / / github. com / rsennrich / nematus ), and marian toolkits ( https : / / github. com / amunmt / marian ).', '7 learning rates of 0. 5 for sgd and 0. 0002 for adam or very similar are shown to work well in nmt implementations including gnmt  #TAUTHOR_TAG, nematus, marian, and opennmt ( http : / / opennmt. net ).', '8 for each mini - batch, sentences are added until the word set perplexity every 50k training sentences for the first training run and every 25k sentences for subsequent runs.', 'for iwslt systems, we evaluate every 25k sentences and then every 6, 250 sentences.', 'training stops when no improvement in perplexity has been seen']",5
"['', 'as  #TAUTHOR_TAG show different levels of effectiveness']","['and bpe systems for all data sets as described in § 2. 1 with the incremental improvement of using adam with rate annealing ( § 3 ).', 'as  #TAUTHOR_TAG show different levels of effectiveness']","['', 'as  #TAUTHOR_TAG show different levels of effectiveness']","['measure the effects of byte pair encoding by training full - word and bpe systems for all data sets as described in § 2. 1 with the incremental improvement of using adam with rate annealing ( § 3 ).', 'as  #TAUTHOR_TAG show different levels of effectiveness for different sub - word vocabulary sizes, we evaluate running bpe with 16k and 32k merge operations.', 'as shown in table 2, sub - word systems outperform full - word systems across the board, despite having fewer total parameters.', 'systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies.', 'based on these results, we recommend 32k as a generally effective vocabulary size and 16k as a contrastive condition when building systems on less than 1 million parallel sentences.', 'to understand the origin of these improvements, we divide the words in each test set into classes based on how the full - word and bpe models handle them and report the unigram f - 1 score for each model on each class.', '']",5
"[' #TAUTHOR_TAG.', ""however, adam's speed and reputation of generally being""]","['annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough "" have made']","['annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough']","['first neural translation models were optimized with stochastic gradient descent  #AUTHOR_TAG.', 'after training for several epochs with a fixed learning rate, the rate is halved at prespecified intervals.', 'this widely used rate "" annealing "" technique takes large steps to move parameters from their initial point to a promising part of the search space followed by increasingly smaller steps to explore that part of the space for a good local optimum.', 'while effective, this approach can be time consuming and relies on hand - crafted learning schedules that may not generalize to different models and data sets.', 'to eliminate the need for schedules, subsequent nmt work trained models using the adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training  #AUTHOR_TAG.', 'model performance is reported to be equivalent to sgd with annealing, though training still takes a considerable amount of time  #AUTHOR_TAG b ).', 'more recent work seeks to accelerate training with the adam algorithm, which applies momentum on a per - parameter basis and automatically adapts step size subject to a user - specified maximum  #AUTHOR_TAG.', 'while this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough "" have made it a popular choice for researchers and nmt toolkit authors 6  #AUTHOR_TAG.', 'while differences in automatic metric scores between sgd and adam - trained systems may be relatively small, they raise the more general question of training effectiveness.', 'in the following section, we explore the relative quality of the optima found by these training algorithms']",4
"[' #TAUTHOR_TAG.', ""however, adam's speed and reputation of generally being""]","['annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough "" have made']","['annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough']","['first neural translation models were optimized with stochastic gradient descent  #AUTHOR_TAG.', 'after training for several epochs with a fixed learning rate, the rate is halved at prespecified intervals.', 'this widely used rate "" annealing "" technique takes large steps to move parameters from their initial point to a promising part of the search space followed by increasingly smaller steps to explore that part of the space for a good local optimum.', 'while effective, this approach can be time consuming and relies on hand - crafted learning schedules that may not generalize to different models and data sets.', 'to eliminate the need for schedules, subsequent nmt work trained models using the adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training  #AUTHOR_TAG.', 'model performance is reported to be equivalent to sgd with annealing, though training still takes a considerable amount of time  #AUTHOR_TAG b ).', 'more recent work seeks to accelerate training with the adam algorithm, which applies momentum on a per - parameter basis and automatically adapts step size subject to a user - specified maximum  #AUTHOR_TAG.', 'while this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing sgd  #TAUTHOR_TAG.', 'however, adam\'s speed and reputation of generally being "" good enough "" have made it a popular choice for researchers and nmt toolkit authors 6  #AUTHOR_TAG.', 'while differences in automatic metric scores between sgd and adam - trained systems may be relatively small, they raise the more general question of training effectiveness.', 'in the following section, we explore the relative quality of the optima found by these training algorithms']",1
"['', 'as  #TAUTHOR_TAG show different levels of effectiveness']","['and bpe systems for all data sets as described in § 2. 1 with the incremental improvement of using adam with rate annealing ( § 3 ).', 'as  #TAUTHOR_TAG show different levels of effectiveness']","['', 'as  #TAUTHOR_TAG show different levels of effectiveness']","['measure the effects of byte pair encoding by training full - word and bpe systems for all data sets as described in § 2. 1 with the incremental improvement of using adam with rate annealing ( § 3 ).', 'as  #TAUTHOR_TAG show different levels of effectiveness for different sub - word vocabulary sizes, we evaluate running bpe with 16k and 32k merge operations.', 'as shown in table 2, sub - word systems outperform full - word systems across the board, despite having fewer total parameters.', 'systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies.', 'based on these results, we recommend 32k as a generally effective vocabulary size and 16k as a contrastive condition when building systems on less than 1 million parallel sentences.', 'to understand the origin of these improvements, we divide the words in each test set into classes based on how the full - word and bpe models handle them and report the unigram f - 1 score for each model on each class.', '']",3
"['of inflection is worth noting  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'even though the english - french']","['of inflection is worth noting  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'even though the english - french']","['off with the strongest.', 'this trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'even though the english - french data is still relatively small ( 220k sentences ), bpe']","['', 'these results further validate claims of the importance of diversity in model ensembles.', 'applying dropout significantly improves all configurations of the czech - english system and some configurations of the english - french system, leveling off with the strongest.', 'this trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'even though the english - french data is still relatively small ( 220k sentences ), bpe leads to a smaller vocabulary of more general translation units, effectively reducing sparsity, while annealing adam can avoid getting stuck in poor local optima.', 'these techniques already lead to better generalization without the need for dropout.', 'finally, we can observe a few key properties of data bootstrapping, the best performing technique on fully strengthened systems.', 'unlike lexicon bias and pre - translation, it modifies only the training data, allowing "" purely neural "" models to be learned from random initialization points.', 'this preserves model diversity, allowing ensembles to benefit as well as single models.', 'further, data bootstrapping is complementary to annealing adam and bpe ; better optimization and a more general vocabulary can make better use of the new training instances.', 'while evaluation on simple vanilla nmt systems would']",3
[' #TAUTHOR_TAG proposed a decision list'],[' #TAUTHOR_TAG proposed a decision list'],[' #TAUTHOR_TAG proposed a decision list'],"['the turkish word "" yuz "" could be interpreted as a noun ( face ), a verb ( swim ) or a number ( hundred ) depending on its context. as we noted before, morphological disambig', '##uation is important for nlp in most of the languages. for instance, although german and french do not have a morphology as rich as turkish,', 'nlp in these languages can still benefit from morphological disambiguation. higher accuracies in nlp tasks such as pos tagging and dependency parsing can be obtained if', 'the morphology of the words are taken into account ( sennrich et al. 2009 ), ( candito et al. 2010 ).', 'we apply our general purpose morphological disambiguation method to these languages and show that high accuracies can be obtained for pos tagging and lemmatization. possible word formations and morphological analyses of the german', 'word "" haus "" and the french word "" savoir "" are given in table 3 and table 4 respectively. there are various approaches proposed for morphological disambiguation based on lexical rules or statistical models.', 'rule based methods apply hand - crafted rules in order to select the correct morphological analyses or eliminate incorrect ones', '( oflazer and kuruoz 1994 ; oflazer and tur 1996 ; daybelge and cicekli 2007 ).  #TAUTHOR_TAG proposed a decision list learning algorithm for extraction of finally, hybrid models which combine statistical and rule based approaches are also proposed ( oflazer and tur', '1996 ; kutlu and cicekli 2013 ). we propose a deep neural architecture followed by the viterbi algorithm for morphological disambiguation of words in a sentence. in this paper we focus on turkish as an example even though the proposed model can be utilized in all morphologically rich languages. we test our approach in german and french in order to prove that the proposed method is able to work well for other languages as well. the network architecture in', 'this paper is designed to produce a classification score for a sequence of n - words. it consists of two layers and a', 'softmax layer. the first layer of the model builds a representation for each word using root embeddings and some syntactic', 'and semantic features. the second layer takes as input the learned word representations and incorporates contextual information', '. a softmax layer uses the output of the second layer to produce', 'a classification score. we use the neural network to produce a score for each n length sequence in a given sentence. we then employ the viterbi algorithm to produce the morphological disambiguation for each word in the sentence by finding the most probable sequence using the output of the soft', '##max layer']",0
"['- speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of']","['forms and part - of - speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of']","['- speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of a million turkish words.', '']","['a natural language processing pipeline morphological disambiguation can be considered at the same level as pos tagging.', 'in order to perform pos tagging in english, various approaches such as rule - based models ( brill 1992 ), statistical models ( brill 1995 ), maximum entropy models ( ratnaparkhi 1997 ), hmms ( cutting et al. 1992 ), crfs ( lafferty, mccallum, and pereira 2001 ) and decision trees ( schmid 1994 ) are proposed.', 'however, morphological disambiguation is a much harder problem in general due to the fact that it requires the classification of both roots, suffixes and the corresponding labels.', 'moreover, compared to an agglutinative language such as turkish, english words can take on a limited number of word forms and part - of - speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of a million turkish words.', 'thus, due to the high number of possible tags and the number of possible analyses in languages with productive morphology, morphological disambiguation is quite different from part - of - speech tagging in english.', 'the previous work on morphological disambiguation in morphologically rich languages can be summarized into three categories : rule based, statistical and hybrid approaches.', '']",0
"['- speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of']","['forms and part - of - speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of']","['- speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of a million turkish words.', '']","['a natural language processing pipeline morphological disambiguation can be considered at the same level as pos tagging.', 'in order to perform pos tagging in english, various approaches such as rule - based models ( brill 1992 ), statistical models ( brill 1995 ), maximum entropy models ( ratnaparkhi 1997 ), hmms ( cutting et al. 1992 ), crfs ( lafferty, mccallum, and pereira 2001 ) and decision trees ( schmid 1994 ) are proposed.', 'however, morphological disambiguation is a much harder problem in general due to the fact that it requires the classification of both roots, suffixes and the corresponding labels.', 'moreover, compared to an agglutinative language such as turkish, english words can take on a limited number of word forms and part - of - speech tags.', ' #TAUTHOR_TAG observe that more than ten thousand tag types exists in a corpus comprised of a million turkish words.', 'thus, due to the high number of possible tags and the number of possible analyses in languages with productive morphology, morphological disambiguation is quite different from part - of - speech tagging in english.', 'the previous work on morphological disambiguation in morphologically rich languages can be summarized into three categories : rule based, statistical and hybrid approaches.', '']",0
['developed in  #TAUTHOR_TAG are presented in lines 1'],['developed in  #TAUTHOR_TAG are presented in lines 1'],"[') and the decision list learning', 'algorithm developed in  #TAUTHOR_TAG are presented in lines 1 and 2']",[' #TAUTHOR_TAG'],0
['developed in  #TAUTHOR_TAG are presented in lines 1'],['developed in  #TAUTHOR_TAG are presented in lines 1'],"[') and the decision list learning', 'algorithm developed in  #TAUTHOR_TAG are presented in lines 1 and 2']",[' #TAUTHOR_TAG'],5
['developed in  #TAUTHOR_TAG are presented in lines 1'],['developed in  #TAUTHOR_TAG are presented in lines 1'],"[') and the decision list learning', 'algorithm developed in  #TAUTHOR_TAG are presented in lines 1 and 2']",[' #TAUTHOR_TAG'],7
"[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geo']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']",[' #TAUTHOR_TAG'],0
"['graph.', 'as shown by  #TAUTHOR_TAG, geo']","['graph.', 'as shown by  #TAUTHOR_TAG, geolocation predictions from text']","['completely fail to geolocate users who are not connected to geolocated components of the graph.', 'as shown by  #TAUTHOR_TAG, geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text - and network - based approach to user geolocation']","['recent spike in interest on user geolocation over social media data has resulted in the development of a range of approaches to automatic geolocation prediction, based on information sources such as the text of messages, social networks, user profile data, and temporal data.', 'text - based methods model the geographical bias of language use in social media, and use it to geolocate non - geotagged users.', 'gazetted expressions  #AUTHOR_TAG and geographical names  #AUTHOR_TAG were used as feature in early work, but were shown to be sparse in coverage.', ' #AUTHOR_TAG used information - theoretic methods to automatically extract location - indicative words for location classification.', 'reported that discriminative approaches ( based on hierarchical classification over adaptive grids ), when optimised properly, are superior to explicit feature selection.', ' #AUTHOR_TAG showed that sparse coding can be used to effectively learn a latent representation of tweet text to use in user geolocation.', ' #AUTHOR_TAG and  #AUTHOR_TAG proposed topic modelbased approaches to geolocation, based on the assumption that words are generated from hidden topics and geographical regions.', ' #AUTHOR_TAG used graphical models to jointly learn spatio - temporal topics for users.', ""the advantage of these generative approaches is that they are able to work with the continuous geographical space directly without any pre - discretisation, but they are algorithmically complex and don't scale well to larger datasets."", ' #AUTHOR_TAG used kernelbased methods to smooth linguistic features over very small grid sizes to alleviate data sparseness.', 'network - based geolocation models, on the other hand, utilise the fact that social media users interact more with people who live nearby.', ' #AUTHOR_TAG and  #AUTHOR_TAG used a twitter reciprocal mention network, and geolocated users based on the geographical coordinates of their friends, by minimising the weighted distance of a given user to their friends.', 'for a reciprocal mention network to be effective, however, a huge amount of twitter data is required.', ' #AUTHOR_TAG showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state - of - theart results.', 'the greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph.', 'as shown by  #TAUTHOR_TAG, geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text - and network - based approach to user geolocation']",0
"[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geo']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']",[' #TAUTHOR_TAG'],1
"[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geo']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']",[' #TAUTHOR_TAG'],4
"['previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us']","['previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us']","['also previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us']","['baseline network - based model of mad - b outperforms the text - based models and also previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us and twitter - world due to the size of the network.', '']",4
"['previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us']","['previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us']","['also previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us']","['baseline network - based model of mad - b outperforms the text - based models and also previous network - based models  #TAUTHOR_TAG.', 'the inference, however, is intractable for twitter - us and twitter - world due to the size of the network.', '']",4
"[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geo']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']","[' #TAUTHOR_TAG.', 'methods which combine the two, however, are rare.', 'in this paper, we present work on twitter user geolocation using both text and network information.', 'our contributions are as follows : ( 1 ) we propose the use of modified adsorption  #AUTHOR_TAG as a baseline networkbased geolocation model, and show that']",[' #TAUTHOR_TAG'],5
"['us  #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['evaluate our models over three pre - existing geotagged twitter datasets : ( 1 ) geotext  #AUTHOR_TAG, ( 2 ) twitter - us  #TAUTHOR_TAG, and ( 3 ) twitter - world  #AUTHOR_TAG.', 'in each dataset, users are represented by a single meta - document, generated by concatenating their tweets.', 'the datasets are pre - partitioned into training, development and test sets, and rebuilt from the original version to include mention information.', 'the first two datasets were constructed to contain mostly english messages.', 'geotext consists of tweets from 9. 5k users : 1895 users are held out for each of development and test data.', 'the primary location of each user is set to the coordinates of their first tweet.', 'twitter - us consists of 449k users, of which 10k users are held out for each of development and test data.', 'the primary location of each user is, once again, set to the coordinates of their first tweet.', 'twitter - world consists of 1. 3m users, of which 10000 each are held out for development and test.', 'unlike the other two datasets, the primary location of users is mapped to the geographic centre of the city where the majority of their tweets were posted']",5
"['model', ', using the method of  #TAUTHOR_TAG. the']","['1 regularised logistic regression model', ', using the method of  #TAUTHOR_TAG.']","['', ', using the method of  #TAUTHOR_TAG. the']","['to 5 × 10 6 for twitter - us and from 4 ×', '10 10 to 1 × 10 7 for twitter - world ), and made inference tractable for twitter - us and twitter - world.  #AUTHOR_TAG report that the time complexity of most network - based geolocation', 'methods is o ( k 2 ) for each node where k is the average number of vertex neighbours. in the case of the collapsed network of twitter - world, k is decreased by a factor of 4000 after setting the celebrity threshold t', 'to 5. we apply celebrity removal over both binary ( "" madcel - b ""', ') and weighted ( "" madcel - w "" ) networks ( using the respective t for each dataset ). the effect of celebrity removal over the development set of twitter - us is shown in figure 2 where it dramatically reduces the graph edge size and simultaneously leads to', 'an improvement in the mean error. a unified geolocation model to address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test', 'node  #AUTHOR_TAG. the label for the dongle node is based on a textbased l 1 regularised logistic regression model', ', using the method of  #TAUTHOR_TAG. the dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way', '']",5
"['model', ', using the method of  #TAUTHOR_TAG. the']","['1 regularised logistic regression model', ', using the method of  #TAUTHOR_TAG.']","['', ', using the method of  #TAUTHOR_TAG. the']","['to 5 × 10 6 for twitter - us and from 4 ×', '10 10 to 1 × 10 7 for twitter - world ), and made inference tractable for twitter - us and twitter - world.  #AUTHOR_TAG report that the time complexity of most network - based geolocation', 'methods is o ( k 2 ) for each node where k is the average number of vertex neighbours. in the case of the collapsed network of twitter - world, k is decreased by a factor of 4000 after setting the celebrity threshold t', 'to 5. we apply celebrity removal over both binary ( "" madcel - b ""', ') and weighted ( "" madcel - w "" ) networks ( using the respective t for each dataset ). the effect of celebrity removal over the development set of twitter - us is shown in figure 2 where it dramatically reduces the graph edge size and simultaneously leads to', 'an improvement in the mean error. a unified geolocation model to address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test', 'node  #AUTHOR_TAG. the label for the dongle node is based on a textbased l 1 regularised logistic regression model', ', using the method of  #TAUTHOR_TAG. the dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way', '']",3
"['automatic identification of information status  #AUTHOR_TAG 1992 ), i. e. categorizing discourse entities into different classes on the given - new scale, has recently been identified as an important issue in natural language processing  #TAUTHOR_TAG ;.', 'it is widely acknowledged']","['automatic identification of information status  #AUTHOR_TAG 1992 ), i. e. categorizing discourse entities into different classes on the given - new scale, has recently been identified as an important issue in natural language processing  #TAUTHOR_TAG ;.', 'it is widely acknowledged']","['automatic identification of information status  #AUTHOR_TAG 1992 ), i. e. categorizing discourse entities into different classes on the given - new scale, has recently been identified as an important issue in natural language processing  #TAUTHOR_TAG ;.', 'it is widely acknowledged']","['automatic identification of information status  #AUTHOR_TAG 1992 ), i. e. categorizing discourse entities into different classes on the given - new scale, has recently been identified as an important issue in natural language processing  #TAUTHOR_TAG ;.', 'it is widely acknowledged that information status and, more generally, information structure, 1 is reflected in word order, in the form of referring expressions as well as in prosody.', 'in computational linguistics, the ability to automatically label text with information status, therefore, could be of great benefit to many applications, including surface realization, text - to - speech synthesis, anaphora resolution, summarization, etc.', 'the task of automatically labeling text with information status, however, is a difficult one.', 'part of the difficulty arises from the fact that, to a certain degree, such labeling requires world knowledge and semantic comprehension of the text, but another obstacle is simply that theoretical notions of information status are not used consistently in the literature.', 'in this paper we outline a system, trained on a small amount of data, that achieves encouraging results on the task of automatically labeling transcribed german radio news data with fine - grained information status labels']",1
"['( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the']","['( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the']","['. this does not rule out', 'further subclassification ( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the highest confusion']","['former paper and in baumann and riester ( to appear ). as it stands, the scheme provides too many labels for our purpose. as a', 'compromise, we group them in seven classes : given, situative, bridging, unused, new, generic and expletive. given. givenness is a central notion in information structure theory.  #AUTHOR_TAG defines givenness of individual - type entities in terms of coreference. if desired, given items can be subclassified, e. g. whether they are pronouns or full noun phrases, and whether the latter are repetitions or short forms of earlier material, or whether they consist of lexically new material ( epithets ). situative. 1 st and 2 nd person pronouns', ', locative and temporal adverbials, usually count', 'as deictic expressions since they refer to elements in the utterance situation. we therefore count them', 'as a separate class. situative entities may, but need not,', 'corefer. bridging. bridging anaphors, as in ( 2a ) above, have received much attention, see e. g.  #AUTHOR_TAG or', ' #AUTHOR_TAG.', 'although they are discourse - new, they share properties with', 'coreference anaphors since they depend on the discourse context. they represent a', 'class which can be easily identified by human annotators but are difficult to capture by automatic techniques. unused. in manual annotation practice, it is very often impossible to decide whether an entity is hearerknown, since this depends on who we assume the hear', '##er to be', '; and even if we agree on a recipient, we may still be mistaken about their knowledge. for example, wolfgang bosbach, deputy chairman of', 'the cdu parliamentary group may be known', 'to parts of a german audience but not to other people.', 'we address', 'this by collecting both hearer - known and hearer - unknown definite expressions into one class unused. this does not rule out', 'further subclassification ( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the highest confusion rate between new and mediated entities may have its roots in this issue', '. new.', 'only ( specific ) indefinites are labeled new. generic. an issue which is not dealt with in  #AUTHOR_TAG are generic expressions as in lions have manes.  #AUTHOR_TAG discuss the task of identifying generic items in a manner similar to the learning tasks presented above, using', 'a bayesian network. we believe it makes sense to integrate genericity detection into information - status prediction.']",1
"['.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes']","['nlp applications.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes : old, mediated']","['nlp applications.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes : old, mediated']","['simpler variant of the task is anaphoricity detection ( discourse - new detection )  #AUTHOR_TAG, which divides discourse entities into anaphoric ( given ) and new.', 'identifying discourse - new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities.', 'in the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for nlp applications.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes : old, mediated and new expressions.', 'this classification, which is described in  #AUTHOR_TAG, has been used for annotating the switchboard dialog corpus  #AUTHOR_TAG, on which both studies are based.', 'most recently,  #AUTHOR_TAG extend their automatic prediction system to a more fine - grained set of 16 subtypes.', 'old.', 'the class of old entities in  #AUTHOR_TAG is not limited to full - fledged anaphors like in example ( 1a ) but also includes cases of generic and first / second person pronouns like in ( 1b ), which may or may not possess a previous mention.', '']",0
"['.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes']","['nlp applications.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes : old, mediated']","['nlp applications.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes : old, mediated']","['simpler variant of the task is anaphoricity detection ( discourse - new detection )  #AUTHOR_TAG, which divides discourse entities into anaphoric ( given ) and new.', 'identifying discourse - new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities.', 'in the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for nlp applications.', ' #AUTHOR_TAG and  #TAUTHOR_TAG developed methods to automatically identify three different classes : old, mediated and new expressions.', 'this classification, which is described in  #AUTHOR_TAG, has been used for annotating the switchboard dialog corpus  #AUTHOR_TAG, on which both studies are based.', 'most recently,  #AUTHOR_TAG extend their automatic prediction system to a more fine - grained set of 16 subtypes.', 'old.', 'the class of old entities in  #AUTHOR_TAG is not limited to full - fledged anaphors like in example ( 1a ) but also includes cases of generic and first / second person pronouns like in ( 1b ), which may or may not possess a previous mention.', '']",0
"['( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the']","['( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the']","['. this does not rule out', 'further subclassification ( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the highest confusion']","['former paper and in baumann and riester ( to appear ). as it stands, the scheme provides too many labels for our purpose. as a', 'compromise, we group them in seven classes : given, situative, bridging, unused, new, generic and expletive. given. givenness is a central notion in information structure theory.  #AUTHOR_TAG defines givenness of individual - type entities in terms of coreference. if desired, given items can be subclassified, e. g. whether they are pronouns or full noun phrases, and whether the latter are repetitions or short forms of earlier material, or whether they consist of lexically new material ( epithets ). situative. 1 st and 2 nd person pronouns', ', locative and temporal adverbials, usually count', 'as deictic expressions since they refer to elements in the utterance situation. we therefore count them', 'as a separate class. situative entities may, but need not,', 'corefer. bridging. bridging anaphors, as in ( 2a ) above, have received much attention, see e. g.  #AUTHOR_TAG or', ' #AUTHOR_TAG.', 'although they are discourse - new, they share properties with', 'coreference anaphors since they depend on the discourse context. they represent a', 'class which can be easily identified by human annotators but are difficult to capture by automatic techniques. unused. in manual annotation practice, it is very often impossible to decide whether an entity is hearerknown, since this depends on who we assume the hear', '##er to be', '; and even if we agree on a recipient, we may still be mistaken about their knowledge. for example, wolfgang bosbach, deputy chairman of', 'the cdu parliamentary group may be known', 'to parts of a german audience but not to other people.', 'we address', 'this by collecting both hearer - known and hearer - unknown definite expressions into one class unused. this does not rule out', 'further subclassification ( known / unknown ) or the possibility of using machine learning techniques to identify this distinction, see  #AUTHOR_TAG', '. the fact that  #TAUTHOR_TAG report the highest confusion rate between new and mediated entities may have its roots in this issue', '. new.', 'only ( specific ) indefinites are labeled new. generic. an issue which is not dealt with in  #AUTHOR_TAG are generic expressions as in lions have manes.  #AUTHOR_TAG discuss the task of identifying generic items in a manner similar to the learning tasks presented above, using', 'a bayesian network. we believe it makes sense to integrate genericity detection into information - status prediction.']",0
"[' #TAUTHOR_TAG, that fires if there is some lexical overlap of nouns ( or compound nouns ) in']","[' #TAUTHOR_TAG, that fires if there is some lexical overlap of nouns ( or compound nouns ) in']","[' #TAUTHOR_TAG, that fires if there is some lexical overlap of nouns ( or compound nouns ) in the preceding 10 sentences.', 'the original label set described in contains 21 labels.', 'here we work with a subset of maximally 12 labels, but also consider smaller subsets of labels and carry out a mapping to the  #AUTHOR_TAG label set ( table 2 ).', '5 we run a 10 - fold cross - validation experiment and report average prediction accuracy.', 'the results are given in table 3a.', 'as an informed baseline, we run the same crossvalidation experiment with a subset of features that roughly correspond to the features of  #AUTHOR_TAG.', 'our models perform statistically significantly better than the baseline ( p < 0. 001, using the approximate randomization test ) for all label sets.', 'table']","['work is based on the dirndl radio news corpus of  #AUTHOR_TAG which has been handannotated with information status labels.', 'we choose a selection of 6668 annotated phrases ( 1420 sentences ).', 'this is an order of magnitude smaller than the annotated switchboard corpus of  #AUTHOR_TAG.', 'we parse each sentence with the german lexical functional grammar of  #AUTHOR_TAG using the xle parser in order to automati -  #AUTHOR_TAG show that there are asymmetries between pairs of information status labels contained in sentences, i. e. certain classes of expressions tend to precede certain other classes.', 'we therefore treat the prediction of is labels as a sequence labeling task.', '4 we train a crf using wapiti  #AUTHOR_TAG, with the features outlined in table 1.', 'we also include a basic "" coreference "" feature, similar to the lexical features of  #TAUTHOR_TAG, that fires if there is some lexical overlap of nouns ( or compound nouns ) in the preceding 10 sentences.', 'the original label set described in contains 21 labels.', 'here we work with a subset of maximally 12 labels, but also consider smaller subsets of labels and carry out a mapping to the  #AUTHOR_TAG label set ( table 2 ).', '5 we run a 10 - fold cross - validation experiment and report average prediction accuracy.', 'the results are given in table 3a.', 'as an informed baseline, we run the same crossvalidation experiment with a subset of features that roughly correspond to the features of  #AUTHOR_TAG.', 'our models perform statistically significantly better than the baseline ( p < 0. 001, using the approximate randomization test ) for all label sets.', 'table 2 : varying the granularity of the label']",3
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beat']","['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that']","['of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g']","['', 'predictions based on the calculation of full length costs ). the real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. therefore, pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an', 'understanding of how the principle of dependency length minimization interacts with other highly predictive principles', 'beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case. 3']",0
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beat']","['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that']","['of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g']","['', 'predictions based on the calculation of full length costs ). the real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. therefore, pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an', 'understanding of how the principle of dependency length minimization interacts with other highly predictive principles', 'beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case. 3']",0
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beat']","['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that']","['of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g']","['', 'predictions based on the calculation of full length costs ). the real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. therefore, pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an', 'understanding of how the principle of dependency length minimization interacts with other highly predictive principles', 'beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case. 3']",0
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that']","['suppose that one wishes to compare the cost of two orderings of the same sentence. the observation that the processing cost of', 'a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it', ', and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so - called complexity profile ( e. g.,', '[ 22 ] ), rendering fair comparison impractical. the problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [', '23 ] and worsens when the sentences being compared differ not only in order but also in content. another challenge is the precision of dependency length that is typically measured in words. lengths in phonemes or syllables shed light on why svo languages show sov order when the object is a short word such as a clitic [ 24 ]. without addressing these issues, the anti - locality effects or long - distance dependencies reviewed by liu et al can', 'neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely ;', 'an effective evaluation of the theoretical framework above can be impossible ( as that framework makes theoretical predictions based on the calculation of full length costs ). the real challenge for', 'psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance : if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the', 'length of the words defining the dependency and that of the intermediate words. therefore', ', pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an understanding of how the principle of dependency length minimization interacts with other highly predictive principles beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case']",0
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that']","['suppose that one wishes to compare the cost of two orderings of the same sentence. the observation that the processing cost of', 'a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it', ', and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so - called complexity profile ( e. g.,', '[ 22 ] ), rendering fair comparison impractical. the problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [', '23 ] and worsens when the sentences being compared differ not only in order but also in content. another challenge is the precision of dependency length that is typically measured in words. lengths in phonemes or syllables shed light on why svo languages show sov order when the object is a short word such as a clitic [ 24 ]. without addressing these issues, the anti - locality effects or long - distance dependencies reviewed by liu et al can', 'neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely ;', 'an effective evaluation of the theoretical framework above can be impossible ( as that framework makes theoretical predictions based on the calculation of full length costs ). the real challenge for', 'psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance : if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the', 'length of the words defining the dependency and that of the intermediate words. therefore', ', pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an understanding of how the principle of dependency length minimization interacts with other highly predictive principles beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case']",0
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that']","['suppose that one wishes to compare the cost of two orderings of the same sentence. the observation that the processing cost of', 'a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it', ', and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so - called complexity profile ( e. g.,', '[ 22 ] ), rendering fair comparison impractical. the problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [', '23 ] and worsens when the sentences being compared differ not only in order but also in content. another challenge is the precision of dependency length that is typically measured in words. lengths in phonemes or syllables shed light on why svo languages show sov order when the object is a short word such as a clitic [ 24 ]. without addressing these issues, the anti - locality effects or long - distance dependencies reviewed by liu et al can', 'neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely ;', 'an effective evaluation of the theoretical framework above can be impossible ( as that framework makes theoretical predictions based on the calculation of full length costs ). the real challenge for', 'psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance : if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the', 'length of the words defining the dependency and that of the intermediate words. therefore', ', pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an understanding of how the principle of dependency length minimization interacts with other highly predictive principles beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case']",0
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beat']","['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that']","['of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g']","['', 'predictions based on the calculation of full length costs ). the real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length', 'measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle', 'beating the other, coexistence, collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance', ': if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the length of the words defining the dependency and that of the intermediate words. therefore, pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an', 'understanding of how the principle of dependency length minimization interacts with other highly predictive principles', 'beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case. 3']",5
"['the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beat']","['of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that']","['suppose that one wishes to compare the cost of two orderings of the same sentence. the observation that the processing cost of', 'a sentence decreases when the length of a dependency increases, does not allow one to conclude that dependency length minimization cannot explain the results because shortening an edge implies moving at least one of the words defining it', ', and every move could imply the reduction of other edges eventually reducing the total sum of dependency lengths or altering the so - called complexity profile ( e. g.,', '[ 22 ] ), rendering fair comparison impractical. the problem of partial calculation of length costs has already been discussed in the context of research on the cost of crossing dependencies [', '23 ] and worsens when the sentences being compared differ not only in order but also in content. another challenge is the precision of dependency length that is typically measured in words. lengths in phonemes or syllables shed light on why svo languages show sov order when the object is a short word such as a clitic [ 24 ]. without addressing these issues, the anti - locality effects or long - distance dependencies reviewed by liu et al can', 'neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely ;', 'an effective evaluation of the theoretical framework above can be impossible ( as that framework makes theoretical predictions based on the calculation of full length costs ). the real challenge for', 'psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [ 20,  #TAUTHOR_TAG : one principle beating the other, coexistence', ', collaboration between principles or the very same trade - off causing the delusion that word order constraints have relaxed dramatically or even disappeared. this is the way of physics. our concern for units of measurement is not a simple matter of precision but one of great theoretical importance : if the length of a dependency is measured in units of word length ( e. g., syllables or phonemes ) then it follows that the length of a dependency will be strongly determined by the', 'length of the words defining the dependency and that of the intermediate words. therefore', ', pressure to reduce dependency lengths implies pressure for compression [ 25, 26 ], linking a principle of word order with a principle that operates ( nonexclusively ) on individual words. an understanding of how the principle of dependency length minimization interacts with other highly predictive principles beyond word order is a fundamental component of a general theory of animal behavior that has human language as a particular case']",5
"['[ 5,  #TAUTHOR_TAG, using']","['[ 5,  #TAUTHOR_TAG, using']","['the embodiedqa task [ 5,  #TAUTHOR_TAG,']","['', 'subsequent to the introduction of the task, several methods have been introduced to solve the embodiedqa task [ 5,  #TAUTHOR_TAG, using some combination of reinforcement learning, behavior cloning and hierarchical control.', 'apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches.', 'in this work, we evaluate simple question - only baselines that never see the environment and receive no form of expert supervision.', 'we examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods.', 'to our surprise, blindfold baselines achieve state - of - the - art performance on the embodiedqa task, except in the case when the agent is spawned extremely close to the object.', 'even in the latter case, blindfold baselines perform surprisingly close to existing state - of - the - art methods.', 'we note that this finding is reminiscent of several recent works in both computer vision and natural language processing, where researchers have found that statistical irregularities in the dataset can enable degenerate methods to perform surprisingly well [ 11, 12, 14, 21 ].', 'our findings suggest that current embodiedqa models are ineffective at leveraging the context from the environment, in fact this context or embodiment in the environment can negatively hamper them.', 'we hope comparison with our baseline']",0
['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5'],['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5'],"['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5 frames of oracle navigation for', '50 epochs with adam and a learning rate of 3']","['', ', we conclude that current methods for the embodiedqa task are not effective at using context from the environment, and in fact this negatively hampers them. this shows that there is room for building new models that leverage the context and embodiment in the environment. oracles : we now examine whether the', 'eqav1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. we reproduce the settings for training the v', '##qa model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5 frames of oracle navigation for', '50 epochs with adam and a learning rate of 3e − 4 using batch size 20. we observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in pacman reduces performance to below the text baselines. for completeness we', 'benchmark an oracle with our bow embedding model in place of the lstm with all other settings kept constant', '. as noted in [ 5 ], we re - iterate that these oracles are far from perfect, as they may not contain the best vantage or context to', '']",0
['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5'],['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5'],"['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5 frames of oracle navigation for', '50 epochs with adam and a learning rate of 3']","['', ', we conclude that current methods for the embodiedqa task are not effective at using context from the environment, and in fact this negatively hampers them. this shows that there is room for building new models that leverage the context and embodiment in the environment. oracles : we now examine whether the', 'eqav1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. we reproduce the settings for training the v', '##qa model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5 frames of oracle navigation for', '50 epochs with adam and a learning rate of 3e − 4 using batch size 20. we observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in pacman reduces performance to below the text baselines. for completeness we', 'benchmark an oracle with our bow embedding model in place of the lstm with all other settings kept constant', '. as noted in [ 5 ], we re - iterate that these oracles are far from perfect, as they may not contain the best vantage or context to', '']",5
['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5'],['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5'],"['model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5 frames of oracle navigation for', '50 epochs with adam and a learning rate of 3']","['', ', we conclude that current methods for the embodiedqa task are not effective at using context from the environment, and in fact this negatively hampers them. this shows that there is room for building new models that leverage the context and embodiment in the environment. oracles : we now examine whether the', 'eqav1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. we reproduce the settings for training the v', '##qa model 2. specifically we train the vqa model described in  #TAUTHOR_TAG on the last 5 frames of oracle navigation for', '50 epochs with adam and a learning rate of 3e − 4 using batch size 20. we observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in pacman reduces performance to below the text baselines. for completeness we', 'benchmark an oracle with our bow embedding model in place of the lstm with all other settings kept constant', '. as noted in [ 5 ], we re - iterate that these oracles are far from perfect, as they may not contain the best vantage or context to', '']",5
[' #TAUTHOR_TAG for agent spawned at'],[' #TAUTHOR_TAG for agent spawned at'],[' #TAUTHOR_TAG for agent spawned at various steps'],"['20 t 50 t any navigation + vqa pacman ( bc ) [ 5 ] 48 bow - cnn vqa - only 56. 5 table 1 : we compare to the published results from  #TAUTHOR_TAG for agent spawned at various steps away from the target : 10, 30, 50, and anywhere in the environment.', 'question - only baselines outperform navigation + vqa methods except when spawned 10 steps from the target object.', ""a vqa - only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation."", '( * ) indicates our reproduction of the model described in [ 5 ] error analysis : to better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the bow model on different question types : here, the color category preposition location color 9. 09 51. 72 53. 31 table 2 : accuracy of the bow model on different question types subsumes color and color _ room both.', 'the particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set ( 2. 44 % ), and the entropy of answer distribution in this class is much higher compared to color and location question types']",5
"['reported in  #TAUTHOR_TAG, we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom - up chart - based parsing algorithm.', 'the algorithm is implemented and a preliminary experiment shows promising']","['reported in  #TAUTHOR_TAG, we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom - up chart - based parsing algorithm.', 'the algorithm is implemented and a preliminary experiment shows promising results']","['a proper selection model must be used.', 'also, the efficiency challenges usually presented by the selection model must be answered.', 'building on the work reported in  #TAUTHOR_TAG, we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom - up chart - based parsing algorithm.', 'the algorithm is implemented and a preliminary experiment shows promising']","['this paper we propose a partial parsing model which achieves robust parsing with a large hpsg grammar.', 'constraint - based precision grammars, like the hpsg grammar we are using for the experiments reported in this paper, typically lack robustness, especially when applied to real world texts.', 'to maximally recover the linguistic knowledge from an unsuccessful parse, a proper selection model must be used.', 'also, the efficiency challenges usually presented by the selection model must be answered.', 'building on the work reported in  #TAUTHOR_TAG, we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom - up chart - based parsing algorithm.', 'the algorithm is implemented and a preliminary experiment shows promising results']",6
"['systems. in  #TAUTHOR_TAG, we have pointed out that']","['systems. in  #TAUTHOR_TAG, we have pointed out that most']","['systems. in  #TAUTHOR_TAG, we have pointed out that']","['test run with chronologically different versions of the erg has shown that, with the increased efforts invested into', 'grammar engineering, the coverage of the specific grammar has shown a very promising improvement over the years. however, it is still unlikely for the specific precision large - scale grammar to', 'achieve full coverage on unseen data without extra robust processing techniques. also, the cost of manually extending the grammar would be too high to be easily acceptable for other precision grammar - based parsing systems. in  #TAUTHOR_TAG, we have pointed out that most applications are only interested in certain aspects of parsing results. full analyses are', 'preferable, but not always necessary. in fact, most of the contemporary deep parsing systems provide as outputs either semantic representations that reflect the', '"" meaning "" of the input, or rather abstract syntactic structures. full representations with all detailed linguistic features ( e. g., typed feature structures in hpsg )', 'are almost never used either as output format or in real applications. take the delph - in hpsg grammars, for instance : minimal recursion semantics ( mrs,  #AUTHOR_TAG ) is used as the semantic', 'representation in these grammars. for recording syntactic structures, derivation trees', '']",6
"['systems. in  #TAUTHOR_TAG, we have pointed out that']","['systems. in  #TAUTHOR_TAG, we have pointed out that most']","['systems. in  #TAUTHOR_TAG, we have pointed out that']","['test run with chronologically different versions of the erg has shown that, with the increased efforts invested into', 'grammar engineering, the coverage of the specific grammar has shown a very promising improvement over the years. however, it is still unlikely for the specific precision large - scale grammar to', 'achieve full coverage on unseen data without extra robust processing techniques. also, the cost of manually extending the grammar would be too high to be easily acceptable for other precision grammar - based parsing systems. in  #TAUTHOR_TAG, we have pointed out that most applications are only interested in certain aspects of parsing results. full analyses are', 'preferable, but not always necessary. in fact, most of the contemporary deep parsing systems provide as outputs either semantic representations that reflect the', '"" meaning "" of the input, or rather abstract syntactic structures. full representations with all detailed linguistic features ( e. g., typed feature structures in hpsg )', 'are almost never used either as output format or in real applications. take the delph - in hpsg grammars, for instance : minimal recursion semantics ( mrs,  #AUTHOR_TAG ) is used as the semantic', 'representation in these grammars. for recording syntactic structures, derivation trees', '']",0
"['systems. in  #TAUTHOR_TAG, we have pointed out that']","['systems. in  #TAUTHOR_TAG, we have pointed out that most']","['systems. in  #TAUTHOR_TAG, we have pointed out that']","['test run with chronologically different versions of the erg has shown that, with the increased efforts invested into', 'grammar engineering, the coverage of the specific grammar has shown a very promising improvement over the years. however, it is still unlikely for the specific precision large - scale grammar to', 'achieve full coverage on unseen data without extra robust processing techniques. also, the cost of manually extending the grammar would be too high to be easily acceptable for other precision grammar - based parsing systems. in  #TAUTHOR_TAG, we have pointed out that most applications are only interested in certain aspects of parsing results. full analyses are', 'preferable, but not always necessary. in fact, most of the contemporary deep parsing systems provide as outputs either semantic representations that reflect the', '"" meaning "" of the input, or rather abstract syntactic structures. full representations with all detailed linguistic features ( e. g., typed feature structures in hpsg )', 'are almost never used either as output format or in real applications. take the delph - in hpsg grammars, for instance : minimal recursion semantics ( mrs,  #AUTHOR_TAG ) is used as the semantic', 'representation in these grammars. for recording syntactic structures, derivation trees', '']",0
"['systems. in  #TAUTHOR_TAG, we have pointed out that']","['systems. in  #TAUTHOR_TAG, we have pointed out that most']","['systems. in  #TAUTHOR_TAG, we have pointed out that']","['test run with chronologically different versions of the erg has shown that, with the increased efforts invested into', 'grammar engineering, the coverage of the specific grammar has shown a very promising improvement over the years. however, it is still unlikely for the specific precision large - scale grammar to', 'achieve full coverage on unseen data without extra robust processing techniques. also, the cost of manually extending the grammar would be too high to be easily acceptable for other precision grammar - based parsing systems. in  #TAUTHOR_TAG, we have pointed out that most applications are only interested in certain aspects of parsing results. full analyses are', 'preferable, but not always necessary. in fact, most of the contemporary deep parsing systems provide as outputs either semantic representations that reflect the', '"" meaning "" of the input, or rather abstract syntactic structures. full representations with all detailed linguistic features ( e. g., typed feature structures in hpsg )', 'are almost never used either as output format or in real applications. take the delph - in hpsg grammars, for instance : minimal recursion semantics ( mrs,  #AUTHOR_TAG ) is used as the semantic', 'representation in these grammars. for recording syntactic structures, derivation trees', '']",0
"[',  #TAUTHOR_TAG formulated']","[',  #TAUTHOR_TAG formulated']","[',  #TAUTHOR_TAG formulated']","['', 'out that the weights of the arcs can be assigned by an estimation function in order to indicate the preference over different fragment analyses. the discovery of such a path can be done in linear time ( o ( | v | + |', 'e | ) ) with the dag - shortest - path algorithm  #AUTHOR_TAG. however, it is not clear ( apart from some simple heuristics ) how the estimation function can be acquired. moreover, by its additive nature, the shortest - path, such a model makes an implicit independence assumption of the estimation function in different edge contexts. based on a similar definition of partial parse,  #TAUTHOR_TAG formulated the following statistical model : the above model contains two probabilistic components : i ) p ( ω |', 'w ) is the conditional probability of a segmentation ω given the input sequence w ; and ii ) p ( t i | w i ) is the conditional probability of an analysis t i for a', 'given subsequence w i in the segmentation. the empirical results have shown that this selection', 'model significantly outperforms the shortest - path based baseline selection model proposed by  #AUTHOR_TAG. the evaluation', ""was done using multiple metrics. while there is no gold - standard corpus for the purpose of partial parse evaluation,  #TAUTHOR_TAG manually compared the parser's partial derivation"", 'trees with the penn treebank annotation for syntactic similarity. furthermore,  #TAUTHOR_TAG evaluated the fragment semantic outputs based on a practical estimation of rmrs similarities described by  #AUTHOR_TAG', '. the semantic outputs of different partial parse selection models were compared to the rmrs outputs from the rasp', 'system  #AUTHOR_TAG. if taken comparatively, all the results suggested that the model in ( 2. ) performed much better than', 'the baseline. but they failed to tell a clear story about the quality of the partial parse selection model. unfortunately', ', the model is approximate because of the independence assumption between the two components ( for simplification ). also, due to the lack of training data,', 'the parameters of the two components were estimated over different', 'data sets in the experiment, which has added further doubt on the consistency of the resulting model. moreover, it is generally not desirable to have different statistical models for full and partial parse selection. ideally', ', a uniform disambiguation model should be used in both cases']",0
"[',  #TAUTHOR_TAG formulated']","[',  #TAUTHOR_TAG formulated']","[',  #TAUTHOR_TAG formulated']","['', 'out that the weights of the arcs can be assigned by an estimation function in order to indicate the preference over different fragment analyses. the discovery of such a path can be done in linear time ( o ( | v | + |', 'e | ) ) with the dag - shortest - path algorithm  #AUTHOR_TAG. however, it is not clear ( apart from some simple heuristics ) how the estimation function can be acquired. moreover, by its additive nature, the shortest - path, such a model makes an implicit independence assumption of the estimation function in different edge contexts. based on a similar definition of partial parse,  #TAUTHOR_TAG formulated the following statistical model : the above model contains two probabilistic components : i ) p ( ω |', 'w ) is the conditional probability of a segmentation ω given the input sequence w ; and ii ) p ( t i | w i ) is the conditional probability of an analysis t i for a', 'given subsequence w i in the segmentation. the empirical results have shown that this selection', 'model significantly outperforms the shortest - path based baseline selection model proposed by  #AUTHOR_TAG. the evaluation', ""was done using multiple metrics. while there is no gold - standard corpus for the purpose of partial parse evaluation,  #TAUTHOR_TAG manually compared the parser's partial derivation"", 'trees with the penn treebank annotation for syntactic similarity. furthermore,  #TAUTHOR_TAG evaluated the fragment semantic outputs based on a practical estimation of rmrs similarities described by  #AUTHOR_TAG', '. the semantic outputs of different partial parse selection models were compared to the rmrs outputs from the rasp', 'system  #AUTHOR_TAG. if taken comparatively, all the results suggested that the model in ( 2. ) performed much better than', 'the baseline. but they failed to tell a clear story about the quality of the partial parse selection model. unfortunately', ', the model is approximate because of the independence assumption between the two components ( for simplification ). also, due to the lack of training data,', 'the parameters of the two components were estimated over different', 'data sets in the experiment, which has added further doubt on the consistency of the resulting model. moreover, it is generally not desirable to have different statistical models for full and partial parse selection. ideally', ', a uniform disambiguation model should be used in both cases']",0
"[',  #TAUTHOR_TAG formulated']","[',  #TAUTHOR_TAG formulated']","[',  #TAUTHOR_TAG formulated']","['', 'out that the weights of the arcs can be assigned by an estimation function in order to indicate the preference over different fragment analyses. the discovery of such a path can be done in linear time ( o ( | v | + |', 'e | ) ) with the dag - shortest - path algorithm  #AUTHOR_TAG. however, it is not clear ( apart from some simple heuristics ) how the estimation function can be acquired. moreover, by its additive nature, the shortest - path, such a model makes an implicit independence assumption of the estimation function in different edge contexts. based on a similar definition of partial parse,  #TAUTHOR_TAG formulated the following statistical model : the above model contains two probabilistic components : i ) p ( ω |', 'w ) is the conditional probability of a segmentation ω given the input sequence w ; and ii ) p ( t i | w i ) is the conditional probability of an analysis t i for a', 'given subsequence w i in the segmentation. the empirical results have shown that this selection', 'model significantly outperforms the shortest - path based baseline selection model proposed by  #AUTHOR_TAG. the evaluation', ""was done using multiple metrics. while there is no gold - standard corpus for the purpose of partial parse evaluation,  #TAUTHOR_TAG manually compared the parser's partial derivation"", 'trees with the penn treebank annotation for syntactic similarity. furthermore,  #TAUTHOR_TAG evaluated the fragment semantic outputs based on a practical estimation of rmrs similarities described by  #AUTHOR_TAG', '. the semantic outputs of different partial parse selection models were compared to the rmrs outputs from the rasp', 'system  #AUTHOR_TAG. if taken comparatively, all the results suggested that the model in ( 2. ) performed much better than', 'the baseline. but they failed to tell a clear story about the quality of the partial parse selection model. unfortunately', ', the model is approximate because of the independence assumption between the two components ( for simplification ). also, due to the lack of training data,', 'the parameters of the two components were estimated over different', 'data sets in the experiment, which has added further doubt on the consistency of the resulting model. moreover, it is generally not desirable to have different statistical models for full and partial parse selection. ideally', ', a uniform disambiguation model should be used in both cases']",0
['models proposed in both  #AUTHOR_TAG and  #TAUTHOR_TAG is that the results'],['models proposed in both  #AUTHOR_TAG and  #TAUTHOR_TAG is that the results'],['common shortcoming of the partial parsing models proposed in both  #AUTHOR_TAG and  #TAUTHOR_TAG is that the results'],"['common shortcoming of the partial parsing models proposed in both  #AUTHOR_TAG and  #TAUTHOR_TAG is that the results of partial parsing are sets of disjoint sub - analyses, either in the form of derivation subtrees, or in the form of mrs fragments.', 'it is not informative enough to show the interconnection across the fragment boundaries.', '']",0
"[' #TAUTHOR_TAG have also pointed out, the']","[' #TAUTHOR_TAG have also pointed out, the']","[' #TAUTHOR_TAG have also pointed out, the evaluation of a partial parser is a very difficult task as such,']","[' #TAUTHOR_TAG have also pointed out, the evaluation of a partial parser is a very difficult task as such, due to the lack of gold - standard annotation for sentences that are not fully analysed by the grammar.', '']",0
"[' #TAUTHOR_TAG have also pointed out, the']","[' #TAUTHOR_TAG have also pointed out, the']","[' #TAUTHOR_TAG have also pointed out, the evaluation of a partial parser is a very difficult task as such,']","[' #TAUTHOR_TAG have also pointed out, the evaluation of a partial parser is a very difficult task as such, due to the lack of gold - standard annotation for sentences that are not fully analysed by the grammar.', '']",0
"['analyzers  #AUTHOR_TAG.', 'our method leverages  #TAUTHOR_TAG']","['analyzers  #AUTHOR_TAG.', ""our method leverages  #TAUTHOR_TAG's formulation of  #AUTHOR_TAG's popular skip - gram model as exponential family principal""]","['with multiple different frame analyzers  #AUTHOR_TAG.', 'our method leverages  #TAUTHOR_TAG']","['', ""while frame semantics provide a structured form for analyzing words with crisp, categorically - labeled concepts, the encoded properties and expectations are implicit. what does it mean to fill a frame's role?"", ""semantic proto - role ( spr ) theory, motivated by  #AUTHOR_TAG's thematic proto - role theory, offers an answer to this."", 'spr replaces categorical roles attempt she said bill would try the same tactic again.', 'we are interested in capturing these spr - based properties and expectations within word embeddings.', 'we present a method that learns frameenriched embeddings from millions of documents that have been semantically parsed with multiple different frame analyzers  #AUTHOR_TAG.', ""our method leverages  #TAUTHOR_TAG's formulation of  #AUTHOR_TAG's popular skip - gram model as exponential family principal component analysis ( epca ) and tensor factorization."", ""this paper's primary contributions are : ( i ) enriching learned word embeddings with multiple, automatically obtained frames from large, disparate corpora ; and ( ii ) demonstrating these enriched embeddings better capture spr - based properties."", ""in so doing, we also generalize  #TAUTHOR_TAG's method to arbitrary tensor dimensions."", 'this allows us to include an arbitrary amount of semantic information when learning embeddings.', 'our variable - size tensor factorization code is available at https : / / github. com']",6
"[""extension of  #TAUTHOR_TAG's tensor factorization""]","[""extension of  #TAUTHOR_TAG's tensor factorization""]","[""extension of  #TAUTHOR_TAG's tensor factorization enriches word embeddings by including syntacticsemantic information not often captured,""]","['presented a way to learn embeddings enriched with multiple, automatically obtained frames from large, disparate corpora.', 'we also presented a qvec evaluation for semantic proto - roles.', ""as demonstrated by our experiments, our extension of  #TAUTHOR_TAG's tensor factorization enriches word embeddings by including syntacticsemantic information not often captured, resulting in consistently higher spr - based correlations."", 'the implementation is available at  #TAUTHOR_TAG']",6
"['3 -', 'tensor baseline of  #TAUTHOR_TAG']","['3 -', 'tensor baseline of  #TAUTHOR_TAG']","['3 -', 'tensor baseline of  #TAUTHOR_TAG. both of  #TAUTHOR_TAG are restricted to a local', 'context and cannot take advantage of frames or any lexical signal that can be derived from frames. overall, we notice that we obtain large']","['', 'tensor baseline of  #TAUTHOR_TAG. both of  #TAUTHOR_TAG are restricted to a local', 'context and cannot take advantage of frames or any lexical signal that can be derived from frames. overall, we notice that we obtain large improvements from models trained on lexical signals that have been derived from', 'frame output ( sep and none ), even if the model itself does not incorporate', 'any frame labels. the embeddings that predict the role filling lexical items ( the green triangles ) correlate higher with spr oracles than the embeddings that predict propbank frames and roles ( red circles ). examining fig. 2a, we see', 'that both model types outperform both the word2vec and  #TAUTHOR_TAG baselines in nearly all model configurations', 'and ablations. we see the highest improvement when predicting role fillers given the frame trigger and the number of tokens separating the two ( the green triangles in the sep rows )', '. comparing fig. 2a to fig. 2b,', 'we see newswire is more amenable to predicting propbank frames and roles. we posit this is a type of out - ofdomain error, as the propbank parser was', 'trained on newswire. we also find that newswire is overall more amenable to incorporating limited framebased features, particularly when predicting propbank using lexical role fillers as part of the con - textual features. we hypothesize this is due to the significantly increased vocabulary size of the wikipedia role fillers ( c. f., tab. 1 ). note, however, that by using all available schema', 'information when predicting propbank, we are able to compensate for the increased vocabulary.', 'in fig. 3 we display the ten nearest neighbors', 'for three randomly sampled trigger words according to two of the highest performing newswire models. they', 'each condition on the trigger and the role filler / trigger separation ; these', '']",4
"['others to consider leveraging linguistic annotations and resources to learn embeddings.', 'both  #TAUTHOR_TAG and  #AUTHOR_TAG a ) incorporate additional syntactic and morphological information in']","['others to consider leveraging linguistic annotations and resources to learn embeddings.', 'both  #TAUTHOR_TAG and  #AUTHOR_TAG a ) incorporate additional syntactic and morphological information in']","['recent popularity of word embeddings have inspired others to consider leveraging linguistic annotations and resources to learn embeddings.', 'both  #TAUTHOR_TAG and  #AUTHOR_TAG a ) incorporate additional syntactic and morphological information in']","['recent popularity of word embeddings have inspired others to consider leveraging linguistic annotations and resources to learn embeddings.', 'both  #TAUTHOR_TAG and  #AUTHOR_TAG a ) incorporate additional syntactic and morphological information in their word embeddings.', ""rothe and schutze ( 2015 )'s use lexical resource entries, such as wordnet synsets, to improve pre - computed word embeddings."", 'through generalized cca,  #AUTHOR_TAG incorporate paraphrased framenet training data.', 'on the applied side,  #AUTHOR_TAG used frame embeddings - produced by training word2vec on tweet - derived semantic frame ( names ) - as additional features in downstream prediction.', ' #AUTHOR_TAG similarly explored the relationship between semantic frames and thematic proto - roles.', 'they proposed using a conditional random field  #AUTHOR_TAG to jointly and conditionally model spr and srl.', "" #AUTHOR_TAG demonstrated slight improvements in jointly and conditionally predicting propbank  #AUTHOR_TAG's semantic role labels and  #AUTHOR_TAG's proto - role labels""]",1
"['. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting']","['evaluation purposes, e. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting gold - standard ccg derivations into the grs in depbank resulted in an fscore']","['evaluation purposes, e. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting gold - standard ccg derivations into the grs in depbank resulted in an fscore']","['has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting gold - standard ccg derivations into the grs in depbank resulted in an fscore of only 85 % ; hence the upper bound on the performance of the ccg parser, using this evaluation scheme, was only 85 %.', 'given that the current best scores for the ptb parsing task are over 90 %, any loss from the conversion process needs to be considered carefully if a fair comparison with ptb parsers is to be achieved.', 'ccgbank was derived from the ptb, and so it might be considered that converting back to the ptb would be a relatively easy task, by essentially reversing the mapping  #AUTHOR_TAG used to create ccgbank.', 'however, there are a number of differences between the two treebanks which make the conversion back far from trivial.', 'first, the corresponding derivations in the treebanks are not isomorphic : a ccg derivation is not simply a relabelling of the nodes in the ptb tree ; there are many constructions, such as coordination and control structures, where the trees are a different shape, as well as having different labels.', 'it is important to realise that  #AUTHOR_TAG invested a significant amount of time and effort in creating the mapping.', 'second, some of the labels in the ptb do not appear in ccgbank, for example the qp label, and these must be added back in ; however, developing rules to insert these labels in the right places is a far from trivial task.', 'there were two approaches we considered for the conversion.', 'one possibility is to associate ptb tree structures with ccg lexical categories, and combine the trees together in step with the category combinations in a ccg derivation - in much the same way that an ltag has elementary trees in the lexicon which are combined using the substitution and adjunction rules of tag.', 'the second approach is to associate conversion rules with each local tree - i. e.', 'a parent and one or two child nodes - which appears in the ccgbank data.', '3 in this paper we took the second approach']",0
"['by  #TAUTHOR_TAG, who convert hpsg analyses from a grammar automatically']","['by  #TAUTHOR_TAG, who convert hpsg analyses from a grammar automatically']","['unary 3 another possible approach has been taken by  #TAUTHOR_TAG, who convert hpsg analyses from a grammar automatically']","['are three types of conversion schema : schemas which introduce nodes for lexical items ; schemas which insert or elide ptb nodes for unary 3 another possible approach has been taken by  #TAUTHOR_TAG, who convert hpsg analyses from a grammar automatically extracted from the ptb back into the ptb.', 'they treat the problem as one of translation, learning a synchronous grammar to perform the mapping.', 'a ptb tree is built from a ccg derivation by running over the derivation in a bottom - up fashion and applying these schemas to the local trees in the derivation']",0
"[', which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2']","['98 %, which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2 %.', '']","[', which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2']","['schemas were developed by manual inspection using section 00 of ccgbank and the ptb as a development set, following the oracle methodology of  #AUTHOR_TAG, in which goldstandard derivations from ccgbank are converted to the new representation and compared with the gold standard for that representation.', 'as well as giving an idea of the difficulty, and success, of the conversion, the resulting numbers provide an up - in total, we annotated 32 unary and 776 binary rule instances ( of the possible 2853 instances ) with conversion schemas, and 162 of the 425 lexical categories.', 'we also implemented a small number of default catch - all cases for the general ccg combinatory rules and for the rules dealing with punctuation, which allowed most of the 2853 rule instances to be covered.', 'considerable time and effort was invested in the creation of these schemas.', 'the oracle conversion results from the gold standard ccgbank to the ptb for section 00 and 23 are shown in table 2.', 'the numbers are bracketing precision, recall, f - score and complete sentence matches, using the evalb evaluation script.', 'note that these figures provide an upper bound on the performance of the ccg parser using evalb, given the current conversion process.', 'the importance of this upper bound should not be underestimated, when the evaluation framework is such that incremental improvements of a few tenths of a percent are routinely presented as improving the state - of - the - art, as is the case with the parseval metrics.', 'the fact that the upper bound here is less than 95 % shows that it is not possible to fairly evaluate the ccg parser on the complete test set.', 'even an upper bound of around 98 %, which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2 %.', '']",0
"['. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting']","['evaluation purposes, e. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting gold - standard ccg derivations into the grs in depbank resulted in an fscore']","['evaluation purposes, e. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting gold - standard ccg derivations into the grs in depbank resulted in an fscore']","['has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e. g.  #TAUTHOR_TAG.', 'the conclusion is that such conversions are surprisingly difficult.', ' #AUTHOR_TAG shows that converting gold - standard ccg derivations into the grs in depbank resulted in an fscore of only 85 % ; hence the upper bound on the performance of the ccg parser, using this evaluation scheme, was only 85 %.', 'given that the current best scores for the ptb parsing task are over 90 %, any loss from the conversion process needs to be considered carefully if a fair comparison with ptb parsers is to be achieved.', 'ccgbank was derived from the ptb, and so it might be considered that converting back to the ptb would be a relatively easy task, by essentially reversing the mapping  #AUTHOR_TAG used to create ccgbank.', 'however, there are a number of differences between the two treebanks which make the conversion back far from trivial.', 'first, the corresponding derivations in the treebanks are not isomorphic : a ccg derivation is not simply a relabelling of the nodes in the ptb tree ; there are many constructions, such as coordination and control structures, where the trees are a different shape, as well as having different labels.', 'it is important to realise that  #AUTHOR_TAG invested a significant amount of time and effort in creating the mapping.', 'second, some of the labels in the ptb do not appear in ccgbank, for example the qp label, and these must be added back in ; however, developing rules to insert these labels in the right places is a far from trivial task.', 'there were two approaches we considered for the conversion.', 'one possibility is to associate ptb tree structures with ccg lexical categories, and combine the trees together in step with the category combinations in a ccg derivation - in much the same way that an ltag has elementary trees in the lexicon which are combined using the substitution and adjunction rules of tag.', 'the second approach is to associate conversion rules with each local tree - i. e.', 'a parent and one or two child nodes - which appears in the ccgbank data.', '3 in this paper we took the second approach']",1
"[', which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2']","['98 %, which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2 %.', '']","[', which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2']","['schemas were developed by manual inspection using section 00 of ccgbank and the ptb as a development set, following the oracle methodology of  #AUTHOR_TAG, in which goldstandard derivations from ccgbank are converted to the new representation and compared with the gold standard for that representation.', 'as well as giving an idea of the difficulty, and success, of the conversion, the resulting numbers provide an up - in total, we annotated 32 unary and 776 binary rule instances ( of the possible 2853 instances ) with conversion schemas, and 162 of the 425 lexical categories.', 'we also implemented a small number of default catch - all cases for the general ccg combinatory rules and for the rules dealing with punctuation, which allowed most of the 2853 rule instances to be covered.', 'considerable time and effort was invested in the creation of these schemas.', 'the oracle conversion results from the gold standard ccgbank to the ptb for section 00 and 23 are shown in table 2.', 'the numbers are bracketing precision, recall, f - score and complete sentence matches, using the evalb evaluation script.', 'note that these figures provide an upper bound on the performance of the ccg parser using evalb, given the current conversion process.', 'the importance of this upper bound should not be underestimated, when the evaluation framework is such that incremental improvements of a few tenths of a percent are routinely presented as improving the state - of - the - art, as is the case with the parseval metrics.', 'the fact that the upper bound here is less than 95 % shows that it is not possible to fairly evaluate the ccg parser on the complete test set.', 'even an upper bound of around 98 %, which is achieved by  #TAUTHOR_TAG, is not sufficient, since this guarantees a loss of at least 2 %.', '']",4
"['by  #TAUTHOR_TAG.', 'even the smallest loss']","['by  #TAUTHOR_TAG.', 'even the smallest loss']","['by  #TAUTHOR_TAG.', 'even the smallest loss']","['question that is often asked of the ccg parsing work is "" why not convert back into the ptb representation and perform a parseval evaluation? "" by showing how difficult the conversion is, we believe that we have finally answered this question, as well as demonstrating comparable performance with the berkeley parser.', 'in addition, we have thrown further doubt on the possible use of the ptb for cross - framework parser evaluation, as recently suggested by  #TAUTHOR_TAG.', 'even the smallest loss due to mapping across representations is significant when a few tenths of a percentage point matter.', 'whether ptb parsers could be competitive on alternative parser evaluations, such as those using gr schemes, for which the ccg parser performs very well, is an open question']",3
"[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","['essential step in comparative reconstruction is to align corresponding phonological segments in the words being compared.', 'to do this, one must search among huge numbers of potential alignments to find those that give a good phonetic fit.', 'this is a hard computational problem, and it becomes exponentially more difficult when more than two strings are being aligned.', 'in this paper i extend the guided - search alignment algorithm of covington  #AUTHOR_TAG to handle more than two strings.', 'the resulting algorithm has been implemented in prolog and gives reasonable results when tested on data from several languages.', 'the comparative method for reconstructing languages consists of at least the following steps :', '1. choose sets of words in the daughter languages that appear to be cognate ;  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once.', 'the alignment step is hard to automate because there are too many possible alignments to choose from.', '']",6
"['them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']","['them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']","['them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']","['comparative method for reconstructing languages consists of at least the following steps :', ""1. choose sets of words in the daughter languages that appear to be cognate ; 275 a regular correspondence, once discovered, can be used to refine one's choice of alignments and even putative cognates."", 'parts of the comparative method have been computerized by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']",6
"[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","['essential step in comparative reconstruction is to align corresponding phonological segments in the words being compared.', 'to do this, one must search among huge numbers of potential alignments to find those that give a good phonetic fit.', 'this is a hard computational problem, and it becomes exponentially more difficult when more than two strings are being aligned.', 'in this paper i extend the guided - search alignment algorithm of covington  #AUTHOR_TAG to handle more than two strings.', 'the resulting algorithm has been implemented in prolog and gives reasonable results when tested on data from several languages.', 'the comparative method for reconstructing languages consists of at least the following steps :', '1. choose sets of words in the daughter languages that appear to be cognate ;  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once.', 'the alignment step is hard to automate because there are too many possible alignments to choose from.', '']",0
"[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","[' #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm']","['essential step in comparative reconstruction is to align corresponding phonological segments in the words being compared.', 'to do this, one must search among huge numbers of potential alignments to find those that give a good phonetic fit.', 'this is a hard computational problem, and it becomes exponentially more difficult when more than two strings are being aligned.', 'in this paper i extend the guided - search alignment algorithm of covington  #AUTHOR_TAG to handle more than two strings.', 'the resulting algorithm has been implemented in prolog and gives reasonable results when tested on data from several languages.', 'the comparative method for reconstructing languages consists of at least the following steps :', '1. choose sets of words in the daughter languages that appear to be cognate ;  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once.', 'the alignment step is hard to automate because there are too many possible alignments to choose from.', '']",0
"['them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']","['them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']","['them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']","['comparative method for reconstructing languages consists of at least the following steps :', ""1. choose sets of words in the daughter languages that appear to be cognate ; 275 a regular correspondence, once discovered, can be used to refine one's choice of alignments and even putative cognates."", 'parts of the comparative method have been computerized by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG, but none of them have tackled the alignment step.', ' #TAUTHOR_TAG presents a workable alignment algorithm for comparing two languages.', 'in this paper i extend that algorithm to handle more than two languages at once']",0
"['alignments are considered distinct  #TAUTHOR_TAG canfield 1996 ).', 'as for multiple strings, if']","['alignments are considered distinct  #TAUTHOR_TAG canfield 1996 ).', 'as for multiple strings, if']","['alignments are considered distinct  #TAUTHOR_TAG canfield 1996 ).', ""as for multiple strings, if two strings have a alignments then n strings have roughly a'~ - 1 alignments, assuming the alignments are generated by aligning the first two strings,""]","['alignment step is hard to automate because there are too many possible alignments to choose from.', 'for example, french le [ l ~ ] and spanish el [ el i can be lined up at least three ways :', 'of these, the second is etymologically correct, and the third would merit consideration if one did not know the etymology.', 'the number of alignments rises exponentially with the length of the strings and the number of strings being aligned.', 'two ten - letter strings have anywhere from 26, 797 to 8, 079, 453 different alignments depending on exactly what alignments are considered distinct  #TAUTHOR_TAG canfield 1996 ).', ""as for multiple strings, if two strings have a alignments then n strings have roughly a'~ - 1 alignments, assuming the alignments are generated by aligning the first two strings, then aligning the third string against the second, and so forth."", ""in fact, the search space isn't quite that large because some combinations are equivalent to others, but it is clearly too large to search exhaustively."", 'fortunately the comparative linguist is not looking for all possible alignments, only the ones that are likely to manifest regular sound correspondences - that is, those with a reasonable degree of phonetic similarity.', 'thus, phonetic similarity can be used to constrain the search']",0
['phonetic similarity criterion used by  #TAUTHOR_TAG is shown in'],['phonetic similarity criterion used by  #TAUTHOR_TAG is shown in'],"['phonetic similarity criterion used by  #TAUTHOR_TAG is shown in table 1.', 'it is obviously just a stand - in for a more sophisticated, perhaps feature - based, system of phonology.', 'the algorithm computes a "" badness "" or']","['phonetic similarity criterion used by  #TAUTHOR_TAG is shown in table 1.', 'it is obviously just a stand - in for a more sophisticated, perhaps feature - based, system of phonology.', 'the algorithm computes a "" badness "" or "" penalty "" for each step ( column ) in the alignment, summing the values to judge the badness of the whole alignment, thus :', 'the alignment with the lowest total badness is the one with the greatest phonetic similarity.', 'note that two separate skips count exactly the same as one complete mismatch ; thus the alignments e - e 1 lare equally valued.', 'in fact, a "" no - alternatingskips rule "" prevents the second one from being generated ; deciding whether [ e ] and [ i ] correspond is left for another, unstated, part of the comparison process.', 'i will explain below why this is not satisfactory.', 'naturally, the alignment with the best overall phonetic similarity is not always the etymologically correct one, although it is usually close ;', 'we are looking for a good phonetic fit, not necessarily the best one']",0
['from  #TAUTHOR_TAG that gives a reduced penalty'],['from  #TAUTHOR_TAG that gives a reduced penalty'],"['computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty']","['is, the algorithm is too reluctant to take skips.', 'the reason, intuitively, is that in this alignment step, there is really only one skip, not two separate skips ( one skipping if ] and one skipping [ p ] ).', 'this becomes even more apparent when more than three strings are being aligned.', 'accordingly, when computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty for adjacent skips in the same string to reflect the fact that affixes tend to be contiguous.', '5 searching the set of alignments the standard way to find the best alignment of two strings is a matrix - based technique known as dynamic programming ( ukkonen 1985, waterman 1995.', 'however, dynamic programming cannot accommodate rules that look ahead along the string to recognize assimilation or metathesis, a possibility that needs to be left open when implementing comparative reconstruction.', 'additionally, generalization of dynamic programming to multiple strings does not entirely appear to be a solved problem ( cf.', 'kececioglu 1993 ).', 'accordingly, i follow  #TAUTHOR_TAG in recasting the problem as a tree search.', 'consider the problem of aligning [ el ] with [ le ].', ' #TAUTHOR_TAG treats this as a process that steps through both strings and, at each step, performs either a "" match "" ( accepting a character from both strings ), a "" skip - l "" ( skipping a character in the first string ), or a "" skip - 2 "" ( skipping a character in the second string ).', 'that results in the search tree shown in fig. 1 ( ignoring covington\'s "" no - alternating - skips rule "" ).', 'the search tree can be generalized to multiple strings by breaking up each step into a series of operations, one on each string, as shown in fig. 2.', 'instead of three choices, match, skip - l, and skip - 2, there are really 2x2 : accept or skip on string 1 and then accept or skip on string 2.', ""one of the four combinations is disallowedyou can't have a step in which no characters are accepted from any string."", 'similarly, if there were three strings, there would be three two - way decisions, leading to eight ( = 2 3 ) states, one of which would be disallowed.', 'using search trees of this type, the decisions necessary to align any number of strings can be strung together']",0
['from  #TAUTHOR_TAG that gives a reduced penalty'],['from  #TAUTHOR_TAG that gives a reduced penalty'],"['computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty']","['is, the algorithm is too reluctant to take skips.', 'the reason, intuitively, is that in this alignment step, there is really only one skip, not two separate skips ( one skipping if ] and one skipping [ p ] ).', 'this becomes even more apparent when more than three strings are being aligned.', 'accordingly, when computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty for adjacent skips in the same string to reflect the fact that affixes tend to be contiguous.', '5 searching the set of alignments the standard way to find the best alignment of two strings is a matrix - based technique known as dynamic programming ( ukkonen 1985, waterman 1995.', 'however, dynamic programming cannot accommodate rules that look ahead along the string to recognize assimilation or metathesis, a possibility that needs to be left open when implementing comparative reconstruction.', 'additionally, generalization of dynamic programming to multiple strings does not entirely appear to be a solved problem ( cf.', 'kececioglu 1993 ).', 'accordingly, i follow  #TAUTHOR_TAG in recasting the problem as a tree search.', 'consider the problem of aligning [ el ] with [ le ].', ' #TAUTHOR_TAG treats this as a process that steps through both strings and, at each step, performs either a "" match "" ( accepting a character from both strings ), a "" skip - l "" ( skipping a character in the first string ), or a "" skip - 2 "" ( skipping a character in the second string ).', 'that results in the search tree shown in fig. 1 ( ignoring covington\'s "" no - alternating - skips rule "" ).', 'the search tree can be generalized to multiple strings by breaking up each step into a series of operations, one on each string, as shown in fig. 2.', 'instead of three choices, match, skip - l, and skip - 2, there are really 2x2 : accept or skip on string 1 and then accept or skip on string 2.', ""one of the four combinations is disallowedyou can't have a step in which no characters are accepted from any string."", 'similarly, if there were three strings, there would be three two - way decisions, leading to eight ( = 2 3 ) states, one of which would be disallowed.', 'using search trees of this type, the decisions necessary to align any number of strings can be strung together']",4
['from  #TAUTHOR_TAG that gives a reduced penalty'],['from  #TAUTHOR_TAG that gives a reduced penalty'],"['computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty']","['is, the algorithm is too reluctant to take skips.', 'the reason, intuitively, is that in this alignment step, there is really only one skip, not two separate skips ( one skipping if ] and one skipping [ p ] ).', 'this becomes even more apparent when more than three strings are being aligned.', 'accordingly, when computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty for adjacent skips in the same string to reflect the fact that affixes tend to be contiguous.', '5 searching the set of alignments the standard way to find the best alignment of two strings is a matrix - based technique known as dynamic programming ( ukkonen 1985, waterman 1995.', 'however, dynamic programming cannot accommodate rules that look ahead along the string to recognize assimilation or metathesis, a possibility that needs to be left open when implementing comparative reconstruction.', 'additionally, generalization of dynamic programming to multiple strings does not entirely appear to be a solved problem ( cf.', 'kececioglu 1993 ).', 'accordingly, i follow  #TAUTHOR_TAG in recasting the problem as a tree search.', 'consider the problem of aligning [ el ] with [ le ].', ' #TAUTHOR_TAG treats this as a process that steps through both strings and, at each step, performs either a "" match "" ( accepting a character from both strings ), a "" skip - l "" ( skipping a character in the first string ), or a "" skip - 2 "" ( skipping a character in the second string ).', 'that results in the search tree shown in fig. 1 ( ignoring covington\'s "" no - alternating - skips rule "" ).', 'the search tree can be generalized to multiple strings by breaking up each step into a series of operations, one on each string, as shown in fig. 2.', 'instead of three choices, match, skip - l, and skip - 2, there are really 2x2 : accept or skip on string 1 and then accept or skip on string 2.', ""one of the four combinations is disallowedyou can't have a step in which no characters are accepted from any string."", 'similarly, if there were three strings, there would be three two - way decisions, leading to eight ( = 2 3 ) states, one of which would be disallowed.', 'using search trees of this type, the decisions necessary to align any number of strings can be strung together']",1
['from  #TAUTHOR_TAG that gives a reduced penalty'],['from  #TAUTHOR_TAG that gives a reduced penalty'],"['computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty']","['is, the algorithm is too reluctant to take skips.', 'the reason, intuitively, is that in this alignment step, there is really only one skip, not two separate skips ( one skipping if ] and one skipping [ p ] ).', 'this becomes even more apparent when more than three strings are being aligned.', 'accordingly, when computing badness i count each skip only once ( assessing it 50 points ), then ignore skips when comparing the segments against each other. i have not implemented the rule from  #TAUTHOR_TAG that gives a reduced penalty for adjacent skips in the same string to reflect the fact that affixes tend to be contiguous.', '5 searching the set of alignments the standard way to find the best alignment of two strings is a matrix - based technique known as dynamic programming ( ukkonen 1985, waterman 1995.', 'however, dynamic programming cannot accommodate rules that look ahead along the string to recognize assimilation or metathesis, a possibility that needs to be left open when implementing comparative reconstruction.', 'additionally, generalization of dynamic programming to multiple strings does not entirely appear to be a solved problem ( cf.', 'kececioglu 1993 ).', 'accordingly, i follow  #TAUTHOR_TAG in recasting the problem as a tree search.', 'consider the problem of aligning [ el ] with [ le ].', ' #TAUTHOR_TAG treats this as a process that steps through both strings and, at each step, performs either a "" match "" ( accepting a character from both strings ), a "" skip - l "" ( skipping a character in the first string ), or a "" skip - 2 "" ( skipping a character in the second string ).', 'that results in the search tree shown in fig. 1 ( ignoring covington\'s "" no - alternating - skips rule "" ).', 'the search tree can be generalized to multiple strings by breaking up each step into a series of operations, one on each string, as shown in fig. 2.', 'instead of three choices, match, skip - l, and skip - 2, there are really 2x2 : accept or skip on string 1 and then accept or skip on string 2.', ""one of the four combinations is disallowedyou can't have a step in which no characters are accepted from any string."", 'similarly, if there were three strings, there would be three two - way decisions, leading to eight ( = 2 3 ) states, one of which would be disallowed.', 'using search trees of this type, the decisions necessary to align any number of strings can be strung together']",5
"['likely to contain the best alignments, thereby narrowing the intractably large search space into something manageable.', 'following  #TAUTHOR_TAG, i implemented a very']","['likely to contain the best alignments, thereby narrowing the intractably large search space into something manageable.', 'following  #TAUTHOR_TAG, i implemented a very']","['likely to contain the best alignments, thereby narrowing the intractably large search space into something manageable.', 'following  #TAUTHOR_TAG, i implemented a very simple pruning strategy.', 'the program keeps track of the badness of']","['goal of the algorithm is, of course, to generate not the whole search tree, but only the parts of it likely to contain the best alignments, thereby narrowing the intractably large search space into something manageable.', 'following  #TAUTHOR_TAG, i implemented a very simple pruning strategy.', 'the program keeps track of the badness of the best complete alignment found so far.', '']",5
"['the morphological dictionary romorphodict  #TAUTHOR_TAG,']","['the morphological dictionary romorphodict  #TAUTHOR_TAG,']","['the morphological dictionary romorphodict  #TAUTHOR_TAG,']","['dataset we used is a romanian language resource containing a total of 480, 722 inflected forms of romanian nouns and adjectives.', 'it was extracted from the text form of the morphological dictionary romorphodict  #TAUTHOR_TAG, which was also used by  #AUTHOR_TAG for their romanian classifier, where every entry has the following structure :', ""here,'form'denotes the inflected form and'description ', the morphosyntactic description, encoding part of speech, gender, number, and case."", ""for the morphosyntactic description, the initial dataset uses the slash ('/') as a disjunct operator meaning that'm / n'stands for'masculine or neuter ', while the dash ('-') is used for the conjunct operator, with'm - n'meaning'masculine and neuter '."", 'in the following, we will see that some of the disjunct gender labels can cause some problems in the extraction of the appropriate gender and subsequently in the classifier.', 'since our interest was in gender, we discarded all the adjectives listed and we isolated the nominative / accusative indefinite ( without the enclitic article ) form.', '']",5
"[', or a feminine  #TAUTHOR_TAG ( barbu,, p. 1939, for both']","['neuter, or a feminine  #TAUTHOR_TAG ( barbu,, p. 1939, for both']","['or a feminine  #TAUTHOR_TAG ( barbu,, p. 1939, for']",[' #TAUTHOR_TAG'],7
"[',, 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have']","[', 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have']","[', 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have achieved promising results']","[', pre - trained language representation models such as gpt  #AUTHOR_TAG ( radford et al.,, 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have achieved promising results in nlp tasks, including reading comprehension  #AUTHOR_TAG, natural language inference  #AUTHOR_TAG and sentiment classification  #AUTHOR_TAG.', 'these models capture contextual information from large - scale unlabelled corpora via well - designed pre - training * equal contribution † corresponding author : minlie huang tasks.', '']",0
['bert  #TAUTHOR_TAG becomes prevalent recently'],"['bert  #TAUTHOR_TAG becomes prevalent recently.', '']","['bert  #TAUTHOR_TAG becomes prevalent recently.', 'these models use deep lstm']","['', ""since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts."", 'thus contextual language representation based on pre - trained models including cove ( mc  #AUTHOR_TAG, elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG ( radford et al.,, 2019 and bert  #TAUTHOR_TAG becomes prevalent recently.', '']",0
['bert  #TAUTHOR_TAG becomes prevalent recently'],"['bert  #TAUTHOR_TAG becomes prevalent recently.', '']","['bert  #TAUTHOR_TAG becomes prevalent recently.', 'these models use deep lstm']","['', ""since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts."", 'thus contextual language representation based on pre - trained models including cove ( mc  #AUTHOR_TAG, elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG ( radford et al.,, 2019 and bert  #TAUTHOR_TAG becomes prevalent recently.', '']",0
['bert  #TAUTHOR_TAG becomes prevalent recently'],"['bert  #TAUTHOR_TAG becomes prevalent recently.', '']","['bert  #TAUTHOR_TAG becomes prevalent recently.', 'these models use deep lstm']","['', ""since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts."", 'thus contextual language representation based on pre - trained models including cove ( mc  #AUTHOR_TAG, elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG ( radford et al.,, 2019 and bert  #TAUTHOR_TAG becomes prevalent recently.', '']",0
"[',, 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have']","[', 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have']","[', 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have achieved promising results']","[', pre - trained language representation models such as gpt  #AUTHOR_TAG ( radford et al.,, 2019, elmo  #AUTHOR_TAG, bert  #TAUTHOR_TAG and xlnet have achieved promising results in nlp tasks, including reading comprehension  #AUTHOR_TAG, natural language inference  #AUTHOR_TAG and sentiment classification  #AUTHOR_TAG.', 'these models capture contextual information from large - scale unlabelled corpora via well - designed pre - training * equal contribution † corresponding author : minlie huang tasks.', '']",1
"['models like bert  #TAUTHOR_TAG,']","['tasks with different settings.', 'compared with the vanilla pretrained models like bert  #TAUTHOR_TAG,']","['models like bert  #TAUTHOR_TAG,']","['task is formulated as follows : given a text sequence x = ( x 1, x 2, · · ·, x n ) of length n, our goal is to acquire the representation of the whole sequence h = ( h 1, h 2, · · ·, h n ) ∈ r n×d that captures the contextual information and the linguistic knowledge simultaneously.', 'in this formulation, d indicates the dimension of the representation vector.', 'figure 1 shows the overview of our model pipeline which contains three stages : 1 ) acquiring the prior sentiment polarity for each word with its corresponding part - of - speech tag ; 2 ) conducting pre - training via two tasks i. e. label - aware masked language modeling and next sentence prediction ; 3 ) fine - tuning on sentiment analysis tasks with different settings.', 'compared with the vanilla pretrained models like bert  #TAUTHOR_TAG, our model enriches the input sequence with its linguistic knowledge including part - of - speech tags and sentiment polarity labels, and utilizes a modified masked language model to capture the relationship between sentence - level sentiment labels and word - level knowledge in addition to context dependency']",4
['than the  #TAUTHOR_TAG'],['than the  #TAUTHOR_TAG'],"['aspect category sentiment classification than the  #TAUTHOR_TAG.', 'to explore']",[' #TAUTHOR_TAG'],4
"['one proposed by  #TAUTHOR_TAG.', 'label']","['one proposed by  #TAUTHOR_TAG.', '']","['to the one proposed by  #TAUTHOR_TAG.', 'label']","['pre - training, label - aware masked language model ( la - mlm ) and next sentence prediction ( nsp ) are adopted as the pre - training tasks where the setting of nsp is identical to the one proposed by  #TAUTHOR_TAG.', 'label - aware masked language model is designed to utilize the linguistic knowledge to grasp the implicit dependency between sentence - level sentiment labels and words in addition to context dependency.', 'it contains two separate sub - tasks, both of which take the position embedding, token embedding and segment embedding as the input.', 'the goal of sub - task # 1 is to recover the masked sequence conditioned on the sentence - level label, as shown in figure 2.', 'in this setting, we add the sentence - level sentiment embedding to the inputs and the model is required to predict the word, partof - speech tag and word - level sentiment polarity individually using the hidden states at the masked positions.', 'this sub - task explicitly exerts the impact of the high - level sentiment label on the words and the linguistic knowledge of words, enhancing the ability of our model to explore the complex connection among them.', '']",5
"['', 'we follow the fine - tuning setting of the existing work  #TAUTHOR_TAG ; :']","['', 'we follow the fine - tuning setting of the existing work  #TAUTHOR_TAG ; :']","['', 'we follow the fine - tuning setting of the existing work  #TAUTHOR_TAG ; :']","['with the ability to utilize linguistic knowledge via pre - training, our model can be finetuned to different sentiment analysis tasks, including sentence - level / aspect - level sentiment classification.', 'we follow the fine - tuning setting of the existing work  #TAUTHOR_TAG ; : sentence - level sentiment classification : the input of this task is a text sequence ( [CLS], x 1, x 2, · · ·, x n, [SEP] ).', 'the sentiment label is obtained based on the hidden state of [CLS].', 'aspect - level sentiment classification : in addition to the text sequence, the input additionally contains an aspect term / aspect category sequence ( a 1, · · ·, a l ).', 'the sentiment label is also acquired based on the hidden state of [CLS].', 'figure 4 illustrates the fine - tuning settings']",5
['used vanilla bert  #TAUTHOR_TAG as'],['used vanilla bert  #TAUTHOR_TAG as'],"['to all the bertstyle pre - training models, we used vanilla bert  #TAUTHOR_TAG as']","['adopted the yelp dataset challenge 2019 2 as our pre - training dataset.', 'this dataset contains 6, 685, 900 reviews with 5 - class review - level sentiment labels.', 'each review consists of 8. 1 sentences and 127. 8 words on average.', 'since our method can adapt to all the bertstyle pre - training models, we used vanilla bert  #TAUTHOR_TAG as the base framework to construct transformer blocks in this paper and leave the exploration of other models like roberta as future work.', '']",5
['than the  #TAUTHOR_TAG'],['than the  #TAUTHOR_TAG'],"['aspect category sentiment classification than the  #TAUTHOR_TAG.', 'to explore']",[' #TAUTHOR_TAG'],5
"['one proposed by  #TAUTHOR_TAG.', 'label']","['one proposed by  #TAUTHOR_TAG.', '']","['to the one proposed by  #TAUTHOR_TAG.', 'label']","['pre - training, label - aware masked language model ( la - mlm ) and next sentence prediction ( nsp ) are adopted as the pre - training tasks where the setting of nsp is identical to the one proposed by  #TAUTHOR_TAG.', 'label - aware masked language model is designed to utilize the linguistic knowledge to grasp the implicit dependency between sentence - level sentiment labels and words in addition to context dependency.', 'it contains two separate sub - tasks, both of which take the position embedding, token embedding and segment embedding as the input.', 'the goal of sub - task # 1 is to recover the masked sequence conditioned on the sentence - level label, as shown in figure 2.', 'in this setting, we add the sentence - level sentiment embedding to the inputs and the model is required to predict the word, partof - speech tag and word - level sentiment polarity individually using the hidden states at the masked positions.', 'this sub - task explicitly exerts the impact of the high - level sentiment label on the words and the linguistic knowledge of words, enhancing the ability of our model to explore the complex connection among them.', '']",3
"['based on masked language model and next sentence prediction  #TAUTHOR_TAG.', 'xlnet : the variant of bert which autoregressively recovers the masked tokens with permutation language model.', 'for fair comparison,']","['based on masked language model and next sentence prediction  #TAUTHOR_TAG.', 'xlnet : the variant of bert which autoregressively recovers the masked tokens with permutation language model.', 'for fair comparison,']","['with several state - of - theart pre - trained language representation models : bert : the pre - trained model based on masked language model and next sentence prediction  #TAUTHOR_TAG.', 'xlnet : the variant of bert which autoregressively recovers the masked tokens with permutation language model.', 'for fair comparison, all the baselines in this paper were set to the base version.', 'the number of parameters in each model is listed in table 1.', '']","['compared sentilr with several state - of - theart pre - trained language representation models : bert : the pre - trained model based on masked language model and next sentence prediction  #TAUTHOR_TAG.', 'xlnet : the variant of bert which autoregressively recovers the masked tokens with permutation language model.', 'for fair comparison, all the baselines in this paper were set to the base version.', 'the number of parameters in each model is listed in table 1.', '']",7
"['for quality differences phrased spontaneously in practice  #TAUTHOR_TAG.', 'in a crowdsourcing study, we test']","['for quality differences phrased spontaneously in practice  #TAUTHOR_TAG.', 'in a crowdsourcing study, we test']","[' #AUTHOR_TAG a ) and one with 17 reasons for quality differences phrased spontaneously in practice  #TAUTHOR_TAG.', 'in a crowdsourcing study, we test']","['', 'section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well - defined quality dimensions taken from theory  #AUTHOR_TAG a ) and one with 17 reasons for quality differences phrased spontaneously in practice  #TAUTHOR_TAG.', 'in a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions ( section 4 ).', 'we find that assessments of overall argumentation quality largely match in theory and practice.', 'nearly all phrased reasons are adequately represented in theory.', 'however, some theoretical quality dimensions seem hard to separate in practice.', 'most importantly, we provide evidence that the observed relative quality differences are reflected in absolute quality ratings.', 'still, our study underpins the fact that the theory - based argumentation quality assessment remains complex.', 'our results do not']",0
"['a is more convincing than b. table 2 : the 17 + 1 practical reason labels given in the corpus of  #TAUTHOR_TAG.', 'covered.', 'in section 3, we use their absolute quality ratings from 1 ( low ) to 3 ( high ) annotated by three experts for']","['a is more convincing than b. table 2 : the 17 + 1 practical reason labels given in the corpus of  #TAUTHOR_TAG.', 'covered.', 'in section 3, we use their absolute quality ratings from 1 ( low ) to 3 ( high ) annotated by three experts for']","['a is more convincing than b. table 2 : the 17 + 1 practical reason labels given in the corpus of  #TAUTHOR_TAG.', 'covered.', 'in section 3, we use their absolute quality ratings from 1 ( low ) to 3 ( high ) annotated by three experts for each dimension of 304 arguments taken from the ukpconvarg1 corpus detailed below']","['a is more convincing than b. table 2 : the 17 + 1 practical reason labels given in the corpus of  #TAUTHOR_TAG.', 'covered.', 'in section 3, we use their absolute quality ratings from 1 ( low ) to 3 ( high ) annotated by three experts for each dimension of 304 arguments taken from the ukpconvarg1 corpus detailed below']",0
"['is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these']","['is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these']","['is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these reasons were used to']","['is an application area where absolute quality ratings of argumentative text are common practice : essay scoring  #AUTHOR_TAG.', ' #AUTHOR_TAG annotated the argumentative strength of essays composing multiple arguments with notable agreement.', 'for single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e. g.,  #AUTHOR_TAG find accepted arguments based on attack relations,  #AUTHOR_TAG rank arguments by their persuasiveness, and  #AUTHOR_TAG b ) rank them by their relevance.', ' #AUTHOR_TAG argue that normative concepts such as fallacies rarely apply to real - life arguments and that they are too sophisticated for operationalization.', 'based on the idea that relative assessment is easier,  #AUTHOR_TAG b ) crowdsourced the ukpconvarg1 corpus.', 'argument pairs ( a, b ) from a debate portal were classified as to which argument is more convincing.', 'without giving any guidelines, the authors also asked for reasons as to why a is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these reasons were used to derive a hierarchical annotation scheme.', '']",0
"['is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these']","['is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these']","['is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these reasons were used to']","['is an application area where absolute quality ratings of argumentative text are common practice : essay scoring  #AUTHOR_TAG.', ' #AUTHOR_TAG annotated the argumentative strength of essays composing multiple arguments with notable agreement.', 'for single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e. g.,  #AUTHOR_TAG find accepted arguments based on attack relations,  #AUTHOR_TAG rank arguments by their persuasiveness, and  #AUTHOR_TAG b ) rank them by their relevance.', ' #AUTHOR_TAG argue that normative concepts such as fallacies rarely apply to real - life arguments and that they are too sophisticated for operationalization.', 'based on the idea that relative assessment is easier,  #AUTHOR_TAG b ) crowdsourced the ukpconvarg1 corpus.', 'argument pairs ( a, b ) from a debate portal were classified as to which argument is more convincing.', 'without giving any guidelines, the authors also asked for reasons as to why a is more convincing than b. in a follow - up study  #TAUTHOR_TAG, these reasons were used to derive a hierarchical annotation scheme.', '']",0
"[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","['hypotheses 1 and 2, we consider all 736 pairs of arguments from  #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', 'for each pair ( a, b ) with a being 1 source code and annotated data : http : / / www. arguana. com more convincing than b, we check whether the ratings of a and b for each dimension ( averaged over all annotators ) show a concordant difference ( i. e., a higher rating for a ), a disconcordant difference ( lower ), or a tie.', 'this way, we can correlate each dimension with all reason labels in table 2 including conv.', '']",0
"['arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as']","['arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as']","['of all arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as each reason refers']","['correlations found imply that the relative quality differences captured are reflected in absolute differences.', 'for explicitness, we computed the mean rating for each quality dimension of all arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.', '']",0
"['the experts.', 'however, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.', 'regarding simplification, the most common practical reasons of  #TAUTHOR_TAG imply what to focus on']","['the experts.', 'however, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.', 'regarding simplification, the most common practical reasons of  #TAUTHOR_TAG imply what to focus on']","['the experts.', 'however, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.', 'regarding simplification, the most common practical reasons of  #TAUTHOR_TAG imply what to focus on']","["", we checked to what extent lay annotators and experts agree in terms of krippendorff's α."", 'on one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of  #AUTHOR_TAG a ).', 'on the other hand, we estimated a reliable rating from the crowd ratings using mace  #AUTHOR_TAG and compared it to the experts.', ""table 5 : mean and mace krippendorff's α agreement between ( a ) the crowd and the experts, ( b ) two independent crowd groups and the experts, ( c ) group 1 and the experts, and ( d ) group 2 and the experts."", 'table 5 ( a ) presents the results.', 'for the mean ratings, most α - values are above. 40.', 'this is similar to the study of  #AUTHOR_TAG b ), where a range of. 27 to. 51 is reported, meaning that lay annotators achieve similar agreement to experts.', 'considering the minimum of mean and mace, we observe the highest agreement for overall quality (. 43 ) - analog to  #AUTHOR_TAG b ).', 'also, global sufficiency has the lowest agreement in both cases.', 'in contrast, the experts hardly said "" cannot judge "" at all, whereas the crowd chose it for about 4 % of all ratings ( most often for global sufficiency ), possibly due to a lack of training.', 'still, we conclude that the crowd generally handles the theory - based quality assessment almost as well as the experts.', 'however, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.', 'regarding simplification, the most common practical reasons of  #TAUTHOR_TAG imply what to focus on']",0
"[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","['hypotheses 1 and 2, we consider all 736 pairs of arguments from  #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', 'for each pair ( a, b ) with a being 1 source code and annotated data : http : / / www. arguana. com more convincing than b, we check whether the ratings of a and b for each dimension ( averaged over all annotators ) show a concordant difference ( i. e., a higher rating for a ), a disconcordant difference ( lower ), or a tie.', 'this way, we can correlate each dimension with all reason labels in table 2 including conv.', '']",3
"['arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as']","['arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as']","['of all arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as each reason refers']","['correlations found imply that the relative quality differences captured are reflected in absolute differences.', 'for explicitness, we computed the mean rating for each quality dimension of all arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.', '']",3
"[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","[' #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', '']","['hypotheses 1 and 2, we consider all 736 pairs of arguments from  #TAUTHOR_TAG where both have been annotated by  #AUTHOR_TAG a ).', 'for each pair ( a, b ) with a being 1 source code and annotated data : http : / / www. arguana. com more convincing than b, we check whether the ratings of a and b for each dimension ( averaged over all annotators ) show a concordant difference ( i. e., a higher rating for a ), a disconcordant difference ( lower ), or a tie.', 'this way, we can correlate each dimension with all reason labels in table 2 including conv.', '']",5
"['arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as']","['arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as']","['of all arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as each reason refers']","['correlations found imply that the relative quality differences captured are reflected in absolute differences.', 'for explicitness, we computed the mean rating for each quality dimension of all arguments from  #AUTHOR_TAG a ) with a particular reason label from  #TAUTHOR_TAG.', 'as each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.', '']",5
['can be stated as follows  #TAUTHOR_TAG : given an alphabet'],"['word list.', 'the second subtask can be stated as follows  #TAUTHOR_TAG : given an alphabet σ,']","['the sequence of letters in a word list.', 'the second subtask can be stated as follows  #TAUTHOR_TAG : given an alphabet']","['', 'given a sequence of letters, there are thus two main subtasks : 1 ) determine whether this is a nonword, 2 ) if so, select and rank candidate words as potential corrections to present to the writer.', 'the first subtask can be accomplished by searching for the sequence of letters in a word list.', 'the second subtask can be stated as follows  #TAUTHOR_TAG : given an alphabet σ, a word list d of strings ∈ σ *, and a string r / ∈ d and ∈ σ *, find w ∈ d such that w is the most likely correction.', 'minimum edit distance is used to select the most likely candidate corrections.', 'the general idea is that a minimum number of edit operations such as insertion and substitution are needed to convert the misspelling into a word.', 'words requiring the smallest numbers of edit operations are selected as the candidate corrections']",5
"['described in  #TAUTHOR_TAG.', '']","['described in  #TAUTHOR_TAG.', 'finally, p l and p p hl are combined using s cm b to rank each word']","['a misspelling r, p l ( r | w ) and p p hl ( r | w ) are calculated for each word in the word list using the algorithm described in  #TAUTHOR_TAG.', '']","[""order to evaluate the effect of pronunciation variation in  #AUTHOR_TAG's spelling correction approach, we compare the performance of the pronunciation model and the combined model with and without pronunciation variation."", 'we implemented the letter and pronunciation spelling correction models as described in section 2. 2.', 'the letter error model p l and the phone error model p p h are trained on the training set.', 'the development set is used to tune the parameters introduced in previous sections.', '7 in order to rank the words as candidate corrections for a misspelling r, p l ( r | w ) and p p hl ( r | w ) are calculated for each word in the word list using the algorithm described in  #TAUTHOR_TAG.', 'finally, p l and p p hl are combined using s cm b to rank each word']",5
"[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has']","['recent spelling correction approaches, edit operations have been extended beyond single character edits and the methods for calculating edit operation weights have become more sophisticated.', 'the spelling error model proposed by  #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has an associated probability that improves the ranking of candidate corrections by modeling how likely particular edits are.', ' #TAUTHOR_TAG estimate the probability of each edit from a corpus of spelling errors.', ' #AUTHOR_TAG extend  #TAUTHOR_TAG to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.', 'they show that including pronunciation information improves performance as compared to  #TAUTHOR_TAG']",0
"[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has']","['recent spelling correction approaches, edit operations have been extended beyond single character edits and the methods for calculating edit operation weights have become more sophisticated.', 'the spelling error model proposed by  #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has an associated probability that improves the ranking of candidate corrections by modeling how likely particular edits are.', ' #TAUTHOR_TAG estimate the probability of each edit from a corpus of spelling errors.', ' #AUTHOR_TAG extend  #TAUTHOR_TAG to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.', 'they show that including pronunciation information improves performance as compared to  #TAUTHOR_TAG']",0
"[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has']","['recent spelling correction approaches, edit operations have been extended beyond single character edits and the methods for calculating edit operation weights have become more sophisticated.', 'the spelling error model proposed by  #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has an associated probability that improves the ranking of candidate corrections by modeling how likely particular edits are.', ' #TAUTHOR_TAG estimate the probability of each edit from a corpus of spelling errors.', ' #AUTHOR_TAG extend  #TAUTHOR_TAG to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.', 'they show that including pronunciation information improves performance as compared to  #TAUTHOR_TAG']",0
"[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', '']","[' #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has']","['recent spelling correction approaches, edit operations have been extended beyond single character edits and the methods for calculating edit operation weights have become more sophisticated.', 'the spelling error model proposed by  #TAUTHOR_TAG allows generic string edit operations up to a certain length.', 'each edit operation also has an associated probability that improves the ranking of candidate corrections by modeling how likely particular edits are.', ' #TAUTHOR_TAG estimate the probability of each edit from a corpus of spelling errors.', ' #AUTHOR_TAG extend  #TAUTHOR_TAG to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.', 'they show that including pronunciation information improves performance as compared to  #TAUTHOR_TAG']",0
['##ing correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],"['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types and weights of edit operations.', 'the idea behind this approach is that a writer starts out with the intended word w in mind, but as it is being written the word passes through a noisy channel resulting in the observed non - word r. in order to determine how likely a candidate correction is, the spelling correction model determines the probability that the word w was the intended word given the misspelling r : p ( w | r ).', '']",0
['##ing correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],"['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types and weights of edit operations.', 'the idea behind this approach is that a writer starts out with the intended word w in mind, but as it is being written the word passes through a noisy channel resulting in the observed non - word r. in order to determine how likely a candidate correction is, the spelling correction model determines the probability that the word w was the intended word given the misspelling r : p ( w | r ).', '']",0
['##ing correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],"['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types and weights of edit operations.', 'the idea behind this approach is that a writer starts out with the intended word w in mind, but as it is being written the word passes through a noisy channel resulting in the observed non - word r. in order to determine how likely a candidate correction is, the spelling correction model determines the probability that the word w was the intended word given the misspelling r : p ( w | r ).', '']",0
['##ing correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types'],"['spelling correction models from  #TAUTHOR_TAG and  #AUTHOR_TAG use the noisy channel model approach to determine the types and weights of edit operations.', 'the idea behind this approach is that a writer starts out with the intended word w in mind, but as it is being written the word passes through a noisy channel resulting in the observed non - word r. in order to determine how likely a candidate correction is, the spelling correction model determines the probability that the word w was the intended word given the misspelling r : p ( w | r ).', '']",0
['describe an extension to  #TAUTHOR_TAG where'],['describe an extension to  #TAUTHOR_TAG where'],['describe an extension to  #TAUTHOR_TAG where the same noisy channel error model is used'],"['describe an extension to  #TAUTHOR_TAG where the same noisy channel error model is used to model phone sequences instead of letter sequences.', 'instead of the word w and the non - word r, the error model considers the pronunciation of the non - word r, pron r, and the pronunciation of the word w, pron w.', 'the error model over phone sequences, called p p h, is just like p l shown in figure 1 except that r and w are replaced with their pronunciations.', 'the model is trained like p l using alignments between phones.', 'since a spelling correction model needs to rank candidate words rather than candidate pronunciations,  #AUTHOR_TAG derive an error model that determines the probability that a word w was spelled as the non - word r based on their pronunciations.', 'their approximation of this model, called p p hl, is also shown in figure 1.', 'p p h ( pron w | pron r ) is the phone error model described above and p ( pron r | r ) is provided by the letter - to - phone model described below']",0
"['by  #TAUTHOR_TAG and  #AUTHOR_TAG appears well - suited for writers of english as a foreign language.', 'the letter and combined models']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG appears well - suited for writers of english as a foreign language.', 'the letter and combined models']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG appears well - suited for writers of english as a foreign language.', 'the letter and combined models']","['noisy channel spelling correction approach developed by  #TAUTHOR_TAG and  #AUTHOR_TAG appears well - suited for writers of english as a foreign language.', 'the letter and combined models outperform the traditional spell checker aspell by a wide margin.', 'although including pronunciation variation does not improve the combined model, it leads to significant improvements in the pronunciation - based model p p hl']",7
"[' #TAUTHOR_TAG, and mapping to other resources ( sche↵czyk & ellsw']","['paraphrase  #TAUTHOR_TAG, and mapping to other resources ( sche↵czyk & ellsworth 2006 ; ferrandez']","['paraphrase  #TAUTHOR_TAG, and mapping to other resources ( sche↵czyk & ellsworth 2006 ; ferr']","['introduction michael ellsworth ( international computer science institute, infinity @ icsi. berkeley. edu ) has been involved with framenet for well over a decade.', 'his chief focus is on semantic relations in framenet ( ruppenhofer et al. 2006 ), how they can be used for paraphrase  #TAUTHOR_TAG, and mapping to other resources ( sche↵czyk & ellsworth 2006 ; ferrandez et al. 2010b ).', 'increasingly, he has examined the connection of framenet to syntax and the constructicon ( torrent & ellsworth 2013 ; ziem & ellsworth to appear ), including in his pending dissertation on the constructions and frame semantics of emotion']",0
"['general language patterns  #TAUTHOR_TAG.', '']","['general language patterns  #TAUTHOR_TAG.', '']","['learning general language patterns  #TAUTHOR_TAG.', 'various extensions']",[' #TAUTHOR_TAG'],0
"['general language patterns  #TAUTHOR_TAG.', '']","['general language patterns  #TAUTHOR_TAG.', '']","['learning general language patterns  #TAUTHOR_TAG.', 'various extensions']",[' #TAUTHOR_TAG'],0
"['##s  #TAUTHOR_TAG.', 'it identified that nominal ie']","['verb - based extractions  #TAUTHOR_TAG.', ""it identified that nominal ie can't be completely syntactic, and,""]","['is a pattern learning approach based on a bootstrapped training data using high precision verb - based extractions  #TAUTHOR_TAG.', ""it identified that nominal ie can't be completely syntactic, and,""]","['the earliest work on nominal open ie is ollie, which is a pattern learning approach based on a bootstrapped training data using high precision verb - based extractions  #TAUTHOR_TAG.', ""it identified that nominal ie can't be completely syntactic, and, at the least, a list of relational nouns ( e. g, mother, director, ceo, capital ) is needed for high precision extraction."", '']",0
"[' #TAUTHOR_TAG, we report yield, since']","[' #TAUTHOR_TAG, we report yield, since']","[' #TAUTHOR_TAG, we report yield, since']","['', 'we randomly sample 2, 000 sentences from newswire and run both ( and other intermediate systems ) on them.', 'we ask two annotators ( students ) to tag if the sentence asserted or implied an extraction.', 'our inter - annotator agreement is 0. 97 and we retain the subset of extractions on which the annotators agree for further analysis.', 'note that while precision and yield ( number of correct extractions ) can be naturally computed by tagging extractions, estimating recall is challenging, as it requires annotators to tag all possible extractions from these sentences.', 'following previous work  #TAUTHOR_TAG, we report yield, since recall is proportional to yield and suffices for system comparisons.', 'table 3 reports the results.', ""we find that ollie's noun patterns have a good yield but poor precision, whereas relnoun 1. 1 has a decent precision of 0. 53, but the yield is much lower."", 'allowing nnp relational nouns in relnoun 1. 1 has a 66 % increase in yield, but precision takes a severe hit.', 'org filtering only helps a little bit in improving precision.', '']",5
"['', 'our study focuses on evaluating transfer learning using bert  #TAUTHOR_TAG to']","['terms from review text.', 'our study focuses on evaluating transfer learning using bert  #TAUTHOR_TAG to']","['is to extract aspect and opinion terms from review text.', 'our study focuses on evaluating transfer learning using bert  #TAUTHOR_TAG to']","['of the tasks in aspect - based sentiment analysis is to extract aspect and opinion terms from review text.', 'our study focuses on evaluating transfer learning using bert  #TAUTHOR_TAG to classify tokens from hotel reviews in bahasa indonesia.', 'we show that the default bert model failed to outperform a simple argmax method.', 'however, changing the default bert tokenizer to our custom one can improve the f 1 scores on our labels of interest by at least 5 %.', 'for i - aspect and b - sentiment, it can even increased the f 1 scores by 11 %.', 'on entity - level evaluation, our tweak on the tokenizer can achieve f 1 scores of 87 % and 89 % for aspect and senti - ment labels respectively.', 'these scores are only 2 % away from the best model by  #AUTHOR_TAG, but with much less training effort ( 8 vs 200 epochs )']",5
"['bahasa indonesia.', 'our main contribution in this study is evaluating bert  #TAUTHOR_TAG as a pre']","['bahasa indonesia.', 'our main contribution in this study is evaluating bert  #TAUTHOR_TAG as a pretrained transformer model on this token classification task on hotel reviews in bahasa indonesia.', 'we also']","['bahasa indonesia.', 'our main contribution in this study is evaluating bert  #TAUTHOR_TAG as a pretrained transformer model on this token classification task on hotel reviews in bahasa indonesia.', 'we also']","['analysis  #AUTHOR_TAG in review text usually consists of multiple aspects.', 'for instance, the following review talks about the location, room, and staff aspects of a hotel, "" excellent location to the tower of london.', ""we also walked to several other areas of interest ; albeit a bit of a trek if you don't mind walking."", 'the room was a typical hotel room in need of a refresh, however clean.', 'the staff couldn\'t have been more professional, they really helped get us a taxi when our pre arranged pickup ran late. "" in this review, some of the sentiment terms are "" excellent "", "" typical "", "" clean "", and "" professional "".', 'in this study, we are focusing on the aspect and opinion term extraction from the reviews to do aspect - based sentiment analysis  #AUTHOR_TAG.', 'while some work has been done in this task  #AUTHOR_TAG, we have not seen a transfer learning approach  #AUTHOR_TAG employed, which should need much less training effort.', 'using transfer learning is especially helpful for low - resource languages  #AUTHOR_TAG, such as bahasa indonesia.', 'our main contribution in this study is evaluating bert  #TAUTHOR_TAG as a pretrained transformer model on this token classification task on hotel reviews in bahasa indonesia.', 'we also found that the current pretrained bert tokenizer has a poor encoder for bahasa indonesia, thus we proposed our own custom tokenizer.', 'we also provided simpler baselines, namely argmax and logistic regression on word embeddings as comparisons']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['', 'for baseline model, we employed two methods : a simple argmax method and logistic regression on word embeddings from fasttext implementation  #AUTHOR_TAG.', 'in the argmax method, we classify a token as the most probable label train test b - aspect 7005 1758 i - aspect 2292 584 b - sentiment 9646 2384 i - sentiment 4265 1067 other 39897 9706 total 63105 15499 table 1 : label distribution label in the training set.', 'for fasttext implementation, we use the skip - gram model and produce 100 - dimensional vectors.', 'we proposed to use transfer learning from pretrained bert - base, multilingual cased  #TAUTHOR_TAG for this token classification problem.', 'we used the implementation in pytorch by  #AUTHOR_TAG 3.', 'we found out that the multilingual cased tokenizer of bert does not recognize some common terms in our dataset, such as "" kamar "" ( room ), "" kendala "" ( issue ), "" wifi "", "" koneksi "" ( connection ), "" bagus "" ( good ), "" bersih "" ( clean ).', 'in the training and validation sets, we found 24, 370 unknown tokens.', 'thus, we encode the token ourselves to have no unknown tokens.', 'for the rest of this paper, we will call this model bert - custom.', 'since the labels are imbalanced, we are using f 1score as the evaluation metric, which is defined as :', 'our experiment setup for bert and bertcustom is to use adam  #AUTHOR_TAG optimizer with 10 −4 as the learning rate and 5 epochs.', 'the batch size is 32 and we are optimizing the cross entropy loss function.', 'we split the training set into 70 : 30 for training and validation sets to tune the hyperparameters and then train with the whole training set before applying the model onto the test set']",5
"['they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve']","['they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve state - of - the - art performance']","['they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve']","['results from our experiments with bio scheme labels are summarized in table 2.', 'we can see in the table that using the default tokenizer cannot beat the baseline f 1 scores for b - aspect and b - sentiment labels.', 'however, changing the tokenizer can improve the f 1 scores by at least 3 https : / / github. com / huggingface / pytorch - transformers 5 %.', 'for i - aspect and b - sentiment, it can increase the f 1 scores by 11 %.', 'on the other hand,  #AUTHOR_TAG trained their model using 200 epochs, while we only use 5 epochs.', 'we also found that simply using word embedding ( fast - text ) is not suitable for this task since it failed to achieve higher f 1 scores compared to a simple argmax method.', 'furthermore, we can see in figure 1 that the model overfits after about 12 iterations ( mini - batches ).', 'table 3 shows the performance on entity level.', 'we are only interested in evaluating the aspect and sentiment labels while actually trained the models with 3 labels.', 'in this case, we increased the number of epochs to 8 since it can yield higher f 1 scores.', 'it is interesting to see that bert is not even better than argmax in this simplified setting.', 'nevertheless, changing the default bert tokenizer is beneficial as well.', 'bert - custom model outperforms argmax by more than 5 % on our labels of interest and only 2 % away from beating the results by  #AUTHOR_TAG.', 'summarized several studies on aspect and opinion terms extraction.', 'some of the methods used are association rule mining  #AUTHOR_TAG, dependency rule parsers  #AUTHOR_TAG, conditional random fields ( crf ) and hidden markov model ( hmm )  #AUTHOR_TAG, topic modelling  #AUTHOR_TAG, and deep learning  #AUTHOR_TAG.', ' #AUTHOR_TAG combines the idea of coupled multilayer attentions ( cmla ) by and double embeddings by  #AUTHOR_TAG on aspect and opinion term extraction on semeval.', 'the work by  #AUTHOR_TAG itself is an improvement from what their prior work on the same task  #AUTHOR_TAG.', 'thus, we only included the work by  #AUTHOR_TAG because they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve state - of - the - art performance not only on sentence - level, but also on token - level tasks, such as for named entity recognition ( ner ).', 'this motivates us to explore bert in our study.', 'this way, we do not need to use dependency parsers or any feature engineering']",0
"['they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve']","['they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve state - of - the - art performance']","['they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve']","['results from our experiments with bio scheme labels are summarized in table 2.', 'we can see in the table that using the default tokenizer cannot beat the baseline f 1 scores for b - aspect and b - sentiment labels.', 'however, changing the tokenizer can improve the f 1 scores by at least 3 https : / / github. com / huggingface / pytorch - transformers 5 %.', 'for i - aspect and b - sentiment, it can increase the f 1 scores by 11 %.', 'on the other hand,  #AUTHOR_TAG trained their model using 200 epochs, while we only use 5 epochs.', 'we also found that simply using word embedding ( fast - text ) is not suitable for this task since it failed to achieve higher f 1 scores compared to a simple argmax method.', 'furthermore, we can see in figure 1 that the model overfits after about 12 iterations ( mini - batches ).', 'table 3 shows the performance on entity level.', 'we are only interested in evaluating the aspect and sentiment labels while actually trained the models with 3 labels.', 'in this case, we increased the number of epochs to 8 since it can yield higher f 1 scores.', 'it is interesting to see that bert is not even better than argmax in this simplified setting.', 'nevertheless, changing the default bert tokenizer is beneficial as well.', 'bert - custom model outperforms argmax by more than 5 % on our labels of interest and only 2 % away from beating the results by  #AUTHOR_TAG.', 'summarized several studies on aspect and opinion terms extraction.', 'some of the methods used are association rule mining  #AUTHOR_TAG, dependency rule parsers  #AUTHOR_TAG, conditional random fields ( crf ) and hidden markov model ( hmm )  #AUTHOR_TAG, topic modelling  #AUTHOR_TAG, and deep learning  #AUTHOR_TAG.', ' #AUTHOR_TAG combines the idea of coupled multilayer attentions ( cmla ) by and double embeddings by  #AUTHOR_TAG on aspect and opinion term extraction on semeval.', 'the work by  #AUTHOR_TAG itself is an improvement from what their prior work on the same task  #AUTHOR_TAG.', 'thus, we only included the work by  #AUTHOR_TAG because they show that we can get the best result by combining the latest work by and  #AUTHOR_TAG.', 'in their paper,  #TAUTHOR_TAG show that they can achieve state - of - the - art performance not only on sentence - level, but also on token - level tasks, such as for named entity recognition ( ner ).', 'this motivates us to explore bert in our study.', 'this way, we do not need to use dependency parsers or any feature engineering']",1
[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],"['embeddings have revolutionized natural language processing by representing words as dense real - valued vectors in a low dimensional space.', 'pre - trained word embeddings such as glove  #AUTHOR_TAG, word2vec  #AUTHOR_TAG and fasttext  #AUTHOR_TAG, trained on large corpora are readily available for use in a variety of tasks.', 'subsequently, there has been emphasis on post - processing the embeddings to improve their performance on downstream tasks  #TAUTHOR_TAG or to induce linguistic properties ( mrksic et al. ;  #AUTHOR_TAG.', 'in particular, the principal component analysis ( pca ) based post - processing algorithm proposed by  #TAUTHOR_TAG has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction  #AUTHOR_TAG.', 'similarly, understanding the geometry of word embeddings is another area of active research  #AUTHOR_TAG.', 'researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from  #AUTHOR_TAG answering the question of optimal dimensionality selection.', 'in contrast to previous work, we explore the dimensional properties of existing pre - trained word embeddings through their principal components.', 'specifically, our contributions are as follows :', '']",0
[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],"['embeddings have revolutionized natural language processing by representing words as dense real - valued vectors in a low dimensional space.', 'pre - trained word embeddings such as glove  #AUTHOR_TAG, word2vec  #AUTHOR_TAG and fasttext  #AUTHOR_TAG, trained on large corpora are readily available for use in a variety of tasks.', 'subsequently, there has been emphasis on post - processing the embeddings to improve their performance on downstream tasks  #TAUTHOR_TAG or to induce linguistic properties ( mrksic et al. ;  #AUTHOR_TAG.', 'in particular, the principal component analysis ( pca ) based post - processing algorithm proposed by  #TAUTHOR_TAG has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction  #AUTHOR_TAG.', 'similarly, understanding the geometry of word embeddings is another area of active research  #AUTHOR_TAG.', 'researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from  #AUTHOR_TAG answering the question of optimal dimensionality selection.', 'in contrast to previous work, we explore the dimensional properties of existing pre - trained word embeddings through their principal components.', 'specifically, our contributions are as follows :', '']",0
[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],"['embeddings have revolutionized natural language processing by representing words as dense real - valued vectors in a low dimensional space.', 'pre - trained word embeddings such as glove  #AUTHOR_TAG, word2vec  #AUTHOR_TAG and fasttext  #AUTHOR_TAG, trained on large corpora are readily available for use in a variety of tasks.', 'subsequently, there has been emphasis on post - processing the embeddings to improve their performance on downstream tasks  #TAUTHOR_TAG or to induce linguistic properties ( mrksic et al. ;  #AUTHOR_TAG.', 'in particular, the principal component analysis ( pca ) based post - processing algorithm proposed by  #TAUTHOR_TAG has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction  #AUTHOR_TAG.', 'similarly, understanding the geometry of word embeddings is another area of active research  #AUTHOR_TAG.', 'researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from  #AUTHOR_TAG answering the question of optimal dimensionality selection.', 'in contrast to previous work, we explore the dimensional properties of existing pre - trained word embeddings through their principal components.', 'specifically, our contributions are as follows :', '']",1
[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],[' #TAUTHOR_TAG or to induce linguistic properties ( mrksic'],"['embeddings have revolutionized natural language processing by representing words as dense real - valued vectors in a low dimensional space.', 'pre - trained word embeddings such as glove  #AUTHOR_TAG, word2vec  #AUTHOR_TAG and fasttext  #AUTHOR_TAG, trained on large corpora are readily available for use in a variety of tasks.', 'subsequently, there has been emphasis on post - processing the embeddings to improve their performance on downstream tasks  #TAUTHOR_TAG or to induce linguistic properties ( mrksic et al. ;  #AUTHOR_TAG.', 'in particular, the principal component analysis ( pca ) based post - processing algorithm proposed by  #TAUTHOR_TAG has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction  #AUTHOR_TAG.', 'similarly, understanding the geometry of word embeddings is another area of active research  #AUTHOR_TAG.', 'researchers have tried to ascertain the importance of dimensionality for word embeddings, with results from  #AUTHOR_TAG answering the question of optimal dimensionality selection.', 'in contrast to previous work, we explore the dimensional properties of existing pre - trained word embeddings through their principal components.', 'specifically, our contributions are as follows :', '']",6
"['evaluate the post - processing algorithm ( ppa ) proposed in  #TAUTHOR_TAG, which']","['evaluate the post - processing algorithm ( ppa ) proposed in  #TAUTHOR_TAG, which']","['evaluate the post - processing algorithm ( ppa ) proposed in  #TAUTHOR_TAG, which']","['this section, we first describe and then evaluate the post - processing algorithm ( ppa ) proposed in  #TAUTHOR_TAG, which achieves high scores on word and semantic textual similarity tasks.', 'the algorithm removes the projections of top principal components from each of the word vectors, making the individual word vectors more discriminative ( refer to algorithm 1 for details ).', 'algorithm 1 : post processing algorithm ppa ( x, d ) data : embedding matrix x, threshold parameter d result : post - processed word embedding matrix x 1 x = x - x ;', '/ / subtract mean embedding / * compute pca components * / 2 ui = pca ( x ), where i = 1, 2..', '. d']",5
"['upon the successful usage of word representations  #AUTHOR_TAG, and word clusters  #TAUTHOR_TAG']","['upon the successful usage of word representations  #AUTHOR_TAG, and word clusters  #TAUTHOR_TAG']","['upon the successful usage of word representations  #AUTHOR_TAG, and word clusters  #TAUTHOR_TAG for ner by utilizing large']","['', '. the workshop on "" noisy user - generated text "" ( wnut ) continued its 2015 shared task on ner on tweets  #AUTHOR_TAG in 2016. in 2016, the task was divided into two parts : ( 1 ) identification of named entities in tweets, and (', '2 ) ner on 10 types of entities, namely person, geo - location, other, company, sports - team, facility, product, music - artist', ', movie, and tv - show. in this paper we introduce two solutions to perform ner on tweets. the first system, which we will refer to as the submitted solution [ st ], was submitted as an entry to the wn', '##ut 2016 ner shared task. it uses random feature [ rf ] dropout for up - sampling the dataset. this system was improved into a semi', '##supervised solution ( our 2 nd solution [ si ] ), which uses additional, unsupervised features. these features have been found to be useful in prior information extraction and ner', 'tasks. the semi - supervised approach circumvents the need to include word n - gram features from any tweets, and builds upon the successful usage of word representations  #AUTHOR_TAG, and word clusters  #TAUTHOR_TAG for ner by utilizing large amounts of unlabelled data', 'or models pre - trained on a large vocabulary. the si system was designed to mitigate the various issues mentioned above, and utilizes the un', '##labelled tokens from the all the available datasets ( including unlabelled test data ) to improve the prediction quality on the evaluation datasets, a form of transductive learning  #AUTHOR_TAG. the si system outperforms st by ~ 7 % ( f1 score ) when using the development set for evaluation, and by ~ 11 % when using the', 'test set ( 1 % higher than the 2 nd best team in', 'the task ). the si model does not utilize any word n - gram lexical features. we believe that the approach taken for si is useful for situations that', 'require refinement or adaptation of an existing classifier to perform well on a new test', 'set. we have released our experimental setup and code at https : / / github. com / napsternxg /', 'twitterner']",0
['of ner systems  #TAUTHOR_TAG'],['of ner systems  #TAUTHOR_TAG'],"['word representations have been shown to improve the accuracy of ner systems  #TAUTHOR_TAG.', 'we used 200 dimensional glove word representations']","['word representations have been shown to improve the accuracy of ner systems  #TAUTHOR_TAG.', 'we used 200 dimensional glove word representations [ wrg ]  #AUTHOR_TAG, which were pre - trained on 6 billion tweets.', 'furthermore, we built a set of word clusters by performing an agglomerative clustering of word representations [ wrftc ] and fine tuning them on the training plus development dataset by running the word2vec model  #AUTHOR_TAG']",0
"['ner tasks  #TAUTHOR_TAG.', 'one algorithm for creating such sets is brown clustering  #AUTHOR_TAG, which produces a hierarchical cluster of words in the corpus while optimizing the likelihood of a language model based on a hidden markov model ( hmm ).', 'we used pre -']","['ner tasks  #TAUTHOR_TAG.', 'one algorithm for creating such sets is brown clustering  #AUTHOR_TAG, which produces a hierarchical cluster of words in the corpus while optimizing the likelihood of a language model based on a hidden markov model ( hmm ).', 'we used pre - trained 1000 brown clusters [ wcbpt ] that were prepared by using a large']","['ner tasks  #TAUTHOR_TAG.', 'one algorithm for creating such sets is brown clustering  #AUTHOR_TAG, which produces a hierarchical cluster of words in the corpus while optimizing the likelihood of a language model based on a hidden markov model ( hmm ).', 'we used pre - trained 1000 brown clusters']","['clusters are word groupings that get generated in an unsupervised fashion, and they have been successfully used as features for ner tasks  #TAUTHOR_TAG.', 'one algorithm for creating such sets is brown clustering  #AUTHOR_TAG, which produces a hierarchical cluster of words in the corpus while optimizing the likelihood of a language model based on a hidden markov model ( hmm ).', 'we used pre - trained 1000 brown clusters [ wcbpt ] that were prepared by using a large corpus of tweets  #AUTHOR_TAG.', 'additionally, we built another set of brown clusters [ wcbd ] with a cluster size of 100 based on all of the available data by using the code provided by  #AUTHOR_TAG 1.', 'furthermore, we also used an implementation 2 of the algorithm proposed by  #AUTHOR_TAG to create 32 ( default option ) additional word clusters from our training plus development data based on the regex and sequential features of the words.', 'we choose to call these clark clusters [ wccc ].', 'additionally, for each token, we also included all word cluster features for their immediate neighbours along with interactive terms ; with the latter capturing the product of the token cluster with the neighbouring cluster']",0
"[' #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised']","[' #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised']","['ner tasks with sparse labelled data  #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of']","['work has shown that semi - supervised algorithms can perform decently for ner tasks with sparse labelled data  #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised word clusters, word representations, and refined gazetteers ; all of which contributed to a cumulative increase in accuracy over our initial submission [ st ] by ~ 11 % when using the test data for evaluation.', 'furthermore, the transition features learned by our model are reflective of correct learning of ner sequences and demonstrate the strength of using the bieuo encoding scheme.', 'additionally, the supervised training of our classifier on features extracted from the unlabelled data, as opposed to lexical token features, reduces the dimensionality of the training data for the classifier and results in increased performance in terms of both accuracy and training time.', 'furthermore, our model can be adjusted on the arrival of new unlabelled data by updating the underlying learned word clusters and representations, and retraining the model on the existing labelled data.', 'as identified by  #TAUTHOR_TAG, the importance of word representations and word clusters increases as the availability of unlabelled data increases.', 'we can add additional entity names to the gazetteers.', 'retraining the model on the same training data would then allow for accommodating to the new feature representations.', 'finally, the random feature dropout based up - sampling can help to increase the amount of training data available, and can also be improved by random swapping of entity types in the training data with their nearest neighbours in the word representations and clusters, or by choosing entities from the most correlated gazetteers.', 'we believe that our described models can help in improving ner on noisy - text, and our open source implementation can be further extended']",0
"[' #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised']","[' #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised']","['ner tasks with sparse labelled data  #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of']","['work has shown that semi - supervised algorithms can perform decently for ner tasks with sparse labelled data  #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised word clusters, word representations, and refined gazetteers ; all of which contributed to a cumulative increase in accuracy over our initial submission [ st ] by ~ 11 % when using the test data for evaluation.', 'furthermore, the transition features learned by our model are reflective of correct learning of ner sequences and demonstrate the strength of using the bieuo encoding scheme.', 'additionally, the supervised training of our classifier on features extracted from the unlabelled data, as opposed to lexical token features, reduces the dimensionality of the training data for the classifier and results in increased performance in terms of both accuracy and training time.', 'furthermore, our model can be adjusted on the arrival of new unlabelled data by updating the underlying learned word clusters and representations, and retraining the model on the existing labelled data.', 'as identified by  #TAUTHOR_TAG, the importance of word representations and word clusters increases as the availability of unlabelled data increases.', 'we can add additional entity names to the gazetteers.', 'retraining the model on the same training data would then allow for accommodating to the new feature representations.', 'finally, the random feature dropout based up - sampling can help to increase the amount of training data available, and can also be improved by random swapping of entity types in the training data with their nearest neighbours in the word representations and clusters, or by choosing entities from the most correlated gazetteers.', 'we believe that our described models can help in improving ner on noisy - text, and our open source implementation can be further extended']",0
"['then retrain our model on the original training data  #TAUTHOR_TAG.', 'in this']","['then retrain our model on the original training data  #TAUTHOR_TAG.', 'in this case, the unlabelled data prevent the classifier from overfitting to the training data by acting as a regularization factor.', 'an alternative approach would']","['then retrain our model on the original training data  #TAUTHOR_TAG.', 'in this case, the unlabelled data prevent the classifier from overfitting to the training data by acting as a regularization factor.', 'an alternative approach would be to train these clusters on a large number of unlabelled tweets that match the time range and search domain of the test tweets']","['described lexicon based solution [ st ] had one major drawback : the most highly weighted features were mainly tokens descriptive of entity types as occurring in the training data.', 'for example, the highest weighted feature for the label u - person was word _ normed : pope.', 'similarly, for many of the other entity types, the highest weighted features were the names or labels of popular entities.', 'although these features help to achieve a decent evaluation score on the development dataset, they can lead to overfitting of the classifier to the vocabulary of the training corpus.', 'in order to circumvent this issue, a semi - supervised  #AUTHOR_TAG solution builds on the general recent success of using word representations and word clusters in ner tasks, while disregarding lexical vocabulary based features.', 'the intuition behind our approach to the 2 nd solution [ si ] was to ensure that the classifier learns higher level representations of the observed tokens.', 'all the features used for our second solution augment the tokens present in the given tweets.', 'this allows us to scale - up the underlying resources, such as gazetteers, and improve word representations and clusters using the new unlabelled test data, while still being able to update the classifier from the initially provided, limited training data.', 'we replicate this behaviour in our classifiers by training our clusters on all of the unlabelled data generated by merging tweet texts from the training, development, and test data ( only un - labelled ) [ tdte ]  #AUTHOR_TAG, and comparing the resulting performance to that obtained with unsupervised training that does not consider the test data [ td ].', 'although it might appear that our classifier has access to the unlabelled test data sequences while learning, it rather is the case that we resemble an online setting where we continuously update our unsupervised features using the new batch of unlabelled test data, and then retrain our model on the original training data  #TAUTHOR_TAG.', 'in this case, the unlabelled data prevent the classifier from overfitting to the training data by acting as a regularization factor.', 'an alternative approach would be to train these clusters on a large number of unlabelled tweets that match the time range and search domain of the test tweets']",3
"[' #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised']","[' #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised']","['ner tasks with sparse labelled data  #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of']","['work has shown that semi - supervised algorithms can perform decently for ner tasks with sparse labelled data  #TAUTHOR_TAG.', 'we leverage this fact in our si model via the use of unsupervised word clusters, word representations, and refined gazetteers ; all of which contributed to a cumulative increase in accuracy over our initial submission [ st ] by ~ 11 % when using the test data for evaluation.', 'furthermore, the transition features learned by our model are reflective of correct learning of ner sequences and demonstrate the strength of using the bieuo encoding scheme.', 'additionally, the supervised training of our classifier on features extracted from the unlabelled data, as opposed to lexical token features, reduces the dimensionality of the training data for the classifier and results in increased performance in terms of both accuracy and training time.', 'furthermore, our model can be adjusted on the arrival of new unlabelled data by updating the underlying learned word clusters and representations, and retraining the model on the existing labelled data.', 'as identified by  #TAUTHOR_TAG, the importance of word representations and word clusters increases as the availability of unlabelled data increases.', 'we can add additional entity names to the gazetteers.', 'retraining the model on the same training data would then allow for accommodating to the new feature representations.', 'finally, the random feature dropout based up - sampling can help to increase the amount of training data available, and can also be improved by random swapping of entity types in the training data with their nearest neighbours in the word representations and clusters, or by choosing entities from the most correlated gazetteers.', 'we believe that our described models can help in improving ner on noisy - text, and our open source implementation can be further extended']",5
['trained on multiple languages simultaneously without changing the model architecture ( ostling and  #TAUTHOR_TAG'],['trained on multiple languages simultaneously without changing the model architecture ( ostling and  #TAUTHOR_TAG'],"['- art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture ( ostling and  #TAUTHOR_TAG.', 'in some cases this']","['work has shown that state - of - the - art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture ( ostling and  #TAUTHOR_TAG.', 'in some cases this leads to improved performance compared to models only trained on a specific language, suggesting that multilingual models learn to share useful knowledge crosslingually through their learned representations.', 'while a large body of research exists on the multilingual mind, the mechanisms explaining knowledge sharing in computational multilingual models remain largely unknown : what kind of knowledge is shared among languages?', '']",0
"['next word prediction  #AUTHOR_TAG ostling and  #AUTHOR_TAG, translation  #TAUTHOR_TAG, morphological reinflection  #AUTHOR_TAG and more  #AUTHOR_TAG.', 'a practical benefit of training models multilingually is']","['next word prediction  #AUTHOR_TAG ostling and  #AUTHOR_TAG, translation  #TAUTHOR_TAG, morphological reinflection  #AUTHOR_TAG and more  #AUTHOR_TAG.', 'a practical benefit of training models multilingually is']","['next word prediction  #AUTHOR_TAG ostling and  #AUTHOR_TAG, translation  #TAUTHOR_TAG, morphological reinflection  #AUTHOR_TAG and more  #AUTHOR_TAG.', 'a practical benefit of training models multilingually is']","['recent advances in neural networks have opened the way to the design of architecturally simple multilingual models for various nlp tasks, such as language modeling or next word prediction  #AUTHOR_TAG ostling and  #AUTHOR_TAG, translation  #TAUTHOR_TAG, morphological reinflection  #AUTHOR_TAG and more  #AUTHOR_TAG.', 'a practical benefit of training models multilingually is to transfer knowledge from high - resource languages to lowresource ones and improve task performance in the latter.', 'here we aim at understanding how linguistic knowledge is transferred among languages, specifically at the syntactic level, which to our knowledge has not been studied so far.', 'assessing the syntactic abilities of monolingual neural lms trained without explicit supervision has been the focus of several recent studies :  #AUTHOR_TAG analyzed the performance of lstm lms at an english subject - verb agreement task, while  #AUTHOR_TAG extended the analysis to various long - range agreement patterns in different languages.', 'the latter study found that state - of - the - art lms trained on a standard loglikelihood objective capture non - trivial patterns of syntactic agreement and can approach the performance levels of humans, even when tested on syntactically well - formed but meaningless ( nonce ) sentences.', 'cross - language interaction during language production and comprehension by human subjects has been widely studied in the fields of bilingualism and second language acquisition ( kellerman and sharwood smith ;  #AUTHOR_TAG under the terms of language transfer or cross - linguistic influence.', 'numerous studies have shown that both the lexicons and the grammars of different languages are not stored independently but together in the mind of bilinguals and second - language learners, leading to observ - able lexical and syntactic transfer effects  #AUTHOR_TAG.', 'for instance, through a crosslingual syntactic priming experiment,  #AUTHOR_TAG showed that bilinguals recently exposed to a given syntactic construction ( passive voice ) in their l1 tend to reuse the same construction in their l2.', 'while the neural networks in this study are not designed to be plausible models of the human mind learning and processing multiple languages, we believe there is interesting potential at the intersection of these research fields']",0
"['that supervision is provided simultaneously in the two languages ( ostling and  #TAUTHOR_TAG.', 'we']","['that supervision is provided simultaneously in the two languages ( ostling and  #TAUTHOR_TAG.', 'we']","['that supervision is provided simultaneously in the two languages ( ostling and  #TAUTHOR_TAG.', 'we leave']","['consider the scenario where l1 is overresourced compared to l2 and train our bilingual models by joint training on a mixed l1 / l2 corpus so that supervision is provided simultaneously in the two languages ( ostling and  #TAUTHOR_TAG.', 'we leave the evaluation of pre - training ( or transfer learning ) methods  #AUTHOR_TAG to future work.', 'the monolingual lm is trained on a small l2 corpus ( lm l2 ).', 'the bilingual lm is trained on a shuffled mix of the same small l2 corpus and a large l1 corpus, where l2 is oversampled to approximately match the amount of l1 sentences ( lm l1 + l2 ).', 'see table 1 for the actual training sizes.', 'for our preliminary experiments we have chosen french as the helper language ( l1 ) and italian as the target language ( l2 ).', 'since french and italian share many morphosyntactic patterns, accuracy on the italian agreement tasks is expected to benefit from adding french sentences to the training data if syntactic transfer occurs.', 'data and training details : we train our lms on french and italian wikipedia articles extracted using the wikiextractor tool.', '1 for each language, we maintain a vocabulary of the 50k most frequent tokens, and replace the remaining tokens by < unk >. for the bilingual lm, all words are prepended with a language tag so that vocabularies are completely disjoint.', 'their union ( 100k types ) is used to train the model.', 'this is the least optimistic scenario for linguistic transfer but also the most controlled one.', 'in future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap.', 'following the setup of  #AUTHOR_TAG, we train 2 - layer lstm models with embedding and hidden layers of 650 dimensions for 40 epochs.', 'the trained models are evaluated on the italian section of the syntactic benchmark provided by  #AUTHOR_TAG, which includes various non - trivial number agreement constructions.', '2 note that all models are trained on a regular corpus likelihood objective and do not receive any specific supervision for the syntactic tasks.', 'table 1 shows the results of our preliminary experiments.', 'the unigram baseline simply picks, for each sentence, the most frequent word form between singular or plural.', 'as an upper - bound we report the agreement accuracy obtained by a monolingual model trained on a large l2 corpus.', 'the effect of mixing the small italian corpus with the large french one does not appear to be major.', 'agreement accuracy increases slightly in the original sentences, where the model is free to rely on collocational cues, but decreases slightly']",5
[' #TAUTHOR_TAG on the sw'],[' #TAUTHOR_TAG on the sw'],[' #TAUTHOR_TAG on the swda corpus as a current state of the art for this'],"['', 'function ( our baseline model ). they also give importance to turn - taking by providing the speaker identity but do not analyse their model in this regard. this approach achieves about 73.', '9 % accuracy on the swda corpus. in another line of research ( ji et al., ;  #AUTHOR_TAG, authors claim that their models take care of the dependency of the utterances within a conversation.  #AUTHOR_TAG use discourse annotation for the word - level language modelling on the sw', '##da corpus and also highlight a limitation that this approach is not scalable to large data. in', 'other approaches a hierarchical convolutional and recurrent neural encoder model are used to learn utterance representation by feeding a whole conversation  #AUTHOR_TAG. the utterance representations are further used to classify da classes using the conditional random field ( crf', ') as a linear classifier. the model can see the past and future utterances at', 'the same time within a conversation, which limits usage in a dialogue system where one can only perceive the preceding utterance as a context', 'but does not know the upcoming utterances. hence, we use a context - based learning approach and regard the 73. 9 % accuracy  #TAUTHOR_TAG on the swda corpus as a current state of the art for this task']",7
"['without context.', 'we also compare our model with  #TAUTHOR_TAG who used the contextbased learning approach achieving']","['without context.', 'we also compare our model with  #TAUTHOR_TAG who used the contextbased learning approach achieving 73. 9 %.', 'our model uses minimal information, such as the context of a few preceding utterances which can be adapted to an online learning tool']","['without context.', 'we also compare our model with  #TAUTHOR_TAG who used the contextbased learning approach achieving 73. 9 %.', 'our model uses minimal information, such as the context of a few preceding utterances which can be adapted to an online learning tool']","['this article, we detail the annotation and modelling of dialogue act corpora, and we find that there is a difference in the way das are annotated and the way they are modelled.', 'we argue to generalise the discourse modelling for conversation within the context of communication.', 'hence, we propose to use the context - based learning approach for the da identification task.', 'we used simple rnn to model the context of preceding utterances.', 'we used the domainindependent pre - trained character language model to represent the utterances.', 'we evaluated the proposed model on the switchboard dialogue act corpus and show the results with and without context.', 'for this corpus, our model achieved an accuracy of 77. 34 % with context compared to 73. 96 % without context.', 'we also compare our model with  #TAUTHOR_TAG who used the contextbased learning approach achieving 73. 9 %.', 'our model uses minimal information, such as the context of a few preceding utterances which can be adapted to an online learning tool such as a spoken dialogue system where one can naturally see the preceding utterances but not the future ones.', 'this makes our model suitable for human - robot / computer interaction which can be easily plugged into any spoken dialogue system']",7
"['data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context,']","['data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context,']","['follow the same data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context,']","['follow the same data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context, then with one, two, and so on preceding utterances in the context.', 'we examined different values for the number of the hidden units of the rnn, empirically 64 was identified as best and used throughout the experiments.', 'we also experimented with the various representations for the speaker id that is concatenated with the respective utterances but could find no differences.', 'as a result, our proposed model uses minimal information for the context.', 'the performance increases from 74 % to about 77 % with context.', 'we run each experiment for ten times and take the average.', 'the model shows robustness providing minimal variance, and using a minimum number of preceding utterances as a context can produce fair results']",3
"['data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context,']","['data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context,']","['follow the same data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context,']","['follow the same data split of 1115 training and 19 test conversations as in the baseline approach  #TAUTHOR_TAG.', 'table 3 shows the results of the proposed model with several setups, first without the context, then with one, two, and so on preceding utterances in the context.', 'we examined different values for the number of the hidden units of the rnn, empirically 64 was identified as best and used throughout the experiments.', 'we also experimented with the various representations for the speaker id that is concatenated with the respective utterances but could find no differences.', 'as a result, our proposed model uses minimal information for the context.', 'the performance increases from 74 % to about 77 % with context.', 'we run each experiment for ten times and take the average.', 'the model shows robustness providing minimal variance, and using a minimum number of preceding utterances as a context can produce fair results']",5
"[""to asymptotic efficiency haven't yet been explored in mt."", 'recently,  #TAUTHOR_TAG formalized dependency']","[""to asymptotic efficiency haven't yet been explored in mt."", 'recently,  #TAUTHOR_TAG formalized dependency']","[""to asymptotic efficiency haven't yet been explored in mt."", 'recently,  #TAUTHOR_TAG formalized dependency parsing as a maximum spanning tree ( mst ) problem, which can be solved in quadratic time relative']","[""is a prime concern in syntactic mt decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in mt."", 'recently,  #TAUTHOR_TAG formalized dependency parsing as a maximum spanning tree ( mst ) problem, which can be solved in quadratic time relative to the length of the sentence.', 'they show that mst parsing is almost as accurate as cubic - time dependency parsing in the case of english, and that it is more accurate with free word order languages.', 'this paper applies mst parsing to mt, and describes how it can be integrated into a phrase - based decoder to compute dependency language model scores.', 'our results show that augmenting a state - ofthe - art phrase - based system with this dependency language model leads to significant improvements in ter ( 0. 92 % ) and bleu ( 0. 45 % ) scores on five nist chinese - english evaluation test sets']",0
"[' #AUTHOR_TAG,  #TAUTHOR_TAG present a quadratic - time']","[' #AUTHOR_TAG,  #TAUTHOR_TAG present a quadratic - time']","[',  #TAUTHOR_TAG present a quadratic - time']","['', 'n 2 ) algorithms exist in the case of dependency parsing. building upon the theoretical', 'work of  #AUTHOR_TAG,  #TAUTHOR_TAG present a quadratic - time dependency parsing', '']",0
"['is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time']","['directly or indirectly headed by hired ( i. e., who, think, they, and hired ) do not constitute a contiguous sequence of words. as we will see later', 'in this section. the most standardly used algorithm for parsing with dependency grammars is presented in', ' #AUTHOR_TAG. it runs in time o ( n 3 ),', 'where n is the length of the sentence. their algorithm exploits the special properties of dependency trees to reduce the', 'worst - case complexity of bilexical parsing, which otherwise requires o ( n 4 ) for bilexical constituency - based parsing. while it seems difficult to improve the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time parsing is possible if trees are not required to be projective. this relaxation entails that dependencies may cross each other rather than being required to be nested, as shown', '']",0
"['is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time']","['directly or indirectly headed by hired ( i. e., who, think, they, and hired ) do not constitute a contiguous sequence of words. as we will see later', 'in this section. the most standardly used algorithm for parsing with dependency grammars is presented in', ' #AUTHOR_TAG. it runs in time o ( n 3 ),', 'where n is the length of the sentence. their algorithm exploits the special properties of dependency trees to reduce the', 'worst - case complexity of bilexical parsing, which otherwise requires o ( n 4 ) for bilexical constituency - based parsing. while it seems difficult to improve the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time parsing is possible if trees are not required to be projective. this relaxation entails that dependencies may cross each other rather than being required to be nested, as shown', '']",0
"['is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time']","['directly or indirectly headed by hired ( i. e., who, think, they, and hired ) do not constitute a contiguous sequence of words. as we will see later', 'in this section. the most standardly used algorithm for parsing with dependency grammars is presented in', ' #AUTHOR_TAG. it runs in time o ( n 3 ),', 'where n is the length of the sentence. their algorithm exploits the special properties of dependency trees to reduce the', 'worst - case complexity of bilexical parsing, which otherwise requires o ( n 4 ) for bilexical constituency - based parsing. while it seems difficult to improve the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time parsing is possible if trees are not required to be projective. this relaxation entails that dependencies may cross each other rather than being required to be nested, as shown', '']",0
"['is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show']","['', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time']","['directly or indirectly headed by hired ( i. e., who, think, they, and hired ) do not constitute a contiguous sequence of words. as we will see later', 'in this section. the most standardly used algorithm for parsing with dependency grammars is presented in', ' #AUTHOR_TAG. it runs in time o ( n 3 ),', 'where n is the length of the sentence. their algorithm exploits the special properties of dependency trees to reduce the', 'worst - case complexity of bilexical parsing, which otherwise requires o ( n 4 ) for bilexical constituency - based parsing. while it seems difficult to improve the asymptotic running time of the eisner algorithm beyond what is presented', 'in  #AUTHOR_TAG,  #TAUTHOR_TAG show o ( n 2 ) - time parsing is possible if trees are not required to be projective. this relaxation entails that dependencies may cross each other rather than being required to be nested, as shown', '']",0
"[' #AUTHOR_TAG,  #TAUTHOR_TAG present a quadratic - time']","[' #AUTHOR_TAG,  #TAUTHOR_TAG present a quadratic - time']","[',  #TAUTHOR_TAG present a quadratic - time']","['', 'n 2 ) algorithms exist in the case of dependency parsing. building upon the theoretical', 'work of  #AUTHOR_TAG,  #TAUTHOR_TAG present a quadratic - time dependency parsing', '']",6
['similarly to  #TAUTHOR_TAG and then describe a modified and more efficient version that can be'],['similarly to  #TAUTHOR_TAG and then describe a modified and more efficient version that can be'],"['now formalize weighted non - projective dependency parsing similarly to  #TAUTHOR_TAG and then describe a modified and more efficient version that can be integrated into a phrasebased decoder.', 'given the single - head constraint, parsing']","['now formalize weighted non - projective dependency parsing similarly to  #TAUTHOR_TAG and then describe a modified and more efficient version that can be integrated into a phrasebased decoder.', 'given the single - head constraint, parsing an input sentence x = ( x 0, x 1, · · ·, x n ) is reduced to labeling each word x j with an index i identifying its head word x i.', '']",5
"['upon the cubic time implementation of  #TAUTHOR_TAG,']","['upon the cubic time implementation of  #TAUTHOR_TAG,']","['upon the cubic time implementation of  #TAUTHOR_TAG,']","['this paper, we presented a non - projective dependency parser whose time - complexity of o ( n 2 ) improves upon the cubic time implementation of  #TAUTHOR_TAG, and does so with little loss in dependency accuracy (. 25 % to. 34 % ).', 'since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase - based decoder during search ( rather than during rescoring ).', 'we use dependency scores as an extra feature in our mt experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5 - gram language model ( 0. 92 % ter and 0. 45 % bleu absolute improvements ).', 'we plan to pursue other research directions using dependency models discussed in this paper.', 'while we use a dependency language model to exemplify the use of hierarchical structure within phrase based decoders, we could extend this work to incorporate dependency features of both sourceand target side.', 'since parsing of the source is relatively inexpensive compared to the target side, it would be relatively easy to condition headmodifier dependencies not only on the two target words, but also on their corresponding chinese words and their relative positions in the chinese tree.', 'this would enable the decoder to capture syntactic reordering without requiring trees to be isomorphic or even projective.', 'it would also be interesting to apply these models to target languages that have free word order, which would presumably benefit more from the flexibility of non - projective dependency models']",5
"['j are both free variables, feature computation in  #TAUTHOR_TAG']","['j are both free variables, feature computation in  #TAUTHOR_TAG']","['j are both free variables, feature computation in  #TAUTHOR_TAG']","['our experiments, we use sets of features that are similar to the ones used in the mcdonald parser, though we make a key modification that yields an asymptotic speedup that ensures a genuine o ( n 2 ) running time.', 'the three feature sets that were used in our experiments are shown in table 2.', 'we write h - word, h - pos, m - word, m - pos to refer to head and modifier words and pos tags, and append a numerical value to shift the word offset either to the left or to the right ( e. g., h - pos + 1 is the pos to the right of the head word ).', 'we use the symbol ∧ to represent feature conjunctions.', 'each feature in the table has a distinct identifier, so that, e. g., the pos features in - between pos features : h - pos are all distinct from m - pos features.', '3 the primary difference between our feature sets and the ones of mcdonald et al. is that their set of "" in between pos features "" includes the set of all tags appearing between each pair of words.', 'extracting all these tags takes time o ( n ) for any arbitrary pair ( i, j ).', 'since i and j are both free variables, feature computation in  #TAUTHOR_TAG takes time o ( n 3 ), even though parsing itself takes o ( n 2 ) time.', 'to make our parser genuinely o ( n 2 ), we modified the set of in - between pos features in two ways.', 'first, we restrict extraction of in - between pos tags to those words that appear within a window of five words relative to either the head or the modifier.', 'while this change alone ensures that feature extraction is now o ( 1 ) for each word pair, this causes a fairly high drop of performance ( dependency accuracy table 4 : dependency parsing experiments on test sentences of any length.', 'the projective parsing algorithm is the one implemented as in ( mc  #AUTHOR_TAG a ), which is known as one of the top performing dependency parsers for english.', 'the o ( n 3 ) non - projective parser of  #TAUTHOR_TAG is slightly more accurate than our version, though ours runs in o ( n 2 ) time. "" local classifier "" refers to non - projective dependency parsing without removing loops as a post - processing step.', 'the result marked with ( * ) identifies the parser used for our mt experiments, which is only about 1 % less accurate than a state - of - the - art dependency parser ( * * ).', 'on our test was down 0. 9 % )']",1
"['j are both free variables, feature computation in  #TAUTHOR_TAG']","['j are both free variables, feature computation in  #TAUTHOR_TAG']","['j are both free variables, feature computation in  #TAUTHOR_TAG']","['our experiments, we use sets of features that are similar to the ones used in the mcdonald parser, though we make a key modification that yields an asymptotic speedup that ensures a genuine o ( n 2 ) running time.', 'the three feature sets that were used in our experiments are shown in table 2.', 'we write h - word, h - pos, m - word, m - pos to refer to head and modifier words and pos tags, and append a numerical value to shift the word offset either to the left or to the right ( e. g., h - pos + 1 is the pos to the right of the head word ).', 'we use the symbol ∧ to represent feature conjunctions.', 'each feature in the table has a distinct identifier, so that, e. g., the pos features in - between pos features : h - pos are all distinct from m - pos features.', '3 the primary difference between our feature sets and the ones of mcdonald et al. is that their set of "" in between pos features "" includes the set of all tags appearing between each pair of words.', 'extracting all these tags takes time o ( n ) for any arbitrary pair ( i, j ).', 'since i and j are both free variables, feature computation in  #TAUTHOR_TAG takes time o ( n 3 ), even though parsing itself takes o ( n 2 ) time.', 'to make our parser genuinely o ( n 2 ), we modified the set of in - between pos features in two ways.', 'first, we restrict extraction of in - between pos tags to those words that appear within a window of five words relative to either the head or the modifier.', 'while this change alone ensures that feature extraction is now o ( 1 ) for each word pair, this causes a fairly high drop of performance ( dependency accuracy table 4 : dependency parsing experiments on test sentences of any length.', 'the projective parsing algorithm is the one implemented as in ( mc  #AUTHOR_TAG a ), which is known as one of the top performing dependency parsers for english.', 'the o ( n 3 ) non - projective parser of  #TAUTHOR_TAG is slightly more accurate than our version, though ours runs in o ( n 2 ) time. "" local classifier "" refers to non - projective dependency parsing without removing loops as a post - processing step.', 'the result marked with ( * ) identifies the parser used for our mt experiments, which is only about 1 % less accurate than a state - of - the - art dependency parser ( * * ).', 'on our test was down 0. 9 % )']",4
"['n 3 ) implementation of  #TAUTHOR_TAG', '']","['n 3 ) implementation of  #TAUTHOR_TAG', '']","['o ( n 3 ) implementation of  #TAUTHOR_TAG', '']","['', '6 %. table 4 shows that the accuracy of', 'our truly o ( n 2 ) parser is only. 25 % to. 34 % worse than the o ( n 3 ) implementation of  #TAUTHOR_TAG', '']",4
"['reported in  #TAUTHOR_TAG,']","['reported in  #TAUTHOR_TAG,']","['reported in  #TAUTHOR_TAG,']","['our experiments, we use a re - implementation of the moses phrase - based decoder.', 'we use the standard features implemented almost exactly as in moses : four translation features ( phrase - based translation probabilities and lexically - weighted probabilities ), word penalty, phrase penalty, linear distortion, and language model score.', 'we also incorporated the lexicalized reordering features of moses, in order to experiment with a baseline that is stronger than the default moses configuration.', 'the language pair for our experiments is chinese - to - english.', 'the training data consists of about 28 million english words and 23. 3 million 5 note that our results on wsj are not exactly the same as those reported in  #TAUTHOR_TAG, since we used slightly different head finding rules.', 'to extract dependencies from treebanks, we used the lth penn converter ( http : / / nlp. cs. lth. se / pennconverter / ), which extracts dependencies that are almost identical to those used for the conll - 2008 shared task.', 'we constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts ( i. e., perform selftraining ) in future work.', 'chinese words drawn from various news parallel corpora distributed by the linguistic data consortium ( ldc ).', '']",4
"['detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tag']","['detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tagger, augmented with multiple word representations.', 'we enhance this']","['submission to the w - nut named entity recognition in twitter task closely follows the approach detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tagger, augmented with multiple word representations.', 'we enhance this approach with updated gazetteers, and with infused phrase embeddings']","['submission to the w - nut named entity recognition in twitter task closely follows the approach detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tagger, augmented with multiple word representations.', 'we enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase.', 'our system achieves a typed f1 of 44. 7, resulting in a third - place finish, despite training only on the official training set.', 'a post - competition analysis indicates that also training on the provided development data improves our performance to 54. 2 f1']",3
"['follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tag']","['follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tagger']","['follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tagger trained online with standard discriminative tagging features, gaze']","['entity recognition ( ner ) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo - location  #AUTHOR_TAG.', 'ner is the first step in many information extraction tasks, but in social media, this task is extremely challenging.', 'the text to be analyzed is unedited and noisy, and covers a much more diverse set of topics than one might expect in newswire.', 'as such, we are quite interested in the w - nut named entity recognition in twitter task  #AUTHOR_TAG as a platform to benchmark and drive forward work on ner in social media.', 'our submission to this competition closely follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tagger trained online with standard discriminative tagging features, gazetteer matches, brown clusters, and word embeddings.', 'we augment this approach with updated gazetteers, phrase embeddings, and infused embeddings that have been adapted to better predict gazetteer membership.', ""our novel infusion technique allows us to adapt existing vectors to ner regardless of their source, by training a typelevel auto - encoder whose hidden layer must predict the corresponding phrase's gazetteer memberships while also recovering the original vector."", 'our submitted system achieved a typed f1 of 44. 7, placing third in the competition, while training only on the provided training data.', 'the competition organizers provided two development sets, one ( dev ) that is close to the training data, with both train and dev being drawn from the year 2010, and another ( dev 2015 ) that is close to the test data, with both dev 2015 and test being drawn from the winter of 2014 - 2015.', 'we present a postcompetition system that achieves an f1 of 54. 2 using the same features and hyper - parameters as our submitted system, except that our tagger is also trained on all provided development data.', ""we close with an analysis of dev 2015's relation to the test set, and argue that these results may overestimate the impact that a small, in - domain training set can have on ner performance""]",3
"['corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized']","['corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized']","['and april 2012.', 'the same corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized']","['', 'our unannotated corpus collects 98m tweets ( 1, 995m tokens ) from between may 2011 and april 2012.', 'the same corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized and post - processed to remove many special unicode characters ; they closely resemble those that appear in the provided training and development sets.', 'furthermore, the corpus consists only of tweets in which the ner system of  #AUTHOR_TAG detects at least one entity.', 'the automatic ner tags are used only to select tweets for inclusion in the corpus, after which the annotations are discarded.', 'filtering our tweets in this way has two immediate effects : first, each tweet is very likely to contain an entity mention.', 'second, the tweets are very long, with an average of 20. 4 tokens per tweet.', ""as the test data is drawn from the winter of 2014 - 2015, we attempted to augment our corpus with more recent data : 13m unannotated english tweets drawn from twitter's public stream, from between april 24 and may 6, 2015."", 'as we had very little recent data, we made no attempt to bias the corpus to be entity - rich.', 'this corpus of recent tweets has an average tweet length of only 13. 8 tokens.', 'our attempts to use this data to build word representations did not improve ner performance on the 2015 development set, regardless of whether we used the data on its own or in combination with our larger corpus']",3
"['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['use only in post - competition analysis. lexical features : recall that our semi', '- markov model allows for both word and phrase - level features. the vast majority of our features are wordlevel, with the representation for a phrase being the sum of the features of its words.', 'our wordlevel features closely follow the set proposed by  #AUTHOR_TAG, covering word identity, the identities', 'of surrounding words within a window of 2 tokens, and prefixes and suffixes up to three characters in length. each word identity feature', ""has three variants, with the first reporting the original word, the second reporting a lowercased version, and the third reporting a summary of the word's"", 'shape ( "" mrs. ""', 'becomes "" aa. "" ) all wordlevel features also have a variant that appends the word', ""' s begin / inside / last / unique position within its entity. our phrase - level features report phrase identity, with lowercased and"", 'word shape variants, along with a bias feature that is always on. phrase identity features allow us to memorize tags for common phrases explicitly. following the standard discriminative tagging paradigm,', 'all features have the tag identity appended to them. representation features : we also produce word - level features corresponding to a number of external representations : gazetteer membership, brown', 'clusters  #AUTHOR_TAG and word embeddings. for gazetteers, we first segment the tweet into longest matching', 'gazetteer phrases, resolving overlapping phrases with a greedy left - toright walk through the tweet. each word then generates a', 'set of features indicating which gazetteers ( if any ) contain its phrase. for cluster representations, we train brown clusters on our unannotated corpus, using the implementation by  #AUTHOR_TAG to build 1,', '000 clusters over types that occur with a minimum frequency of 10.  #AUTHOR_TAG, each word generates indicators for bit prefixes of its binary cluster signature, for prefixes of length 2, 4 8 and 12. for word embeddings, we use an in - house java re - implementation of word2vec  #AUTHOR_TAG a ) to build 300 - dimensional vector representations for all types that occur at', 'least 10 times in our unannotated corpus. each word then reports a real - valued feature ( as opposed to an indicator ) for', 'each of the 300 dimensions in its vector representation. a single random vector', 'is created to represent all out - ofvocabulary words. our vectors and clusters', 'cover 2. 5 million types. note that we do not include part - of - speech tags as features, as they were not found to be useful', 'by  #TAUTHOR_TAG']",3
"['- implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['systems above [ a ] + [ u ] are intended to demonstrate our development process.', 'our baseline is our attempt to re - implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have a huge impact on ner performance for all dev and test sets.', 'we then performed a careful hyper - parameter sweep using the two provided development sets, resulting in the inc. regularization system.', 'the hyper - parameters suggested by  #TAUTHOR_TAG ( e = 10, c = 0. 01, p = 10 ) were selected to work well with and without representations.', 'we found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge ( e = 30, c = 0. 001, p = 8 ).', 'although we revisited these settings periodically, these hyperparameters have proved to be quite stable, and we use them for all remaining experiments.', 'the next three systems test the three extensions described in section 3.', 'neither [ p ] hrase vectors nor [ u ] pdated gazetteers were able to improve both dev and dev 2015 when applied alone, while the [ a ] dapted vectors did boost performance on both sets, increasing average f - measure by 0. 6.', 'in particular, the adapted vectors improved the rare entity types such as movie and sports team.', 'unfortunately, these improvements do not seem to carry over to the test set.', 'as we combine the ideas with', ', we see even larger improvements on both development sets.', 'note that adapted vectors implicitly include phrase vectors, as those are the vectors that have been adapted.', 'these ideas may work better in combination because both our phrase vectors and our updated gazetteers include many noisy phrasal entries, but their sources of noise are independent, allowing one to compensate for the other']",3
"['- implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['systems above [ a ] + [ u ] are intended to demonstrate our development process.', 'our baseline is our attempt to re - implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have a huge impact on ner performance for all dev and test sets.', 'we then performed a careful hyper - parameter sweep using the two provided development sets, resulting in the inc. regularization system.', 'the hyper - parameters suggested by  #TAUTHOR_TAG ( e = 10, c = 0. 01, p = 10 ) were selected to work well with and without representations.', 'we found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge ( e = 30, c = 0. 001, p = 8 ).', 'although we revisited these settings periodically, these hyperparameters have proved to be quite stable, and we use them for all remaining experiments.', 'the next three systems test the three extensions described in section 3.', 'neither [ p ] hrase vectors nor [ u ] pdated gazetteers were able to improve both dev and dev 2015 when applied alone, while the [ a ] dapted vectors did boost performance on both sets, increasing average f - measure by 0. 6.', 'in particular, the adapted vectors improved the rare entity types such as movie and sports team.', 'unfortunately, these improvements do not seem to carry over to the test set.', 'as we combine the ideas with', ', we see even larger improvements on both development sets.', 'note that adapted vectors implicitly include phrase vectors, as those are the vectors that have been adapted.', 'these ideas may work better in combination because both our phrase vectors and our updated gazetteers include many noisy phrasal entries, but their sources of noise are independent, allowing one to compensate for the other']",3
"['detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tag']","['detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tagger, augmented with multiple word representations.', 'we enhance this']","['submission to the w - nut named entity recognition in twitter task closely follows the approach detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tagger, augmented with multiple word representations.', 'we enhance this approach with updated gazetteers, and with infused phrase embeddings']","['submission to the w - nut named entity recognition in twitter task closely follows the approach detailed by  #TAUTHOR_TAG, who use a discriminative, semi - markov tagger, augmented with multiple word representations.', 'we enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase.', 'our system achieves a typed f1 of 44. 7, resulting in a third - place finish, despite training only on the official training set.', 'a post - competition analysis indicates that also training on the provided development data improves our performance to 54. 2 f1']",5
"['follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tag']","['follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tagger']","['follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tagger trained online with standard discriminative tagging features, gaze']","['entity recognition ( ner ) is the task of finding rigid designators as they appear in free text and assigning them to coarse types such as person or geo - location  #AUTHOR_TAG.', 'ner is the first step in many information extraction tasks, but in social media, this task is extremely challenging.', 'the text to be analyzed is unedited and noisy, and covers a much more diverse set of topics than one might expect in newswire.', 'as such, we are quite interested in the w - nut named entity recognition in twitter task  #AUTHOR_TAG as a platform to benchmark and drive forward work on ner in social media.', 'our submission to this competition closely follows  #TAUTHOR_TAG, who advocate the use of a semi - markov tagger trained online with standard discriminative tagging features, gazetteer matches, brown clusters, and word embeddings.', 'we augment this approach with updated gazetteers, phrase embeddings, and infused embeddings that have been adapted to better predict gazetteer membership.', ""our novel infusion technique allows us to adapt existing vectors to ner regardless of their source, by training a typelevel auto - encoder whose hidden layer must predict the corresponding phrase's gazetteer memberships while also recovering the original vector."", 'our submitted system achieved a typed f1 of 44. 7, placing third in the competition, while training only on the provided training data.', 'the competition organizers provided two development sets, one ( dev ) that is close to the training data, with both train and dev being drawn from the year 2010, and another ( dev 2015 ) that is close to the test data, with both dev 2015 and test being drawn from the winter of 2014 - 2015.', 'we present a postcompetition system that achieves an f1 of 54. 2 using the same features and hyper - parameters as our submitted system, except that our tagger is also trained on all provided development data.', ""we close with an analysis of dev 2015's relation to the test set, and argue that these results may overestimate the impact that a small, in - domain training set can have on ner performance""]",5
"['corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized']","['corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized']","['and april 2012.', 'the same corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized']","['', 'our unannotated corpus collects 98m tweets ( 1, 995m tokens ) from between may 2011 and april 2012.', 'the same corpus is used by  #TAUTHOR_TAG.', 'these tweets have been tokenized and post - processed to remove many special unicode characters ; they closely resemble those that appear in the provided training and development sets.', 'furthermore, the corpus consists only of tweets in which the ner system of  #AUTHOR_TAG detects at least one entity.', 'the automatic ner tags are used only to select tweets for inclusion in the corpus, after which the annotations are discarded.', 'filtering our tweets in this way has two immediate effects : first, each tweet is very likely to contain an entity mention.', 'second, the tweets are very long, with an average of 20. 4 tokens per tweet.', ""as the test data is drawn from the winter of 2014 - 2015, we attempted to augment our corpus with more recent data : 13m unannotated english tweets drawn from twitter's public stream, from between april 24 and may 6, 2015."", 'as we had very little recent data, we made no attempt to bias the corpus to be entity - rich.', 'this corpus of recent tweets has an average tweet length of only 13. 8 tokens.', 'our attempts to use this data to build word representations did not improve ner performance on the 2015 development set, regardless of whether we used the data on its own or in combination with our larger corpus']",5
"['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['use only in post - competition analysis. lexical features : recall that our semi', '- markov model allows for both word and phrase - level features. the vast majority of our features are wordlevel, with the representation for a phrase being the sum of the features of its words.', 'our wordlevel features closely follow the set proposed by  #AUTHOR_TAG, covering word identity, the identities', 'of surrounding words within a window of 2 tokens, and prefixes and suffixes up to three characters in length. each word identity feature', ""has three variants, with the first reporting the original word, the second reporting a lowercased version, and the third reporting a summary of the word's"", 'shape ( "" mrs. ""', 'becomes "" aa. "" ) all wordlevel features also have a variant that appends the word', ""' s begin / inside / last / unique position within its entity. our phrase - level features report phrase identity, with lowercased and"", 'word shape variants, along with a bias feature that is always on. phrase identity features allow us to memorize tags for common phrases explicitly. following the standard discriminative tagging paradigm,', 'all features have the tag identity appended to them. representation features : we also produce word - level features corresponding to a number of external representations : gazetteer membership, brown', 'clusters  #AUTHOR_TAG and word embeddings. for gazetteers, we first segment the tweet into longest matching', 'gazetteer phrases, resolving overlapping phrases with a greedy left - toright walk through the tweet. each word then generates a', 'set of features indicating which gazetteers ( if any ) contain its phrase. for cluster representations, we train brown clusters on our unannotated corpus, using the implementation by  #AUTHOR_TAG to build 1,', '000 clusters over types that occur with a minimum frequency of 10.  #AUTHOR_TAG, each word generates indicators for bit prefixes of its binary cluster signature, for prefixes of length 2, 4 8 and 12. for word embeddings, we use an in - house java re - implementation of word2vec  #AUTHOR_TAG a ) to build 300 - dimensional vector representations for all types that occur at', 'least 10 times in our unannotated corpus. each word then reports a real - valued feature ( as opposed to an indicator ) for', 'each of the 300 dimensions in its vector representation. a single random vector', 'is created to represent all out - ofvocabulary words. our vectors and clusters', 'cover 2. 5 million types. note that we do not include part - of - speech tags as features, as they were not found to be useful', 'by  #TAUTHOR_TAG']",5
"['- implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['systems above [ a ] + [ u ] are intended to demonstrate our development process.', 'our baseline is our attempt to re - implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have a huge impact on ner performance for all dev and test sets.', 'we then performed a careful hyper - parameter sweep using the two provided development sets, resulting in the inc. regularization system.', 'the hyper - parameters suggested by  #TAUTHOR_TAG ( e = 10, c = 0. 01, p = 10 ) were selected to work well with and without representations.', 'we found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge ( e = 30, c = 0. 001, p = 8 ).', 'although we revisited these settings periodically, these hyperparameters have proved to be quite stable, and we use them for all remaining experiments.', 'the next three systems test the three extensions described in section 3.', 'neither [ p ] hrase vectors nor [ u ] pdated gazetteers were able to improve both dev and dev 2015 when applied alone, while the [ a ] dapted vectors did boost performance on both sets, increasing average f - measure by 0. 6.', 'in particular, the adapted vectors improved the rare entity types such as movie and sports team.', 'unfortunately, these improvements do not seem to carry over to the test set.', 'as we combine the ideas with', ', we see even larger improvements on both development sets.', 'note that adapted vectors implicitly include phrase vectors, as those are the vectors that have been adapted.', 'these ideas may work better in combination because both our phrase vectors and our updated gazetteers include many noisy phrasal entries, but their sources of noise are independent, allowing one to compensate for the other']",5
"['- implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have']","['systems above [ a ] + [ u ] are intended to demonstrate our development process.', 'our baseline is our attempt to re - implement the provided baseline in our code base, and includes all lexical features and the baseline gazetteers.', 'c & g 2015 adds brown clusters and word embeddings to create a complete re - implementation of  #TAUTHOR_TAG.', 'we can see that these representations have a huge impact on ner performance for all dev and test sets.', 'we then performed a careful hyper - parameter sweep using the two provided development sets, resulting in the inc. regularization system.', 'the hyper - parameters suggested by  #TAUTHOR_TAG ( e = 10, c = 0. 01, p = 10 ) were selected to work well with and without representations.', 'we found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge ( e = 30, c = 0. 001, p = 8 ).', 'although we revisited these settings periodically, these hyperparameters have proved to be quite stable, and we use them for all remaining experiments.', 'the next three systems test the three extensions described in section 3.', 'neither [ p ] hrase vectors nor [ u ] pdated gazetteers were able to improve both dev and dev 2015 when applied alone, while the [ a ] dapted vectors did boost performance on both sets, increasing average f - measure by 0. 6.', 'in particular, the adapted vectors improved the rare entity types such as movie and sports team.', 'unfortunately, these improvements do not seem to carry over to the test set.', 'as we combine the ideas with', ', we see even larger improvements on both development sets.', 'note that adapted vectors implicitly include phrase vectors, as those are the vectors that have been adapted.', 'these ideas may work better in combination because both our phrase vectors and our updated gazetteers include many noisy phrasal entries, but their sources of noise are independent, allowing one to compensate for the other']",5
"['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['use only in post - competition analysis. lexical features : recall that our semi', '- markov model allows for both word and phrase - level features. the vast majority of our features are wordlevel, with the representation for a phrase being the sum of the features of its words.', 'our wordlevel features closely follow the set proposed by  #AUTHOR_TAG, covering word identity, the identities', 'of surrounding words within a window of 2 tokens, and prefixes and suffixes up to three characters in length. each word identity feature', ""has three variants, with the first reporting the original word, the second reporting a lowercased version, and the third reporting a summary of the word's"", 'shape ( "" mrs. ""', 'becomes "" aa. "" ) all wordlevel features also have a variant that appends the word', ""' s begin / inside / last / unique position within its entity. our phrase - level features report phrase identity, with lowercased and"", 'word shape variants, along with a bias feature that is always on. phrase identity features allow us to memorize tags for common phrases explicitly. following the standard discriminative tagging paradigm,', 'all features have the tag identity appended to them. representation features : we also produce word - level features corresponding to a number of external representations : gazetteer membership, brown', 'clusters  #AUTHOR_TAG and word embeddings. for gazetteers, we first segment the tweet into longest matching', 'gazetteer phrases, resolving overlapping phrases with a greedy left - toright walk through the tweet. each word then generates a', 'set of features indicating which gazetteers ( if any ) contain its phrase. for cluster representations, we train brown clusters on our unannotated corpus, using the implementation by  #AUTHOR_TAG to build 1,', '000 clusters over types that occur with a minimum frequency of 10.  #AUTHOR_TAG, each word generates indicators for bit prefixes of its binary cluster signature, for prefixes of length 2, 4 8 and 12. for word embeddings, we use an in - house java re - implementation of word2vec  #AUTHOR_TAG a ) to build 300 - dimensional vector representations for all types that occur at', 'least 10 times in our unannotated corpus. each word then reports a real - valued feature ( as opposed to an indicator ) for', 'each of the 300 dimensions in its vector representation. a single random vector', 'is created to represent all out - ofvocabulary words. our vectors and clusters', 'cover 2. 5 million types. note that we do not include part - of - speech tags as features, as they were not found to be useful', 'by  #TAUTHOR_TAG']",4
"['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['have summarized our entry to the first w - nut named entity recognition in twitter task.', 'our entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings, and gazetteer - infused phrase embeddings.', 'our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source.', 'taken together with improved hyper - parameters, these extensions improve the approach of  #TAUTHOR_TAG by 2. 6 fmeasure on a completely blind test.', 'our final submission achieves a test f - measure of 44. 7, placing third in the competition, and could have achieved an f - measure of 54. 2 had we included all development data as training data.', 'we have also presented a discussion of how the most recent development set relates to the test set, arguing that these results likely over - estimate the impact of a small, in - domain training set on ner performance']",4
"['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['have summarized our entry to the first w - nut named entity recognition in twitter task.', 'our entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings, and gazetteer - infused phrase embeddings.', 'our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source.', 'taken together with improved hyper - parameters, these extensions improve the approach of  #TAUTHOR_TAG by 2. 6 fmeasure on a completely blind test.', 'our final submission achieves a test f - measure of 44. 7, placing third in the competition, and could have achieved an f - measure of 54. 2 had we included all development data as training data.', 'we have also presented a discussion of how the most recent development set relates to the test set, arguing that these results likely over - estimate the impact of a small, in - domain training set on ner performance']",4
"['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['to be useful', 'by  #TAUTHOR_TAG']","['use only in post - competition analysis. lexical features : recall that our semi', '- markov model allows for both word and phrase - level features. the vast majority of our features are wordlevel, with the representation for a phrase being the sum of the features of its words.', 'our wordlevel features closely follow the set proposed by  #AUTHOR_TAG, covering word identity, the identities', 'of surrounding words within a window of 2 tokens, and prefixes and suffixes up to three characters in length. each word identity feature', ""has three variants, with the first reporting the original word, the second reporting a lowercased version, and the third reporting a summary of the word's"", 'shape ( "" mrs. ""', 'becomes "" aa. "" ) all wordlevel features also have a variant that appends the word', ""' s begin / inside / last / unique position within its entity. our phrase - level features report phrase identity, with lowercased and"", 'word shape variants, along with a bias feature that is always on. phrase identity features allow us to memorize tags for common phrases explicitly. following the standard discriminative tagging paradigm,', 'all features have the tag identity appended to them. representation features : we also produce word - level features corresponding to a number of external representations : gazetteer membership, brown', 'clusters  #AUTHOR_TAG and word embeddings. for gazetteers, we first segment the tweet into longest matching', 'gazetteer phrases, resolving overlapping phrases with a greedy left - toright walk through the tweet. each word then generates a', 'set of features indicating which gazetteers ( if any ) contain its phrase. for cluster representations, we train brown clusters on our unannotated corpus, using the implementation by  #AUTHOR_TAG to build 1,', '000 clusters over types that occur with a minimum frequency of 10.  #AUTHOR_TAG, each word generates indicators for bit prefixes of its binary cluster signature, for prefixes of length 2, 4 8 and 12. for word embeddings, we use an in - house java re - implementation of word2vec  #AUTHOR_TAG a ) to build 300 - dimensional vector representations for all types that occur at', 'least 10 times in our unannotated corpus. each word then reports a real - valued feature ( as opposed to an indicator ) for', 'each of the 300 dimensions in its vector representation. a single random vector', 'is created to represent all out - ofvocabulary words. our vectors and clusters', 'cover 2. 5 million types. note that we do not include part - of - speech tags as features, as they were not found to be useful', 'by  #TAUTHOR_TAG']",6
"['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['have summarized our entry to the first w - nut named entity recognition in twitter task.', 'our entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings, and gazetteer - infused phrase embeddings.', 'our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source.', 'taken together with improved hyper - parameters, these extensions improve the approach of  #TAUTHOR_TAG by 2. 6 fmeasure on a completely blind test.', 'our final submission achieves a test f - measure of 44. 7, placing third in the competition, and could have achieved an f - measure of 54. 2 had we included all development data as training data.', 'we have also presented a discussion of how the most recent development set relates to the test set, arguing that these results likely over - estimate the impact of a small, in - domain training set on ner performance']",6
"['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings,']","['have summarized our entry to the first w - nut named entity recognition in twitter task.', 'our entry extends the work of  #TAUTHOR_TAG with updated lexicons, phrase embeddings, and gazetteer - infused phrase embeddings.', 'our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source.', 'taken together with improved hyper - parameters, these extensions improve the approach of  #TAUTHOR_TAG by 2. 6 fmeasure on a completely blind test.', 'our final submission achieves a test f - measure of 44. 7, placing third in the competition, and could have achieved an f - measure of 54. 2 had we included all development data as training data.', 'we have also presented a discussion of how the most recent development set relates to the test set, arguing that these results likely over - estimate the impact of a small, in - domain training set on ner performance']",6
"['specific lexicons  #TAUTHOR_TAG.', 'research has also shown that merging these two forms']","['lexicons  #TAUTHOR_TAG.', 'research has also shown that merging these two forms']","['specific lexicons  #TAUTHOR_TAG.', 'research has also shown that merging these two forms']",[' #TAUTHOR_TAG'],0
['in  #TAUTHOR_TAG to process the'],['in  #TAUTHOR_TAG to process the'],"['compatible with the previous work, we follow the procedure in  #TAUTHOR_TAG to process']","['formalize ed as a multi - class classification problem.', 'given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger of some event in the pre - defined event set or not? the current token along with its context in the sentence constitute an event trigger candidate.', 'in order to make it compatible with the previous work, we follow the procedure in  #TAUTHOR_TAG to process the trigger candidates for cnn.', 'in particular, we limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary.', 'let 2n + 1 be the fixed window size, and w = [ w 0, w 1,..., w n,..., w 2n 1, w 2n ] be some trigger candidate where the current token is positioned in the middle of the window ( token w n ).', 'before entering cnn, each token w i is first transformed into a real - valued vector x i using the concatenation of the following vectors :', '']",5
['in  #TAUTHOR_TAG to process the'],['in  #TAUTHOR_TAG to process the'],"['compatible with the previous work, we follow the procedure in  #TAUTHOR_TAG to process']","['formalize ed as a multi - class classification problem.', 'given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger of some event in the pre - defined event set or not? the current token along with its context in the sentence constitute an event trigger candidate.', 'in order to make it compatible with the previous work, we follow the procedure in  #TAUTHOR_TAG to process the trigger candidates for cnn.', 'in particular, we limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary.', 'let 2n + 1 be the fixed window size, and w = [ w 0, w 1,..., w n,..., w 2n 1, w 2n ] be some trigger candidate where the current token is positioned in the middle of the window ( token w n ).', 'before entering cnn, each token w i is first transformed into a real - valued vector x i using the concatenation of the following vectors :', '']",5
"['parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically,']","['parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically,']","['and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically, we employ the window sizes in the set']","['apply the same parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically, we employ the window sizes in the set { 2, 3, 4, 5 } for the convolution operation with 150 filters for each window size.', 'the window size of the trigger candidate is 31 while the dimensionality of the position embeddings and entity type embeddings is 50.', 'we use word2vec from  #AUTHOR_TAG b ) as the pretrained word embeddings.', 'the other parameters include the dropout rate [UNK] = 0. 5, the mini - batch size = 50, the predefined threshold for the l 2 norms = 3.', 'following the previous studies  #TAUTHOR_TAG, we evaluate the models on the ace 2005 corpus with 33 event subtypes.', 'in order to make it compatible, we use the same test set with 40 newswire articles, the same development set with 30 other documents and the same training set with the remaining 529 documents.', 'all the data preprocessing and evaluation criteria follow those in  #TAUTHOR_TAG']",5
"['parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically,']","['parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically,']","['and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically, we employ the window sizes in the set']","['apply the same parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically, we employ the window sizes in the set { 2, 3, 4, 5 } for the convolution operation with 150 filters for each window size.', 'the window size of the trigger candidate is 31 while the dimensionality of the position embeddings and entity type embeddings is 50.', 'we use word2vec from  #AUTHOR_TAG b ) as the pretrained word embeddings.', 'the other parameters include the dropout rate [UNK] = 0. 5, the mini - batch size = 50, the predefined threshold for the l 2 norms = 3.', 'following the previous studies  #TAUTHOR_TAG, we evaluate the models on the ace 2005 corpus with 33 event subtypes.', 'in order to make it compatible, we use the same test set with 40 newswire articles, the same development set with 30 other documents and the same training set with the remaining 529 documents.', 'all the data preprocessing and evaluation criteria follow those in  #TAUTHOR_TAG']",5
"['parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically,']","['parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically,']","['and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically, we employ the window sizes in the set']","['apply the same parameters and resources as  #TAUTHOR_TAG to ensure the compatible comparison.', 'specifically, we employ the window sizes in the set { 2, 3, 4, 5 } for the convolution operation with 150 filters for each window size.', 'the window size of the trigger candidate is 31 while the dimensionality of the position embeddings and entity type embeddings is 50.', 'we use word2vec from  #AUTHOR_TAG b ) as the pretrained word embeddings.', 'the other parameters include the dropout rate [UNK] = 0. 5, the mini - batch size = 50, the predefined threshold for the l 2 norms = 3.', 'following the previous studies  #TAUTHOR_TAG, we evaluate the models on the ace 2005 corpus with 33 event subtypes.', 'in order to make it compatible, we use the same test set with 40 newswire articles, the same development set with 30 other documents and the same training set with the remaining 529 documents.', 'all the data preprocessing and evaluation criteria follow those in  #TAUTHOR_TAG']",5
"['e, the cnn model in  #TAUTHOR_TAG ( cnn ),']","['', '2 ) the neural network models, i. e, the cnn model in  #TAUTHOR_TAG ( cnn ),']","['e, the cnn model in  #TAUTHOR_TAG ( cnn ), the dynamic multi - pooling cnn model (']",[' #TAUTHOR_TAG'],5
"['by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model']","['by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model']","[') studies aim to overcome this issue by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model']","['', 'the domain adaptation ( da ) studies aim to overcome this issue by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model outperformed the feature - based models in the cross - domain setting.', 'in this section, we compare nc - cnn with the cnn model in  #TAUTHOR_TAG ( as well as the other models above ) in the da setting to further investigate their effectiveness']",5
"[') and webblogs ( wl ).', 'following  #TAUTHOR_TAG, we']","['also uses the ace 2005 dataset but focuses more on the difference between domains.', 'the ace 2005 corpus includes 6 different domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and webblogs ( wl ).', 'following  #TAUTHOR_TAG, we']","[') and webblogs ( wl ).', 'following  #TAUTHOR_TAG, we use news']","['section also uses the ace 2005 dataset but focuses more on the difference between domains.', 'the ace 2005 corpus includes 6 different domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and webblogs ( wl ).', 'following  #TAUTHOR_TAG, we use news ( the union of bn and nw ) as the source domain and bc, cts, wl and un as four different target domains 3.', 'we take half of bc as the development set and use the remaining data for testing.', 'our data split is the same as that in  #AUTHOR_TAG b table 2 reports the performance of the systems with 5 - fold cross validation.', 'note that we focus on the systems exploiting only the sentence level information in this section.', 'for each system, we train a model on the training data of the source domain and evaluate this model on the test set of the source domain ( in - domain performance ) as well as on the four target domains bc, cts, wl and un']",5
"['+ local, joint + local + global, b - rnn, and cnn is obtained from the actual systems in the original work  #TAUTHOR_TAG a ).', 'the']","['systems maxent, joint + local, joint + local + global, b - rnn, and cnn is obtained from the actual systems in the original work  #TAUTHOR_TAG a ).', 'the']","['emphasize that the performance of the systems maxent, joint + local, joint + local + global, b - rnn, and cnn is obtained from the actual systems in the original work  #TAUTHOR_TAG a ).', 'the performance of dm - cnn, on the other hand, is from our re - implementation of the system in  #AUTHOR_TAG using the same hyper - parameters and resources']","['emphasize that the performance of the systems maxent, joint + local, joint + local + global, b - rnn, and cnn is obtained from the actual systems in the original work  #TAUTHOR_TAG a ).', 'the performance of dm - cnn, on the other hand, is from our re - implementation of the system in  #AUTHOR_TAG using the same hyper - parameters and resources as cnn and nc - cnn for a fair comparison.', 'from the table, we see that nc - cnn is significantly better than the other models on the source domain.', 'this is consistent with the conclusions in section 3. 2 and further confirms the effectiveness of nc - cnn.', 'more importantly, nc - cnn outperforms cnn and the other models on the target domains bc, cts and un, and performs comparably with cnn on wl.', 'the performance improvement is significant on bc and un ( p < 0. 05 ), thereby verifying the robustness of nc - cnn for ed across domains']",5
"['e, the cnn model in  #TAUTHOR_TAG ( cnn ),']","['', '2 ) the neural network models, i. e, the cnn model in  #TAUTHOR_TAG ( cnn ),']","['e, the cnn model in  #TAUTHOR_TAG ( cnn ), the dynamic multi - pooling cnn model (']",[' #TAUTHOR_TAG'],4
"['by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model']","['by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model']","[') studies aim to overcome this issue by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model']","['', 'the domain adaptation ( da ) studies aim to overcome this issue by developing robust techniques across domains.', 'the best reported system in the da setting for ed is  #TAUTHOR_TAG, which demonstrated that the cnn model outperformed the feature - based models in the cross - domain setting.', 'in this section, we compare nc - cnn with the cnn model in  #TAUTHOR_TAG ( as well as the other models above ) in the da setting to further investigate their effectiveness']",0
"['broadcasting [ 4,  #TAUTHOR_TAG']","['broadcasting [ 4,  #TAUTHOR_TAG']","['sport broadcasting [ 4,  #TAUTHOR_TAG.', 'as an example, figure 1 shows a data - structure containing statistics']",[' #TAUTHOR_TAG'],0
"['broadcasting [ 4,  #TAUTHOR_TAG']","['broadcasting [ 4,  #TAUTHOR_TAG']","['sport broadcasting [ 4,  #TAUTHOR_TAG.', 'as an example, figure 1 shows a data - structure containing statistics']",[' #TAUTHOR_TAG'],0
"[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","['. this task stems from the neural machine translation ( nmt ) domain, and early work [ 1, 15', ',  #TAUTHOR_TAG represent the data records as a single sequence of facts to be entirely translated into natural language. wiseman et al.  #TAUTHOR_TAG show the limits of traditional nmt systems on larger structured - data, where nmt systems fail to accurately extract salient elements.', 'to improve these models, a number of work [ 16, 28, 40 ] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. for example, puduppully et al. [ 28 ] propose a two - step decoder which', 'first targets specific records and then use them as a plan for the actual text generation. similarly, li et al. [ 16 ] proposed', 'a delayed copy mechanism where their decoder also acts in two steps : 1 ) using a classical lstm decoder to generate delexicalized text and 2 ) using a pointer network [ 38 ] to replace place', '##holders by records from the input data. closer to our work, very recent work [ 18, 17, 29 ] have proposed to take into account the data', 'structure. more particularly, puduppully et al. [ 29 ] follow entity - centric theories [ 10, 20 ] and propose a model based on dynamic entity representation at decoding time. it', 'consists in conditioning the decoder on entity representations that are updated during inference at each decoding step. on the other hand, liu et al. [ 18, 17 ] rather focus on introducing structure into', '']",0
"[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","['. this task stems from the neural machine translation ( nmt ) domain, and early work [ 1, 15', ',  #TAUTHOR_TAG represent the data records as a single sequence of facts to be entirely translated into natural language. wiseman et al.  #TAUTHOR_TAG show the limits of traditional nmt systems on larger structured - data, where nmt systems fail to accurately extract salient elements.', 'to improve these models, a number of work [ 16, 28, 40 ] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. for example, puduppully et al. [ 28 ] propose a two - step decoder which', 'first targets specific records and then use them as a plan for the actual text generation. similarly, li et al. [ 16 ] proposed', 'a delayed copy mechanism where their decoder also acts in two steps : 1 ) using a classical lstm decoder to generate delexicalized text and 2 ) using a pointer network [ 38 ] to replace place', '##holders by records from the input data. closer to our work, very recent work [ 18, 17, 29 ] have proposed to take into account the data', 'structure. more particularly, puduppully et al. [ 29 ] follow entity - centric theories [ 10, 20 ] and propose a model based on dynamic entity representation at decoding time. it', 'consists in conditioning the decoder on entity representations that are updated during inference at each decoding step. on the other hand, liu et al. [ 18, 17 ] rather focus on introducing structure into', '']",0
"['previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure']","['previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of']","['outlined in section 2, most previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of each element from']","['outlined in section 2, most previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of each element from the data - structure, we propose a hierarchical encoder which relies on two modules.', '']",0
"['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'ble']","['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and']","['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and']","['evaluate our model through two types of metrics.', 'the bleu score [ 23 ] aims at measuring to what extent the generated descriptions are literally closed to the ground truth.', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and that of a human by computing the number of co - occurrences for ngrams ( n ∈ 1, 2, 3, 4 ) between the generated candidate and the ground truth.', 'we use the implementation code released by [ 27 ].', 'information extraction - oriented metrics.', 'these metrics estimate the ability of our model to integrate elements from the table in its descriptions.', 'particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ.', 'to do so, we follow the protocol presented in  #TAUTHOR_TAG.', 'first, we apply an information extraction ( ie ) system trained on labeled relations from the gold descriptions of the rotowire train dataset.', 'entity - value pairs are extracted from the descriptions.', 'for example, in the sentence isaiah thomas led the team in scoring, totaling 23 points [... ]., an ie tool will extract the pair ( isaiah thomas, 23, pts ).', '']",0
"['of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder']","['of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder - decoder system with copy mechanism.', '• li [ 16 ] is a standard encoder - decoder with a delayed copy mechanism : text is first']","['of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder - decoder system with copy mechanism.', '• li [ 16 ] is a standard encoder - decoder with a delayed copy mechanism : text is first']","['compare our hierarchical model against three systems.', 'for each of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder - decoder system with copy mechanism.', '• li [ 16 ] is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network.', '• puduppully - plan [ 28 ] acts in two steps : a first standard encoder - decoder generates a plan, i. e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan.', '• puduppully - updt [ 29 ].', 'it consists in a standard encoder - decoder, with an added module aimed at updating record representations during the generation process.', 'at each decoding step, a gated recurrent network computes which records should be updated and what should be their new representation']",0
,,,,0
"['28,  #TAUTHOR_TAG']","['be a classical module as used in [ 28,  #TAUTHOR_TAG']","['28,  #TAUTHOR_TAG.', 'our']","['data - to - text systems use rnns as encoders ( such as grus or lstms ), these architectures have however some limitations.', 'indeed, they require in practice their input to be fed sequentially.', 'this way of encoding unordered sequences ( i. e. collections of entities ) implicitly assumes an arbitrary order within the collection which, as demonstrated by vinyals et al. [ 37 ], significantly impacts the learning performance.', 'to address these shortcomings, we propose a new structured - data encoder assuming that structures should be hierarchically captured.', 'our contribution focuses on the encoding of the data - structure, thus the decoder is chosen to be a classical module as used in [ 28,  #TAUTHOR_TAG.', 'our contribution is threefold :', '- we model the general structure of the data using a two - level architecture, first encoding all entities on the basis of their elements, then encoding the data structure on the basis of its entities ; - we introduce the transformer encoder [ 36 ] in data - to - text models to ensure robust encoding of each element / entities in comparison to all others, no matter their initial positioning ; - we integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder.', 'we report experiments on the rotowire benchmark  #TAUTHOR_TAG which contains around 5k statistical tables of nba basketball games paired with humanwritten descriptions.', 'our model is compared to several state - of - the - art models.', 'results show that the proposed architecture outperforms previous models on bleu score and is generally better on qualitative metrics.', 'in the following, we first present a state - of - the art of data - to - text literature ( section 2 ), and then describe our proposed hierarchical data encoder ( section 3 ).', 'the evaluation protocol is presented in section 4, followed by the results ( section 5 ).', 'section 6 concludes the paper and presents perspectives']",3
"['previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure']","['previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of']","['outlined in section 2, most previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of each element from']","['outlined in section 2, most previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of each element from the data - structure, we propose a hierarchical encoder which relies on two modules.', '']",3
"['28,  #TAUTHOR_TAG']","['be a classical module as used in [ 28,  #TAUTHOR_TAG']","['28,  #TAUTHOR_TAG.', 'our']","['data - to - text systems use rnns as encoders ( such as grus or lstms ), these architectures have however some limitations.', 'indeed, they require in practice their input to be fed sequentially.', 'this way of encoding unordered sequences ( i. e. collections of entities ) implicitly assumes an arbitrary order within the collection which, as demonstrated by vinyals et al. [ 37 ], significantly impacts the learning performance.', 'to address these shortcomings, we propose a new structured - data encoder assuming that structures should be hierarchically captured.', 'our contribution focuses on the encoding of the data - structure, thus the decoder is chosen to be a classical module as used in [ 28,  #TAUTHOR_TAG.', 'our contribution is threefold :', '- we model the general structure of the data using a two - level architecture, first encoding all entities on the basis of their elements, then encoding the data structure on the basis of its entities ; - we introduce the transformer encoder [ 36 ] in data - to - text models to ensure robust encoding of each element / entities in comparison to all others, no matter their initial positioning ; - we integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder.', 'we report experiments on the rotowire benchmark  #TAUTHOR_TAG which contains around 5k statistical tables of nba basketball games paired with humanwritten descriptions.', 'our model is compared to several state - of - the - art models.', 'results show that the proposed architecture outperforms previous models on bleu score and is generally better on qualitative metrics.', 'in the following, we first present a state - of - the art of data - to - text literature ( section 2 ), and then describe our proposed hierarchical data encoder ( section 3 ).', 'the evaluation protocol is presented in section 4, followed by the results ( section 5 ).', 'section 6 concludes the paper and presents perspectives']",5
"['for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder']","['for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder - decoder architecture [ 2 ].', 'because our contribution focuses on the encoding process, we chose the decoding module used in [ 28,  #TAUTHOR_TAG : a two - layers lstm network with a copy mechanism.', 'in order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data - structure']","['θ ) the probability of the model to generate the adequate description y for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder - decoder architecture [ 2 ].', 'because our contribution focuses on the encoding process, we chose the decoding module used in [ 28,  #TAUTHOR_TAG : a two - layers lstm network with a copy mechanism.', 'in order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data - structure']","['', 'thus, the full sequence of words can be noted as y = y 1 : t.', '• the dataset d is a collection of n aligned ( data - structure, description ) pairs ( s, y ).', 'for instance, figure 1 illustrates a data - structure associated with a description.', 'the data - structure includes a set of entities ( hawks, magic, al horford, jeff teague,... ).', 'the entity jeff teague is modeled as a set of records { ( pts, 17 ), ( reb, 0 ), ( ast, 7 )... } in which, e. g., the record ( pts, 17 ) is characterized by a key ( pts ) and a value ( 17 ).', 'for each data - structure s in d, the objective function aims to generate a descriptiony as close as possible to the ground truth y. this objective function optimizes the following log - likelihood over the whole dataset d :', 'where θ stands for the model parameters and p ( y = y | s ; θ ) the probability of the model to generate the adequate description y for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder - decoder architecture [ 2 ].', 'because our contribution focuses on the encoding process, we chose the decoding module used in [ 28,  #TAUTHOR_TAG : a two - layers lstm network with a copy mechanism.', 'in order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data - structure']",5
"['for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder']","['for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder - decoder architecture [ 2 ].', 'because our contribution focuses on the encoding process, we chose the decoding module used in [ 28,  #TAUTHOR_TAG : a two - layers lstm network with a copy mechanism.', 'in order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data - structure']","['θ ) the probability of the model to generate the adequate description y for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder - decoder architecture [ 2 ].', 'because our contribution focuses on the encoding process, we chose the decoding module used in [ 28,  #TAUTHOR_TAG : a two - layers lstm network with a copy mechanism.', 'in order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data - structure']","['', 'thus, the full sequence of words can be noted as y = y 1 : t.', '• the dataset d is a collection of n aligned ( data - structure, description ) pairs ( s, y ).', 'for instance, figure 1 illustrates a data - structure associated with a description.', 'the data - structure includes a set of entities ( hawks, magic, al horford, jeff teague,... ).', 'the entity jeff teague is modeled as a set of records { ( pts, 17 ), ( reb, 0 ), ( ast, 7 )... } in which, e. g., the record ( pts, 17 ) is characterized by a key ( pts ) and a value ( 17 ).', 'for each data - structure s in d, the objective function aims to generate a descriptiony as close as possible to the ground truth y. this objective function optimizes the following log - likelihood over the whole dataset d :', 'where θ stands for the model parameters and p ( y = y | s ; θ ) the probability of the model to generate the adequate description y for table s. during inference, we generate the sequencey * with the maximum a posteriori probability conditioned on table s. using the chain rule, we get :', 'this equation is intractable in practice, we approximate a solution using beam search, as in [ 18, 17, 28, 29,  #TAUTHOR_TAG.', 'our model follows the encoder - decoder architecture [ 2 ].', 'because our contribution focuses on the encoding process, we chose the decoding module used in [ 28,  #TAUTHOR_TAG : a two - layers lstm network with a copy mechanism.', 'in order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data - structure']",5
"['previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure']","['previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of']","['outlined in section 2, most previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of each element from']","['outlined in section 2, most previous work [ 16, 28, 29,  #TAUTHOR_TAG 40 ] make use of flat encoders that do not exploit the data structure.', 'to keep the semantics of each element from the data - structure, we propose a hierarchical encoder which relies on two modules.', '']",5
"['of several types of entities, we used the ro - towire dataset  #TAUTHOR_TAG.', 'it includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the']","['of several types of entities, we used the ro - towire dataset  #TAUTHOR_TAG.', 'it includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the']","['of several types of entities, we used the ro - towire dataset  #TAUTHOR_TAG.', 'it includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of figure 1.', 'the descriptions are professionally written and average 337 words with a vocabulary size']","['evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data - structure made of several types of entities, we used the ro - towire dataset  #TAUTHOR_TAG.', 'it includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of figure 1.', 'the descriptions are professionally written and average 337 words with a vocabulary size of 11. 3k.', 'there are 39 different record keys, and the average number of records ( resp. entities ) in a single data - structure is 628 ( resp. 28 ).', 'entities are of two types, either team or player, and player descriptions depend on their involvement in the game.', 'we followed the data partitions introduced with the dataset and used a train / validation / test sets of respectively 3, 398 / 727 / 728 ( data - structure, description ) pairs']",5
"['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'ble']","['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and']","['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and']","['evaluate our model through two types of metrics.', 'the bleu score [ 23 ] aims at measuring to what extent the generated descriptions are literally closed to the ground truth.', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and that of a human by computing the number of co - occurrences for ngrams ( n ∈ 1, 2, 3, 4 ) between the generated candidate and the ground truth.', 'we use the implementation code released by [ 27 ].', 'information extraction - oriented metrics.', 'these metrics estimate the ability of our model to integrate elements from the table in its descriptions.', 'particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ.', 'to do so, we follow the protocol presented in  #TAUTHOR_TAG.', 'first, we apply an information extraction ( ie ) system trained on labeled relations from the gold descriptions of the rotowire train dataset.', 'entity - value pairs are extracted from the descriptions.', 'for example, in the sentence isaiah thomas led the team in scoring, totaling 23 points [... ]., an ie tool will extract the pair ( isaiah thomas, 23, pts ).', '']",5
"['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'ble']","['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and']","['', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and']","['evaluate our model through two types of metrics.', 'the bleu score [ 23 ] aims at measuring to what extent the generated descriptions are literally closed to the ground truth.', 'the second category designed by  #TAUTHOR_TAG is more qualitative.', 'bleu score.', 'the bleu score [ 23 ] is commonly used as an evaluation metric in text generation tasks.', 'it estimates the correspondence between a machine output and that of a human by computing the number of co - occurrences for ngrams ( n ∈ 1, 2, 3, 4 ) between the generated candidate and the ground truth.', 'we use the implementation code released by [ 27 ].', 'information extraction - oriented metrics.', 'these metrics estimate the ability of our model to integrate elements from the table in its descriptions.', 'particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ.', 'to do so, we follow the protocol presented in  #TAUTHOR_TAG.', 'first, we apply an information extraction ( ie ) system trained on labeled relations from the gold descriptions of the rotowire train dataset.', 'entity - value pairs are extracted from the descriptions.', 'for example, in the sentence isaiah thomas led the team in scoring, totaling 23 points [... ]., an ie tool will extract the pair ( isaiah thomas, 23, pts ).', '']",5
"['of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder']","['of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder - decoder system with copy mechanism.', '• li [ 16 ] is a standard encoder - decoder with a delayed copy mechanism : text is first']","['of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder - decoder system with copy mechanism.', '• li [ 16 ] is a standard encoder - decoder with a delayed copy mechanism : text is first']","['compare our hierarchical model against three systems.', 'for each of them, we report the results of the best performing models presented in each paper.', '• wiseman  #TAUTHOR_TAG is a standard encoder - decoder system with copy mechanism.', '• li [ 16 ] is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network.', '• puduppully - plan [ 28 ] acts in two steps : a first standard encoder - decoder generates a plan, i. e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan.', '• puduppully - updt [ 29 ].', 'it consists in a standard encoder - decoder, with an added module aimed at updating record representations during the generation process.', 'at each decoding step, a gated recurrent network computes which records should be updated and what should be their new representation']",5
"['used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for']","['used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for']","['decoder is the one used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for the encoder module, both']","['decoder is the one used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for the encoder module, both the low - level and high - level encoders use a two - layers multi - head self - attention with two heads.', 'to fit with the small number of record keys in our dataset  #TAUTHOR_TAG, their embedding size is fixed to 20.', 'the size of the record value embeddings and hidden layers of the transformer encoders are both set to 300.', 'we use dropout at rate 0. 5.', 'the models are trained with a batch size of 64.', 'we follow the training procedure in [ 36 ] and train the model for a fixed number of 25k updates, and average the weights of the last 5 checkpoints ( at every 1k updates ) to ensure more stability across runs.', 'all models were trained with the adam optimizer [ 13 ] ; the initial learning rate is 0. 001, and is reduced by half every 10k steps.', 'we used beam search with beam size of 5 during inference.', 'all the models are implemented in opennmt - py [ 14 ].', 'all code is available at https : / / github. com / kaijuml / data - to - text -']",5
"['used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for']","['used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for']","['decoder is the one used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for the encoder module, both']","['decoder is the one used in [ 28, 29,  #TAUTHOR_TAG with the same hyper - parameters.', 'for the encoder module, both the low - level and high - level encoders use a two - layers multi - head self - attention with two heads.', 'to fit with the small number of record keys in our dataset  #TAUTHOR_TAG, their embedding size is fixed to 20.', 'the size of the record value embeddings and hidden layers of the transformer encoders are both set to 300.', 'we use dropout at rate 0. 5.', 'the models are trained with a batch size of 64.', 'we follow the training procedure in [ 36 ] and train the model for a fixed number of 25k updates, and average the weights of the last 5 checkpoints ( at every 1k updates ) to ensure more stability across runs.', 'all models were trained with the adam optimizer [ 13 ] ; the initial learning rate is 0. 001, and is reduced by half every 10k steps.', 'we used beam search with beam size of 5 during inference.', 'all the models are implemented in opennmt - py [ 14 ].', 'all code is available at https : / / github. com / kaijuml / data - to - text -']",5
,,,,5
"[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","[', and early work [ 1, 15', ',  #TAUTHOR_TAG represent the']","['. this task stems from the neural machine translation ( nmt ) domain, and early work [ 1, 15', ',  #TAUTHOR_TAG represent the data records as a single sequence of facts to be entirely translated into natural language. wiseman et al.  #TAUTHOR_TAG show the limits of traditional nmt systems on larger structured - data, where nmt systems fail to accurately extract salient elements.', 'to improve these models, a number of work [ 16, 28, 40 ] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. for example, puduppully et al. [ 28 ] propose a two - step decoder which', 'first targets specific records and then use them as a plan for the actual text generation. similarly, li et al. [ 16 ] proposed', 'a delayed copy mechanism where their decoder also acts in two steps : 1 ) using a classical lstm decoder to generate delexicalized text and 2 ) using a pointer network [ 38 ] to replace place', '##holders by records from the input data. closer to our work, very recent work [ 18, 17, 29 ] have proposed to take into account the data', 'structure. more particularly, puduppully et al. [ 29 ] follow entity - centric theories [ 10, 20 ] and propose a model based on dynamic entity representation at decoding time. it', 'consists in conditioning the decoder on entity representations that are updated during inference at each decoding step. on the other hand, liu et al. [ 18, 17 ] rather focus on introducing structure into', '']",4
"['of  #TAUTHOR_TAG to train features with 150 million parameters,']","['of  #TAUTHOR_TAG to train features with 150 million parameters,']","['of discriminative training to log linear - based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters.', 'in this paper, we propose to scale up discriminative training of  #TAUTHOR_TAG to train features with 150 million parameters,']","['recently, the application of discriminative training to log linear - based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters.', 'in this paper, we propose to scale up discriminative training of  #TAUTHOR_TAG to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning.', 'the experimental results confirm the effectiveness of our proposals on nist mt06 set over a strong baseline']",6
['training of  #TAUTHOR_TAG is a recent'],"['translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent']","['', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their']","['- of - the - art statistical machine translation systems based on a log - linear framework are parameterized by { λ, φ }, where the feature weights λ are discriminatively trained  #AUTHOR_TAG b ;  #AUTHOR_TAG by directly optimizing them against a translation - oriented metric such as bleu.', 'the feature parameters φ can be roughly divided into two categories : dense feature that measures the plausibility of each translation rule from a particular aspect, e. g., the rule translation probabilities p ( f | e ) and p ( e | f ) ; and sparse feature that fires when certain phenomena is observed, e. g., when a frequent word pair co - occured in a rule.', 'in contrast to λ, feature parameters in φ are usually modeled by generative models for dense features, or by indicator functions for sparse ones.', 'it is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their work to a scaled - up task of discriminative training of the features of a strong hierarchical phrase - based model and confirm its effectiveness empirically.', 'in this work, we further consider the application of discriminative training to pruned model.', 'various pruning techniques  #AUTHOR_TAG have been proposed recently to filter translation rules.', 'one common consequence of pruning is that the probability distribution of many surviving rules become deficient, i. e. f p ( f | e ) < 1.', 'in practice, others have chosen either to leave the pruned rules as it - is, or simply to re - normalize the probability mass by distributing the pruned mass to surviving rules proportionally.', 'we argue that both approaches are suboptimal, and propose a more principled method to re - distribute the probability mass, i. e. using discriminative training with some translation criterion.', '']",6
['training of  #TAUTHOR_TAG is a recent'],"['translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent']","['', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their']","['- of - the - art statistical machine translation systems based on a log - linear framework are parameterized by { λ, φ }, where the feature weights λ are discriminatively trained  #AUTHOR_TAG b ;  #AUTHOR_TAG by directly optimizing them against a translation - oriented metric such as bleu.', 'the feature parameters φ can be roughly divided into two categories : dense feature that measures the plausibility of each translation rule from a particular aspect, e. g., the rule translation probabilities p ( f | e ) and p ( e | f ) ; and sparse feature that fires when certain phenomena is observed, e. g., when a frequent word pair co - occured in a rule.', 'in contrast to λ, feature parameters in φ are usually modeled by generative models for dense features, or by indicator functions for sparse ones.', 'it is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their work to a scaled - up task of discriminative training of the features of a strong hierarchical phrase - based model and confirm its effectiveness empirically.', 'in this work, we further consider the application of discriminative training to pruned model.', 'various pruning techniques  #AUTHOR_TAG have been proposed recently to filter translation rules.', 'one common consequence of pruning is that the probability distribution of many surviving rules become deficient, i. e. f p ( f | e ) < 1.', 'in practice, others have chosen either to leave the pruned rules as it - is, or simply to re - normalize the probability mass by distributing the pruned mass to surviving rules proportionally.', 'we argue that both approaches are suboptimal, and propose a more principled method to re - distribute the probability mass, i. e. using discriminative training with some translation criterion.', '']",6
"['update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for']","['update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for']","['one update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for each τ.', 'across different τ, we find that the first iteration provides']","['ensure the correctness of our implementation, we show in fig 2, the first five ebw updates with τ = 0. 10.', 'as shown, the utility function log ( u ( θ ) ) increases monotonically but is countered by the kl term, resulting in a smaller but consistent increase of the objective function o ( θ ).', 'this monotonicallyincreasing trend of the objective function confirms the correctness of our implementation since ebw algorithm is a bound - based technique that ensures growth transformations between updates.', 'we then explore the optimal setting for τ which controls the contribution of the regularization term.', 'specifically, we perform grid search, exploring values of τ from 0. 1 to 0. 75.', 'for each τ, we run several iterations of discriminative training where each iteration involves one simultaneous update of p ( f | e ) and p ( e | f ) according to eq. 4, followed by one update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for each τ.', 'across different τ, we find that the first iteration provides most of the gain while the subsequent iterations provide additional, smaller gain with occassional performance degradation ; thus the translation performance is not always monotonically increasing over iteration.', '']",6
['training of  #TAUTHOR_TAG to train two features'],"['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features']","['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features']","['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features of a state - of - the - art hierarchical phrasebased system, namely : p ( f | e ) and p ( e | f ).', 'compared to  #TAUTHOR_TAG, we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set.', 'the number of parameters under consideration amounts to 150 million.', 'our experiments show that discriminative training these two features ( out of 50 ) gives around 0. 40 bleu point improvement, which is consistent with the conclusion of  #TAUTHOR_TAG but in a much larger - scale system.', 'furthermore, we apply the algorithm to redistribute the probability mass of p ( f | e ) and p ( e | f ) that is commonly lost due to conventional model pruning.', 'previous techniques either leave the probability mass as it is or distribute it proportionally among the surviving rules.', 'we show that our proposal of using discriminative training to redistribute the mass empirically performs better, demonstrating the effectiveness of our proposal']",6
['training of  #TAUTHOR_TAG is a recent'],"['translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent']","['', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their']","['- of - the - art statistical machine translation systems based on a log - linear framework are parameterized by { λ, φ }, where the feature weights λ are discriminatively trained  #AUTHOR_TAG b ;  #AUTHOR_TAG by directly optimizing them against a translation - oriented metric such as bleu.', 'the feature parameters φ can be roughly divided into two categories : dense feature that measures the plausibility of each translation rule from a particular aspect, e. g., the rule translation probabilities p ( f | e ) and p ( e | f ) ; and sparse feature that fires when certain phenomena is observed, e. g., when a frequent word pair co - occured in a rule.', 'in contrast to λ, feature parameters in φ are usually modeled by generative models for dense features, or by indicator functions for sparse ones.', 'it is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their work to a scaled - up task of discriminative training of the features of a strong hierarchical phrase - based model and confirm its effectiveness empirically.', 'in this work, we further consider the application of discriminative training to pruned model.', 'various pruning techniques  #AUTHOR_TAG have been proposed recently to filter translation rules.', 'one common consequence of pruning is that the probability distribution of many surviving rules become deficient, i. e. f p ( f | e ) < 1.', 'in practice, others have chosen either to leave the pruned rules as it - is, or simply to re - normalize the probability mass by distributing the pruned mass to surviving rules proportionally.', 'we argue that both approaches are suboptimal, and propose a more principled method to re - distribute the probability mass, i. e. using discriminative training with some translation criterion.', '']",0
['training of  #TAUTHOR_TAG is a recent'],"['translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent']","['', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their']","['- of - the - art statistical machine translation systems based on a log - linear framework are parameterized by { λ, φ }, where the feature weights λ are discriminatively trained  #AUTHOR_TAG b ;  #AUTHOR_TAG by directly optimizing them against a translation - oriented metric such as bleu.', 'the feature parameters φ can be roughly divided into two categories : dense feature that measures the plausibility of each translation rule from a particular aspect, e. g., the rule translation probabilities p ( f | e ) and p ( e | f ) ; and sparse feature that fires when certain phenomena is observed, e. g., when a frequent word pair co - occured in a rule.', 'in contrast to λ, feature parameters in φ are usually modeled by generative models for dense features, or by indicator functions for sparse ones.', 'it is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion.', 'the maximum expected bleu training of  #TAUTHOR_TAG is a recent effort towards this direction, and in this paper, we extend their work to a scaled - up task of discriminative training of the features of a strong hierarchical phrase - based model and confirm its effectiveness empirically.', 'in this work, we further consider the application of discriminative training to pruned model.', 'various pruning techniques  #AUTHOR_TAG have been proposed recently to filter translation rules.', 'one common consequence of pruning is that the probability distribution of many surviving rules become deficient, i. e. f p ( f | e ) < 1.', 'in practice, others have chosen either to leave the pruned rules as it - is, or simply to re - normalize the probability mass by distributing the pruned mass to surviving rules proportionally.', 'we argue that both approaches are suboptimal, and propose a more principled method to re - distribute the probability mass, i. e. using discriminative training with some translation criterion.', '']",4
"['. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['19 and 47. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['a mixed genre of news wire, broadcast news, web - blog and comes from various sources such as ldc, hk hansard and un data. in total, there are 50 dense features in our translation system. in', 'addition to the standard features which include the rule translation probabilities, we incorporate features that are found useful for developing a state - of - the - art baseline, e. g. provenancebased lexical features  #AUTHOR_TAG. we use', 'a large 6 - gram language model, which we train on a 10 billion words monolingual corpus, including the english side of our parallel corpora plus other corpora such as gig', '##aword ( ldc2011t07 ) and google news', '. to prevent possible over - fitting, we only kept the rules that have at most three terminal words ( plus up to two nonterminals ) on the source side, resulting in a grammar with 167 million rules', '. our discriminative training procedure includes updating both λ and θ, and we follow  #AUTHOR_TAG to optimize them in an alternate manner. that is, when we optimize θ via ebw, we keep λ fixed and when we optimize λ, we keep λ fixed.', 'we use pro  #AUTHOR_TAG to tune λ. for discriminative training of θ, we use a subset of 550 thousands of parallel sentences selected from the entire training data, mainly to allow for faster experimental cycle ; they mainly come', 'from news and web - blog domains. for each sentence of this subset, we generate 500 - best of unique hypotheses', 'using the baseline model. the 1 - best and the oracle bleu scores for this subset are 40. 19 and 47. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative training of p ( f | e ) and p ( e | f ), which in practice affects around 150 million of parameters ; hence the title. for the tuning and development sets, we set aside 127', '##5 and 1239 sentences respectively from ldc2010e30 corpus. the tune set is used by', 'pro for tuning λ while the dev set is used to decide the best dt model. as for the blind test set', "", we report the performance on the nist mt06 evaluation set, which consists of 1644 sentences from news and web - blog domains. our baseline system's performance on mt06 is"", '39. 91 which is among the best number ever published so far in the community. table 1 compares the key components of our', 'baseline system with that of  #TAUTHOR_TAG. as shown, we are working with a stronger system than  #TAUTHOR_TAG, especially in terms', 'of the number of parameters under consideration | θ |. he &']",4
"['. 2, which is omitted in  #TAUTHOR_TAG.', 'for conciseness, we drop the conditions and write p (']","['eq. 1 to eq. 2, which is omitted in  #TAUTHOR_TAG.', 'for conciseness, we drop the conditions and write p ( e i | f i ) as p ( e i ).', 'we write eq. 1']","['. 2, which is omitted in  #TAUTHOR_TAG.', 'for conciseness, we drop the conditions and write p ( e i']","['describe the process to simplify eq. 1 to eq. 2, which is omitted in  #TAUTHOR_TAG.', 'for conciseness, we drop the conditions and write p ( e i | f i ) as p ( e i ).', 'we write eq. 1 again below as eq. 5.', 'we first focus on the first sentence e 1 / f 1 and expand the related terms from the equation as follow :', 'expanding the inner summation, we arrive at :', '']",4
"[', following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['given the current λ :', 'where b ( e 1... e n ) is the bleu score of the concatenated hypothesis of the entire training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['the entire training data { f n, e n } n n = 1, and current parameterization { λ, φ }, we decode the source side of training data f n to produce hypothesis { e n } n n = 1.', 'our goal is to update φ towards φ that maximizes the expected bleu scores of the entire training data given the current λ :', 'where b ( e 1... e n ) is the bleu score of the concatenated hypothesis of the entire training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over all possible combinations ofe 1... e n, which is intractable.', '']",5
"[', following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['given the current λ :', 'where b ( e 1... e n ) is the bleu score of the concatenated hypothesis of the entire training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['the entire training data { f n, e n } n n = 1, and current parameterization { λ, φ }, we decode the source side of training data f n to produce hypothesis { e n } n n = 1.', 'our goal is to update φ towards φ that maximizes the expected bleu scores of the entire training data given the current λ :', 'where b ( e 1... e n ) is the bleu score of the concatenated hypothesis of the entire training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over all possible combinations ofe 1... e n, which is intractable.', '']",5
"[', following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['given the current λ :', 'where b ( e 1... e n ) is the bleu score of the concatenated hypothesis of the entire training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over']","['the entire training data { f n, e n } n n = 1, and current parameterization { λ, φ }, we decode the source side of training data f n to produce hypothesis { e n } n n = 1.', 'our goal is to update φ towards φ that maximizes the expected bleu scores of the entire training data given the current λ :', 'where b ( e 1... e n ) is the bleu score of the concatenated hypothesis of the entire training data, following  #TAUTHOR_TAG.', 'eq. 1 summarizes over all possible combinations ofe 1... e n, which is intractable.', '']",5
"['. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['19 and 47. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['a mixed genre of news wire, broadcast news, web - blog and comes from various sources such as ldc, hk hansard and un data. in total, there are 50 dense features in our translation system. in', 'addition to the standard features which include the rule translation probabilities, we incorporate features that are found useful for developing a state - of - the - art baseline, e. g. provenancebased lexical features  #AUTHOR_TAG. we use', 'a large 6 - gram language model, which we train on a 10 billion words monolingual corpus, including the english side of our parallel corpora plus other corpora such as gig', '##aword ( ldc2011t07 ) and google news', '. to prevent possible over - fitting, we only kept the rules that have at most three terminal words ( plus up to two nonterminals ) on the source side, resulting in a grammar with 167 million rules', '. our discriminative training procedure includes updating both λ and θ, and we follow  #AUTHOR_TAG to optimize them in an alternate manner. that is, when we optimize θ via ebw, we keep λ fixed and when we optimize λ, we keep λ fixed.', 'we use pro  #AUTHOR_TAG to tune λ. for discriminative training of θ, we use a subset of 550 thousands of parallel sentences selected from the entire training data, mainly to allow for faster experimental cycle ; they mainly come', 'from news and web - blog domains. for each sentence of this subset, we generate 500 - best of unique hypotheses', 'using the baseline model. the 1 - best and the oracle bleu scores for this subset are 40. 19 and 47. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative training of p ( f | e ) and p ( e | f ), which in practice affects around 150 million of parameters ; hence the title. for the tuning and development sets, we set aside 127', '##5 and 1239 sentences respectively from ldc2010e30 corpus. the tune set is used by', 'pro for tuning λ while the dev set is used to decide the best dt model. as for the blind test set', "", we report the performance on the nist mt06 evaluation set, which consists of 1644 sentences from news and web - blog domains. our baseline system's performance on mt06 is"", '39. 91 which is among the best number ever published so far in the community. table 1 compares the key components of our', 'baseline system with that of  #TAUTHOR_TAG. as shown, we are working with a stronger system than  #TAUTHOR_TAG, especially in terms', 'of the number of parameters under consideration | θ |. he &']",5
"['. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['19 and 47. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative']","['a mixed genre of news wire, broadcast news, web - blog and comes from various sources such as ldc, hk hansard and un data. in total, there are 50 dense features in our translation system. in', 'addition to the standard features which include the rule translation probabilities, we incorporate features that are found useful for developing a state - of - the - art baseline, e. g. provenancebased lexical features  #AUTHOR_TAG. we use', 'a large 6 - gram language model, which we train on a 10 billion words monolingual corpus, including the english side of our parallel corpora plus other corpora such as gig', '##aword ( ldc2011t07 ) and google news', '. to prevent possible over - fitting, we only kept the rules that have at most three terminal words ( plus up to two nonterminals ) on the source side, resulting in a grammar with 167 million rules', '. our discriminative training procedure includes updating both λ and θ, and we follow  #AUTHOR_TAG to optimize them in an alternate manner. that is, when we optimize θ via ebw, we keep λ fixed and when we optimize λ, we keep λ fixed.', 'we use pro  #AUTHOR_TAG to tune λ. for discriminative training of θ, we use a subset of 550 thousands of parallel sentences selected from the entire training data, mainly to allow for faster experimental cycle ; they mainly come', 'from news and web - blog domains. for each sentence of this subset, we generate 500 - best of unique hypotheses', 'using the baseline model. the 1 - best and the oracle bleu scores for this subset are 40. 19 and 47. 06', 'respectively. following  #TAUTHOR_TAG, we focus on', 'discriminative training of p ( f | e ) and p ( e | f ), which in practice affects around 150 million of parameters ; hence the title. for the tuning and development sets, we set aside 127', '##5 and 1239 sentences respectively from ldc2010e30 corpus. the tune set is used by', 'pro for tuning λ while the dev set is used to decide the best dt model. as for the blind test set', "", we report the performance on the nist mt06 evaluation set, which consists of 1644 sentences from news and web - blog domains. our baseline system's performance on mt06 is"", '39. 91 which is among the best number ever published so far in the community. table 1 compares the key components of our', 'baseline system with that of  #TAUTHOR_TAG. as shown, we are working with a stronger system than  #TAUTHOR_TAG, especially in terms', 'of the number of parameters under consideration | θ |. he &']",5
"['update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for']","['update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for']","['one update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for each τ.', 'across different τ, we find that the first iteration provides']","['ensure the correctness of our implementation, we show in fig 2, the first five ebw updates with τ = 0. 10.', 'as shown, the utility function log ( u ( θ ) ) increases monotonically but is countered by the kl term, resulting in a smaller but consistent increase of the objective function o ( θ ).', 'this monotonicallyincreasing trend of the objective function confirms the correctness of our implementation since ebw algorithm is a bound - based technique that ensures growth transformations between updates.', 'we then explore the optimal setting for τ which controls the contribution of the regularization term.', 'specifically, we perform grid search, exploring values of τ from 0. 1 to 0. 75.', 'for each τ, we run several iterations of discriminative training where each iteration involves one simultaneous update of p ( f | e ) and p ( e | f ) according to eq. 4, followed by one update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for each τ.', 'across different τ, we find that the first iteration provides most of the gain while the subsequent iterations provide additional, smaller gain with occassional performance degradation ; thus the translation performance is not always monotonically increasing over iteration.', '']",5
['training of  #TAUTHOR_TAG to train two features'],"['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features']","['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features']","['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features of a state - of - the - art hierarchical phrasebased system, namely : p ( f | e ) and p ( e | f ).', 'compared to  #TAUTHOR_TAG, we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set.', 'the number of parameters under consideration amounts to 150 million.', 'our experiments show that discriminative training these two features ( out of 50 ) gives around 0. 40 bleu point improvement, which is consistent with the conclusion of  #TAUTHOR_TAG but in a much larger - scale system.', 'furthermore, we apply the algorithm to redistribute the probability mass of p ( f | e ) and p ( e | f ) that is commonly lost due to conventional model pruning.', 'previous techniques either leave the probability mass as it is or distribute it proportionally among the surviving rules.', 'we show that our proposal of using discriminative training to redistribute the mass empirically performs better, demonstrating the effectiveness of our proposal']",5
"['update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for']","['update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for']","['one update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for each τ.', 'across different τ, we find that the first iteration provides']","['ensure the correctness of our implementation, we show in fig 2, the first five ebw updates with τ = 0. 10.', 'as shown, the utility function log ( u ( θ ) ) increases monotonically but is countered by the kl term, resulting in a smaller but consistent increase of the objective function o ( θ ).', 'this monotonicallyincreasing trend of the objective function confirms the correctness of our implementation since ebw algorithm is a bound - based technique that ensures growth transformations between updates.', 'we then explore the optimal setting for τ which controls the contribution of the regularization term.', 'specifically, we perform grid search, exploring values of τ from 0. 1 to 0. 75.', 'for each τ, we run several iterations of discriminative training where each iteration involves one simultaneous update of p ( f | e ) and p ( e | f ) according to eq. 4, followed by one update of λ via pro ( as in  #TAUTHOR_TAG.', 'in total, we run 10 such iterations for each τ.', 'across different τ, we find that the first iteration provides most of the gain while the subsequent iterations provide additional, smaller gain with occassional performance degradation ; thus the translation performance is not always monotonically increasing over iteration.', '']",3
['training of  #TAUTHOR_TAG to train two features'],"['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features']","['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features']","['this paper, we first extend the maximum expected bleu training of  #TAUTHOR_TAG to train two features of a state - of - the - art hierarchical phrasebased system, namely : p ( f | e ) and p ( e | f ).', 'compared to  #TAUTHOR_TAG, we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set.', 'the number of parameters under consideration amounts to 150 million.', 'our experiments show that discriminative training these two features ( out of 50 ) gives around 0. 40 bleu point improvement, which is consistent with the conclusion of  #TAUTHOR_TAG but in a much larger - scale system.', 'furthermore, we apply the algorithm to redistribute the probability mass of p ( f | e ) and p ( e | f ) that is commonly lost due to conventional model pruning.', 'previous techniques either leave the probability mass as it is or distribute it proportionally among the surviving rules.', 'we show that our proposal of using discriminative training to redistribute the mass empirically performs better, demonstrating the effectiveness of our proposal']",3
[' #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence'],"[' #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence.', ' #AUTHOR_TAG used labeled']","['borninyear, corporationacquired, others  #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence.', ' #AUTHOR_TAG used labeled']","['be able to answer the questions "" what causes ebola? "", "" what are the duties of a medical doctor? "", "" what are the differences between a terrorist and a victim? "", "" which are the animals that have wings but cannot fly? "" one requires knowledge about verb - based relations.', 'over the years, researchers have developed various relation learning algorithms.', 'some  #AUTHOR_TAG targeted specific relations like borninyear, corporationacquired, others  #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence.', ' #AUTHOR_TAG used labeled data to learn relations,  #AUTHOR_TAG used information encoded in the structured wikipedia documents,  #AUTHOR_TAG bootstrapped patterns.', 'as a result various knowledge bases have been produced like topicsignatures  #AUTHOR_TAG, conceptnet  #AUTHOR_TAG, yago  #AUTHOR_TAG, nell  #AUTHOR_TAG and reverb  #TAUTHOR_TAG.', 'despite the many efforts to date, yet there is no universal repository ( or even a system ), which for a given term it can immediately return all verb relations related to the term.', 'however, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term.', 'if available, such resource can aid different natural language processing tasks such as preposition sense disambiguation  #AUTHOR_TAG, selectional preferences  #AUTHOR_TAG, question answering  #AUTHOR_TAG and textual entailment  #AUTHOR_TAG']",0
[' #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence'],"[' #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence.', ' #AUTHOR_TAG used labeled']","['borninyear, corporationacquired, others  #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence.', ' #AUTHOR_TAG used labeled']","['be able to answer the questions "" what causes ebola? "", "" what are the duties of a medical doctor? "", "" what are the differences between a terrorist and a victim? "", "" which are the animals that have wings but cannot fly? "" one requires knowledge about verb - based relations.', 'over the years, researchers have developed various relation learning algorithms.', 'some  #AUTHOR_TAG targeted specific relations like borninyear, corporationacquired, others  #TAUTHOR_TAG extracted any phrase denoting a relation in an english sentence.', ' #AUTHOR_TAG used labeled data to learn relations,  #AUTHOR_TAG used information encoded in the structured wikipedia documents,  #AUTHOR_TAG bootstrapped patterns.', 'as a result various knowledge bases have been produced like topicsignatures  #AUTHOR_TAG, conceptnet  #AUTHOR_TAG, yago  #AUTHOR_TAG, nell  #AUTHOR_TAG and reverb  #TAUTHOR_TAG.', 'despite the many efforts to date, yet there is no universal repository ( or even a system ), which for a given term it can immediately return all verb relations related to the term.', 'however, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term.', 'if available, such resource can aid different natural language processing tasks such as preposition sense disambiguation  #AUTHOR_TAG, selectional preferences  #AUTHOR_TAG, question answering  #AUTHOR_TAG and textual entailment  #AUTHOR_TAG']",0
['the relations are not specified in advance  #TAUTHOR_TAG'],['the relations are not specified in advance  #TAUTHOR_TAG'],"['the relations are not specified in advance  #TAUTHOR_TAG.', 'however, recently developed openie systems']","['of attention has been payed on learning is - a and part - of relations  #AUTHOR_TAG.', 'others  #AUTHOR_TAG have focused on learning specific relations like borninyear, employedby and corporationacquired.', 'however to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required  #AUTHOR_TAG and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance  #TAUTHOR_TAG.', 'however, recently developed openie systems like textrunner  #AUTHOR_TAG and reverb  #TAUTHOR_TAG surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in english sentences.', ' #AUTHOR_TAG define relation to be any verb - prep, adj - noun construction.', 'while such systems are great at learning general relations, they are not guided but simply gather in an undifferentiated way whatever happens to be contained in their input.', 'in order to be able to extract all verb relations associated with a given term, such systems need to part - of - speech tag and parse a large document collection, then they have to extract all verb constructions and all arguments matching specific sets of patterns which were written by humans ( or experts ).', 'finally, they must filter out the information and retrieve only those verb relations that are associated with the specific term.', 'once compiled the repository is straightforward to query and use, however if a term is not present in the compiled repository, repeating the whole process on a new document collection becomes time consuming and unpractical.', 'the main objective and contribution of our research is the development of a dynamic and flexible knowledge harvesting procedure, which for any given term can learn on the fly verb based relations associated with the term in a very fast and accurate manner']",0
['the relations are not specified in advance  #TAUTHOR_TAG'],['the relations are not specified in advance  #TAUTHOR_TAG'],"['the relations are not specified in advance  #TAUTHOR_TAG.', 'however, recently developed openie systems']","['of attention has been payed on learning is - a and part - of relations  #AUTHOR_TAG.', 'others  #AUTHOR_TAG have focused on learning specific relations like borninyear, employedby and corporationacquired.', 'however to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required  #AUTHOR_TAG and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance  #TAUTHOR_TAG.', 'however, recently developed openie systems like textrunner  #AUTHOR_TAG and reverb  #TAUTHOR_TAG surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in english sentences.', ' #AUTHOR_TAG define relation to be any verb - prep, adj - noun construction.', 'while such systems are great at learning general relations, they are not guided but simply gather in an undifferentiated way whatever happens to be contained in their input.', 'in order to be able to extract all verb relations associated with a given term, such systems need to part - of - speech tag and parse a large document collection, then they have to extract all verb constructions and all arguments matching specific sets of patterns which were written by humans ( or experts ).', 'finally, they must filter out the information and retrieve only those verb relations that are associated with the specific term.', 'once compiled the repository is straightforward to query and use, however if a term is not present in the compiled repository, repeating the whole process on a new document collection becomes time consuming and unpractical.', 'the main objective and contribution of our research is the development of a dynamic and flexible knowledge harvesting procedure, which for any given term can learn on the fly verb based relations associated with the term in a very fast and accurate manner']",0
"['4  #TAUTHOR_TAG, which similarly to our']","['4  #TAUTHOR_TAG, which similarly to our']","['##b 4  #TAUTHOR_TAG, which similarly to our']","['our comparative study with existing systems, we used reverb 4  #TAUTHOR_TAG, which similarly to our approach was specifically designed to learn verb - based relations from unstructured texts.', 'currently, reverb has extracted relations from clueweb09 5 and wikipedia, which have been freely distributed to the public.', 'reverb learns relations by taking as input any document and applies pos - tagging, np - chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.', 'according to  #TAUTHOR_TAG reverb outperforms textrunner  #AUTHOR_TAG and the open wikipedia extractor woe  #AUTHOR_TAG in terms of the quantity and quality of the learned relations.', 'for comparison, we took five terms from our experiment : ant, bomb, president, terrorists, virus and collected all verbs found by reverb in the clueweb09 and wikipedia triples']",0
['reverb  #TAUTHOR_TAG and show that'],['reverb  #TAUTHOR_TAG and show that'],[' #TAUTHOR_TAG and show that'],[' #TAUTHOR_TAG'],4
"['4  #TAUTHOR_TAG, which similarly to our']","['4  #TAUTHOR_TAG, which similarly to our']","['##b 4  #TAUTHOR_TAG, which similarly to our']","['our comparative study with existing systems, we used reverb 4  #TAUTHOR_TAG, which similarly to our approach was specifically designed to learn verb - based relations from unstructured texts.', 'currently, reverb has extracted relations from clueweb09 5 and wikipedia, which have been freely distributed to the public.', 'reverb learns relations by taking as input any document and applies pos - tagging, np - chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.', 'according to  #TAUTHOR_TAG reverb outperforms textrunner  #AUTHOR_TAG and the open wikipedia extractor woe  #AUTHOR_TAG in terms of the quantity and quality of the learned relations.', 'for comparison, we took five terms from our experiment : ant, bomb, president, terrorists, virus and collected all verbs found by reverb in the clueweb09 and wikipedia triples']",3
['our approach using human based evaluation and have compared results against the reverb  #TAUTHOR_TAG system and existing'],['our approach using human based evaluation and have compared results against the reverb  #TAUTHOR_TAG system and existing'],['our approach using human based evaluation and have compared results against the reverb  #TAUTHOR_TAG system and'],"['key contribution is the development of a semi - supervised procedure, which starts with a term and a verb to learn from web documents a large and diverse set of verb relations.', 'we have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers.', 'we have evaluated the accuracy of our approach using human based evaluation and have compared results against the reverb  #TAUTHOR_TAG system and existing knowledge bases like nell  #AUTHOR_TAG, yago  #AUTHOR_TAG and conceptnet  #AUTHOR_TAG.', 'our study showed that despite their completeness these resources lack verb - based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure.', 'in the future, we would like to test the usefulness of the generated resources in nlp applications']",3
"['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning']","['advances in deep learning have shown exceptional results in language - related tasks such as machine translation, question answering, or sentiment analysis.', 'however, the supervised approaches that capture the underlying statistical patterns in language are not sufficient in perceiving the interactive nature of communication and how humans use it for coordination.', 'it is thus crucial to learn to communicate by interaction, i. e., communication must emerge out of necessity.', 'such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols.', 'in these games, communication success is the only supervision during learning, and the meaning of the emergent messages gets grounded during the game.', 'in  #TAUTHOR_TAG, the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in ( havrylov and titov 2017 ), the message is considered to be a sequence of symbols.', '( lazaridou et al. 2018 ) demonstrates that successful communication can also emerge in environments which present raw pixel input.', 'copyright c 2020, association for the advancement of artificial intelligence ( www. aaai. org ).', 'all rights reserved.', ' #TAUTHOR_TAG further extends the scope of mode of communication by also studying the emergence of non - verbal communication.', 'while these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication.', 'historically, written language systems have shown complex patterns in evolution over time.', 'moreover, the process of writing requires sophisticated graphomotor skills which involves both linguistic and non - linguistic factors.', 'thus writing systems can be considered crucial for understanding autonomous system development.', 'we are further motivated by the work in ( ganin et al. 2018 ), where the authors demonstrate that artificial agents can produce visual representations similar to those created by humans.', 'this can only be achieved by giving them access to the same tools that we use to recreate the world around us.', 'we extend this idea to study emergence of writing systems']",0
"['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning']","['advances in deep learning have shown exceptional results in language - related tasks such as machine translation, question answering, or sentiment analysis.', 'however, the supervised approaches that capture the underlying statistical patterns in language are not sufficient in perceiving the interactive nature of communication and how humans use it for coordination.', 'it is thus crucial to learn to communicate by interaction, i. e., communication must emerge out of necessity.', 'such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols.', 'in these games, communication success is the only supervision during learning, and the meaning of the emergent messages gets grounded during the game.', 'in  #TAUTHOR_TAG, the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in ( havrylov and titov 2017 ), the message is considered to be a sequence of symbols.', '( lazaridou et al. 2018 ) demonstrates that successful communication can also emerge in environments which present raw pixel input.', 'copyright c 2020, association for the advancement of artificial intelligence ( www. aaai. org ).', 'all rights reserved.', ' #TAUTHOR_TAG further extends the scope of mode of communication by also studying the emergence of non - verbal communication.', 'while these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication.', 'historically, written language systems have shown complex patterns in evolution over time.', 'moreover, the process of writing requires sophisticated graphomotor skills which involves both linguistic and non - linguistic factors.', 'thus writing systems can be considered crucial for understanding autonomous system development.', 'we are further motivated by the work in ( ganin et al. 2018 ), where the authors demonstrate that artificial agents can produce visual representations similar to those created by humans.', 'this can only be achieved by giving them access to the same tools that we use to recreate the world around us.', 'we extend this idea to study emergence of writing systems']",0
"['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning']","['advances in deep learning have shown exceptional results in language - related tasks such as machine translation, question answering, or sentiment analysis.', 'however, the supervised approaches that capture the underlying statistical patterns in language are not sufficient in perceiving the interactive nature of communication and how humans use it for coordination.', 'it is thus crucial to learn to communicate by interaction, i. e., communication must emerge out of necessity.', 'such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols.', 'in these games, communication success is the only supervision during learning, and the meaning of the emergent messages gets grounded during the game.', 'in  #TAUTHOR_TAG, the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in ( havrylov and titov 2017 ), the message is considered to be a sequence of symbols.', '( lazaridou et al. 2018 ) demonstrates that successful communication can also emerge in environments which present raw pixel input.', 'copyright c 2020, association for the advancement of artificial intelligence ( www. aaai. org ).', 'all rights reserved.', ' #TAUTHOR_TAG further extends the scope of mode of communication by also studying the emergence of non - verbal communication.', 'while these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication.', 'historically, written language systems have shown complex patterns in evolution over time.', 'moreover, the process of writing requires sophisticated graphomotor skills which involves both linguistic and non - linguistic factors.', 'thus writing systems can be considered crucial for understanding autonomous system development.', 'we are further motivated by the work in ( ganin et al. 2018 ), where the authors demonstrate that artificial agents can produce visual representations similar to those created by humans.', 'this can only be achieved by giving them access to the same tools that we use to recreate the world around us.', 'we extend this idea to study emergence of writing systems']",0
"['is 0. because of the high dimensional search space introduced due to brushstrokes, we use proximal policy optimization ( ppo )  #TAUTHOR_TAG']","['is 0. because of the high dimensional search space introduced due to brushstrokes, we use proximal policy optimization ( ppo )  #TAUTHOR_TAG']","['intermediate timesteps, the payoff is 0. because of the high dimensional search space introduced due to brushstrokes, we use proximal policy optimization ( ppo )  #TAUTHOR_TAG']","['both the agents, we pose the learning of communication protocols as maximization of the expected return er [ r ( r ) ], where r is the reward function.', 'the payoff is 1 for both the agents iff r φ ( f s ( s θ ( r ( m i ), h i, v ) ), u ) = t, where i is the last timestep of the episode.', 'in all other cases and intermediate timesteps, the payoff is 0. because of the high dimensional search space introduced due to brushstrokes, we use proximal policy optimization ( ppo )  #TAUTHOR_TAG for optimizing the weights of sender and receiver agents']",0
"['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement']","['successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning']","['advances in deep learning have shown exceptional results in language - related tasks such as machine translation, question answering, or sentiment analysis.', 'however, the supervised approaches that capture the underlying statistical patterns in language are not sufficient in perceiving the interactive nature of communication and how humans use it for coordination.', 'it is thus crucial to learn to communicate by interaction, i. e., communication must emerge out of necessity.', 'such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language.', 'several recent works  #TAUTHOR_TAG, have shown that in multi - agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols.', 'in these games, communication success is the only supervision during learning, and the meaning of the emergent messages gets grounded during the game.', 'in  #TAUTHOR_TAG, the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in ( havrylov and titov 2017 ), the message is considered to be a sequence of symbols.', '( lazaridou et al. 2018 ) demonstrates that successful communication can also emerge in environments which present raw pixel input.', 'copyright c 2020, association for the advancement of artificial intelligence ( www. aaai. org ).', 'all rights reserved.', ' #TAUTHOR_TAG further extends the scope of mode of communication by also studying the emergence of non - verbal communication.', 'while these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication.', 'historically, written language systems have shown complex patterns in evolution over time.', 'moreover, the process of writing requires sophisticated graphomotor skills which involves both linguistic and non - linguistic factors.', 'thus writing systems can be considered crucial for understanding autonomous system development.', 'we are further motivated by the work in ( ganin et al. 2018 ), where the authors demonstrate that artificial agents can produce visual representations similar to those created by humans.', 'this can only be achieved by giving them access to the same tools that we use to recreate the world around us.', 'we extend this idea to study emergence of writing systems']",1
"['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazari']","['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazaridou et al. 2018 ).', 'there are two players, a sender and a receiver.', 'from']","['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazari']","['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazaridou et al. 2018 ).', 'there are two players, a sender and a receiver.', 'from a given set of images', 'where the sender only has access to the target image t ; distractor aware ( d - aware ) : where the sender has access to the candidate set c = t ∪ d. in both these variations, the sender has to come up with a message m l = { m j } l j = 1, which is a sequence of l brushstrokes.', '']",4
"['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazari']","['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazaridou et al. 2018 ).', 'there are two players, a sender and a receiver.', 'from']","['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazari']","['our work, we have used two referential game setups that are slight modifications to the ones used in  #TAUTHOR_TAG ; lazaridou et al. 2018 ).', 'there are two players, a sender and a receiver.', 'from a given set of images', 'where the sender only has access to the target image t ; distractor aware ( d - aware ) : where the sender has access to the candidate set c = t ∪ d. in both these variations, the sender has to come up with a message m l = { m j } l j = 1, which is a sequence of l brushstrokes.', '']",6
[' #TAUTHOR_TAG ]'],[' #TAUTHOR_TAG ]'],['proposed system outperforms the baseline keyword spotting model in [  #TAUTHOR_TAG ]'],"['propose smoothed max pooling loss and its application to keyword spotting systems.', 'the proposed approach jointly trains an encoder ( to detect keyword parts ) and a decoder ( to detect whole keyword ) in a semi - supervised manner.', 'the proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame - level labeling from lvcsr ( large vocabulary continuous speech recognition ), making further optimization possible.', 'the proposed system outperforms the baseline keyword spotting model in [  #TAUTHOR_TAG ] due to increased optimizability.', 'further, it can be more easily adapted for on - device learning applications due to reduced dependency on lvcsr']",4
"[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['proposed model uses the same encoder / decoder structure as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.', 'in [ 1 ], both encoder and decoder models are trained with cross entropy ( ce ) loss using frame level labels.', 'in the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.', 'the proposed smoothed max pooling loss doesnt strictly depend on phoneme - level alignment, allowing better optimization than the  #TAUTHOR_TAG']",4
"[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['proposed model uses the same encoder / decoder structure as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.', 'in [ 1 ], both encoder and decoder models are trained with cross entropy ( ce ) loss using frame level labels.', 'in the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.', 'the proposed smoothed max pooling loss doesnt strictly depend on phoneme - level alignment, allowing better optimization than the  #TAUTHOR_TAG']",4
"[' #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and']","[' #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and']","['decoder architecture with the baseline in [  #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and']","['compare the model trained with the new smoothed max pooling loss on encoder / decoder architecture with the baseline in [  #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and the proposed model have the same architecture.', 'only the training losses are different.', 'details of the setup are discussed below']",4
[' #TAUTHOR_TAG ] as the baseline and use the same structure'],[' #TAUTHOR_TAG ] as the baseline and use the same structure'],['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure'],"['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure for testing all other models.', 'as shown in fig. 1, the model has 7 svdf layers and 3 linear bottleneck dense layers.', 'for detailed architectural parameters, please refer to [  #TAUTHOR_TAG ].', '']",4
"['models  #TAUTHOR_TAG, max1 - max4 )']","['models  #TAUTHOR_TAG, max1 - max4 )']","['various models  #TAUTHOR_TAG, max1 - max4 )']","['show effectiveness of the proposed approach, we evaluated falsereject ( fr ) and false - accept ( fa ) tradeoff across various models described in section 3.', 'all models are converted to inference models using tensorflow lites quantization [ 19 ].', 'table 2 summarizes fr rates of models in fig. 3 and 4 at selected fa rate ( 0. 1 fa per hour measured on 64k re - recorded tv noise set ).', 'fig. 3 shows the roc curves of various models  #TAUTHOR_TAG, max1 - max4 ) across different conditions.', 'figure 4 shows the roc curves of max4 - max7 models across different conditions.', 'across model types and evaluation conditions max4 smp smp shows the best accuracy and roc curve.', 'max3 ce mp model also performs better than the  #TAUTHOR_TAG but not as good as max4.', 'other variations max2 ( has only decoder loss ) and max1 ( has ctc encoder loss ) performed worse than  #TAUTHOR_TAG', 'comparison among models with max pooling and different smoothing options ( fig. 4 ) shows that max4 smp smp ( smoothed max poling on both encoder and decoder ) performs the best and outperforms max7 ( no smoothing on encoder and decoder max pooling loss ).', ""especially the proposed max4 model reduces fr rate to nearly half of the  #TAUTHOR_TAG in clean accented and noisy inside - vehicle conditions, where it's more difficult to obtain training data with accurate alignments""]",4
"['models  #TAUTHOR_TAG, max1 - max4 )']","['models  #TAUTHOR_TAG, max1 - max4 )']","['various models  #TAUTHOR_TAG, max1 - max4 )']","['show effectiveness of the proposed approach, we evaluated falsereject ( fr ) and false - accept ( fa ) tradeoff across various models described in section 3.', 'all models are converted to inference models using tensorflow lites quantization [ 19 ].', 'table 2 summarizes fr rates of models in fig. 3 and 4 at selected fa rate ( 0. 1 fa per hour measured on 64k re - recorded tv noise set ).', 'fig. 3 shows the roc curves of various models  #TAUTHOR_TAG, max1 - max4 ) across different conditions.', 'figure 4 shows the roc curves of max4 - max7 models across different conditions.', 'across model types and evaluation conditions max4 smp smp shows the best accuracy and roc curve.', 'max3 ce mp model also performs better than the  #TAUTHOR_TAG but not as good as max4.', 'other variations max2 ( has only decoder loss ) and max1 ( has ctc encoder loss ) performed worse than  #TAUTHOR_TAG', 'comparison among models with max pooling and different smoothing options ( fig. 4 ) shows that max4 smp smp ( smoothed max poling on both encoder and decoder ) performs the best and outperforms max7 ( no smoothing on encoder and decoder max pooling loss ).', ""especially the proposed max4 model reduces fr rate to nearly half of the  #TAUTHOR_TAG in clean accented and noisy inside - vehicle conditions, where it's more difficult to obtain training data with accurate alignments""]",4
"['models  #TAUTHOR_TAG, max1 - max4 )']","['models  #TAUTHOR_TAG, max1 - max4 )']","['various models  #TAUTHOR_TAG, max1 - max4 )']","['show effectiveness of the proposed approach, we evaluated falsereject ( fr ) and false - accept ( fa ) tradeoff across various models described in section 3.', 'all models are converted to inference models using tensorflow lites quantization [ 19 ].', 'table 2 summarizes fr rates of models in fig. 3 and 4 at selected fa rate ( 0. 1 fa per hour measured on 64k re - recorded tv noise set ).', 'fig. 3 shows the roc curves of various models  #TAUTHOR_TAG, max1 - max4 ) across different conditions.', 'figure 4 shows the roc curves of max4 - max7 models across different conditions.', 'across model types and evaluation conditions max4 smp smp shows the best accuracy and roc curve.', 'max3 ce mp model also performs better than the  #TAUTHOR_TAG but not as good as max4.', 'other variations max2 ( has only decoder loss ) and max1 ( has ctc encoder loss ) performed worse than  #TAUTHOR_TAG', 'comparison among models with max pooling and different smoothing options ( fig. 4 ) shows that max4 smp smp ( smoothed max poling on both encoder and decoder ) performs the best and outperforms max7 ( no smoothing on encoder and decoder max pooling loss ).', ""especially the proposed max4 model reduces fr rate to nearly half of the  #TAUTHOR_TAG in clean accented and noisy inside - vehicle conditions, where it's more difficult to obtain training data with accurate alignments""]",4
['the  #TAUTHOR_TAG with ce loss by relative 22'],['the  #TAUTHOR_TAG with ce loss by relative'],['the  #TAUTHOR_TAG with ce loss by relative 22'],"['presented smoothed max pooling loss for training keyword spotting model with improved optimizability.', 'experiments show that the proposed approach outperforms the  #TAUTHOR_TAG with ce loss by relative 22 % - 54 % across a variety of conditions.', 'further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the  #TAUTHOR_TAG.', 'the proposed approach provides further benefits of reducing dependence on lvcsr to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on - device learning [ 20 ] [ 21 ]']",4
['the  #TAUTHOR_TAG with ce loss by relative 22'],['the  #TAUTHOR_TAG with ce loss by relative'],['the  #TAUTHOR_TAG with ce loss by relative 22'],"['presented smoothed max pooling loss for training keyword spotting model with improved optimizability.', 'experiments show that the proposed approach outperforms the  #TAUTHOR_TAG with ce loss by relative 22 % - 54 % across a variety of conditions.', 'further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the  #TAUTHOR_TAG.', 'the proposed approach provides further benefits of reducing dependence on lvcsr to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on - device learning [ 20 ] [ 21 ]']",4
[' #TAUTHOR_TAG 13 ] further improved'],[' #TAUTHOR_TAG 13 ] further improved'],['- end trainable dnn approaches [  #TAUTHOR_TAG 13 ] further improved'],"['detection has become an important frontend service for asr - based assistant interfaces ( e. g. hey google, alexa, hey siri ).', 'as assistant technology spreads to more ubiquitous use - cases ( mobile, iot ), reducing resource consumption ( memory and computation ) while improving accuracy has been the key success criteria of keyword spotting techniques.', 'following the successes in general asr [ 2, 3 ], the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [ 4, 5, 6, 7, 8, 9, 10, 11 ].', 'such works include dnn + temporal integration [ 4, 5, 11, 12 ], and hmm + dnn hybrid approaches [ 6, 7, 8, 9, 10 ].', 'recently introduced end - to - end trainable dnn approaches [  #TAUTHOR_TAG 13 ] further improved accuracy and lowered resource requirements using highly optimizable system design.', 'in general, training of such dnn based systems required framelevel labels generated by lvcsr systems [ 14,  #TAUTHOR_TAG ].', '']",0
[' #TAUTHOR_TAG 13 ] further improved'],[' #TAUTHOR_TAG 13 ] further improved'],['- end trainable dnn approaches [  #TAUTHOR_TAG 13 ] further improved'],"['detection has become an important frontend service for asr - based assistant interfaces ( e. g. hey google, alexa, hey siri ).', 'as assistant technology spreads to more ubiquitous use - cases ( mobile, iot ), reducing resource consumption ( memory and computation ) while improving accuracy has been the key success criteria of keyword spotting techniques.', 'following the successes in general asr [ 2, 3 ], the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [ 4, 5, 6, 7, 8, 9, 10, 11 ].', 'such works include dnn + temporal integration [ 4, 5, 11, 12 ], and hmm + dnn hybrid approaches [ 6, 7, 8, 9, 10 ].', 'recently introduced end - to - end trainable dnn approaches [  #TAUTHOR_TAG 13 ] further improved accuracy and lowered resource requirements using highly optimizable system design.', 'in general, training of such dnn based systems required framelevel labels generated by lvcsr systems [ 14,  #TAUTHOR_TAG ].', '']",0
[' #TAUTHOR_TAG 13 ] further improved'],[' #TAUTHOR_TAG 13 ] further improved'],['- end trainable dnn approaches [  #TAUTHOR_TAG 13 ] further improved'],"['detection has become an important frontend service for asr - based assistant interfaces ( e. g. hey google, alexa, hey siri ).', 'as assistant technology spreads to more ubiquitous use - cases ( mobile, iot ), reducing resource consumption ( memory and computation ) while improving accuracy has been the key success criteria of keyword spotting techniques.', 'following the successes in general asr [ 2, 3 ], the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [ 4, 5, 6, 7, 8, 9, 10, 11 ].', 'such works include dnn + temporal integration [ 4, 5, 11, 12 ], and hmm + dnn hybrid approaches [ 6, 7, 8, 9, 10 ].', 'recently introduced end - to - end trainable dnn approaches [  #TAUTHOR_TAG 13 ] further improved accuracy and lowered resource requirements using highly optimizable system design.', 'in general, training of such dnn based systems required framelevel labels generated by lvcsr systems [ 14,  #TAUTHOR_TAG ].', '']",0
['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ('],"['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ( k + 1 ) outputs y e corresponding to phoneme - like sound units ( fig. 1 ).', 'the decoder model takes the encoder output as input and generates binary output y d that predicts existence of a keyword.', 'the model is fed with acoustic input features at each frame ( generated every 10ms ), and generates prediction labels at each frame in a streaming manner.', 'in [  #TAUTHOR_TAG ], the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.', 'in [  #TAUTHOR_TAG ], the encoder model is trained to predict phonemelevel labels provided from lvcsr.', ""both encoder and decoder models use ce - loss defined in eq ( 1 ) and ( 2 ), where xt = [ xt−c l, · · ·, xt, · · ·, xt + c r ], xt is spectral feature of d - dimension, yi ( xt, w ) stands for ith dimension of network's softmax output, w is network weight, and ct is a frame - level label at frame t."", 'in [  #TAUTHOR_TAG ], target label sequence consists of intervals of repeated labels which we call runs.', 'these label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension.', 'while such model behavior can be trained end - to - end, the labels need to be provided from a lvcsr system which is typically non - end - to - end system [ 2 ].', 'the timing and accuracy of labels from lvcsr system can limit the accuracy of the trained model']",0
['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ('],"['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ( k + 1 ) outputs y e corresponding to phoneme - like sound units ( fig. 1 ).', 'the decoder model takes the encoder output as input and generates binary output y d that predicts existence of a keyword.', 'the model is fed with acoustic input features at each frame ( generated every 10ms ), and generates prediction labels at each frame in a streaming manner.', 'in [  #TAUTHOR_TAG ], the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.', 'in [  #TAUTHOR_TAG ], the encoder model is trained to predict phonemelevel labels provided from lvcsr.', ""both encoder and decoder models use ce - loss defined in eq ( 1 ) and ( 2 ), where xt = [ xt−c l, · · ·, xt, · · ·, xt + c r ], xt is spectral feature of d - dimension, yi ( xt, w ) stands for ith dimension of network's softmax output, w is network weight, and ct is a frame - level label at frame t."", 'in [  #TAUTHOR_TAG ], target label sequence consists of intervals of repeated labels which we call runs.', 'these label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension.', 'while such model behavior can be trained end - to - end, the labels need to be provided from a lvcsr system which is typically non - end - to - end system [ 2 ].', 'the timing and accuracy of labels from lvcsr system can limit the accuracy of the trained model']",0
['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ('],"['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ( k + 1 ) outputs y e corresponding to phoneme - like sound units ( fig. 1 ).', 'the decoder model takes the encoder output as input and generates binary output y d that predicts existence of a keyword.', 'the model is fed with acoustic input features at each frame ( generated every 10ms ), and generates prediction labels at each frame in a streaming manner.', 'in [  #TAUTHOR_TAG ], the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.', 'in [  #TAUTHOR_TAG ], the encoder model is trained to predict phonemelevel labels provided from lvcsr.', ""both encoder and decoder models use ce - loss defined in eq ( 1 ) and ( 2 ), where xt = [ xt−c l, · · ·, xt, · · ·, xt + c r ], xt is spectral feature of d - dimension, yi ( xt, w ) stands for ith dimension of network's softmax output, w is network weight, and ct is a frame - level label at frame t."", 'in [  #TAUTHOR_TAG ], target label sequence consists of intervals of repeated labels which we call runs.', 'these label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension.', 'while such model behavior can be trained end - to - end, the labels need to be provided from a lvcsr system which is typically non - end - to - end system [ 2 ].', 'the timing and accuracy of labels from lvcsr system can limit the accuracy of the trained model']",0
"[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['proposed model uses the same encoder / decoder structure as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.', 'in [ 1 ], both encoder and decoder models are trained with cross entropy ( ce ) loss using frame level labels.', 'in the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.', 'the proposed smoothed max pooling loss doesnt strictly depend on phoneme - level alignment, allowing better optimization than the  #TAUTHOR_TAG']",5
"[' #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","[' #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","['used the same frontend feature extract as the baseline [  #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","['used the same frontend feature extract as the baseline [  #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts and stacks a 40 - d feature vector of log - mel filter - bank energies at each frame and stacks them to generate input feature vector xt. refer to [  #TAUTHOR_TAG ] for further details']",5
[' #TAUTHOR_TAG ] as the baseline and use the same structure'],[' #TAUTHOR_TAG ] as the baseline and use the same structure'],['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure'],"['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure for testing all other models.', 'as shown in fig. 1, the model has 7 svdf layers and 3 linear bottleneck dense layers.', 'for detailed architectural parameters, please refer to [  #TAUTHOR_TAG ].', '']",5
[' #TAUTHOR_TAG ] as the baseline and use the same structure'],[' #TAUTHOR_TAG ] as the baseline and use the same structure'],['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure'],"['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure for testing all other models.', 'as shown in fig. 1, the model has 7 svdf layers and 3 linear bottleneck dense layers.', 'for detailed architectural parameters, please refer to [  #TAUTHOR_TAG ].', '']",5
"['models  #TAUTHOR_TAG, max1 - max4 )']","['models  #TAUTHOR_TAG, max1 - max4 )']","['various models  #TAUTHOR_TAG, max1 - max4 )']","['show effectiveness of the proposed approach, we evaluated falsereject ( fr ) and false - accept ( fa ) tradeoff across various models described in section 3.', 'all models are converted to inference models using tensorflow lites quantization [ 19 ].', 'table 2 summarizes fr rates of models in fig. 3 and 4 at selected fa rate ( 0. 1 fa per hour measured on 64k re - recorded tv noise set ).', 'fig. 3 shows the roc curves of various models  #TAUTHOR_TAG, max1 - max4 ) across different conditions.', 'figure 4 shows the roc curves of max4 - max7 models across different conditions.', 'across model types and evaluation conditions max4 smp smp shows the best accuracy and roc curve.', 'max3 ce mp model also performs better than the  #TAUTHOR_TAG but not as good as max4.', 'other variations max2 ( has only decoder loss ) and max1 ( has ctc encoder loss ) performed worse than  #TAUTHOR_TAG', 'comparison among models with max pooling and different smoothing options ( fig. 4 ) shows that max4 smp smp ( smoothed max poling on both encoder and decoder ) performs the best and outperforms max7 ( no smoothing on encoder and decoder max pooling loss ).', ""especially the proposed max4 model reduces fr rate to nearly half of the  #TAUTHOR_TAG in clean accented and noisy inside - vehicle conditions, where it's more difficult to obtain training data with accurate alignments""]",5
"[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","[' #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder']","['proposed model uses the same encoder / decoder structure as [  #TAUTHOR_TAG ] ( fig. 1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.', 'in [ 1 ], both encoder and decoder models are trained with cross entropy ( ce ) loss using frame level labels.', 'in the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.', 'the proposed smoothed max pooling loss doesnt strictly depend on phoneme - level alignment, allowing better optimization than the  #TAUTHOR_TAG']",3
['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature'],['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ('],"['the  #TAUTHOR_TAG and the proposed model have an encoder which takes spectral domain feature xt as input and generate ( k + 1 ) outputs y e corresponding to phoneme - like sound units ( fig. 1 ).', 'the decoder model takes the encoder output as input and generates binary output y d that predicts existence of a keyword.', 'the model is fed with acoustic input features at each frame ( generated every 10ms ), and generates prediction labels at each frame in a streaming manner.', 'in [  #TAUTHOR_TAG ], the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.', 'in [  #TAUTHOR_TAG ], the encoder model is trained to predict phonemelevel labels provided from lvcsr.', ""both encoder and decoder models use ce - loss defined in eq ( 1 ) and ( 2 ), where xt = [ xt−c l, · · ·, xt, · · ·, xt + c r ], xt is spectral feature of d - dimension, yi ( xt, w ) stands for ith dimension of network's softmax output, w is network weight, and ct is a frame - level label at frame t."", 'in [  #TAUTHOR_TAG ], target label sequence consists of intervals of repeated labels which we call runs.', 'these label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension.', 'while such model behavior can be trained end - to - end, the labels need to be provided from a lvcsr system which is typically non - end - to - end system [ 2 ].', 'the timing and accuracy of labels from lvcsr system can limit the accuracy of the trained model']",3
"[' #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and']","[' #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and']","['decoder architecture with the baseline in [  #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and']","['compare the model trained with the new smoothed max pooling loss on encoder / decoder architecture with the baseline in [  #TAUTHOR_TAG ].', 'both the  #TAUTHOR_TAG and the proposed model have the same architecture.', 'only the training losses are different.', 'details of the setup are discussed below']",3
"[' #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","[' #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","['used the same frontend feature extract as the baseline [  #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","['used the same frontend feature extract as the baseline [  #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts and stacks a 40 - d feature vector of log - mel filter - bank energies at each frame and stacks them to generate input feature vector xt. refer to [  #TAUTHOR_TAG ] for further details']",3
[' #TAUTHOR_TAG ] as the baseline and use the same structure'],[' #TAUTHOR_TAG ] as the baseline and use the same structure'],['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure'],"['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure for testing all other models.', 'as shown in fig. 1, the model has 7 svdf layers and 3 linear bottleneck dense layers.', 'for detailed architectural parameters, please refer to [  #TAUTHOR_TAG ].', '']",3
"['utterances with the keywords ok google and hey google.', 'data augmentation similar to [  #TAUTHOR_TAG ] has been used for better robustness.', 'evaluation is done on four data sets separate from training data, representing diverse environmental conditions - clean non - accented set contains 170']","['anonymized utterances with the keywords ok google and hey google.', 'data augmentation similar to [  #TAUTHOR_TAG ] has been used for better robustness.', 'evaluation is done on four data sets separate from training data, representing diverse environmental conditions - clean non - accented set contains 170k non - accented english utterances of keywords in quiet condition.', 'clean accented has 138k english utterances of keyword with australian, british, and indian accents in quiet conditions.', 'query logs contains 58k utterances from anonymized voice']","['training data consists of 2. 1 million anonymized utterances with the keywords ok google and hey google.', 'data augmentation similar to [  #TAUTHOR_TAG ] has been used for better robustness.', 'evaluation is done on four data sets separate from training data, representing diverse environmental conditions - clean non - accented set contains 170k non - accented english utterances of keywords in quiet condition.', 'clean accented has 138k english utterances of keyword with australian, british, and indian accents in quiet conditions.', 'query logs contains 58k utterances from anonym']","['training data consists of 2. 1 million anonymized utterances with the keywords ok google and hey google.', 'data augmentation similar to [  #TAUTHOR_TAG ] has been used for better robustness.', 'evaluation is done on four data sets separate from training data, representing diverse environmental conditions - clean non - accented set contains 170k non - accented english utterances of keywords in quiet condition.', 'clean accented has 138k english utterances of keyword with australian, british, and indian accents in quiet conditions.', 'query logs contains 58k utterances from anonymized voice search queries.', 'in - vehicle set has 144k utterances with the keywords recorded inside cars while driving, which includes significant amount of noises from road, engine, and fans.', 'all sets are augmented with 64k negative utterances, which are re - recorded tv noise']",3
"[' #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","[' #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","['used the same frontend feature extract as the baseline [  #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts']","['used the same frontend feature extract as the baseline [  #TAUTHOR_TAG ] in our experiments.', 'the front - end extracts and stacks a 40 - d feature vector of log - mel filter - bank energies at each frame and stacks them to generate input feature vector xt. refer to [  #TAUTHOR_TAG ] for further details']",7
[' #TAUTHOR_TAG ] as the baseline and use the same structure'],[' #TAUTHOR_TAG ] as the baseline and use the same structure'],['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure'],"['selected e2e 318k architecture in [  #TAUTHOR_TAG ] as the baseline and use the same structure for testing all other models.', 'as shown in fig. 1, the model has 7 svdf layers and 3 linear bottleneck dense layers.', 'for detailed architectural parameters, please refer to [  #TAUTHOR_TAG ].', '']",7
"['', 'the  #TAUTHOR_TAG']","['recovering non - local dependencies in syntactic dependency structures.', 'the  #TAUTHOR_TAG']","['recovering non - local dependencies in syntactic dependency structures.', 'the  #TAUTHOR_TAG']","['describe an algorithm for recovering non - local dependencies in syntactic dependency structures.', 'the  #TAUTHOR_TAG for a similar task for phrase structure trees is extended with machine learning techniques.', 'the algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment.', 'evaluating the algorithm on the penn treebank shows an improvement of both precision and recall, compared to the results presented in  #TAUTHOR_TAG']",6
"['.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine']","['structures.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine']","['in local dependency structures.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine learning techniques,']","['have presented an algorithm for recovering longdistance dependencies in local dependency structures.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine learning techniques, and use dependency structures instead of constituency trees.', '']",6
"['', 'the  #TAUTHOR_TAG']","['recovering non - local dependencies in syntactic dependency structures.', 'the  #TAUTHOR_TAG']","['recovering non - local dependencies in syntactic dependency structures.', 'the  #TAUTHOR_TAG']","['describe an algorithm for recovering non - local dependencies in syntactic dependency structures.', 'the  #TAUTHOR_TAG for a similar task for phrase structure trees is extended with machine learning techniques.', 'the algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment.', 'evaluating the algorithm on the penn treebank shows an improvement of both precision and recall, compared to the results presented in  #TAUTHOR_TAG']",4
"[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non""]","[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local""]","['', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for""]","['- local dependencies ( also called long - distance, long - range or unbounded ) appear in many frequent linguistic phenomena, such as passive, whmovement, control and raising etc.', 'although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention.', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local dependencies after parsing has been performed."", 'more specifically,  #TAUTHOR_TAG for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non - local dependencies.', 'from a training corpus with annotated empty nodes  #TAUTHOR_TAG first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus "" licensing "" corresponding non - local dependencies.', '']",4
"['4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover free']","['4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover free']","['4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover']","['parser output p r f p r f overall 0. 80 0. 70 0. 75 0. 73 0. 63 0. 68 table 4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover free empty nodes ( without antecedents ), we look for nonlocal dependencies, which corresponds to identification of co - indexed empty nodes ( note, however, the modifications we describe in section 2, when we actually transform free empty nodes into co - indexed empty nodes )']",4
['experiments reported in  #TAUTHOR_TAG'],"['. g., in the experiments reported in  #TAUTHOR_TAG']",['experiments reported in  #TAUTHOR_TAG the'],[' #TAUTHOR_TAG'],4
"['.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine']","['structures.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine']","['in local dependency structures.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine learning techniques,']","['have presented an algorithm for recovering longdistance dependencies in local dependency structures.', 'we extend the pattern matching approach of  #TAUTHOR_TAG with machine learning techniques, and use dependency structures instead of constituency trees.', '']",4
"[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non""]","[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local""]","['', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for""]","['- local dependencies ( also called long - distance, long - range or unbounded ) appear in many frequent linguistic phenomena, such as passive, whmovement, control and raising etc.', 'although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention.', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local dependencies after parsing has been performed."", 'more specifically,  #TAUTHOR_TAG for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non - local dependencies.', 'from a training corpus with annotated empty nodes  #TAUTHOR_TAG first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus "" licensing "" corresponding non - local dependencies.', '']",0
"[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non""]","[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local""]","['', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for""]","['- local dependencies ( also called long - distance, long - range or unbounded ) appear in many frequent linguistic phenomena, such as passive, whmovement, control and raising etc.', 'although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention.', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local dependencies after parsing has been performed."", 'more specifically,  #TAUTHOR_TAG for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non - local dependencies.', 'from a training corpus with annotated empty nodes  #TAUTHOR_TAG first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus "" licensing "" corresponding non - local dependencies.', '']",0
"[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non""]","[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local""]","['', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for""]","['- local dependencies ( also called long - distance, long - range or unbounded ) appear in many frequent linguistic phenomena, such as passive, whmovement, control and raising etc.', 'although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention.', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local dependencies after parsing has been performed."", 'more specifically,  #TAUTHOR_TAG for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non - local dependencies.', 'from a training corpus with annotated empty nodes  #TAUTHOR_TAG first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus "" licensing "" corresponding non - local dependencies.', '']",0
['experiments reported in  #TAUTHOR_TAG'],"['. g., in the experiments reported in  #TAUTHOR_TAG']",['experiments reported in  #TAUTHOR_TAG the'],[' #TAUTHOR_TAG'],0
"[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non""]","[""included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local""]","['', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for""]","['- local dependencies ( also called long - distance, long - range or unbounded ) appear in many frequent linguistic phenomena, such as passive, whmovement, control and raising etc.', 'although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention.', ""in  #AUTHOR_TAG long - range dependencies are included in parser's probabilistic model, while  #TAUTHOR_TAG presents a method for recovering non - local dependencies after parsing has been performed."", 'more specifically,  #TAUTHOR_TAG for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non - local dependencies.', 'from a training corpus with annotated empty nodes  #TAUTHOR_TAG first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus "" licensing "" corresponding non - local dependencies.', '']",1
"['', 'as in  #TAUTHOR_TAG, our patterns are minimal connected fragments containing both nodes involved in a']","['patterns.', 'as in  #TAUTHOR_TAG, our patterns are minimal connected fragments containing both nodes involved in a']","['', 'as in  #TAUTHOR_TAG, our patterns are minimal connected fragments containing both nodes involved in']","['', 'as in  #TAUTHOR_TAG, our patterns are minimal connected fragments containing both nodes involved in a non - local dependency.', 'however, in our case these fragments are not connected sets of local trees, but shortest paths in local dependency graphs, leading from heads to non - local dependents.', 'patterns do not include pos tags of the involved words, but only labels of the dependencies.', '']",5
"['presented in  #TAUTHOR_TAG, we measured the overall']","['presented in  #TAUTHOR_TAG, we measured the overall']","['presented in  #TAUTHOR_TAG, we measured the overall performance of']","['', 'the table also shows the number of times a pattern ( together with a specific non - local dependency label ) actually occurs in the whole penn treebank corpus ( the column dependency count ).', 'in order to compare our results to the results presented in  #TAUTHOR_TAG, we measured the overall performance of the algorithm across patterns and non - local dependency labels.', '']",7
"['presented in  #TAUTHOR_TAG, we measured the overall']","['presented in  #TAUTHOR_TAG, we measured the overall']","['presented in  #TAUTHOR_TAG, we measured the overall performance of']","['', 'the table also shows the number of times a pattern ( together with a specific non - local dependency label ) actually occurs in the whole penn treebank corpus ( the column dependency count ).', 'in order to compare our results to the results presented in  #TAUTHOR_TAG, we measured the overall performance of the algorithm across patterns and non - local dependency labels.', '']",7
"['4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover free']","['4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover free']","['4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover']","['parser output p r f p r f overall 0. 80 0. 70 0. 75 0. 73 0. 63 0. 68 table 4 : results from  #AUTHOR_TAG.', 'it is difficult to make a strict comparison of our results and those in  #TAUTHOR_TAG.', 'the two algorithms are designed for slightly different purposes : while  #TAUTHOR_TAG allows one to recover free empty nodes ( without antecedents ), we look for nonlocal dependencies, which corresponds to identification of co - indexed empty nodes ( note, however, the modifications we describe in section 2, when we actually transform free empty nodes into co - indexed empty nodes )']",7
"['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['most frequently applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to be a good choice for this kind of task : the top three results for english and the top two results for german were obtained by participants who employed them in one way or another.', 'hidden markov models were employed by four of the systems that took part in the shared task  #TAUTHOR_TAG.', 'however, they were always used in combination with other learning techniques.', ' #AUTHOR_TAG also applied the related conditional markov models for combining classifiers.', 'learning methods that were based on connectionist approaches were applied by four systems.', ' #AUTHOR_TAG used robust risk minimization, which is a winnow technique.', ' #TAUTHOR_TAG employed the same technique in a combination of learners.', 'voted perceptrons were applied to the shared task data by  #AUTHOR_TAG a ) and hammerton used a recurrent neural network ( long short - term memory ) for finding named entities.', 'other learning approaches were employed less frequently.', 'two teams used adaboost.', 'mh  #AUTHOR_TAG b ;  #AUTHOR_TAG and two other groups employed memory - based learning  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG.', 'transformation - based learning  #TAUTHOR_TAG, support vector machines  #AUTHOR_TAG and conditional random fields ( mc  #AUTHOR_TAG were applied by one system each.', 'combination of different learning systems has proven to be a good method for obtaining excellent results.', 'five participating groups have applied system combination.', ' #TAUTHOR_TAG tested different methods for combining the results of four systems and found that robust risk minimization worked best.', ' #AUTHOR_TAG employed a stacked learning system which contains hidden markov models, maximum entropy models and conditional markov models.', ' #AUTHOR_TAG stacked two learners and obtained better performance.', ' #AUTHOR_TAG applied both stacking and voting to three learners.', ' #AUTHOR_TAG employed both voting and bagging for combining classifiers']",0
"['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['most frequently applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to be a good choice for this kind of task : the top three results for english and the top two results for german were obtained by participants who employed them in one way or another.', 'hidden markov models were employed by four of the systems that took part in the shared task  #TAUTHOR_TAG.', 'however, they were always used in combination with other learning techniques.', ' #AUTHOR_TAG also applied the related conditional markov models for combining classifiers.', 'learning methods that were based on connectionist approaches were applied by four systems.', ' #AUTHOR_TAG used robust risk minimization, which is a winnow technique.', ' #TAUTHOR_TAG employed the same technique in a combination of learners.', 'voted perceptrons were applied to the shared task data by  #AUTHOR_TAG a ) and hammerton used a recurrent neural network ( long short - term memory ) for finding named entities.', 'other learning approaches were employed less frequently.', 'two teams used adaboost.', 'mh  #AUTHOR_TAG b ;  #AUTHOR_TAG and two other groups employed memory - based learning  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG.', 'transformation - based learning  #TAUTHOR_TAG, support vector machines  #AUTHOR_TAG and conditional random fields ( mc  #AUTHOR_TAG were applied by one system each.', 'combination of different learning systems has proven to be a good method for obtaining excellent results.', 'five participating groups have applied system combination.', ' #TAUTHOR_TAG tested different methods for combining the results of four systems and found that robust risk minimization worked best.', ' #AUTHOR_TAG employed a stacked learning system which contains hidden markov models, maximum entropy models and conditional markov models.', ' #AUTHOR_TAG stacked two learners and obtained better performance.', ' #AUTHOR_TAG applied both stacking and voting to three learners.', ' #AUTHOR_TAG employed both voting and bagging for combining classifiers']",0
"['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['most frequently applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to be a good choice for this kind of task : the top three results for english and the top two results for german were obtained by participants who employed them in one way or another.', 'hidden markov models were employed by four of the systems that took part in the shared task  #TAUTHOR_TAG.', 'however, they were always used in combination with other learning techniques.', ' #AUTHOR_TAG also applied the related conditional markov models for combining classifiers.', 'learning methods that were based on connectionist approaches were applied by four systems.', ' #AUTHOR_TAG used robust risk minimization, which is a winnow technique.', ' #TAUTHOR_TAG employed the same technique in a combination of learners.', 'voted perceptrons were applied to the shared task data by  #AUTHOR_TAG a ) and hammerton used a recurrent neural network ( long short - term memory ) for finding named entities.', 'other learning approaches were employed less frequently.', 'two teams used adaboost.', 'mh  #AUTHOR_TAG b ;  #AUTHOR_TAG and two other groups employed memory - based learning  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG.', 'transformation - based learning  #TAUTHOR_TAG, support vector machines  #AUTHOR_TAG and conditional random fields ( mc  #AUTHOR_TAG were applied by one system each.', 'combination of different learning systems has proven to be a good method for obtaining excellent results.', 'five participating groups have applied system combination.', ' #TAUTHOR_TAG tested different methods for combining the results of four systems and found that robust risk minimization worked best.', ' #AUTHOR_TAG employed a stacked learning system which contains hidden markov models, maximum entropy models and conditional markov models.', ' #AUTHOR_TAG stacked two learners and obtained better performance.', ' #AUTHOR_TAG applied both stacking and voting to three learners.', ' #AUTHOR_TAG employed both voting and bagging for combining classifiers']",0
"['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['most frequently applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to be a good choice for this kind of task : the top three results for english and the top two results for german were obtained by participants who employed them in one way or another.', 'hidden markov models were employed by four of the systems that took part in the shared task  #TAUTHOR_TAG.', 'however, they were always used in combination with other learning techniques.', ' #AUTHOR_TAG also applied the related conditional markov models for combining classifiers.', 'learning methods that were based on connectionist approaches were applied by four systems.', ' #AUTHOR_TAG used robust risk minimization, which is a winnow technique.', ' #TAUTHOR_TAG employed the same technique in a combination of learners.', 'voted perceptrons were applied to the shared task data by  #AUTHOR_TAG a ) and hammerton used a recurrent neural network ( long short - term memory ) for finding named entities.', 'other learning approaches were employed less frequently.', 'two teams used adaboost.', 'mh  #AUTHOR_TAG b ;  #AUTHOR_TAG and two other groups employed memory - based learning  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG.', 'transformation - based learning  #TAUTHOR_TAG, support vector machines  #AUTHOR_TAG and conditional random fields ( mc  #AUTHOR_TAG were applied by one system each.', 'combination of different learning systems has proven to be a good method for obtaining excellent results.', 'five participating groups have applied system combination.', ' #TAUTHOR_TAG tested different methods for combining the results of four systems and found that robust risk minimization worked best.', ' #AUTHOR_TAG employed a stacked learning system which contains hidden markov models, maximum entropy models and conditional markov models.', ' #AUTHOR_TAG stacked two learners and obtained better performance.', ' #AUTHOR_TAG applied both stacking and voting to three learners.', ' #AUTHOR_TAG employed both voting and bagging for combining classifiers']",0
"['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to']","['most frequently applied technique in the conll - 2003 shared task is the maximum entropy model.', 'five systems used this statistical learning method.', 'three systems used maximum entropy models in isolation  #AUTHOR_TAG.', 'two more systems used them in combination with other techniques  #TAUTHOR_TAG.', 'maximum entropy models seem to be a good choice for this kind of task : the top three results for english and the top two results for german were obtained by participants who employed them in one way or another.', 'hidden markov models were employed by four of the systems that took part in the shared task  #TAUTHOR_TAG.', 'however, they were always used in combination with other learning techniques.', ' #AUTHOR_TAG also applied the related conditional markov models for combining classifiers.', 'learning methods that were based on connectionist approaches were applied by four systems.', ' #AUTHOR_TAG used robust risk minimization, which is a winnow technique.', ' #TAUTHOR_TAG employed the same technique in a combination of learners.', 'voted perceptrons were applied to the shared task data by  #AUTHOR_TAG a ) and hammerton used a recurrent neural network ( long short - term memory ) for finding named entities.', 'other learning approaches were employed less frequently.', 'two teams used adaboost.', 'mh  #AUTHOR_TAG b ;  #AUTHOR_TAG and two other groups employed memory - based learning  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG.', 'transformation - based learning  #TAUTHOR_TAG, support vector machines  #AUTHOR_TAG and conditional random fields ( mc  #AUTHOR_TAG were applied by one system each.', 'combination of different learning systems has proven to be a good method for obtaining excellent results.', 'five participating groups have applied system combination.', ' #TAUTHOR_TAG tested different methods for combining the results of four systems and found that robust risk minimization worked best.', ' #AUTHOR_TAG employed a stacked learning system which contains hidden markov models, maximum entropy models and conditional markov models.', ' #AUTHOR_TAG stacked two learners and obtained better performance.', ' #AUTHOR_TAG applied both stacking and voting to three learners.', ' #AUTHOR_TAG employed both voting and bagging for combining classifiers']",0
"['combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training']","['combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training data.', 'the inclusion of extra named']","['for english as a part in a combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training data.', 'the inclusion of extra named']","['', 'four groups examined the usability of unannotated data, either for extracting training instances  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG or obtaining extra named entities for gazetteers  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'a reasonable number of groups have also employed unannotated data for obtaining capitalization features for words.', 'one participating team has used externally trained named entity recognition systems for english as a part in a combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training data.', 'the inclusion of extra named entity recognition systems seems to have worked well  #TAUTHOR_TAG.', 'generally the systems that only used gazetteers seem to gain more than systems that have used unannotated data for other purposes than obtaining capitalization information.', 'however, the gain differences between the two approaches are most obvious for english for which better gazetteers are available.', 'with the exception of the result of  #AUTHOR_TAG, there is not much difference in the german results between the gains obtained by using gazetteers and those obtained by using unannotated data']",0
"['combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training']","['combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training data.', 'the inclusion of extra named']","['for english as a part in a combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training data.', 'the inclusion of extra named']","['', 'four groups examined the usability of unannotated data, either for extracting training instances  #AUTHOR_TAG hendrickx and van den  #AUTHOR_TAG or obtaining extra named entities for gazetteers  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'a reasonable number of groups have also employed unannotated data for obtaining capitalization features for words.', 'one participating team has used externally trained named entity recognition systems for english as a part in a combined system  #TAUTHOR_TAG.', 'with extra information compared to while using only the available training data.', 'the inclusion of extra named entity recognition systems seems to have worked well  #TAUTHOR_TAG.', 'generally the systems that only used gazetteers seem to gain more than systems that have used unannotated data for other purposes than obtaining capitalization information.', 'however, the gain differences between the two approaches are most obvious for english for which better gazetteers are available.', 'with the exception of the result of  #AUTHOR_TAG, there is not much difference in the german results between the gains obtained by using gazetteers and those obtained by using unannotated data']",0
[' #TAUTHOR_TAG achieved the'],[' #TAUTHOR_TAG achieved the'],"[' #TAUTHOR_TAG achieved the highest overall f β = 1 rate.', 'however, the difference between their']","['', 'the performances of the sixteen systems on the two test data sets can be found in table 5.', 'for english, the combined classifier of  #TAUTHOR_TAG achieved the highest overall f β = 1 rate.', 'however, the difference between their performance and that of the maximum entropy approach of  #AUTHOR_TAG is not significant.', 'an important feature of the best system that other participants did not use, was the inclusion of the output of two externally trained named entity recognizers in the combination process.', '']",0
[' #TAUTHOR_TAG achieved the'],[' #TAUTHOR_TAG achieved the'],"[' #TAUTHOR_TAG achieved the highest overall f β = 1 rate.', 'however, the difference between their']","['', 'the performances of the sixteen systems on the two test data sets can be found in table 5.', 'for english, the combined classifier of  #TAUTHOR_TAG achieved the highest overall f β = 1 rate.', 'however, the difference between their performance and that of the maximum entropy approach of  #AUTHOR_TAG is not significant.', 'an important feature of the best system that other participants did not use, was the inclusion of the output of two externally trained named entity recognizers in the combination process.', '']",0
[' #TAUTHOR_TAG achieved the'],[' #TAUTHOR_TAG achieved the'],"[' #TAUTHOR_TAG achieved the highest overall f β = 1 rate.', 'however, the difference between their']","['', 'the performances of the sixteen systems on the two test data sets can be found in table 5.', 'for english, the combined classifier of  #TAUTHOR_TAG achieved the highest overall f β = 1 rate.', 'however, the difference between their performance and that of the maximum entropy approach of  #AUTHOR_TAG is not significant.', 'an important feature of the best system that other participants did not use, was the inclusion of the output of two externally trained named entity recognizers in the combination process.', '']",0
['as robust risk minimization  #TAUTHOR_TAG'],['as robust risk minimization  #TAUTHOR_TAG'],"['as robust risk minimization  #TAUTHOR_TAG.', 'apart from the training data, this system']","['have described the conll - 2003 shared task : language - independent named entity recognition.', 'sixteen systems have processed english and german named entity data.', 'the best performance for both languages has been obtained by a combined learning system that used maximum entropy models, transformation - based learning, hidden markov models as well as robust risk minimization  #TAUTHOR_TAG.', 'apart from the training data, this system also employed gazetteers and the output of two externally trained named entity recognizers.', 'the performance of the system of  #AUTHOR_TAG was not significantly different from the best performance for english and the method of  #AUTHOR_TAG and the approach of  #AUTHOR_TAG were not significantly worse than the best result for german.', 'eleven teams have incorporated information other than the training data in their system.', 'four of them have obtained error reductions of 15 % or more for english and one has managed this for german.', 'the resources used by these systems, gazetteers and externally trained named entity systems, still require a lot of manual work.', 'systems that employed unannotated data, obtained performance gains around 5 %.', 'the search for an excellent method for taking advantage of the fast amount of available raw text, remains open']",0
"['- internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG. debates in online public forums ( e. g. fig. 1 ) differ from']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG. debates in online public forums ( e. g. fig. 1 ) differ from debates in congress and on company discussion sites in two ways. first, the language is different. online debaters are highly involved, often using emotional and colorful language to make their points. these debates are also personal, giving a strong sense of the indi - vid', '##ual making', 'the argument, and whether s / he favors emotive or factual modes of expression, e. g. let me answer.... no! ( p2 in fig', '. 3 ). other', ""common features are sarcasm, e. g. i'm obviously ignorant. look how many times i've been"", 'given the title ( p', ""##4 in fig. 1 ), questioning another's evidence or assumptions : yes there is always room for human error, but is one"", ""accident that hasn't happened yet enough cause to get rid of a capital punishment? ( p2 in fig. 3 ), and insults : or is it because you are ignorant? ( p3 in fig. 1 ). these properties may function to engage the audience and persuade them to form a"", 'particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64 % over several topics  #TAUTHOR_TAG second, the', 'affordances of different online debate sites', 'provide differential support for dialogic relations between forum participants. for example, the research of  #TAUTHOR_TAG,', 'does not explicitly model dialogue or author relations. however debates in our corpus vary greatly by topic on two dialogic factors : ( 1 ) the percent of posts that are rebuttals to prior posts, and ( 2 ) the number of posts per author. the first 5 columns of', 'table 2 shows the variation in these dimensions by topic. in this paper we show that information', 'about dialogic relations between authors ( source factors ) improves performance for stance classification, when compared to models that only have access to properties of the argument. we model source relations with a graph, and add this information to classifiers operating on the text of a post. sec. 2 describes the corpus and our approach. our corpus is publicly available, see  #AUTHOR_TAG. we show in sec. 3 that modeling source properties improves performance when the debates are highly dialogic', '. we leave a more detailed comparison to previous work to sec. 3 so that we can contrast previous work with our approach']",0
"['- internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG. debates in online public forums ( e. g. fig. 1 ) differ from']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG. debates in online public forums ( e. g. fig. 1 ) differ from debates in congress and on company discussion sites in two ways. first, the language is different. online debaters are highly involved, often using emotional and colorful language to make their points. these debates are also personal, giving a strong sense of the indi - vid', '##ual making', 'the argument, and whether s / he favors emotive or factual modes of expression, e. g. let me answer.... no! ( p2 in fig', '. 3 ). other', ""common features are sarcasm, e. g. i'm obviously ignorant. look how many times i've been"", 'given the title ( p', ""##4 in fig. 1 ), questioning another's evidence or assumptions : yes there is always room for human error, but is one"", ""accident that hasn't happened yet enough cause to get rid of a capital punishment? ( p2 in fig. 3 ), and insults : or is it because you are ignorant? ( p3 in fig. 1 ). these properties may function to engage the audience and persuade them to form a"", 'particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64 % over several topics  #TAUTHOR_TAG second, the', 'affordances of different online debate sites', 'provide differential support for dialogic relations between forum participants. for example, the research of  #TAUTHOR_TAG,', 'does not explicitly model dialogue or author relations. however debates in our corpus vary greatly by topic on two dialogic factors : ( 1 ) the percent of posts that are rebuttals to prior posts, and ( 2 ) the number of posts per author. the first 5 columns of', 'table 2 shows the variation in these dimensions by topic. in this paper we show that information', 'about dialogic relations between authors ( source factors ) improves performance for stance classification, when compared to models that only have access to properties of the argument. we model source relations with a graph, and add this information to classifiers operating on the text of a post. sec. 2 describes the corpus and our approach. our corpus is publicly available, see  #AUTHOR_TAG. we show in sec. 3 that modeling source properties improves performance when the debates are highly dialogic', '. we leave a more detailed comparison to previous work to sec. 3 so that we can contrast previous work with our approach']",0
"['- internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG. debates in online public forums ( e. g. fig. 1 ) differ from']","[') company - internal discussion sites  #AUTHOR_TAG ; and ( 3 ) online social and political public forums  #TAUTHOR_TAG ; wang and rose, 2010 ;  #AUTHOR_TAG. debates in online public forums ( e. g. fig. 1 ) differ from debates in congress and on company discussion sites in two ways. first, the language is different. online debaters are highly involved, often using emotional and colorful language to make their points. these debates are also personal, giving a strong sense of the indi - vid', '##ual making', 'the argument, and whether s / he favors emotive or factual modes of expression, e. g. let me answer.... no! ( p2 in fig', '. 3 ). other', ""common features are sarcasm, e. g. i'm obviously ignorant. look how many times i've been"", 'given the title ( p', ""##4 in fig. 1 ), questioning another's evidence or assumptions : yes there is always room for human error, but is one"", ""accident that hasn't happened yet enough cause to get rid of a capital punishment? ( p2 in fig. 3 ), and insults : or is it because you are ignorant? ( p3 in fig. 1 ). these properties may function to engage the audience and persuade them to form a"", 'particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64 % over several topics  #TAUTHOR_TAG second, the', 'affordances of different online debate sites', 'provide differential support for dialogic relations between forum participants. for example, the research of  #TAUTHOR_TAG,', 'does not explicitly model dialogue or author relations. however debates in our corpus vary greatly by topic on two dialogic factors : ( 1 ) the percent of posts that are rebuttals to prior posts, and ( 2 ) the number of posts per author. the first 5 columns of', 'table 2 shows the variation in these dimensions by topic. in this paper we show that information', 'about dialogic relations between authors ( source factors ) improves performance for stance classification, when compared to models that only have access to properties of the argument. we model source relations with a graph, and add this information to classifiers operating on the text of a post. sec. 2 describes the corpus and our approach. our corpus is publicly available, see  #AUTHOR_TAG. we show in sec. 3 that modeling source properties improves performance when the debates are highly dialogic', '. we leave a more detailed comparison to previous work to sec. 3 so that we can contrast previous work with our approach']",0
['labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow'],['labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow'],['approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow the standard supervised learning'],"['trigger labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow the standard supervised learning paradigm. for each event type, experts first write annotation guidelines.', '']",0
['labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow'],['labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow'],['approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow the standard supervised learning'],"['trigger labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow the standard supervised learning paradigm. for each event type, experts first write annotation guidelines.', '']",4
"['of  #TAUTHOR_TAG, modifying mechanisms relevant for features and']","['of  #TAUTHOR_TAG, modifying mechanisms relevant for features and']","['requires annotated triggers for each target event type.', 'therefore, we implemented our system by adapting the state - of - the - art fully - supervised event extraction system of  #TAUTHOR_TAG, modifying mechanisms relevant for features and']","['section describes the method we designed to implement the seed - based approach.', 'to assess our approach, we compare it ( section 4 ) with the common fully - supervised approach, which requires annotated triggers for each target event type.', 'therefore, we implemented our system by adapting the state - of - the - art fully - supervised event extraction system of  #TAUTHOR_TAG, modifying mechanisms relevant for features and for trigger labels, as described below.', 'hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure']",4
"['classification part in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', 'given']","['classification part in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', 'given']","['in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', 'given a set of new target event types t we classify']","['implement the seed - based approach for trigger labeling, we adapt only the trigger classification part in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', '']",4
['implemented by  #TAUTHOR_TAG ('],"['implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the']","['the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and']","['evaluate our seed - based approach ( section 2 ) in comparison to the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and 559 training documents.', 'however, some evaluation settings differ :  #TAUTHOR_TAG train a multi - class model for all 33 ace - 2005 event types, and classify all tokens in the test documents into these event types.', 'our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.', 'we next describe how this setup is addressed in our evaluation']",4
['labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow'],['labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow'],['approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow the standard supervised learning'],"['trigger labeling approaches  #AUTHOR_TAG b ;  #TAUTHOR_TAG follow the standard supervised learning paradigm. for each event type, experts first write annotation guidelines.', '']",6
"['of  #TAUTHOR_TAG, modifying mechanisms relevant for features and']","['of  #TAUTHOR_TAG, modifying mechanisms relevant for features and']","['requires annotated triggers for each target event type.', 'therefore, we implemented our system by adapting the state - of - the - art fully - supervised event extraction system of  #TAUTHOR_TAG, modifying mechanisms relevant for features and']","['section describes the method we designed to implement the seed - based approach.', 'to assess our approach, we compare it ( section 4 ) with the common fully - supervised approach, which requires annotated triggers for each target event type.', 'therefore, we implemented our system by adapting the state - of - the - art fully - supervised event extraction system of  #TAUTHOR_TAG, modifying mechanisms relevant for features and for trigger labels, as described below.', 'hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure']",6
"['classification part in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', 'given']","['classification part in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', 'given']","['in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', 'given a set of new target event types t we classify']","['implement the seed - based approach for trigger labeling, we adapt only the trigger classification part in the  #TAUTHOR_TAG fully - supervised system, ignoring arguments.', '']",6
['implemented by  #TAUTHOR_TAG ('],"['implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the']","['the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and']","['evaluate our seed - based approach ( section 2 ) in comparison to the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and 559 training documents.', 'however, some evaluation settings differ :  #TAUTHOR_TAG train a multi - class model for all 33 ace - 2005 event types, and classify all tokens in the test documents into these event types.', 'our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.', 'we next describe how this setup is addressed in our evaluation']",6
['system of  #TAUTHOR_TAG labels triggers and'],['system of  #TAUTHOR_TAG labels triggers and'],['event extraction system of  #TAUTHOR_TAG labels triggers and'],"['event extraction system of  #TAUTHOR_TAG labels triggers and their arguments for a set of target event types l, for which annotated training documents are provided.', 'the system utilizes a structured perceptron with beam search  #AUTHOR_TAG.', '']",3
['implemented by  #TAUTHOR_TAG ('],"['implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the']","['the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and']","['evaluate our seed - based approach ( section 2 ) in comparison to the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and 559 training documents.', 'however, some evaluation settings differ :  #TAUTHOR_TAG train a multi - class model for all 33 ace - 2005 event types, and classify all tokens in the test documents into these event types.', 'our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.', 'we next describe how this setup is addressed in our evaluation']",3
['implemented by  #TAUTHOR_TAG ('],"['implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the']","['the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and']","['evaluate our seed - based approach ( section 2 ) in comparison to the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and 559 training documents.', 'however, some evaluation settings differ :  #TAUTHOR_TAG train a multi - class model for all 33 ace - 2005 event types, and classify all tokens in the test documents into these event types.', 'our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.', 'we next describe how this setup is addressed in our evaluation']",3
"['system of  #TAUTHOR_TAG ( its globally optimized run, when labeling both']","['system of  #TAUTHOR_TAG ( its globally optimized run, when labeling both']","['of  #TAUTHOR_TAG ( its globally optimized run, when labeling both triggers and arguments ).', 'we also compare to']","['', ""we compare our system's performance to the published trigger classification results of the baseline system of  #TAUTHOR_TAG ( its globally optimized run, when labeling both triggers and arguments )."", 'we also compare to the sentence - level system in  #AUTHOR_TAG which uses the same dataset split.', 'our system outperforms the fully - supervised baseline by 5. 7 % f 1, which is statistically significant ( two - tailed wilcoxon test, p < 0. 05 ).', 'this shows that there is no performance hit for the seed - based method on this dataset, even though it does not require any annotated data for new tested events, thus saving costly annotation efforts']",3
['implemented by  #TAUTHOR_TAG ('],"['implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the']","['the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and']","['evaluate our seed - based approach ( section 2 ) in comparison to the fully - supervised approach implemented by  #TAUTHOR_TAG ( section 3 ).', 'to maintain comparability, we use the ace - 2005 documents with the same split as in  #AUTHOR_TAG b ;  #TAUTHOR_TAG to 40 test documents and 559 training documents.', 'however, some evaluation settings differ :  #TAUTHOR_TAG train a multi - class model for all 33 ace - 2005 event types, and classify all tokens in the test documents into these event types.', 'our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.', 'we next describe how this setup is addressed in our evaluation']",5
"['technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical']","['technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical']","['the grammar engine technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical advances in']","['language generation systems require content planning and format for the selected subject domain as input and specifics about the natural language in order to generate text  #AUTHOR_TAG, of which the latter tend to be bootstrappable for related languages ( de  #AUTHOR_TAG.', 'our nlg system uses ontologies to represent domain knowledge.', 'as for language, we are interested in runyankore, a bantu language indigenous to south western uganda.', 'the highly agglutinative structure and complex verbal morphology of runyankore make existing nlg systems based on templates inapplicable  #AUTHOR_TAG.', 'there have been efforts undertaken to apply the grammar engine technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for runyankore.', 'we present our implementation of these algorithms and required linguistic annotations as a protege 5. x plugin.', 'also ensures no typographical errors are made in the xml file.', 'these annotation fields are mandatory, and we allowed for the use of 0 as the nc for the pos which is not a noun.', 'these restrictions to input were achieved using document filters.', 'the xml file is queried during the verbalization process so as to obtain the required annotations that are needed for the algorithms']",0
"['specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', '']","['verbalization and pluralization presented in  #AUTHOR_TAG a ;  #AUTHOR_TAG c ) as a java application.', 'the cfg specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', '']","['presented in  #AUTHOR_TAG a ;  #AUTHOR_TAG c ) as a java application.', 'the cfg specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', 'we used this tool for three main reasons : our grammar engine implementation was done in java, so we wanted a java tool as well ; we wanted a small cfg implementation']","['implemented the algorithms for verbalization and pluralization presented in  #AUTHOR_TAG a ;  #AUTHOR_TAG c ) as a java application.', 'the cfg specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', ""we used this tool for three main reasons : our grammar engine implementation was done in java, so we wanted a java tool as well ; we wanted a small cfg implementation for reasonable performance ; and their tool extended purdom's algorithm to fulfill context - dependent rule coverage ( cdrc ), which generates more and simpler sentences."", 'a sample of the generated text is presented below :', ""• buri rupapura rwamakuru n'ekihandiiko ekishohoziibwe, ( generated from : newspaper publication )"", '• buri ntaama nerya ebinyaansi byoona, ( gener -', 'the generated text is saved in a text file, which ensures that the text can be linked to other application scenarios.', 'we are working on a better design to present the sentences within the tool, for interaction during multi - modal ontology development.', ""the grammar engine can be launched through the'runyankore > verbalize'submenu under the'tools'menu in protege 5. x."", 'the jar file is available from https : / / github. com / runyankorenlg / runyankorenlgsystem']",0
"['technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical']","['technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical']","['the grammar engine technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical advances in']","['language generation systems require content planning and format for the selected subject domain as input and specifics about the natural language in order to generate text  #AUTHOR_TAG, of which the latter tend to be bootstrappable for related languages ( de  #AUTHOR_TAG.', 'our nlg system uses ontologies to represent domain knowledge.', 'as for language, we are interested in runyankore, a bantu language indigenous to south western uganda.', 'the highly agglutinative structure and complex verbal morphology of runyankore make existing nlg systems based on templates inapplicable  #AUTHOR_TAG.', 'there have been efforts undertaken to apply the grammar engine technique instead  #AUTHOR_TAG a ;  #TAUTHOR_TAG c ), which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for runyankore.', 'we present our implementation of these algorithms and required linguistic annotations as a protege 5. x plugin.', 'also ensures no typographical errors are made in the xml file.', 'these annotation fields are mandatory, and we allowed for the use of 0 as the nc for the pos which is not a noun.', 'these restrictions to input were achieved using document filters.', 'the xml file is queried during the verbalization process so as to obtain the required annotations that are needed for the algorithms']",5
"['specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', '']","['verbalization and pluralization presented in  #AUTHOR_TAG a ;  #AUTHOR_TAG c ) as a java application.', 'the cfg specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', '']","['presented in  #AUTHOR_TAG a ;  #AUTHOR_TAG c ) as a java application.', 'the cfg specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', 'we used this tool for three main reasons : our grammar engine implementation was done in java, so we wanted a java tool as well ; we wanted a small cfg implementation']","['implemented the algorithms for verbalization and pluralization presented in  #AUTHOR_TAG a ;  #AUTHOR_TAG c ) as a java application.', 'the cfg specified in  #TAUTHOR_TAG was implemented using the cfg java tool  #AUTHOR_TAG.', ""we used this tool for three main reasons : our grammar engine implementation was done in java, so we wanted a java tool as well ; we wanted a small cfg implementation for reasonable performance ; and their tool extended purdom's algorithm to fulfill context - dependent rule coverage ( cdrc ), which generates more and simpler sentences."", 'a sample of the generated text is presented below :', ""• buri rupapura rwamakuru n'ekihandiiko ekishohoziibwe, ( generated from : newspaper publication )"", '• buri ntaama nerya ebinyaansi byoona, ( gener -', 'the generated text is saved in a text file, which ensures that the text can be linked to other application scenarios.', 'we are working on a better design to present the sentences within the tool, for interaction during multi - modal ontology development.', ""the grammar engine can be launched through the'runyankore > verbalize'submenu under the'tools'menu in protege 5. x."", 'the jar file is available from https : / / github. com / runyankorenlg / runyankorenlgsystem']",5
['very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the digital humanities  #TAUTHOR_TAG a )'],['very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the digital humanities  #TAUTHOR_TAG a )'],"['training neural word embeddings  #AUTHOR_TAG.', 'as svd ppmi performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the digital humanities  #TAUTHOR_TAG a )']","['semantics can be broadly conceived as a staged approach to capture the semantics of a lexical item in focus via contextual patterns.', 'concordances are probably the most simple scheme to examine contextual semantic effects, but leave semantic inferences entirely to the human observer.', ""a more complex layer is reached with collocations which can be identified automatically via statistical word co - occurrence metrics ( manning and schutze, 1999 ;  #AUTHOR_TAG, two of which are incorporated in jeseme as well : positive pointwise mutual information ( ppmi ), developed by  #AUTHOR_TAG as an improvement over the probability ratio of normal pointwise mutual information ( pmi ;  #AUTHOR_TAG ) and pearson's χ 2, commonly used for testing the association between categorical variables ( e. g., pos tags ) and considered to be more robust than pmi when facing sparse information ( manning and schutze, 1999 )."", 'the currently most sophisticated and most influential approach to distributional semantics employs word embeddings, i. e., low ( usually 300 - 500 ) dimensional vector word representations of both semantic and syntactic information.', 'alternative approaches are e. g., graph - based algorithms  #AUTHOR_TAG or ranking functions from information retrieval  #AUTHOR_TAG.', 'the premier example for word embeddings is skip - gram negative sampling, which is part of the word2vec family of algorithms  #AUTHOR_TAG.', 'the random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words  #AUTHOR_TAG a, 2017 ).', 'word embeddings based on singular value decomposition ( svd ; historically popular in the form of latent semantic analysis  #AUTHOR_TAG ) are not affected by this problem.', ' #AUTHOR_TAG created svd ppmi after investigating the implicit operations performed while training neural word embeddings  #AUTHOR_TAG.', 'as svd ppmi performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the digital humanities  #TAUTHOR_TAG a )']",0
"['.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans']","['rare, e. g.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans']","['rare, e. g.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans']","['use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics ( see e. g.,  #AUTHOR_TAG ).', 'there exist already several tools for performing statistical analysis on user provided corpora, e. g., wordsmith 3 or the ucs toolkit, 4 as well as interactive websites for exploring precompiled corpora, e. g., the "" advanced "" interface for google books  #AUTHOR_TAG or diacollo  #AUTHOR_TAG.', 'meanwhile, word embeddings and their application to diachronic semantics have become a novel state - of - the - art methodology lacking, however, off - the - shelves analysis tools easy to use for a typically non - technical audience.', 'most work is centered around word2vec ( e. g.,  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG b ) ), whereas alternative approaches are rare, e. g.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time  #TAUTHOR_TAG and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.', 'this information can then be exploited for automatic  #AUTHOR_TAG or manual  #AUTHOR_TAG interpretation.', 'research.', 'we employ five corpora, including the four largest diachronic corpora of acceptable quality for english and german.', 'the google books ngram corpus ( gb ;  #AUTHOR_TAG,  #AUTHOR_TAG contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ).', 'gb is multilingual ; its english subcorpus is further divided into regional segments ( british, us ) and genres ( general language and fiction texts ).', 'it can be argued to be not so useful for digital humanities research due to digitalization artifacts and its opaque and unbalanced nature, yet the english fiction part is least effected by these problems  #AUTHOR_TAG.', 'we use its german ( gb german ) and english fiction ( gb fiction ) subcorpora.', 'the corpus of historical american english 5 ( coha ;  #AUTHOR_TAG ) covers texts from 1800 to 2009 from multiple genres balanced for each decade, and contains annotations for lemmata.', ""the deutsches textarchiv 6 ( dta,'german text archive';  #AUTHOR_TAG ;  #AUTHOR_TAG ) is a german diachronic corpus and consists of manually transcribed books selected for their representativeness and balance between genres."", 'a major benefit of dta are its annotation layers which offer both orthographic normalization ( mapping arch']",0
"['.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans']","['rare, e. g.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans']","['rare, e. g.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans']","['use of statistical methods is getting more and more the status of a commonly shared methodology in diachronic linguistics ( see e. g.,  #AUTHOR_TAG ).', 'there exist already several tools for performing statistical analysis on user provided corpora, e. g., wordsmith 3 or the ucs toolkit, 4 as well as interactive websites for exploring precompiled corpora, e. g., the "" advanced "" interface for google books  #AUTHOR_TAG or diacollo  #AUTHOR_TAG.', 'meanwhile, word embeddings and their application to diachronic semantics have become a novel state - of - the - art methodology lacking, however, off - the - shelves analysis tools easy to use for a typically non - technical audience.', 'most work is centered around word2vec ( e. g.,  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG b ) ), whereas alternative approaches are rare, e. g.,  #AUTHOR_TAG using glove  #AUTHOR_TAG and  #TAUTHOR_TAG using svd ppmi.', 'embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time  #TAUTHOR_TAG and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.', 'this information can then be exploited for automatic  #AUTHOR_TAG or manual  #AUTHOR_TAG interpretation.', 'research.', 'we employ five corpora, including the four largest diachronic corpora of acceptable quality for english and german.', 'the google books ngram corpus ( gb ;  #AUTHOR_TAG,  #AUTHOR_TAG contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ).', 'gb is multilingual ; its english subcorpus is further divided into regional segments ( british, us ) and genres ( general language and fiction texts ).', 'it can be argued to be not so useful for digital humanities research due to digitalization artifacts and its opaque and unbalanced nature, yet the english fiction part is least effected by these problems  #AUTHOR_TAG.', 'we use its german ( gb german ) and english fiction ( gb fiction ) subcorpora.', 'the corpus of historical american english 5 ( coha ;  #AUTHOR_TAG ) covers texts from 1800 to 2009 from multiple genres balanced for each decade, and contains annotations for lemmata.', ""the deutsches textarchiv 6 ( dta,'german text archive';  #AUTHOR_TAG ;  #AUTHOR_TAG ) is a german diachronic corpus and consists of manually transcribed books selected for their representativeness and balance between genres."", 'a major benefit of dta are its annotation layers which offer both orthographic normalization ( mapping arch']",0
"['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could']","['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure']","['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure']","['values changed over time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure 2 : screenshot of jeseme\'s result page when searching for the lexical item "" heart "" in coha. be potentially misleading by implying a', 'constant meaning of those words used as the background ( which are actually positioned by their meaning at a single point', 'in time ). typical context offers two graphs, one for χ 2 and one for ppmi, arranged in tabs. values in typical context graphs are normalized to make them comparable across different metrics. finally, relative frequency plots the relative frequency measure against all', ""words above the minimum frequency threshold ( see section 4 ). all graphs are accompanied by a short explanation and a form for adding further words to the graph under scrutiny. the result page also provides a link to the corresponding corpus, to help users trace jeseme's computational results. as an example, consider"", '']",5
"['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could']","['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure']","['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure']","['values changed over time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure 2 : screenshot of jeseme\'s result page when searching for the lexical item "" heart "" in coha. be potentially misleading by implying a', 'constant meaning of those words used as the background ( which are actually positioned by their meaning at a single point', 'in time ). typical context offers two graphs, one for χ 2 and one for ppmi, arranged in tabs. values in typical context graphs are normalized to make them comparable across different metrics. finally, relative frequency plots the relative frequency measure against all', ""words above the minimum frequency threshold ( see section 4 ). all graphs are accompanied by a short explanation and a form for adding further words to the graph under scrutiny. the result page also provides a link to the corresponding corpus, to help users trace jeseme's computational results. as an example, consider"", '']",3
"['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could']","['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure']","['time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure']","['values changed over time. we follow  #AUTHOR_TAG in choosing such a visualization, while we refrain from using the two - dimensional projection used in other studies  #TAUTHOR_TAG. we stipulate that the latter could figure 2 : screenshot of jeseme\'s result page when searching for the lexical item "" heart "" in coha. be potentially misleading by implying a', 'constant meaning of those words used as the background ( which are actually positioned by their meaning at a single point', 'in time ). typical context offers two graphs, one for χ 2 and one for ppmi, arranged in tabs. values in typical context graphs are normalized to make them comparable across different metrics. finally, relative frequency plots the relative frequency measure against all', ""words above the minimum frequency threshold ( see section 4 ). all graphs are accompanied by a short explanation and a form for adding further words to the graph under scrutiny. the result page also provides a link to the corresponding corpus, to help users trace jeseme's computational results. as an example, consider"", '']",4
"['##eme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', '']","['long periods of time.', 'in contrast to other corpus exploration tools, jeseme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', 'jeseme is also']","['long periods of time.', 'in contrast to other corpus exploration tools, jeseme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', '']","['presented jeseme, the jena semantic explorer, an interactive website and rest api for exploring changes in lexical semantics over long periods of time.', 'in contrast to other corpus exploration tools, jeseme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', 'jeseme is also the first tool of its kind and under continuous development.', 'future technical work will add functionality to compare words across corpora which might require a mapping between embeddings  #TAUTHOR_TAG and provide optional stemming routines.', 'both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long - term availability.', ""finally, we will conduct a user study to investigate jeseme's potential for the digital humanities community""]",7
"['##eme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', '']","['long periods of time.', 'in contrast to other corpus exploration tools, jeseme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', 'jeseme is also']","['long periods of time.', 'in contrast to other corpus exploration tools, jeseme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', '']","['presented jeseme, the jena semantic explorer, an interactive website and rest api for exploring changes in lexical semantics over long periods of time.', 'in contrast to other corpus exploration tools, jeseme is based on cutting - edge word embedding technology  #TAUTHOR_TAG a, 2017 ) and provides access to five popular corpora for the english and german language.', 'jeseme is also the first tool of its kind and under continuous development.', 'future technical work will add functionality to compare words across corpora which might require a mapping between embeddings  #TAUTHOR_TAG and provide optional stemming routines.', 'both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long - term availability.', ""finally, we will conduct a user study to investigate jeseme's potential for the digital humanities community""]",2
[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pre'],[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained'],"[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide']","['regarding bias and stereotypes expressed in text and subsequently incorporated in learned language models is currently a vivid field.', ' #AUTHOR_TAG show that learned embeddings exhibit every linguistic bias documented in the field of psychology ( such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race ).', ' #AUTHOR_TAG show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and  #AUTHOR_TAG suggest that biases might in fact be amplified by embedding models.', 'several researchers have also investigated ways to counter stereotypes and biases in learned language models.', 'while the seminal work by  #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide insights into the possibilities of learning embeddings that are gender neutral.', ' #AUTHOR_TAG outline a way of training a recurrent neural network for word - based language modelling such that the model is gender neutral.', ' #AUTHOR_TAG discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by  #TAUTHOR_TAG, to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.', ' #AUTHOR_TAG contest the approaches to debiasing word embeddings presented by  #TAUTHOR_TAG and  #AUTHOR_TAG, arguing that while the bias is reduced when measured according to its definition, i. e., dampening the impact of the general gender direction in the vector space, "" the actual effect is mostly hiding the bias, not removing it "".', ' #AUTHOR_TAG claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.', 'our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for swedish.', 'we are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.', 'our experiments are therefore tightly tied to a real - world use case where gender bias would have potentially serious ramifications.', 'we also provide further evidence of the inability of the debiasing method proposed by  #TAUTHOR_TAG to handle the type of bias we are concerned with']",0
[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pre'],[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained'],"[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide']","['regarding bias and stereotypes expressed in text and subsequently incorporated in learned language models is currently a vivid field.', ' #AUTHOR_TAG show that learned embeddings exhibit every linguistic bias documented in the field of psychology ( such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race ).', ' #AUTHOR_TAG show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and  #AUTHOR_TAG suggest that biases might in fact be amplified by embedding models.', 'several researchers have also investigated ways to counter stereotypes and biases in learned language models.', 'while the seminal work by  #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide insights into the possibilities of learning embeddings that are gender neutral.', ' #AUTHOR_TAG outline a way of training a recurrent neural network for word - based language modelling such that the model is gender neutral.', ' #AUTHOR_TAG discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by  #TAUTHOR_TAG, to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.', ' #AUTHOR_TAG contest the approaches to debiasing word embeddings presented by  #TAUTHOR_TAG and  #AUTHOR_TAG, arguing that while the bias is reduced when measured according to its definition, i. e., dampening the impact of the general gender direction in the vector space, "" the actual effect is mostly hiding the bias, not removing it "".', ' #AUTHOR_TAG claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.', 'our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for swedish.', 'we are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.', 'our experiments are therefore tightly tied to a real - world use case where gender bias would have potentially serious ramifications.', 'we also provide further evidence of the inability of the debiasing method proposed by  #TAUTHOR_TAG to handle the type of bias we are concerned with']",0
[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pre'],[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained'],"[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide']","['regarding bias and stereotypes expressed in text and subsequently incorporated in learned language models is currently a vivid field.', ' #AUTHOR_TAG show that learned embeddings exhibit every linguistic bias documented in the field of psychology ( such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race ).', ' #AUTHOR_TAG show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and  #AUTHOR_TAG suggest that biases might in fact be amplified by embedding models.', 'several researchers have also investigated ways to counter stereotypes and biases in learned language models.', 'while the seminal work by  #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide insights into the possibilities of learning embeddings that are gender neutral.', ' #AUTHOR_TAG outline a way of training a recurrent neural network for word - based language modelling such that the model is gender neutral.', ' #AUTHOR_TAG discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by  #TAUTHOR_TAG, to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.', ' #AUTHOR_TAG contest the approaches to debiasing word embeddings presented by  #TAUTHOR_TAG and  #AUTHOR_TAG, arguing that while the bias is reduced when measured according to its definition, i. e., dampening the impact of the general gender direction in the vector space, "" the actual effect is mostly hiding the bias, not removing it "".', ' #AUTHOR_TAG claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.', 'our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for swedish.', 'we are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.', 'our experiments are therefore tightly tied to a real - world use case where gender bias would have potentially serious ramifications.', 'we also provide further evidence of the inability of the debiasing method proposed by  #TAUTHOR_TAG to handle the type of bias we are concerned with']",0
['in  #TAUTHOR_TAG to the pre'],"['in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding']",['apply the debiasing methodology in  #TAUTHOR_TAG to the pre'],"['apply the debiasing methodology in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific.', 'the definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e. g., he - she, and motherfather.', 'in our setting, there are 10 such pairs.', 'the gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space.', 'we use the same methodology for growing a seed set of gender specific words into a larger set as described in  #TAUTHOR_TAG, and end up with 486 manually curated gender specific words, including e. g., farfar ( paternal grandfather ), tvillingsystrar ( twin sisters ), and matriark ( matriarch ).', 'the definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs ( i. e. w 1 − w 2 ), and then factorizing the mean - centered difference vectors using pca, retaining only the first principal component, which will act as the gender direction.', 'the vector space is then hard debiased 1 in the sense that the gen - der direction b is removed from the embeddings of all non - gender specific words w using orthogonal projection : w = w − b × w · b b · b.', 'the approach described by  #TAUTHOR_TAG includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.', 'the equality set is application specific, and since the current investigation of swedish language embeddings does not naturally lend itself to include an equality set, the debiasing of the embeddings does not involve equalization in our case.', 'we apply the method described above to all pretrained embeddings in table 3, as well as to the token vectors generated by elmo and bert.', 'although it is not clear whether the proposed debiasing method is applicable to embeddings produced by contextualized language models, we argue that it is reasonable to treat the contextualized models as black boxes, and rely only on their output, given the proposed use case']",0
[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pre'],[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained'],"[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide']","['regarding bias and stereotypes expressed in text and subsequently incorporated in learned language models is currently a vivid field.', ' #AUTHOR_TAG show that learned embeddings exhibit every linguistic bias documented in the field of psychology ( such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race ).', ' #AUTHOR_TAG show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and  #AUTHOR_TAG suggest that biases might in fact be amplified by embedding models.', 'several researchers have also investigated ways to counter stereotypes and biases in learned language models.', 'while the seminal work by  #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide insights into the possibilities of learning embeddings that are gender neutral.', ' #AUTHOR_TAG outline a way of training a recurrent neural network for word - based language modelling such that the model is gender neutral.', ' #AUTHOR_TAG discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by  #TAUTHOR_TAG, to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.', ' #AUTHOR_TAG contest the approaches to debiasing word embeddings presented by  #TAUTHOR_TAG and  #AUTHOR_TAG, arguing that while the bias is reduced when measured according to its definition, i. e., dampening the impact of the general gender direction in the vector space, "" the actual effect is mostly hiding the bias, not removing it "".', ' #AUTHOR_TAG claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.', 'our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for swedish.', 'we are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.', 'our experiments are therefore tightly tied to a real - world use case where gender bias would have potentially serious ramifications.', 'we also provide further evidence of the inability of the debiasing method proposed by  #TAUTHOR_TAG to handle the type of bias we are concerned with']",3
['in  #TAUTHOR_TAG to the pre'],"['in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding']",['apply the debiasing methodology in  #TAUTHOR_TAG to the pre'],"['apply the debiasing methodology in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific.', 'the definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e. g., he - she, and motherfather.', 'in our setting, there are 10 such pairs.', 'the gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space.', 'we use the same methodology for growing a seed set of gender specific words into a larger set as described in  #TAUTHOR_TAG, and end up with 486 manually curated gender specific words, including e. g., farfar ( paternal grandfather ), tvillingsystrar ( twin sisters ), and matriark ( matriarch ).', 'the definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs ( i. e. w 1 − w 2 ), and then factorizing the mean - centered difference vectors using pca, retaining only the first principal component, which will act as the gender direction.', 'the vector space is then hard debiased 1 in the sense that the gen - der direction b is removed from the embeddings of all non - gender specific words w using orthogonal projection : w = w − b × w · b b · b.', 'the approach described by  #TAUTHOR_TAG includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.', 'the equality set is application specific, and since the current investigation of swedish language embeddings does not naturally lend itself to include an equality set, the debiasing of the embeddings does not involve equalization in our case.', 'we apply the method described above to all pretrained embeddings in table 3, as well as to the token vectors generated by elmo and bert.', 'although it is not clear whether the proposed debiasing method is applicable to embeddings produced by contextualized language models, we argue that it is reasonable to treat the contextualized models as black boxes, and rely only on their output, given the proposed use case']",3
['in  #TAUTHOR_TAG to the pre'],"['in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding']",['apply the debiasing methodology in  #TAUTHOR_TAG to the pre'],"['apply the debiasing methodology in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific.', 'the definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e. g., he - she, and motherfather.', 'in our setting, there are 10 such pairs.', 'the gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space.', 'we use the same methodology for growing a seed set of gender specific words into a larger set as described in  #TAUTHOR_TAG, and end up with 486 manually curated gender specific words, including e. g., farfar ( paternal grandfather ), tvillingsystrar ( twin sisters ), and matriark ( matriarch ).', 'the definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs ( i. e. w 1 − w 2 ), and then factorizing the mean - centered difference vectors using pca, retaining only the first principal component, which will act as the gender direction.', 'the vector space is then hard debiased 1 in the sense that the gen - der direction b is removed from the embeddings of all non - gender specific words w using orthogonal projection : w = w − b × w · b b · b.', 'the approach described by  #TAUTHOR_TAG includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.', 'the equality set is application specific, and since the current investigation of swedish language embeddings does not naturally lend itself to include an equality set, the debiasing of the embeddings does not involve equalization in our case.', 'we apply the method described above to all pretrained embeddings in table 3, as well as to the token vectors generated by elmo and bert.', 'although it is not clear whether the proposed debiasing method is applicable to embeddings produced by contextualized language models, we argue that it is reasonable to treat the contextualized models as black boxes, and rely only on their output, given the proposed use case']",3
[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pre'],[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained'],"[' #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide']","['regarding bias and stereotypes expressed in text and subsequently incorporated in learned language models is currently a vivid field.', ' #AUTHOR_TAG show that learned embeddings exhibit every linguistic bias documented in the field of psychology ( such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race ).', ' #AUTHOR_TAG show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and  #AUTHOR_TAG suggest that biases might in fact be amplified by embedding models.', 'several researchers have also investigated ways to counter stereotypes and biases in learned language models.', 'while the seminal work by  #AUTHOR_TAG a  #TAUTHOR_TAG concerns the identification and mitigation of gender bias in pretrained word embeddings,  #AUTHOR_TAG provide insights into the possibilities of learning embeddings that are gender neutral.', ' #AUTHOR_TAG outline a way of training a recurrent neural network for word - based language modelling such that the model is gender neutral.', ' #AUTHOR_TAG discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by  #TAUTHOR_TAG, to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.', ' #AUTHOR_TAG contest the approaches to debiasing word embeddings presented by  #TAUTHOR_TAG and  #AUTHOR_TAG, arguing that while the bias is reduced when measured according to its definition, i. e., dampening the impact of the general gender direction in the vector space, "" the actual effect is mostly hiding the bias, not removing it "".', ' #AUTHOR_TAG claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words.', 'our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for swedish.', 'we are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.', 'our experiments are therefore tightly tied to a real - world use case where gender bias would have potentially serious ramifications.', 'we also provide further evidence of the inability of the debiasing method proposed by  #TAUTHOR_TAG to handle the type of bias we are concerned with']",5
['in  #TAUTHOR_TAG to the pre'],"['in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding']",['apply the debiasing methodology in  #TAUTHOR_TAG to the pre'],"['apply the debiasing methodology in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific.', 'the definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e. g., he - she, and motherfather.', 'in our setting, there are 10 such pairs.', 'the gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space.', 'we use the same methodology for growing a seed set of gender specific words into a larger set as described in  #TAUTHOR_TAG, and end up with 486 manually curated gender specific words, including e. g., farfar ( paternal grandfather ), tvillingsystrar ( twin sisters ), and matriark ( matriarch ).', 'the definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs ( i. e. w 1 − w 2 ), and then factorizing the mean - centered difference vectors using pca, retaining only the first principal component, which will act as the gender direction.', 'the vector space is then hard debiased 1 in the sense that the gen - der direction b is removed from the embeddings of all non - gender specific words w using orthogonal projection : w = w − b × w · b b · b.', 'the approach described by  #TAUTHOR_TAG includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.', 'the equality set is application specific, and since the current investigation of swedish language embeddings does not naturally lend itself to include an equality set, the debiasing of the embeddings does not involve equalization in our case.', 'we apply the method described above to all pretrained embeddings in table 3, as well as to the token vectors generated by elmo and bert.', 'although it is not clear whether the proposed debiasing method is applicable to embeddings produced by contextualized language models, we argue that it is reasonable to treat the contextualized models as black boxes, and rely only on their output, given the proposed use case']",5
['in  #TAUTHOR_TAG to the pre'],"['in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding']",['apply the debiasing methodology in  #TAUTHOR_TAG to the pre'],"['apply the debiasing methodology in  #TAUTHOR_TAG to the pretrained embedddings.', 'debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific.', 'the definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e. g., he - she, and motherfather.', 'in our setting, there are 10 such pairs.', 'the gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space.', 'we use the same methodology for growing a seed set of gender specific words into a larger set as described in  #TAUTHOR_TAG, and end up with 486 manually curated gender specific words, including e. g., farfar ( paternal grandfather ), tvillingsystrar ( twin sisters ), and matriark ( matriarch ).', 'the definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs ( i. e. w 1 − w 2 ), and then factorizing the mean - centered difference vectors using pca, retaining only the first principal component, which will act as the gender direction.', 'the vector space is then hard debiased 1 in the sense that the gen - der direction b is removed from the embeddings of all non - gender specific words w using orthogonal projection : w = w − b × w · b b · b.', 'the approach described by  #TAUTHOR_TAG includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.', 'the equality set is application specific, and since the current investigation of swedish language embeddings does not naturally lend itself to include an equality set, the debiasing of the embeddings does not involve equalization in our case.', 'we apply the method described above to all pretrained embeddings in table 3, as well as to the token vectors generated by elmo and bert.', 'although it is not clear whether the proposed debiasing method is applicable to embeddings produced by contextualized language models, we argue that it is reasonable to treat the contextualized models as black boxes, and rely only on their output, given the proposed use case']",5
"['of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are']","['of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are']","['of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are extracted for']","['far, we have shown that all swedish pretrained embeddings included in this study exhibit some degree of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are extracted for the original and debiased embeddings, and human evaluators are used to asses the number of appropriate and stereotypical pairs in the respective representations.', ' #AUTHOR_TAG b ) used 10 crowdworkers to classify the analogy pairs as being appropriate or stereotypical.', '']",5
"['of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are']","['of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are']","['of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are extracted for']","['far, we have shown that all swedish pretrained embeddings included in this study exhibit some degree of gender bias when applied to a real - world scenario.', 'we now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of  #TAUTHOR_TAG.', 'in this setting, a number of analogy pairs are extracted for the original and debiased embeddings, and human evaluators are used to asses the number of appropriate and stereotypical pairs in the respective representations.', ' #AUTHOR_TAG b ) used 10 crowdworkers to classify the analogy pairs as being appropriate or stereotypical.', '']",4
"[' #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied']","['tasks with promising results, e. g.  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied']","['where many tasks are aimed at discovering novel information.', 'recent work has applied such models to various tasks with promising results, e. g.  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied the basic model of this class, the dirichlet process mixture model (']","['non - parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data.', 'this property is particularly interesting for nlp where many tasks are aimed at discovering novel information.', 'recent work has applied such models to various tasks with promising results, e. g.  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied the basic model of this class, the dirichlet process mixture model ( dpmm ), to lexical - semantic verb clustering with encouraging results.', 'the task involves discovering classes of verbs similar in terms of their syntactic - semantic properties ( e. g. motion class for travel, walk, run, etc. ).', 'such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling.', ' #AUTHOR_TAG although some fixed classifications are available these are not comprehensive and are inadequate for specific domains.', 'furthermore,  #TAUTHOR_TAG used a constrained version of the dpmm in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand.', 'this supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used.', 'like other forms of supervision, these constraints require manual annotation and it is important to maximize the benefits obtained from it.', 'therefore it is natural to consider active learning  #AUTHOR_TAG in order to focus the supervision on clusterings on which the model is uncertain.', 'in this work, we propose a simple yet effective active learning method employing uncertainty based sampling.', 'the effectiveness of the al method is demonstrated on two datasets, one of which has multiple gold standards']",0
"[' #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied']","['tasks with promising results, e. g.  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied']","['where many tasks are aimed at discovering novel information.', 'recent work has applied such models to various tasks with promising results, e. g.  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied the basic model of this class, the dirichlet process mixture model (']","['non - parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data.', 'this property is particularly interesting for nlp where many tasks are aimed at discovering novel information.', 'recent work has applied such models to various tasks with promising results, e. g.  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #TAUTHOR_TAG applied the basic model of this class, the dirichlet process mixture model ( dpmm ), to lexical - semantic verb clustering with encouraging results.', 'the task involves discovering classes of verbs similar in terms of their syntactic - semantic properties ( e. g. motion class for travel, walk, run, etc. ).', 'such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling.', ' #AUTHOR_TAG although some fixed classifications are available these are not comprehensive and are inadequate for specific domains.', 'furthermore,  #TAUTHOR_TAG used a constrained version of the dpmm in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand.', 'this supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used.', 'like other forms of supervision, these constraints require manual annotation and it is important to maximize the benefits obtained from it.', 'therefore it is natural to consider active learning  #AUTHOR_TAG in order to focus the supervision on clusterings on which the model is uncertain.', 'in this work, we propose a simple yet effective active learning method employing uncertainty based sampling.', 'the effectiveness of the al method is demonstrated on two datasets, one of which has multiple gold standards']",0
"['different ones.', 'following  #TAUTHOR_TAG, for each instance that does not belong to a linked - group, the sampler is restricted to choose components that']","['different ones.', 'following  #TAUTHOR_TAG, for each instance that does not belong to a linked - group, the sampler is restricted to choose components that']","['different ones.', 'following  #TAUTHOR_TAG, for each instance that does not belong to a linked - group, the sampler is restricted to choose components that do not contain instances cannot - linked with']","['dpmms, the parameters of each component are generated by a dirichlet process ( dp ) which can be seen as a distribution over distributions.', 'each instance, represented by its features, is generated by the component it is assigned to.', 'the components discovered correspond to the clusters.', 'the prior probability of assigning an instance to a particular component is proportionate to the number of instances already assigned to it, in other words, the dpmm exhibits the "" rich get richer "" property.', 'a popular metaphor to describe the dpmm which exhibits an equivalent clustering property is the chinese restaurant process ( crp ).', 'customers ( instances ) arrive at a chinese restaurant which has an infinite number of tables ( components ).', 'each customer sits at one of the tables that is either occupied or vacant with popular tables attracting more customers.', ' #AUTHOR_TAG, parameter estimation is performed using gibbs sampling by sampling the assignment z i of each instance x i given all the others z −i and the data x :', 'in eq. 1 p ( z i = z | z −i ) is the crp prior and p ( x i | z i = z, x −i ) is the distribution that generates instance x i given it has been assigned to component z. this sampling scheme is possible because the assignments in the model are exchangeable, i. e. their order is not relevant.', 'the constrained version of the dpmm uses pairwise constraints over instances in order to adapt the clustering discovered.', ' #AUTHOR_TAG, a pair of instances is either linked together ( must - link ) or not ( cannotlink ).', 'for example, charge and run should form a must - link if the aim is to cluster motion verbs together, but they should form a cannot - link if we are interested in bill verbs.', 'all links are assumed to be consistent with each other.', 'in order to incorporate the constraints in the dpmm, the gibbs sampling scheme is modified so that mustlinked instances are generated by the same component and cannot - linked instances always by different ones.', 'following  #TAUTHOR_TAG, for each instance that does not belong to a linked - group, the sampler is restricted to choose components that do not contain instances cannot - linked with it.', 'for instances in a linked - group, their assignment is sampled jointly, again taking into account their cannot - links.', 'this is performed by adding each instance of the linked - group successively to the same component.', 'in terms of the crp metaphor, customers connected with must - links arrive at the restaurant and choose a table jointly, respecting their cannot - links with other customers']",5
"['- measure  #AUTHOR_TAG and v - beta  #TAUTHOR_TAG.', '']","['substantially.', 'we evaluate our results using three information theoretic measures : variation of information ( meila, 2007 ), v - measure  #AUTHOR_TAG and v - beta  #TAUTHOR_TAG.', '']","['- measure  #AUTHOR_TAG and v - beta  #TAUTHOR_TAG.', '']","['our experiments we used two verb clustering datasets, one from general english  #AUTHOR_TAG and one from the biomedical domain  #AUTHOR_TAG.', 'in both datasets the features for each verb are its subcategorization frames ( scfs ) which capture the syntactic context in which it occurs.', 'they were acquired automatically using a domain - independent statistical parsing toolkit, rasp  #AUTHOR_TAG, and a classifier which identifies verbal scfs.', 'as a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument - adjunct distinction.', ""the general english dataset contains 204 verbs belonging to 17 fine - grained classes in levin's  #AUTHOR_TAG taxonomy so that each class contains 12 verbs."", 'the biomedical dataset consists of 193 medium to high frequency verbs from a corpus of 2230 full - text articles from 3 biomedical journals.', 'a team of linguists and biologists created a three - level gold standard with 16, 34 and 50 classes.', 'both datasets were pre - processed using non - negative matrix factorization  #AUTHOR_TAG which decomposes a large sparse matrix into two dense matrices ( of lower dimensionality ) with non - negative values.', 'in all experiments 35 dimensions were kept.', 'preliminary experiments with different number of dimensions kept did not affect the performance substantially.', 'we evaluate our results using three information theoretic measures : variation of information ( meila, 2007 ), v - measure  #AUTHOR_TAG and v - beta  #TAUTHOR_TAG.', 'all three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness.', 'homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering.', 'completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard.', 'v - beta balances these properties explicitly by taking into account the ratio of the number of cluster discovered over the number of classes in the gold standard.', 'while an ideal clustering should have both properties, naively improving one of them can be harmful for the other.', 'compared to the more commonly used f - measure  #AUTHOR_TAG, these measures have the advantage that they do not assume a mapping between clusters and classes']",5
"[' #TAUTHOR_TAG, we applied a dual rnn in']","[' #TAUTHOR_TAG, we applied a dual rnn in']","['hidden feature representations for both the modality.', 'these features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker.', 'experimental evidence shows the potential of the approach.', 'in our previous work  #TAUTHOR_TAG, we applied a dual rnn in']","['', 'in [ 5 ], frame and supra - segmental level features ( such as pitch and spectral contours ) are derived from the speech signal.', 'textual information is used by spotting keywords that emphases the emotional states of the speaker.', 'the work in [ 6 ] also presents an approach to exploit the acoustic and lexical content.', 'in particular, they explored conventional acoustic features from the speech signal while the textual information is derived from the bag of word representation.', 'recently, deep neural network ( dnn ) has shown to provide good results for modeling acoustic and textual information for emotion identification.', 'in [ 5 ], textual and acoustic information of the utterance are used by a dnn to obtain hidden feature representations for both the modality.', 'these features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker.', 'experimental evidence shows the potential of the approach.', 'in our previous work  #TAUTHOR_TAG, we applied a dual rnn in order to obtain a richer representation by blending the content and acoustic knowledge.', 'in this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework.', 'the proposed attention mechanism is trained to exploit both textual and acoustic information in tandem.', 'we refer to this attention method as the multi - hop.', 'the multi - hop attention is designed to select relevant parts of the textual data, which']",0
"['5 ] identified emotional key phrases and salience of verbal cues from both phoneme sequences and words.', 'recently,  #TAUTHOR_TAG 18 ] combined acoustic information and conversation transcripts using a neural']","['[ 16, 17 ].', 'multi - modal approaches using acoustic features and textual information have been investigated.', '[ 5 ] identified emotional key phrases and salience of verbal cues from both phoneme sequences and words.', 'recently,  #TAUTHOR_TAG 18 ] combined acoustic information and conversation transcripts using a neural network - based model to']","['5 ] identified emotional key phrases and salience of verbal cues from both phoneme sequences and words.', 'recently,  #TAUTHOR_TAG 18 ] combined acoustic information and conversation transcripts using a neural network - based model to']","['', 'more complex models such as [ 15 ] were designed to better learn nonlinear decision boundaries of emotional speech and achieved the best - recorded performance in audio modality models on iemocap dataset [ 8 ].', 'several neural network models with attention mechanism have been proposed to efficiently focus on a prominent part of speech and learn temporal dependency within whole utterance [ 16, 17 ].', 'multi - modal approaches using acoustic features and textual information have been investigated.', '[ 5 ] identified emotional key phrases and salience of verbal cues from both phoneme sequences and words.', 'recently,  #TAUTHOR_TAG 18 ] combined acoustic information and conversation transcripts using a neural network - based model to improve emotion classification accuracy.', 'however, none of these studies utilized attention method over audio and text modality in tandem for contextual understanding of the emotion in audio recording']",0
"['modality  #TAUTHOR_TAG 21 ].', 'as opposed to this']","['modality  #TAUTHOR_TAG 21 ].', 'as opposed to this approach,']","['proposed mha model.', 'previous research used multi - modal information independently using neural network model by concatenating features from each modality  #TAUTHOR_TAG 21 ].', 'as opposed to this approach,']","['propose a novel multi - hop attention method to predict the importance of audio and text, referred to multi - hop attention ( mha ).', 'figure 1 shows the architecture of the proposed mha model.', 'previous research used multi - modal information independently using neural network model by concatenating features from each modality  #TAUTHOR_TAG 21 ].', 'as opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content ( and vice - versa ).', 'first, the acoustic and textual data are encoded with the audio - bre and text - bre, respectively, using equation ( 1 ).', '']",0
"[' #TAUTHOR_TAG, we applied a dual rnn in']","[' #TAUTHOR_TAG, we applied a dual rnn in']","['hidden feature representations for both the modality.', 'these features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker.', 'experimental evidence shows the potential of the approach.', 'in our previous work  #TAUTHOR_TAG, we applied a dual rnn in']","['', 'in [ 5 ], frame and supra - segmental level features ( such as pitch and spectral contours ) are derived from the speech signal.', 'textual information is used by spotting keywords that emphases the emotional states of the speaker.', 'the work in [ 6 ] also presents an approach to exploit the acoustic and lexical content.', 'in particular, they explored conventional acoustic features from the speech signal while the textual information is derived from the bag of word representation.', 'recently, deep neural network ( dnn ) has shown to provide good results for modeling acoustic and textual information for emotion identification.', 'in [ 5 ], textual and acoustic information of the utterance are used by a dnn to obtain hidden feature representations for both the modality.', 'these features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker.', 'experimental evidence shows the potential of the approach.', 'in our previous work  #TAUTHOR_TAG, we applied a dual rnn in order to obtain a richer representation by blending the content and acoustic knowledge.', 'in this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework.', 'the proposed attention mechanism is trained to exploit both textual and acoustic information in tandem.', 'we refer to this attention method as the multi - hop.', 'the multi - hop attention is designed to select relevant parts of the textual data, which']",6
"['this research is extended work from previous research  #TAUTHOR_TAG, we use the']","['this research is extended work from previous research  #TAUTHOR_TAG, we use the']","['this research is extended work from previous research  #TAUTHOR_TAG, we use the same feature extraction method as done in our']","['this research is extended work from previous research  #TAUTHOR_TAG, we use the same feature extraction method as done in our previous work.', 'after extracting 40 - dimensional mel - frequency cepstral coefficients ( mfcc ) feature ( frame size is set to 25 ms at a rate of 10 ms with the hamming window ) using kaldi [ 22 ], we concatenate it with its first, second order derivates, making the feature dimension to 120.', 'we also extract prosodic features by using opensmile toolkit [ 23 ] and appending it to the audio feature vector.', 'in preparing the textual dataset, we first use the ground - truth transcripts of the iemocap dataset.', 'in a practical scenario where we may not access to transcripts of the audio, we obtain all of the transcripts from the speech signal using a commercial asr system [ 24 ] ( the performance of the asr system is word error rate ( wer ) of 5. 53 % ).', 'we apply word - tokenizer to the transcripts and obtain sequential data for textual input.', 'the maximum length of an audio segment is set to 750 based on the implementation choices presented in [ 25 ] and 128 for the textual input which covers the maximum length of the tokenized transcripts.', 'we minimize the cross - entropy loss function using ( equation ( 2 ) ) the adam optimizer [ 26 ] with a learning rate of 1e - 3 and gradients clipped with a norm value of 1.', 'for the purposes of regularization, we apply the dropout method, 30 %.', 'the number of hidden units and the number of layers in the rnn for each model ( bre and mha ) are optimized on the development set']",6
"['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train']","['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train']","['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train a recurrent encoder to predict the categorical class of']","['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train a recurrent encoder to predict the categorical class of a given audio signal.', 'to model the sequential nature of the speech signal, we use a bidirectional recurrent encoder ( bre ) as shown in the figure 1 ( a ).', 'we also added a residual connection to the model for promoting convergence during training [ 20 ].', 'a sequence of feature vectors is fed as input to the bre, which leads to the formation of hidden states of the model as given by the following equation :', 'where f θ, f θ are the forward and backward long short - term memory ( lstm ) with weight parameter θ, ht represents the hidden state at t - th time step, and xt represents the t - th mfcc features in audio signal.', 'the hidden representations ( − → h t, ← − h t ) from forward / backward lstms are concatenated for produce the feature, ot.', 'to follow previous research  #TAUTHOR_TAG, we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o a t.', '']",1
"['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train']","['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train']","['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train a recurrent encoder to predict the categorical class of']","['by the architecture used in  #TAUTHOR_TAG 17, 19 ], we train a recurrent encoder to predict the categorical class of a given audio signal.', 'to model the sequential nature of the speech signal, we use a bidirectional recurrent encoder ( bre ) as shown in the figure 1 ( a ).', 'we also added a residual connection to the model for promoting convergence during training [ 20 ].', 'a sequence of feature vectors is fed as input to the bre, which leads to the formation of hidden states of the model as given by the following equation :', 'where f θ, f θ are the forward and backward long short - term memory ( lstm ) with weight parameter θ, ht represents the hidden state at t - th time step, and xt represents the t - th mfcc features in audio signal.', 'the hidden representations ( − → h t, ← − h t ) from forward / backward lstms are concatenated for produce the feature, ot.', 'to follow previous research  #TAUTHOR_TAG, we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o a t.', '']",5
"['works  #TAUTHOR_TAG 18 ],']","['works  #TAUTHOR_TAG 18 ],']","['consistent comparison with previous works  #TAUTHOR_TAG 18 ], all utterances labeled "" excitement "" are merged with']","['train and evaluate our model, we use the interactive emotional dyadic motion capture ( iemocap ) [ 8 ] dataset, which includes five sessions of utterances between two speakers ( one male and one female ).', 'total 10 unique speakers participated in this work.', 'for consistent comparison with previous works  #TAUTHOR_TAG 18 ], all utterances labeled "" excitement "" are merged with those labeled "" happiness "".', 'we assign single categorical emotion to the utterance with majority of annotators agreed on the emotion labels.', '']",5
"['as neutral class, supporting the claims of  #TAUTHOR_TAG 25 ].']","['neutral class, supporting the claims of  #TAUTHOR_TAG 25 ].']","['class, supporting the claims of  #TAUTHOR_TAG 25 ].']","['', '##2 ) and 4. 6 % ( 0. 765 to 0. 730 ) relative in wa, receptively', '. even with the erroneous transcripts ( wer = 5. 53 % ), however, the proposed approach table 1. model performance comparisons. the top 2 bestperforming models ( according to the wa ) are marked in bold. the "" - asr "" models', 'use asr - processed transcripts from the google cloud speech api. the "" a "" and "" t "" in modality indicate "" audio "" and "" text ""', ', receptively. ( mha - 2 - asr ) outperforms the best baseline system ( mdre ) by 1. 6 % relative ( 0. 718 to 0. 730 ) in terms of wa.', 'figure 2 shows the confusion matrices of the proposed systems. in audio - bre ( fig. 2 ( a ) ), most of the emotion', 'labels are frequently misclassified as neutral class, supporting the claims of  #TAUTHOR_TAG 25 ]. the text - bre shows improvement in classifying most of the labels in fig. 2 ( b ). in', '']",5
"['modality  #TAUTHOR_TAG 21 ].', 'as opposed to this']","['modality  #TAUTHOR_TAG 21 ].', 'as opposed to this approach,']","['proposed mha model.', 'previous research used multi - modal information independently using neural network model by concatenating features from each modality  #TAUTHOR_TAG 21 ].', 'as opposed to this approach,']","['propose a novel multi - hop attention method to predict the importance of audio and text, referred to multi - hop attention ( mha ).', 'figure 1 shows the architecture of the proposed mha model.', 'previous research used multi - modal information independently using neural network model by concatenating features from each modality  #TAUTHOR_TAG 21 ].', 'as opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content ( and vice - versa ).', 'first, the acoustic and textual data are encoded with the audio - bre and text - bre, respectively, using equation ( 1 ).', '']",4
"['as neutral class, supporting the claims of  #TAUTHOR_TAG 25 ].']","['neutral class, supporting the claims of  #TAUTHOR_TAG 25 ].']","['class, supporting the claims of  #TAUTHOR_TAG 25 ].']","['', '##2 ) and 4. 6 % ( 0. 765 to 0. 730 ) relative in wa, receptively', '. even with the erroneous transcripts ( wer = 5. 53 % ), however, the proposed approach table 1. model performance comparisons. the top 2 bestperforming models ( according to the wa ) are marked in bold. the "" - asr "" models', 'use asr - processed transcripts from the google cloud speech api. the "" a "" and "" t "" in modality indicate "" audio "" and "" text ""', ', receptively. ( mha - 2 - asr ) outperforms the best baseline system ( mdre ) by 1. 6 % relative ( 0. 718 to 0. 730 ) in terms of wa.', 'figure 2 shows the confusion matrices of the proposed systems. in audio - bre ( fig. 2 ( a ) ), most of the emotion', 'labels are frequently misclassified as neutral class, supporting the claims of  #TAUTHOR_TAG 25 ]. the text - bre shows improvement in classifying most of the labels in fig. 2 ( b ). in', '']",3
"['this work, we use the datasets released by  #TAUTHOR_TAG and heot dataset']","['this work, we use the datasets released by  #TAUTHOR_TAG and heot dataset']","['this work, we use the datasets released by  #TAUTHOR_TAG and heot dataset']","['this work, we use the datasets released by  #TAUTHOR_TAG and heot dataset provided by ( mathur et al. 2018 ).', 'the datasets obtained pass through these steps of processing : ( i ) removal of punctuatios, stopwords, urls, numbers, emoticons, etc.', 'this was then followed by transliteration using the xlit - crowd conversion dictionary 3 and translation of each word to english using hindi to english dictionary 4.', 'to deal with the spelling variations, we manually added some common variations of popular hinglish words.', 'final dictionary comprised of 7200 word pairs.', 'additionally, to deal with profane words, which are not present in xlit - crowd, we had to make a profanity dictionary ( with 209 profane words ) as well.', 'table 1 gives some examples from the dictionary']",5
"['training embeddings for the processed tweets.', 'the embeddings were trained on both the datasets provided by  #TAUTHOR_TAG and heot.', 'these embeddings help to']","['training embeddings for the processed tweets.', 'the embeddings were trained on both the datasets provided by  #TAUTHOR_TAG and heot.', 'these embeddings help to']","['training embeddings for the processed tweets.', 'the embeddings were trained on both the datasets provided by  #TAUTHOR_TAG and heot.', 'these embeddings help to']","['tried glove ( pennington, socher, and manning 2014 ) and twitter word2vec ( godin et al. 2015 ) code for training embeddings for the processed tweets.', 'the embeddings were trained on both the datasets provided by  #TAUTHOR_TAG and heot.', 'these embeddings help to learn distributed representations of tweets.', 'after experimentation, we kept the size of embeddings fixed to 100']",5
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",5
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",5
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",5
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",5
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",0
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",4
"['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model']","['the heot and  #TAUTHOR_TAG datasets contain tweets which are annotated in three categories : offensive, abusive and none ( or benign ).', 'some examples from the dataset are shown in table 2.', 'we use a lstm based classifier model for training our model to classify these tweets into these three categories.', 'an overview of the model is given in the figure 1.', 'the model consists of one layer of lstm followed by three dense layers.', 'the lstm layer uses a dropout value of 0. 2.', 'categorical crossentropy loss was used for the last layer due to the presence of multiple classes.', 'we use adam optimizer along with l2 regularisation to prevent overfitting.', 'as indicated by the figure 1, the model was initially trained on the dataset provided by  #TAUTHOR_TAG, and then re - trained on the heot dataset so as to benefit from the transfer of learned features in the last stage.', 'the model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.', 'results table 3 shows the performance of our model ( after getting trained on  #TAUTHOR_TAG on the heot dataset averaged over three runs.', 'we also compare results on pre - trained embeddings.', 'as shown in the table, our model when given glove embeddings performs better than all other models.', 'for comparison purposes, in table 4 we have also evaluated our results on the dataset by  #TAUTHOR_TAG']",3
"['( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolck']","['( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolcke, and harper 2004,']","['over the past decade ( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolcke, and har']","['', 'this half of the book will be more enjoyable for readers of this journal, who are presumably interested in more general questions of computation and language than the step - by - step tutorial format of the first half of the book.', 'the details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high - utility context - free grammars for speech recognition.', 'the primary shortcoming of this presentation lies in perpetuating the false dichotomy between "" grammar - based "" and "" data - driven "" approaches to language modeling for speech recognition, which motivates the final chapter of the book.', ""in fact, the authors'approach is both grammar - based and data - driven, given the corpus - based grammar specialization and pcfg estimation, which the authors themselves demonstrate to be indispensable."", 'robust grammar - based language modeling is a topic that has received a fair bit of attention over the past decade ( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolcke, and harper 2004, among others ), and while this line of research has not focused on the use of manually built, narrow - domain feature grammars, there is enough similarity between the approach described in this book and the']",0
"['( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolck']","['( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolcke, and harper 2004,']","['over the past decade ( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolcke, and har']","['', 'this half of the book will be more enjoyable for readers of this journal, who are presumably interested in more general questions of computation and language than the step - by - step tutorial format of the first half of the book.', 'the details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high - utility context - free grammars for speech recognition.', 'the primary shortcoming of this presentation lies in perpetuating the false dichotomy between "" grammar - based "" and "" data - driven "" approaches to language modeling for speech recognition, which motivates the final chapter of the book.', ""in fact, the authors'approach is both grammar - based and data - driven, given the corpus - based grammar specialization and pcfg estimation, which the authors themselves demonstrate to be indispensable."", 'robust grammar - based language modeling is a topic that has received a fair bit of attention over the past decade ( chelba and jelinek 2000 ;  #TAUTHOR_TAG ; roark 2001 ; wang, stolcke, and harper 2004, among others ), and while this line of research has not focused on the use of manually built, narrow - domain feature grammars, there is enough similarity between the approach described in this book and the']",3
"['while filtering noise, via domain adaptation  #AUTHOR_TAG classifying connectives  #AUTHOR_TAG or multi - task learning  #TAUTHOR_TAG, and shows promising']","['syndata while filtering noise, via domain adaptation  #AUTHOR_TAG classifying connectives  #AUTHOR_TAG or multi - task learning  #TAUTHOR_TAG, and shows promising']","['while filtering noise, via domain adaptation  #AUTHOR_TAG classifying connectives  #AUTHOR_TAG or multi - task learning  #TAUTHOR_TAG, and shows promising']","['', ' #AUTHOR_TAG attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data.', 'these data are usually called as syn - * corresponding author.', 'thetic implicit data ( hereafter syndata ).', ' #AUTHOR_TAG argue that syndata has two drawbacks : 1 ) meaning shifts in some cases when removing connectives, and 2 ) a different word distribution with the real implicit data.', 'they also show that using syndata directly degrades the performance.', 'recent work seeks to derive valuable information from syndata while filtering noise, via domain adaptation  #AUTHOR_TAG classifying connectives  #AUTHOR_TAG or multi - task learning  #TAUTHOR_TAG, and shows promising results.', 'society reckon existence youth problems, but many young people think themselves no problems.', '']",0
"['', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation']","['the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['', 'the possible reason is that only while is ambiguous about comp and t emp, while as, when and since are all ambiguous about t emp and cont, among top 10 connectives in our bisyndata.', 'meanwhile the amount of labeled data for comp is relatively small.', 'overall, using bisyndata under our multi - task model achieves significant improvements on the english drr imp.', 'we believe the reasons for the improvements are twofold : 1 ) the added synthetic english instances from our bisyndata can alleviate the meaning shift problem, and 2 ) a multi - task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.', 'considering some of the english connectives ( e. g., while ) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives.', 'specifically, we first discard as, when, while and since in top 20 connectives, and get 22, 999 synthetic instances.', 'then, we leverage these instances in two different ways : 1 ) using them in our multi - task model as above, and 2 ) using them as additional training data directly after mapping unambiguous connectives into relations.', 'both methods using only unambiguous connectives do not achieve better performance.', 'one possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.', 'we also compare m t n bi with recent systems using additional training data.', ' #AUTHOR_TAG select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation classification on the labeled rst corpus  #AUTHOR_TAG, which defines different discourse relations with that in the pdtb.', 'the results are shown in table 3.', 'although  #TAUTHOR_TAG achieve the stateof - the - art performance ( line 5 )']",0
"['', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation']","['the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['', 'the possible reason is that only while is ambiguous about comp and t emp, while as, when and since are all ambiguous about t emp and cont, among top 10 connectives in our bisyndata.', 'meanwhile the amount of labeled data for comp is relatively small.', 'overall, using bisyndata under our multi - task model achieves significant improvements on the english drr imp.', 'we believe the reasons for the improvements are twofold : 1 ) the added synthetic english instances from our bisyndata can alleviate the meaning shift problem, and 2 ) a multi - task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.', 'considering some of the english connectives ( e. g., while ) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives.', 'specifically, we first discard as, when, while and since in top 20 connectives, and get 22, 999 synthetic instances.', 'then, we leverage these instances in two different ways : 1 ) using them in our multi - task model as above, and 2 ) using them as additional training data directly after mapping unambiguous connectives into relations.', 'both methods using only unambiguous connectives do not achieve better performance.', 'one possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.', 'we also compare m t n bi with recent systems using additional training data.', ' #AUTHOR_TAG select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation classification on the labeled rst corpus  #AUTHOR_TAG, which defines different discourse relations with that in the pdtb.', 'the results are shown in table 3.', 'although  #TAUTHOR_TAG achieve the stateof - the - art performance ( line 5 )']",0
"['', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit']","['directly.', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit']","['from explicit instances via connective classification, which are used as additional training data directly.', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit data using multi - task learning models and gain improvements.', 'different from']","['line of research related to drr imp tries to take advantage of explicit discourse data.', ' #AUTHOR_TAG predict the absent connectives based on a language model.', 'using these predicted connectives as features is proven to be helpful.', 'biran and mc  #AUTHOR_TAG aggregate word - pair features that are collected around the same connectives, which can effectively alleviate the feature sparsity problem.', 'more recently,  #AUTHOR_TAG and consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them.', ' #AUTHOR_TAG propose to gather weakly labeled data from explicit instances via connective classification, which are used as additional training data directly.', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit data using multi - task learning models and gain improvements.', 'different from all the above work, we construct additional training data from a bilingual corpus.', 'multi - task neural networks have been successfully used for many nlp tasks.', 'for example,  #AUTHOR_TAG jointly train models for the partof - speech tagging, chunking, named entity recognition and semantic role labeling using convolutional network.', ' #AUTHOR_TAG successfully combine the tasks of query classification and ranking for web search using a deep multi - task neural network.', ' #AUTHOR_TAG explore multi - task sequence to sequence learning for constituency parsing, image caption generation and machine translation']",0
"['in our experiments. following  #TAUTHOR_TAG, we alternate']","['in our experiments. following  #TAUTHOR_TAG, we alternately use two tasks to train', 'the model, one']","['efficient on both memory and computational cost. in addition, the simplicity of m t n allows us to focus on measuring the quality of bisyndata. we use the cross -', 'entropy loss function and minibatch adagrad  #AUTHOR_TAG to optimize parameters. pre - trained word embeddings are fixed. we find that fine - tuning word embeddings during training leads to severe over', '##fitting in our experiments. following  #TAUTHOR_TAG, we alternately use two tasks to train', 'the model, one task per epoch. for tasks on']","['', 'simply averages embeddings of words to represent arguments, as v arg 1 and v arg 2. these two vectors are then concatenated and transformed through two non - linear hidden layers. finally, the corresponding sof tmax layer is used to perform classification. m t n', 'ignores the word order in arguments and uses two hidden layers to capture the interactions between two arguments. the idea behind', 'm t n is borrowed from  #AUTHOR_TAG, where a deep averaging network achieves close to the state - ofthe - art performance on', 'text classification. though m t n is simple, it is easy', 'to train and efficient on both memory and computational cost. in addition, the simplicity of m t n allows us to focus on measuring the quality of bisyndata. we use the cross -', 'entropy loss function and minibatch adagrad  #AUTHOR_TAG to optimize parameters. pre - trained word embeddings are fixed. we find that fine - tuning word embeddings during training leads to severe over', '##fitting in our experiments. following  #TAUTHOR_TAG, we alternately use two tasks to train', 'the model, one task per epoch. for tasks on both the', 'pdtb and cdtb, we use the same hyper - parameters. the dimension of word embedding is 100. we', 'set the size of l 2 to 200, and l 3 to 100. relu is used as the non - linear function. different learning rates 0.', '005 and 0. 001 are used in the main and auxiliary tasks, respectively. to avoid over', '##fitting, we randomly drop out 20 % words in each argument following  #AUTHOR_TAG. all hyper - parameters are tuned on', 'the development set']",5
"['', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation']","['the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['', 'the possible reason is that only while is ambiguous about comp and t emp, while as, when and since are all ambiguous about t emp and cont, among top 10 connectives in our bisyndata.', 'meanwhile the amount of labeled data for comp is relatively small.', 'overall, using bisyndata under our multi - task model achieves significant improvements on the english drr imp.', 'we believe the reasons for the improvements are twofold : 1 ) the added synthetic english instances from our bisyndata can alleviate the meaning shift problem, and 2 ) a multi - task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.', 'considering some of the english connectives ( e. g., while ) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives.', 'specifically, we first discard as, when, while and since in top 20 connectives, and get 22, 999 synthetic instances.', 'then, we leverage these instances in two different ways : 1 ) using them in our multi - task model as above, and 2 ) using them as additional training data directly after mapping unambiguous connectives into relations.', 'both methods using only unambiguous connectives do not achieve better performance.', 'one possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.', 'we also compare m t n bi with recent systems using additional training data.', ' #AUTHOR_TAG select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation classification on the labeled rst corpus  #AUTHOR_TAG, which defines different discourse relations with that in the pdtb.', 'the results are shown in table 3.', 'although  #TAUTHOR_TAG achieve the stateof - the - art performance ( line 5 )']",1
"['', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation']","['the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks']","['', 'the possible reason is that only while is ambiguous about comp and t emp, while as, when and since are all ambiguous about t emp and cont, among top 10 connectives in our bisyndata.', 'meanwhile the amount of labeled data for comp is relatively small.', 'overall, using bisyndata under our multi - task model achieves significant improvements on the english drr imp.', 'we believe the reasons for the improvements are twofold : 1 ) the added synthetic english instances from our bisyndata can alleviate the meaning shift problem, and 2 ) a multi - task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.', 'considering some of the english connectives ( e. g., while ) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives.', 'specifically, we first discard as, when, while and since in top 20 connectives, and get 22, 999 synthetic instances.', 'then, we leverage these instances in two different ways : 1 ) using them in our multi - task model as above, and 2 ) using them as additional training data directly after mapping unambiguous connectives into relations.', 'both methods using only unambiguous connectives do not achieve better performance.', 'one possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.', 'we also compare m t n bi with recent systems using additional training data.', ' #AUTHOR_TAG select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data.', ' #TAUTHOR_TAG use a multi - task model with three auxiliary tasks : 1 ) conn : connective classification on explicit instances, 2 ) exp : relation classification on the labeled explicit instances in the pdtb, and 3 ) rst : relation classification on the labeled rst corpus  #AUTHOR_TAG, which defines different discourse relations with that in the pdtb.', 'the results are shown in table 3.', 'although  #TAUTHOR_TAG achieve the stateof - the - art performance ( line 5 )']",4
"['', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit']","['directly.', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit']","['from explicit instances via connective classification, which are used as additional training data directly.', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit data using multi - task learning models and gain improvements.', 'different from']","['line of research related to drr imp tries to take advantage of explicit discourse data.', ' #AUTHOR_TAG predict the absent connectives based on a language model.', 'using these predicted connectives as features is proven to be helpful.', 'biran and mc  #AUTHOR_TAG aggregate word - pair features that are collected around the same connectives, which can effectively alleviate the feature sparsity problem.', 'more recently,  #AUTHOR_TAG and consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them.', ' #AUTHOR_TAG propose to gather weakly labeled data from explicit instances via connective classification, which are used as additional training data directly.', ' #AUTHOR_TAG and  #TAUTHOR_TAG combine explicit and implicit data using multi - task learning models and gain improvements.', 'different from all the above work, we construct additional training data from a bilingual corpus.', 'multi - task neural networks have been successfully used for many nlp tasks.', 'for example,  #AUTHOR_TAG jointly train models for the partof - speech tagging, chunking, named entity recognition and semantic role labeling using convolutional network.', ' #AUTHOR_TAG successfully combine the tasks of query classification and ranking for web search using a deep multi - task neural network.', ' #AUTHOR_TAG explore multi - task sequence to sequence learning for constituency parsing, image caption generation and machine translation']",4
"[';  #TAUTHOR_TAG.', 'moreover,']","['2009 ;  #TAUTHOR_TAG.', 'moreover,']","[' #TAUTHOR_TAG.', 'moreover,']","['', 'therefore, many approaches have been proposed to learn word segmentation suitable for smt.', 'these approaches were either complicated  #AUTHOR_TAG, or of high computational complexity ( chung and gildea 2009 ;  #TAUTHOR_TAG.', '']",0
"['as word packing  #AUTHOR_TAG and pseudo - word  #TAUTHOR_TAG.', 'as character could preserve more meanings than word in chinese, it']","['as word packing  #AUTHOR_TAG and pseudo - word  #TAUTHOR_TAG.', 'as character could preserve more meanings than word in chinese, it']","['as word packing  #AUTHOR_TAG and pseudo - word  #TAUTHOR_TAG.', 'as character could preserve more meanings than word in chinese, it seems that a character']","['', 'as can be seen, ccorpus has less ut and utp than wcorpus, i. e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in statistical modeling  #AUTHOR_TAG.', 'another advantage of character alignment is the reduction in alignment errors caused by word seg - table 2 # ut and # utp in ccorpus and wcorpus mentation errors.', 'for example, "" [UNK] [UNK] ( cheney ) "" and "" [UNK] ( will ) "" are wrongly merged into one word [UNK] [UNK] [UNK] by the word segmenter, and [UNK] [UNK] [UNK] wrongly aligns to a comma in english sentence in the word alignment ; however, both [UNK] and [UNK] align to "" cheney "" correctly in the character alignment.', 'however, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing  #AUTHOR_TAG and pseudo - word  #TAUTHOR_TAG.', 'as character could preserve more meanings than word in chinese, it seems that a character can be wrongly aligned to many english words by the aligner.', 'however, we found this can be avoided to a great extent by the basic features ( co - occurrence and distortion ) used by many alignment models.', 'for example, we observed that']",0
"['by  #TAUTHOR_TAG.', 'comparing']","['by  #TAUTHOR_TAG.', 'comparing']","['by  #TAUTHOR_TAG.', 'comparing']","['first evaluate the alignment quality.', 'the method discussed in section 3 was used to compare character and word alignment.', 'as can be seen from table 3, the systems using character as wsa outperformed the ones using word as wsa in both small - scale ( row 3 - 5 ) and large - scale task ( row 6 - 8 ) with all segmentations.', 'this gain can be attributed to the small vocabulary size ( sparsity ) for character alignment.', 'the observation is consistent with  #AUTHOR_TAG which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing wsa and wsr.', 'we then evaluated the translation performance.', 'the baselines are fully word - based mt systems ( wordsys ), i. e. using word as both wsa and wsr, and fully character - based systems ( charsys ).', 'table word table 4 translation evaluation of wordsys and proposed system using bleu - sbp  #AUTHOR_TAG 4 compares wordsys to our proposed system.', 'significant testing was carried out using bootstrap re - sampling method proposed by  #AUTHOR_TAG with a 95 % confidence level.', 'we see that our proposed systems outperformed wordsys in all segmentation specifications settings.', 'table 5 lists the results of charsys in small - scale task.', 'in this setting, we gradually set the phrase length and the distortion limits of the phrase - based decoder ( context size ) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as wsr for fair comparison with wordsys as suggested by  #TAUTHOR_TAG.', '']",5
"['squad  #TAUTHOR_TAG.', 'however,']","['squad  #TAUTHOR_TAG.', 'however,']","['squad  #TAUTHOR_TAG.', 'however,']",[' #TAUTHOR_TAG'],0
"['1 ).', ' #TAUTHOR_TAG conducted experiments with a german translation']","['1 ).', ' #TAUTHOR_TAG conducted experiments with a german translation']","['evaluation of ss or sr have been created so far ( see table 1 ).', ' #TAUTHOR_TAG conducted experiments with a german translation']","['german datasets for evaluation of ss or sr have been created so far ( see table 1 ).', ' #TAUTHOR_TAG conducted experiments with a german translation of an english dataset  #AUTHOR_TAG, but argued that the dataset ( gur65 ) is too small ( it contains only 65 noun pairs ), and does not model sr.', 'thus, she created a german dataset containing 350 word pairs ( gur350 ) containing nouns, verbs and adjectives that are connected by classical and non - classical relations  #AUTHOR_TAG.', 'however, the dataset is biased towards strong classical relations, as word pairs were manually selected.', ' #AUTHOR_TAG semi - automatically created word pairs from domain - specific corpora.', 'the resulting zg222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations.', 'hence, it is particularly suited for analyzing the capability of a measure to estimate sr']",0
"['dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for']","['dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for']","['2 gives an overview of our experimental results on three german datasets. best values for each dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for the les measure,', 'we give the results for considering : ( i ) only the first paragraph ( + first ) and ( ii ) the']","['', 'category pairs', '. table 2 gives an overview of our experimental results on three german datasets. best values for each dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for the les measure,', 'we give the results for considering : ( i ) only the first paragraph ( + first ) and ( ii ) the full text (', '+ full ). for the path length based measures', ', we give the values for averaging over all category pairs ( + avg ), or taking the best sr', 'value computed among the pairs ( + best ). for each dataset, we', ""report pearson's correlation r with human judgments on pairs that are found in both resources ( both ). otherwise, the results would not be comparable. we additionally use"", 'a subset containing only noun - noun pairs ( both nn ). this comparison is fairer, because article titles in wikipedia are usually nouns', '. table 2 also gives the inter annotator agreement for each subset. it constitutes an upper bound of a measure', ""' s performance""]",0
"['dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for']","['dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for']","['2 gives an overview of our experimental results on three german datasets. best values for each dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for the les measure,', 'we give the results for considering : ( i ) only the first paragraph ( + first ) and ( ii ) the']","['', 'category pairs', '. table 2 gives an overview of our experimental results on three german datasets. best values for each dataset and knowledge source are in bold. we use the p g measure in optimal configuration', 'as reported by  #TAUTHOR_TAG. for the les measure,', 'we give the results for considering : ( i ) only the first paragraph ( + first ) and ( ii ) the full text (', '+ full ). for the path length based measures', ', we give the values for averaging over all category pairs ( + avg ), or taking the best sr', 'value computed among the pairs ( + best ). for each dataset, we', ""report pearson's correlation r with human judgments on pairs that are found in both resources ( both ). otherwise, the results would not be comparable. we additionally use"", 'a subset containing only noun - noun pairs ( both nn ). this comparison is fairer, because article titles in wikipedia are usually nouns', '. table 2 also gives the inter annotator agreement for each subset. it constitutes an upper bound of a measure', ""' s performance""]",5
"['published by  #TAUTHOR_TAG, ranging from']","['published by  #TAUTHOR_TAG, ranging from 0. 69 - 0. 75.', 'for gur350, the']","['published by  #TAUTHOR_TAG, ranging from']","['results on gur65 using germanet are very close to those published by  #TAUTHOR_TAG, ranging from 0. 69 - 0. 75.', 'for gur350, the performance drops to 0. 38 - 0. 50, due to the lower upper bound, and because germanet does not model sr well.', 'these findings are endorsed by an even more significant performance drop on zg222.', 'the measures based on wikipedia behave less uniformly.', 'les yields acceptable results on gur350, but is generally not among the best performing measures.', 'lc + avg yields the best performance on gur65, but is outperformed on the other datasets by p l + best, which performs equally good for all datasets.', 'if we compare germanet based and wikipedia based measures, we find that the knowledge source has a major influence on performance.', '']",3
['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],"['', '( ir', '##vine and callison -  #AUTHOR_TAG or adding an auxiliary autoencoding task on monolingual data  #AUTHOR_TAG. however, both methods have problems with low', '##resource and syntactically divergent language pairs. back translation assumes enough data to create a functional nmt system, an unrealistic requirement in low - resource scenarios, while autoencoding target sentences by definition will not be able to learn source', '- target word reordering. this paper proposes a method to create pseudoparallel sentences', 'for nmt for language pairs with divergent syntactic structures. prior to nmt, word reordering was a major challenge for statistical machine translation ( smt ), and many techniques have emerged over the years to address', 'this challenge ( xia and mc  #AUTHOR_TAG. importantly, even simple heuristic reordering methods with a few handcre', '##ated rules have been shown to be highly effective in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules usually function solely in high - resourced languages such as english with', 'high - quality synguages, but limited success on real low - resource settings and syntactically divergent language pairs  #AUTHOR_TAG guzman et al., 2019 ). hence we', 'focus on semi - supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. however, similar pre - ordering methods have not proven useful', 'in nmt  #AUTHOR_TAG, largely because high - resource scenarios nm', '##t is much more effective at learning reordering than previous smt methods were  #AUTHOR_TAG. however, in low - resource scenarios it is less realistic to', '']",5
['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],"['', '( ir', '##vine and callison -  #AUTHOR_TAG or adding an auxiliary autoencoding task on monolingual data  #AUTHOR_TAG. however, both methods have problems with low', '##resource and syntactically divergent language pairs. back translation assumes enough data to create a functional nmt system, an unrealistic requirement in low - resource scenarios, while autoencoding target sentences by definition will not be able to learn source', '- target word reordering. this paper proposes a method to create pseudoparallel sentences', 'for nmt for language pairs with divergent syntactic structures. prior to nmt, word reordering was a major challenge for statistical machine translation ( smt ), and many techniques have emerged over the years to address', 'this challenge ( xia and mc  #AUTHOR_TAG. importantly, even simple heuristic reordering methods with a few handcre', '##ated rules have been shown to be highly effective in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules usually function solely in high - resourced languages such as english with', 'high - quality synguages, but limited success on real low - resource settings and syntactically divergent language pairs  #AUTHOR_TAG guzman et al., 2019 ). hence we', 'focus on semi - supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. however, similar pre - ordering methods have not proven useful', 'in nmt  #AUTHOR_TAG, largely because high - resource scenarios nm', '##t is much more effective at learning reordering than previous smt methods were  #AUTHOR_TAG. however, in low - resource scenarios it is less realistic to', '']",5
[' #TAUTHOR_TAG or learning from aligned parallel'],[' #TAUTHOR_TAG or learning from aligned parallel'],"['on linguistic knowledge  #TAUTHOR_TAG or learning from aligned parallel data ( xia and mc  #AUTHOR_TAG, and in principle our pseudo - corpus creation paradigm is compatible with any of these methods.', 'specifically, in this work we utilize']","['', 'instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in smt.', 'reordering can be done either using rules based on linguistic knowledge  #TAUTHOR_TAG or learning from aligned parallel data ( xia and mc  #AUTHOR_TAG, and in principle our pseudo - corpus creation paradigm is compatible with any of these methods.', 'specifically, in this work we utilize rule - based methods, as our goal is to improve translation of low - resource languages, where large quantities of high - quality parallel data do not exist and we posit that current data - driven reordering methods are unlikely to function well.', '']",5
[' #TAUTHOR_TAG or learning from aligned parallel'],[' #TAUTHOR_TAG or learning from aligned parallel'],"['on linguistic knowledge  #TAUTHOR_TAG or learning from aligned parallel data ( xia and mc  #AUTHOR_TAG, and in principle our pseudo - corpus creation paradigm is compatible with any of these methods.', 'specifically, in this work we utilize']","['', 'instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in smt.', 'reordering can be done either using rules based on linguistic knowledge  #TAUTHOR_TAG or learning from aligned parallel data ( xia and mc  #AUTHOR_TAG, and in principle our pseudo - corpus creation paradigm is compatible with any of these methods.', 'specifically, in this work we utilize rule - based methods, as our goal is to improve translation of low - resource languages, where large quantities of high - quality parallel data do not exist and we posit that current data - driven reordering methods are unlikely to function well.', '']",5
"['decoder.', 'as noted above, we use hf  #TAUTHOR_TAG as our']","['decoder.', 'as noted above, we use hf  #TAUTHOR_TAG as our re - ordering rule.', 'hf was designed for transforming english into japanese order, but we use it as - is for the uyghur - english pair as well to demonstrate that simple, linguistically motivated rules']","['in the decoder.', 'as noted above, we use hf  #TAUTHOR_TAG as our']","['both language pairs, we use an attentionbased encoder - decoder nmt model with a onelayer bidirectional lstm as the encoder and onelayer uni - directional lstm as the decoder.', '4 embeddings and lstm states were set to 300 and 256 dimensions respectively.', 'target word embeddings are shared with the softmax weight matrix in the decoder.', 'as noted above, we use hf  #TAUTHOR_TAG as our re - ordering rule.', 'hf was designed for transforming english into japanese order, but we use it as - is for the uyghur - english pair as well to demonstrate that simple, linguistically motivated rules can generalize across pairs with similar syntax with little or no modification.', 'further details regarding the experimental settings are in the supplementary material']",5
['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],['in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules'],"['', '( ir', '##vine and callison -  #AUTHOR_TAG or adding an auxiliary autoencoding task on monolingual data  #AUTHOR_TAG. however, both methods have problems with low', '##resource and syntactically divergent language pairs. back translation assumes enough data to create a functional nmt system, an unrealistic requirement in low - resource scenarios, while autoencoding target sentences by definition will not be able to learn source', '- target word reordering. this paper proposes a method to create pseudoparallel sentences', 'for nmt for language pairs with divergent syntactic structures. prior to nmt, word reordering was a major challenge for statistical machine translation ( smt ), and many techniques have emerged over the years to address', 'this challenge ( xia and mc  #AUTHOR_TAG. importantly, even simple heuristic reordering methods with a few handcre', '##ated rules have been shown to be highly effective in closing syntactic gaps (  #AUTHOR_TAG ;  #TAUTHOR_TAG ; fig. 1 ). because these rules usually function solely in high - resourced languages such as english with', 'high - quality synguages, but limited success on real low - resource settings and syntactically divergent language pairs  #AUTHOR_TAG guzman et al., 2019 ). hence we', 'focus on semi - supervised methods in this paper. tactic analysis tools, a linguist with rudimentary knowledge of the structure of the target language can create them in short order using these tools. however, similar pre - ordering methods have not proven useful', 'in nmt  #AUTHOR_TAG, largely because high - resource scenarios nm', '##t is much more effective at learning reordering than previous smt methods were  #AUTHOR_TAG. however, in low - resource scenarios it is less realistic to', '']",0
[' #TAUTHOR_TAG or learning from aligned parallel'],[' #TAUTHOR_TAG or learning from aligned parallel'],"['on linguistic knowledge  #TAUTHOR_TAG or learning from aligned parallel data ( xia and mc  #AUTHOR_TAG, and in principle our pseudo - corpus creation paradigm is compatible with any of these methods.', 'specifically, in this work we utilize']","['', 'instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in smt.', 'reordering can be done either using rules based on linguistic knowledge  #TAUTHOR_TAG or learning from aligned parallel data ( xia and mc  #AUTHOR_TAG, and in principle our pseudo - corpus creation paradigm is compatible with any of these methods.', 'specifically, in this work we utilize rule - based methods, as our goal is to improve translation of low - resource languages, where large quantities of high - quality parallel data do not exist and we posit that current data - driven reordering methods are unlikely to function well.', '']",0
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG.', 'these accuracy gains come with marginal computational costs']",[' #TAUTHOR_TAG'],6
['follows  #TAUTHOR_TAG. parses are'],"['best parses from n parsers, then follows  #TAUTHOR_TAG. parses are weighted by the estimated', 'probabilities from the parser']","['##s from n parsers, then follows  #TAUTHOR_TAG. parses are']","['', 'a parser as if they are 1 - best parses from n parsers, then follows  #TAUTHOR_TAG. parses are weighted by the estimated', 'probabilities from the parser. given n trees and their weights, the model computes a constituent', '']",6
['follows  #TAUTHOR_TAG. parses are'],"['best parses from n parsers, then follows  #TAUTHOR_TAG. parses are weighted by the estimated', 'probabilities from the parser']","['##s from n parsers, then follows  #TAUTHOR_TAG. parses are']","['', 'a parser as if they are 1 - best parses from n parsers, then follows  #TAUTHOR_TAG. parses are weighted by the estimated', 'probabilities from the parser. given n trees and their weights, the model computes a constituent', '']",0
"['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '. approach ( 1 )']","['', ' #AUTHOR_TAG where novel predic - tions from model combination ( stacking ) were worse than baseline performance. the difference is that novel predictions with fusion better incorporate', 'model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. preliminary extensions : here, we summarize two extensions to fusion which have yet to show benefits. the first extension explores applying fusion to dependency parsing. we explored two ways to apply fusion when starting from constituency parses : ( 1 ) fuse constituents and then convert them to dependencies and', '( 2 ) convert to dependencies then fuse the dependencies as in  #TAUTHOR_TAG', '']",0
['follows  #TAUTHOR_TAG. parses are'],"['best parses from n parsers, then follows  #TAUTHOR_TAG. parses are weighted by the estimated', 'probabilities from the parser']","['##s from n parsers, then follows  #TAUTHOR_TAG. parses are']","['', 'a parser as if they are 1 - best parses from n parsers, then follows  #TAUTHOR_TAG. parses are weighted by the estimated', 'probabilities from the parser. given n trees and their weights, the model computes a constituent', '']",5
"['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '. approach ( 1 )']","['', ' #AUTHOR_TAG where novel predic - tions from model combination ( stacking ) were worse than baseline performance. the difference is that novel predictions with fusion better incorporate', 'model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. preliminary extensions : here, we summarize two extensions to fusion which have yet to show benefits. the first extension explores applying fusion to dependency parsing. we explored two ways to apply fusion when starting from constituency parses : ( 1 ) fuse constituents and then convert them to dependencies and', '( 2 ) convert to dependencies then fuse the dependencies as in  #TAUTHOR_TAG', '']",5
"['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '. approach ( 1 )']","['', ' #AUTHOR_TAG where novel predic - tions from model combination ( stacking ) were worse than baseline performance. the difference is that novel predictions with fusion better incorporate', 'model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. preliminary extensions : here, we summarize two extensions to fusion which have yet to show benefits. the first extension explores applying fusion to dependency parsing. we explored two ways to apply fusion when starting from constituency parses : ( 1 ) fuse constituents and then convert them to dependencies and', '( 2 ) convert to dependencies then fuse the dependencies as in  #TAUTHOR_TAG', '']",5
"['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '. approach ( 1 )']","['', ' #AUTHOR_TAG where novel predic - tions from model combination ( stacking ) were worse than baseline performance. the difference is that novel predictions with fusion better incorporate', 'model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. preliminary extensions : here, we summarize two extensions to fusion which have yet to show benefits. the first extension explores applying fusion to dependency parsing. we explored two ways to apply fusion when starting from constituency parses : ( 1 ) fuse constituents and then convert them to dependencies and', '( 2 ) convert to dependencies then fuse the dependencies as in  #TAUTHOR_TAG', '']",3
"['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '.']","['dependencies as in  #TAUTHOR_TAG', '. approach ( 1 )']","['', ' #AUTHOR_TAG where novel predic - tions from model combination ( stacking ) were worse than baseline performance. the difference is that novel predictions with fusion better incorporate', 'model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. preliminary extensions : here, we summarize two extensions to fusion which have yet to show benefits. the first extension explores applying fusion to dependency parsing. we explored two ways to apply fusion when starting from constituency parses : ( 1 ) fuse constituents and then convert them to dependencies and', '( 2 ) convert to dependencies then fuse the dependencies as in  #TAUTHOR_TAG', '']",3
"[', memory augmented models have been proposed  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG and  #AUTHOR_TAG attended to retrieval models, lacking']","['based reasoning, memory augmented models have been proposed  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG and  #AUTHOR_TAG attended to retrieval models, lacking']","[', memory augmented models have been proposed  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG and  #AUTHOR_TAG attended to retrieval models, lacking']","['', 'compared with traditional pipeline solutions  #AUTHOR_TAG, end - to - end approaches recently gain much attention  #AUTHOR_TAG a ;  #AUTHOR_TAG, because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand - crafted state labels.', 'to effectively incorporate kb information and perform knowledge - * corresponding author based reasoning, memory augmented models have been proposed  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG and  #AUTHOR_TAG attended to retrieval models, lacking the ability of generation, while others incorporated the memory ( i. e. end - to - end memory networks, abbreviated as memn  #AUTHOR_TAG ) and copy mechanism  #AUTHOR_TAG into a sequential generative architecture.', 'however, most models tended to confound the dialog history with kb tuples and simply stored them into one memory.', 'a shared memory forces the memory reader to reason over the two different types of data, which makes the task harder, especially when the memory is large.', 'to explore this problem,  #AUTHOR_TAG very recently proposed to separate memories for modeling dialog context and kb results.', '']",0
"['s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in']","['s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in']","['s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in the dialog history']","['symbols are defined in table 1, and more details can be found in the supplementary material.', 'we omit the subscript e or s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel  #TAUTHOR_TAG x x = { x1,..., xn, $ }, the dialog history y y = { y1, · · ·, ym }, the expected response bi one kb tuple, actually the corresponding entity b b = { b1, · · ·, b l, $ }, the kb tuples p t re = { ptre, 1, · · ·, ptre, m }, dialog pointer index set.', 'p t re supervised information for copying words in dialog history p t rs = { ptrs, 1, · · ·, ptrs, m }, kb pointer index set.', 'p t rs supervised information for copying entities in kb tuples table 1 : notation table. where xb z ∈ x or b is the dialog history or kb tuples according to the subscript ( e or s ) and n xb + 1 is the sentinel position index as n xb is equal to the dialog history length n or the number of kb triples l. the idea behind eq. 1 is that we can obtain the positions of where to copy by matching the target text with the dialog history or kb information.', 'furthermore, we hope this provides the model with an accurate guidance of how to activate the two long - term memories']",5
"['s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in']","['s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in']","['s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in the dialog history']","['symbols are defined in table 1, and more details can be found in the supplementary material.', 'we omit the subscript e or s 2, following  #TAUTHOR_TAG to define each pointer index set :', 'symbol definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel  #TAUTHOR_TAG x x = { x1,..., xn, $ }, the dialog history y y = { y1, · · ·, ym }, the expected response bi one kb tuple, actually the corresponding entity b b = { b1, · · ·, b l, $ }, the kb tuples p t re = { ptre, 1, · · ·, ptre, m }, dialog pointer index set.', 'p t re supervised information for copying words in dialog history p t rs = { ptrs, 1, · · ·, ptrs, m }, kb pointer index set.', 'p t rs supervised information for copying entities in kb tuples table 1 : notation table. where xb z ∈ x or b is the dialog history or kb tuples according to the subscript ( e or s ) and n xb + 1 is the sentinel position index as n xb is equal to the dialog history length n or the number of kb triples l. the idea behind eq. 1 is that we can obtain the positions of where to copy by matching the target text with the dialog history or kb information.', 'furthermore, we hope this provides the model with an accurate guidance of how to activate the two long - term memories']",5
['incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ('],"['incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ( subject, relation, object ) representation of kb information']",['incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ('],"['', 'we also incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ( subject, relation, object ) representation of kb information as  #AUTHOR_TAG b ).', 'more details can be found in the supplementary material.', 'having written dialog history and kb tuples into e - memnn and s - memnn, we then use the wm to interact with them ( to query and reason over them ) to generate the response.', 'at each decoder step, the attn - ctrl, instantiated as a gru, dynamically generates the query vector q t as follows :', '']",5
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",5
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",5
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",5
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",5
['incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ('],"['incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ( subject, relation, object ) representation of kb information']",['incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ('],"['', 'we also incorporate additional temporal information and speaker information into dialog utterances as  #TAUTHOR_TAG and adopt a ( subject, relation, object ) representation of kb information as  #AUTHOR_TAG b ).', 'more details can be found in the supplementary material.', 'having written dialog history and kb tuples into e - memnn and s - memnn, we then use the wm to interact with them ( to query and reason over them ) to generate the response.', 'at each decoder step, the attn - ctrl, instantiated as a gru, dynamically generates the query vector q t as follows :', '']",6
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",6
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",4
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",4
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",4
"['with less parameters than', ' #TAUTHOR_TAG on task 5.']","['with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between']","[', as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5.']","['conclude that the separation of context memory and kb memory benefits the performance, as wmm2seq performs well with less parameters than', ' #TAUTHOR_TAG on task 5. finally, we additionally analysis how the multi - hop attention mechanism helps by showing the performance differences between the hop k = 1 and the default hop k = 3. though multi - hop attention strengthens the reasoning ability and improves the results,', 'we find that the performance difference between the hops k = 1 and k = 3 is', '']",4
"[', has been widely studied in recent years  #TAUTHOR_TAG.', '']","['answering, has been widely studied in recent years  #TAUTHOR_TAG.', '']","['has been widely studied in recent years  #TAUTHOR_TAG.', '']",[' #TAUTHOR_TAG'],0
"[', has been widely studied in recent years  #TAUTHOR_TAG.', '']","['answering, has been widely studied in recent years  #TAUTHOR_TAG.', '']","['has been widely studied in recent years  #TAUTHOR_TAG.', '']",[' #TAUTHOR_TAG'],0
"['and relations through learning of an embedding space for representing all relations and question words  #TAUTHOR_TAG, in which each']","['and relations through learning of an embedding space for representing all relations and question words  #TAUTHOR_TAG, in which each']","['and relations through learning of an embedding space for representing all relations and question words  #TAUTHOR_TAG, in which each relation']","['paradigm in proposed approaches for relation extraction in kbqa is based on semantic parsing in which questions were parsed and turned into logical forms in order to query the knowledge base  #AUTHOR_TAG.', 'however, most of the recent approaches  #AUTHOR_TAG are based on automatically extracted features of terms ; thanks to the prominent performance of neural network on representation learning  #AUTHOR_TAG a, b ).', 'from another point of view, two mainstreams for extracting relations in kbqa are studied : ( a ) using a classifier which chooses the most probable relation among all  #AUTHOR_TAG ; ( b ) matching questions and relations through learning of an embedding space for representing all relations and question words  #TAUTHOR_TAG, in which each relation is considered either as a meaningful sequence of words or as a unique entity.', ' #AUTHOR_TAG considered the relation prediction, as well as the whole kbqa problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention.', 'to this aim, they']",0
"['works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark']","['works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark dataset of the simple question answering, namely simplequestions,']","['the previous works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark dataset of the simple question answering, namely simplequestions, which was originally introduced by  #AUTHOR_TAG.', 'this dataset contains 108442 questions gathered with the help of english - speaking annotators.', ' #TAUTHOR_TAG proposed a new benchmark for evaluating relation extraction task on simplequestion.', 'in this benchmark, every question, whose entity is replaced by a unique token, is labeled with its ground truth relation as its positive label, and all other relation of the gold entity that is mentioned in the question are considered as negative labels.', 'we use the same dataset which contains 72239, 10310 and 20610 question samples as train, validation, and test sets respectively']",0
"[' #TAUTHOR_TAG', 'is an']","[' #TAUTHOR_TAG', 'is an']","[', ampcnn  #TAUTHOR_TAG', 'is an']","['( 3 ) two - channel lexical and semantic text matching. the results of these experiments are reported in table 2. as can be seen', 'in the results, the model accuracy ( % ) arc i  #AUTHOR_TAG 89. 35 arc ii  #AUTHOR_TAG 88. 44 matchpyramid 91', '. 75 bimpm  #AUTHOR_TAG 90. 73 impact', 'of adding an extra input channel is obvious as it is compared with single one. the better', 'performance of the semantic channel shows the importance of semantic text matching in the qa system. the further improvement', 'using an additional channel for lexical match indicates that although lexical match is considered implicitly in the normal matchpyramid model, it is not enough for considering this', ""issue in the qa task and the proposed two - channel model can better cover both semantic and lexical similarities. in the next step of our experiments, we added the q'- r network to the q'- q network and evaluated the new combined architecture,"", 'presented in figure 1, on the same dataset. table', '3 reports the performance of our model on classifying the relations in comparison with the state - of - the - art models. in this', 'table, ampcnn  #TAUTHOR_TAG', 'is an attentive max - pooling cnn for matching a question with all relations. apcnn ( dos  #AUTHOR_TAG and abc', '##nn  #TAUTHOR_TAG both employ an attentive pooling mechanism. these two models model accuracy ( % ) amp', '']",0
"[', following  #TAUTHOR_TAG in its place, so that we will have a question pool in which']","['words.', 'in this regard, following  #TAUTHOR_TAG in its place, so that we will have a question pool in which']","['', 'in this regard, following  #TAUTHOR_TAG in its place, so that we will have a question pool in which']","['', 'in this regard, following  #TAUTHOR_TAG in its place, so that we will have a question pool in which each question is labeled with its relation that can be considered as a paraphrase of that question.', '']",5
"['works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark']","['works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark dataset of the simple question answering, namely simplequestions,']","['the previous works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark dataset of the simple question answering, namely simplequestions, which was originally introduced by  #AUTHOR_TAG.', 'this dataset contains 108442 questions gathered with the help of english - speaking annotators.', ' #TAUTHOR_TAG proposed a new benchmark for evaluating relation extraction task on simplequestion.', 'in this benchmark, every question, whose entity is replaced by a unique token, is labeled with its ground truth relation as its positive label, and all other relation of the gold entity that is mentioned in the question are considered as negative labels.', 'we use the same dataset which contains 72239, 10310 and 20610 question samples as train, validation, and test sets respectively']",5
"['works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark']","['works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark dataset of the simple question answering, namely simplequestions,']","['the previous works by  #TAUTHOR_TAG and  #AUTHOR_TAG, we use the common benchmark dataset of the simple question answering, namely simplequestions, which was originally introduced by  #AUTHOR_TAG.', 'this dataset contains 108442 questions gathered with the help of english - speaking annotators.', ' #TAUTHOR_TAG proposed a new benchmark for evaluating relation extraction task on simplequestion.', 'in this benchmark, every question, whose entity is replaced by a unique token, is labeled with its ground truth relation as its positive label, and all other relation of the gold entity that is mentioned in the question are considered as negative labels.', 'we use the same dataset which contains 72239, 10310 and 20610 question samples as train, validation, and test sets respectively']",5
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering (']","[' #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering (']","['keyphrases for a given document refer to a group of phrases that represent the document.', 'although we often come across texts from different domains such as scientific papers, news articles and blogs, which are labeled with keyphrases by the authors, a large portion of the web content remains untagged.', 'while keyphrases are excellent means for providing a concise summary of a document, recent research results have suggested that the task of automatically identifying keyphrases from a document is by no means trivial.', 'researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction.', 'supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ;,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'a disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains.', 'unsupervised approaches could be a viable alternative in this regard.', 'the unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling ( e. g.,  #AUTHOR_TAG ), graph - based ranking ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG b ) ).', 'while these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue.', 'worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated']",0
"[', textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering']","['recently proposed systems, namely, textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering - based']","['recently proposed systems, namely, textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering - based approach  #AUTHOR_TAG b ).', 'since']","['', 'is there any system that can generalize fairly well across various domains?', 'we seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions.', 'more specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics.', 'these algorithms represent the ma - jor directions in this research area, including tfidf and four recently proposed systems, namely, textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering - based approach  #AUTHOR_TAG b ).', 'since none of these systems ( except textrank ) are publicly available, we reimplement all of them and make them freely available for research purposes.', '1 to our knowledge, this is the first attempt to compare the performance of state - of - the - art unsupervised keyphrase extraction systems on multiple datasets']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['', 'we report results on all 308 articles in our evaluation.', 'the inspec dataset is a collection of 2, 000 abstracts from journal papers including the paper title.', 'each document has two sets of keyphrases assigned by the indexers : the controlled keyphrases, which are keyphrases that appear in the inspec thesaurus ; and the uncontrolled keyphrases, which do not necessarily appear in the thesaurus.', 'this is a relatively popular dataset for automatic keyphrase extraction, as it was first used by  #AUTHOR_TAG and later by  #TAUTHOR_TAG and  #AUTHOR_TAG b ).', 'in our evaluation, we use the set of 500 abstracts designated by these previous approaches as the test set and its set of uncontrolled keyphrases.', 'note that the average document length for this dataset is the smallest among all our datasets.', 'the nus keyphrase corpus  #AUTHOR_TAG includes 211 scientific conference papers with lengths between 4 to 12 pages.', 'each paper has one or more sets of keyphrases assigned by its authors and other annotators.', 'we use all the 211 papers in our evaluation.', 'since the number of annotators can be different for different documents and the annotators are not specified along with the annotations, we decide to take the union 1 see http : / / www. hlt. utdallas. edu / saidul / code. html for details.', 'of all the gold standard keyphrases from all the sets to construct one single set of annotation for each paper.', 'as table 1 shows, each nus paper, both in terms of the average number of tokens ( 8291 ) and candidate phrases ( 2027 ) per paper, is more than five times larger than any document from any other corpus.', 'hence, the number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four.', 'finally, the icsi meeting corpus  #AUTHOR_TAG, which is annotated by  #AUTHOR_TAG a ), includes 161 meeting transcriptions.', ""following liu et al., we remove topic segments marked as'chitchat'and'digit'from the dataset and use all the remaining segments for evaluation."", 'each transcript contains three sets of keyphrases produced by the same three human annotators.', 'since it is possible to associate each set of keyphrases with its annotator']",0
"[', verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in']","['partof - speech tags ( e. g., nouns, adjectives, verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in']","[', nouns, adjectives, verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in']","['1 : candidate lexical unit selection the first step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics.', 'commonly used heuristics include ( 1 ) using a stop word list to remove non - keywords ( e. g.,  #AUTHOR_TAG b ) ) and ( 2 ) allowing words with certain partof - speech tags ( e. g., nouns, adjectives, verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in all of our experiments, we follow  #AUTHOR_TAG and select as candidates words with the following penn treebank tags : nn, nns, nnp, nnps, and jj, which are obtained using the stanford pos tagger  #AUTHOR_TAG.', 'table 1 : corpus statistics for the four datasets used in this paper.', 'a candidate word / phrase, typically a sequence of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase.', 'the u / b / t / o distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other higher order n - grams.', 'step 2 : lexical unit ranking once the candidate list is generated, the next task is to rank these lexical units.', 'to accomplish this, it is necessary to build a representation of the input text for the ranking algorithm.', '']",0
"[', verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in']","['partof - speech tags ( e. g., nouns, adjectives, verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in']","[', nouns, adjectives, verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in']","['1 : candidate lexical unit selection the first step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics.', 'commonly used heuristics include ( 1 ) using a stop word list to remove non - keywords ( e. g.,  #AUTHOR_TAG b ) ) and ( 2 ) allowing words with certain partof - speech tags ( e. g., nouns, adjectives, verbs ) to be considered candidate keywords  #TAUTHOR_TAG,  #AUTHOR_TAG a ),  #AUTHOR_TAG ).', 'in all of our experiments, we follow  #AUTHOR_TAG and select as candidates words with the following penn treebank tags : nn, nns, nnp, nnps, and jj, which are obtained using the stanford pos tagger  #AUTHOR_TAG.', 'table 1 : corpus statistics for the four datasets used in this paper.', 'a candidate word / phrase, typically a sequence of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase.', 'the u / b / t / o distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other higher order n - grams.', 'step 2 : lexical unit ranking once the candidate list is generated, the next task is to rank these lexical units.', 'to accomplish this, it is necessary to build a representation of the input text for the ranking algorithm.', '']",0
"['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each']","['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each']","['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each vertex corresponds']","['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each vertex corresponds to a word type.', 'a weight, w ij, is assigned to the edge connecting the two vertices, v i and v j, and its value is the number of times the corresponding word types co - occur within a window of w words in the associated text.', '']",0
"['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each']","['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each']","['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each vertex corresponds']","['the textrank algorithm  #TAUTHOR_TAG, a text is represented by a graph.', 'each vertex corresponds to a word type.', 'a weight, w ij, is assigned to the edge connecting the two vertices, v i and v j, and its value is the number of times the corresponding word types co - occur within a window of w words in the associated text.', '']",0
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering (']","[' #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering (']","['keyphrases for a given document refer to a group of phrases that represent the document.', 'although we often come across texts from different domains such as scientific papers, news articles and blogs, which are labeled with keyphrases by the authors, a large portion of the web content remains untagged.', 'while keyphrases are excellent means for providing a concise summary of a document, recent research results have suggested that the task of automatically identifying keyphrases from a document is by no means trivial.', 'researchers have explored both supervised and unsupervised techniques to address the problem of automatic keyphrase extraction.', 'supervised methods typically recast this problem as a binary classification task, where a model is trained on annotated data to determine whether a given phrase is a keyphrase or not ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG ;,  #AUTHOR_TAG,  #AUTHOR_TAG ).', 'a disadvantage of supervised approaches is that they require a lot of training data and yet show bias towards the domain on which they are trained, undermining their ability to generalize well to new domains.', 'unsupervised approaches could be a viable alternative in this regard.', 'the unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling ( e. g.,  #AUTHOR_TAG ), graph - based ranking ( e. g.,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG a ) ), and clustering ( e. g.,  #AUTHOR_TAG,  #AUTHOR_TAG b ) ).', 'while these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue.', 'worse still, each of them is based on a set of assumptions, which may only hold for the dataset on which they are evaluated']",1
"[', textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering']","['recently proposed systems, namely, textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering - based']","['recently proposed systems, namely, textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering - based approach  #AUTHOR_TAG b ).', 'since']","['', 'is there any system that can generalize fairly well across various domains?', 'we seek to gain a better understanding of the state of the art in unsupervised keyphrase extraction by examining the aforementioned questions.', 'more specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics.', 'these algorithms represent the ma - jor directions in this research area, including tfidf and four recently proposed systems, namely, textrank  #TAUTHOR_TAG, singlerank  #AUTHOR_TAG, expandrank  #AUTHOR_TAG, and a clustering - based approach  #AUTHOR_TAG b ).', 'since none of these systems ( except textrank ) are publicly available, we reimplement all of them and make them freely available for research purposes.', '1 to our knowledge, this is the first attempt to compare the performance of state - of - the - art unsupervised keyphrase extraction systems on multiple datasets']",5
"['##rank and singlerank setup following  #TAUTHOR_TAG and  #AUTHOR_TAG, we set the co - occurrence window size for textrank and singlerank to 2 and 10, respectively, as these parameter values have']","['##rank and singlerank setup following  #TAUTHOR_TAG and  #AUTHOR_TAG, we set the co - occurrence window size for textrank and singlerank to 2 and 10, respectively, as these parameter values have']","['##rank and singlerank setup following  #TAUTHOR_TAG and  #AUTHOR_TAG, we set the co - occurrence window size for textrank and singlerank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets.', 'expandrank setup  #AUTHOR_TAG, we find']","['##rank and singlerank setup following  #TAUTHOR_TAG and  #AUTHOR_TAG, we set the co - occurrence window size for textrank and singlerank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets.', 'expandrank setup  #AUTHOR_TAG, we find the 5 nearest neighbors for each document from the remaining documents in the same corpus.', 'the other parameters are set in the same way as in singlerank.', 'keycluster setup as argued by  #AUTHOR_TAG b ), wikipedia - based relatedness is computationally expensive to compute.', 'as a result, we follow them by computing the co - occurrence - based relatedness instead, using a window of size 10.', 'then, we cluster the candidate words using spectral clustering, and use the frequent word list that they generously provided us to post - process the resulting keyphrases by filtering out those that are frequent unigrams']",5
['recall of 100. while  #TAUTHOR_TAG and'],"[', none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphras']","['as a result, none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphras']","['##phrase extraction evaluations. in inspec, not all gold - standard keyphrases appear in their associated document, and as a result, none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphrases in our evaluation,  #AUTHOR_TAG and liu et al.', '']",4
['recall of 100. while  #TAUTHOR_TAG and'],"[', none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphras']","['as a result, none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphras']","['##phrase extraction evaluations. in inspec, not all gold - standard keyphrases appear in their associated document, and as a result, none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphrases in our evaluation,  #AUTHOR_TAG and liu et al.', '']",3
['recall of 100. while  #TAUTHOR_TAG and'],"[', none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphras']","['as a result, none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphras']","['##phrase extraction evaluations. in inspec, not all gold - standard keyphrases appear in their associated document, and as a result, none of the five systems we', 'consider in this paper can achieve a recall of 100. while  #TAUTHOR_TAG and our reimplementations use all of these gold - standard keyphrases in our evaluation,  #AUTHOR_TAG and liu et al.', '']",3
"['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover,']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['##i is the task of identifying the different senses ( uses ) of a target word in a given text.', 'wsi is a field of significant value, because it aims to overcome the limitations originated by representing word senses as a fixed - list of dictionary definitions.', 'these limitations of hand - crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts  #AUTHOR_TAG, and the inability to reflect the exact content of the context in which a target word appears ( veronis, 2004 ).', 'given the significance of wsi, the objective assessment and comparison of wsi methods is crucial.', 'the first effort to evaluate wsi methods under a common framework ( evaluation schemes & dataset ) was undertaken in the semeval - 2007 wsi task ( swsi )  #AUTHOR_TAG, where two separate evaluation schemes were employed.', 'the first one, unsupervised evaluation, treats the wsi results as clusters of target word contexts and gold standard ( gs ) senses as classes.', 'the traditional clustering measure of f - score  #AUTHOR_TAG is used to assess the performance of wsi systems.', 'the second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to gs senses.', 'in the next step, the testing corpus is used to measure the performance of systems in a word sense disambiguation ( wsd ) setting.', 'a significant limitation of f - score is that it does not evaluate the make up of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score might also fail to evaluate clusters which are not matched to any gs class due to their small size.', 'these two limitations define the matching problem of f - score  #TAUTHOR_TAG which can lead to : ( 1 ) identical scores between different clustering solutions, and ( 2 ) inaccurate assessment of the clustering quality.', 'the supervised evaluation scheme employs a method in order to map the automatically induced clusters to gs senses.', 'as a result, this process might change the distribution of clusters by mapping more than one clusters to the same gs sense.', 'the outcome of this process might be more helpful for systems that produce a large number of clusters.', 'in this paper, we focus on analysing the semeval - 2007 wsi evaluation schemes showing their deficiencies.', 'subsequently, we present the use of v - measure  #TAUTHOR_TAG as an evaluation measure that can overcome the current limitations of f - score.', 'finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of wsd performance.', 'the proposed evaluation setting will be applied in the semev']",0
"['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover,']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['##i is the task of identifying the different senses ( uses ) of a target word in a given text.', 'wsi is a field of significant value, because it aims to overcome the limitations originated by representing word senses as a fixed - list of dictionary definitions.', 'these limitations of hand - crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts  #AUTHOR_TAG, and the inability to reflect the exact content of the context in which a target word appears ( veronis, 2004 ).', 'given the significance of wsi, the objective assessment and comparison of wsi methods is crucial.', 'the first effort to evaluate wsi methods under a common framework ( evaluation schemes & dataset ) was undertaken in the semeval - 2007 wsi task ( swsi )  #AUTHOR_TAG, where two separate evaluation schemes were employed.', 'the first one, unsupervised evaluation, treats the wsi results as clusters of target word contexts and gold standard ( gs ) senses as classes.', 'the traditional clustering measure of f - score  #AUTHOR_TAG is used to assess the performance of wsi systems.', 'the second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to gs senses.', 'in the next step, the testing corpus is used to measure the performance of systems in a word sense disambiguation ( wsd ) setting.', 'a significant limitation of f - score is that it does not evaluate the make up of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score might also fail to evaluate clusters which are not matched to any gs class due to their small size.', 'these two limitations define the matching problem of f - score  #TAUTHOR_TAG which can lead to : ( 1 ) identical scores between different clustering solutions, and ( 2 ) inaccurate assessment of the clustering quality.', 'the supervised evaluation scheme employs a method in order to map the automatically induced clusters to gs senses.', 'as a result, this process might change the distribution of clusters by mapping more than one clusters to the same gs sense.', 'the outcome of this process might be more helpful for systems that produce a large number of clusters.', 'in this paper, we focus on analysing the semeval - 2007 wsi evaluation schemes showing their deficiencies.', 'subsequently, we present the use of v - measure  #TAUTHOR_TAG as an evaluation measure that can overcome the current limitations of f - score.', 'finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of wsd performance.', 'the proposed evaluation setting will be applied in the semev']",0
"[' #TAUTHOR_TAG. the former situation is present,']","[' #TAUTHOR_TAG. the former situation is present,']","['cluster  #TAUTHOR_TAG. the former situation is present, due']","['', 'either by not evaluating the entire membership of a cluster, or by not evaluating every cluster  #TAUTHOR_TAG. the former situation is present, due to the fact that f - score does not consider the make', '- up of the clusters beyond the majority class  #TAUTHOR_TAG. for example, in', '']",0
"['sizes  #TAUTHOR_TAG.', 'formulas 4 and 5 define h ( c ) and h ( c | gs ).', 'finally h and c']","['sizes  #TAUTHOR_TAG.', 'formulas 4 and 5 define h ( c ) and h ( c | gs ).', 'finally h and c']","['sizes  #TAUTHOR_TAG.', 'formulas 4 and 5 define h ( c ) and h ( c | gs ).', 'finally h and c can be combined and']","['any other case.', 'in the worst case, completeness will be equal to 0, particularly when h ( c | gs ) is maximal and equal to h ( c ).', 'this happens when each gs class is included in all clusters with a distribution equal to the distribution of sizes  #TAUTHOR_TAG.', 'formulas 4 and 5 define h ( c ) and h ( c | gs ).', 'finally h and c can be combined and produce v - measure, which is the harmonic mean of homogeneity and completeness.', 'returning to our clustering example in table 1, its v - measure is equal to 0. 275.', 'in section 2. 3, we also presented an additional clustering ( table 3 ), which had the same f - score as the clustering in table 1, despite the fact that it intuitively had a better completeness and homogeneity.', '']",0
"['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover,']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['##i is the task of identifying the different senses ( uses ) of a target word in a given text.', 'wsi is a field of significant value, because it aims to overcome the limitations originated by representing word senses as a fixed - list of dictionary definitions.', 'these limitations of hand - crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts  #AUTHOR_TAG, and the inability to reflect the exact content of the context in which a target word appears ( veronis, 2004 ).', 'given the significance of wsi, the objective assessment and comparison of wsi methods is crucial.', 'the first effort to evaluate wsi methods under a common framework ( evaluation schemes & dataset ) was undertaken in the semeval - 2007 wsi task ( swsi )  #AUTHOR_TAG, where two separate evaluation schemes were employed.', 'the first one, unsupervised evaluation, treats the wsi results as clusters of target word contexts and gold standard ( gs ) senses as classes.', 'the traditional clustering measure of f - score  #AUTHOR_TAG is used to assess the performance of wsi systems.', 'the second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to gs senses.', 'in the next step, the testing corpus is used to measure the performance of systems in a word sense disambiguation ( wsd ) setting.', 'a significant limitation of f - score is that it does not evaluate the make up of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score might also fail to evaluate clusters which are not matched to any gs class due to their small size.', 'these two limitations define the matching problem of f - score  #TAUTHOR_TAG which can lead to : ( 1 ) identical scores between different clustering solutions, and ( 2 ) inaccurate assessment of the clustering quality.', 'the supervised evaluation scheme employs a method in order to map the automatically induced clusters to gs senses.', 'as a result, this process might change the distribution of clusters by mapping more than one clusters to the same gs sense.', 'the outcome of this process might be more helpful for systems that produce a large number of clusters.', 'in this paper, we focus on analysing the semeval - 2007 wsi evaluation schemes showing their deficiencies.', 'subsequently, we present the use of v - measure  #TAUTHOR_TAG as an evaluation measure that can overcome the current limitations of f - score.', 'finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of wsd performance.', 'the proposed evaluation setting will be applied in the semev']",4
"[' #TAUTHOR_TAG. the former situation is present,']","[' #TAUTHOR_TAG. the former situation is present,']","['cluster  #TAUTHOR_TAG. the former situation is present, due']","['', 'either by not evaluating the entire membership of a cluster, or by not evaluating every cluster  #TAUTHOR_TAG. the former situation is present, due to the fact that f - score does not consider the make', '- up of the clusters beyond the majority class  #TAUTHOR_TAG. for example, in', '']",4
"['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover,']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score']","['##i is the task of identifying the different senses ( uses ) of a target word in a given text.', 'wsi is a field of significant value, because it aims to overcome the limitations originated by representing word senses as a fixed - list of dictionary definitions.', 'these limitations of hand - crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts  #AUTHOR_TAG, and the inability to reflect the exact content of the context in which a target word appears ( veronis, 2004 ).', 'given the significance of wsi, the objective assessment and comparison of wsi methods is crucial.', 'the first effort to evaluate wsi methods under a common framework ( evaluation schemes & dataset ) was undertaken in the semeval - 2007 wsi task ( swsi )  #AUTHOR_TAG, where two separate evaluation schemes were employed.', 'the first one, unsupervised evaluation, treats the wsi results as clusters of target word contexts and gold standard ( gs ) senses as classes.', 'the traditional clustering measure of f - score  #AUTHOR_TAG is used to assess the performance of wsi systems.', 'the second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to gs senses.', 'in the next step, the testing corpus is used to measure the performance of systems in a word sense disambiguation ( wsd ) setting.', 'a significant limitation of f - score is that it does not evaluate the make up of clusters beyond the majority class  #TAUTHOR_TAG.', 'moreover, f - score might also fail to evaluate clusters which are not matched to any gs class due to their small size.', 'these two limitations define the matching problem of f - score  #TAUTHOR_TAG which can lead to : ( 1 ) identical scores between different clustering solutions, and ( 2 ) inaccurate assessment of the clustering quality.', 'the supervised evaluation scheme employs a method in order to map the automatically induced clusters to gs senses.', 'as a result, this process might change the distribution of clusters by mapping more than one clusters to the same gs sense.', 'the outcome of this process might be more helpful for systems that produce a large number of clusters.', 'in this paper, we focus on analysing the semeval - 2007 wsi evaluation schemes showing their deficiencies.', 'subsequently, we present the use of v - measure  #TAUTHOR_TAG as an evaluation measure that can overcome the current limitations of f - score.', 'finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of wsd performance.', 'the proposed evaluation setting will be applied in the semev']",6
"['. e. homogeneity and completeness  #TAUTHOR_TAG.', 'homogeneity refers to']","['to 0. 714.', 'as it can be observed, f - score assesses the quality of a clustering solution by considering two different angles, i. e. homogeneity and completeness  #TAUTHOR_TAG.', 'homogeneity refers to']","['. e. homogeneity and completeness  #TAUTHOR_TAG.', 'homogeneity refers to']","['', 'finally, the f - score of the entire clustering solution is defined as the weighted average of the f - scores of each gs sense ( formula 1 ), where q is the number of gs senses and n is the total number of target word ings 1 gs 2 gs 3 cl 1 500 100 100 cl 2 100 500 100 cl 3 100 100 500 stances.', 'if the clustering is identical to the original classes in the datasets, f - score will be equal to one.', 'in the example of table 1, f - score is equal to 0. 714.', 'as it can be observed, f - score assesses the quality of a clustering solution by considering two different angles, i. e. homogeneity and completeness  #TAUTHOR_TAG.', 'homogeneity refers to the degree that each cluster consists of data points, which primarily belong to a single gs class.', 'on the other hand, completeness refers to the degree that each gs class consists of data points, which have primarily been assigned to a single cluster.', 'a perfect homogeneity would result in a precision equal to 1, while a perfect completeness would result in a recall equal to 1.', 'purity and entropy  #AUTHOR_TAG are also used in swsi as complementary measures.', '']",3
"['quality of a clustering solution by explicitly measuring its homogeneity and its completeness  #TAUTHOR_TAG.', 'recall that homogeneity refers to']","['quality of a clustering solution by explicitly measuring its homogeneity and its completeness  #TAUTHOR_TAG.', 'recall that homogeneity refers to']","['', 'v - measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness  #TAUTHOR_TAG.', 'recall that homogeneity refers to the degree that each cluster']","['table 2, we observe that no system managed to outperform the 1c1w baseline in terms of f - score.', 'at the same time, some systems participating in swsi were able to outperform the equivalent of the 1c1w baseline ( mfs ) in the supervised evaluation.', 'for example, ubc - as achieved the best f - score close to the 1c1w baseline.', 'however, by looking at its supervised recall, we observe that it is below the mfs baseline.', 'a clustering solution, which achieves high supervised recall, does not necessarily achieve high fscore.', 'one reason for that stems from the fact that f - score penalises systems for getting the number of gs classes wrongly, as in 1c1inst baseline.', 'according to  #AUTHOR_TAG, supervised evaluation seems to be more neutral regarding the number of induced clusters, because clusters are mapped into a weighted vector of senses, and therefore inducing a number of clusters similar to the number of senses is not a requirement for good results.', 'however, a large number of clusters might also lead to an unreliable mapping of clusters to gs senses.', 'for example, high supervised recall also means high purity and low entropy as in i2r, but not vice versa as in uoy.', 'uoy produces a large number of clean clusters, in effect suffering from an unreliable mapping of clusters to senses due to the lack of adequate training data.', 'moreover, an additional supervised evaluation of wsi methods using a different dataset split resulted in a different ranking, in which all of the systems outperformed the mfs baseline  #AUTHOR_TAG.', 'this result indicates that the supervised evaluation might not provide a reliable estimation of wsd performance, particularly in the case where the mapping relies on a single dataset split.', '3 semeval - 2010 wsi evaluation setting 3. 1 unsupervised evaluation using v - measure let us assume that the dataset of a target word tw comprises of n instances ( data points ).', 'these data points are divided into two partitions, i. e. a set of automatically generated clusters c = { c j | j = 1...', 'n } and a set of gold standard classes gs = { gs i | gs = 1...', 'm }. moreover, let a ij be the number of data points, which are members of class gs i and elements of cluster c j.', 'v - measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness  #TAUTHOR_TAG.', 'recall that homogeneity refers to the degree that each cluster consists of data points which primarily belong to a single gs class.', '']",3
"['other case  #TAUTHOR_TAG.', 'symmetrically to homogeneity, completeness refers to']","['other case  #TAUTHOR_TAG.', 'symmetrically to homogeneity, completeness refers to']","['gs | c ) h ( gs ) in any other case  #TAUTHOR_TAG.', 'symmetrically to homogeneity, completeness refers to']","['there is only a single class ( h ( gs ) = 0 ), any clustering would produce a perfectly homogeneous solution.', 'in the worst case, the class distribution within each cluster is equal to the overall class distribution ( h ( gs | c ) = h ( gs ) ), i. e. clustering provides no new information.', 'overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity ( h ) of a clustering solution is 1 if there is only a single class, and 1− h ( gs | c ) h ( gs ) in any other case  #TAUTHOR_TAG.', 'symmetrically to homogeneity, completeness refers to the degree that each gs class consists of data points, which have primarily been assigned to a single cluster.', 'to evaluate completeness, v - measure examines the distribution of cluster assignments within each class.', 'the conditional entropy of the cluster given the class distribution, h ( c | gs ), quantifies the remaining entropy ( uncertainty ) of the cluster given that the class distribution is known.', 'consequently, when h ( c | gs ) is 0, we have the perfectly complete solution, since all the data points of a class belong to the same cluster.', 'therefore, symmetrically to homogeneity, the completeness c of a clustering solution is 1 if there is only a single cluster ( h ( c ) = 0 ), and 1']",3
"['sizes  #TAUTHOR_TAG.', 'this worst solution is not equivalent to']","['sizes  #TAUTHOR_TAG.', 'this worst solution is not equivalent to']","['of cluster sizes  #TAUTHOR_TAG.', 'this worst solution is not equivalent to the 1c1inst, hence completeness of 1']","['table 4, we also observe that the 1c1inst baseline achieves a high performance.', 'in nouns only i2r is able to outperform this baseline, while in verbs the 1c1inst baseline achieves the highest result.', 'by the definition of homogeneity ( section 3. 1 ), this baseline is perfectly homogeneous, since each cluster contains one instance of a single sense.', 'however, its completeness is not 0, as one might intuitively expect.', 'this is due to the fact that v - measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes  #TAUTHOR_TAG.', 'this worst solution is not equivalent to the 1c1inst, hence completeness of 1c1inst is greater than 0.', 'additionally, completeness of this baseline benefits from the fact that around 18 % of gs senses have only one instance in the test set.', 'note however, that on average this baseline achieves a lower completeness than most of the systems.', 'another observation from table 4 is that upv si and uoy have a better ranking than in table 2.', '']",3
"[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","['dirichlet allocation ( lda ) is a topic modeling technique for textual data [ 5 ] that is widely applied in software engineering  #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ] for different tasks such as requirements engineering [ 15 ], software architecture [ 10 ], source code analysis [ 9 ], defect reports [ 16 ], testing [ 14 ] and to bibliometric analysis of software engineering literature [ 11, 22 ].', 'a survey on topic modelling in software engineering has been conducted [ 24 ] and a book titled "" the art and science of analyzing software data "" [ 4 ] devoted a chapter for lda analysis [ 6 ].', 'many sources give methodological guidance on how to apply lda topic modeling in software engineering  #TAUTHOR_TAG, 3, 19 ].', 'given all this, we think it is fair to say that lda topic modelling is a relevant data analysis technique in empirical software engineering research.', 'the quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [ 13 ], perplexity of measure in the test data [ 13 ], or silhouette coefficient of resulting topics [ 19 ].', 'other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co - occurrences in publicly available texts [ 23 ], or stability which investigates similarity of topics between different runs  #TAUTHOR_TAG.', 'recently,  #TAUTHOR_TAG claimed that the instability of topics is one major shortcoming of this technique.', '']",0
"[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","['dirichlet allocation ( lda ) is a topic modeling technique for textual data [ 5 ] that is widely applied in software engineering  #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ] for different tasks such as requirements engineering [ 15 ], software architecture [ 10 ], source code analysis [ 9 ], defect reports [ 16 ], testing [ 14 ] and to bibliometric analysis of software engineering literature [ 11, 22 ].', 'a survey on topic modelling in software engineering has been conducted [ 24 ] and a book titled "" the art and science of analyzing software data "" [ 4 ] devoted a chapter for lda analysis [ 6 ].', 'many sources give methodological guidance on how to apply lda topic modeling in software engineering  #TAUTHOR_TAG, 3, 19 ].', 'given all this, we think it is fair to say that lda topic modelling is a relevant data analysis technique in empirical software engineering research.', 'the quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [ 13 ], perplexity of measure in the test data [ 13 ], or silhouette coefficient of resulting topics [ 19 ].', 'other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co - occurrences in publicly available texts [ 23 ], or stability which investigates similarity of topics between different runs  #TAUTHOR_TAG.', 'recently,  #TAUTHOR_TAG claimed that the instability of topics is one major shortcoming of this technique.', '']",0
"[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","['dirichlet allocation ( lda ) is a topic modeling technique for textual data [ 5 ] that is widely applied in software engineering  #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ] for different tasks such as requirements engineering [ 15 ], software architecture [ 10 ], source code analysis [ 9 ], defect reports [ 16 ], testing [ 14 ] and to bibliometric analysis of software engineering literature [ 11, 22 ].', 'a survey on topic modelling in software engineering has been conducted [ 24 ] and a book titled "" the art and science of analyzing software data "" [ 4 ] devoted a chapter for lda analysis [ 6 ].', 'many sources give methodological guidance on how to apply lda topic modeling in software engineering  #TAUTHOR_TAG, 3, 19 ].', 'given all this, we think it is fair to say that lda topic modelling is a relevant data analysis technique in empirical software engineering research.', 'the quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [ 13 ], perplexity of measure in the test data [ 13 ], or silhouette coefficient of resulting topics [ 19 ].', 'other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co - occurrences in publicly available texts [ 23 ], or stability which investigates similarity of topics between different runs  #TAUTHOR_TAG.', 'recently,  #TAUTHOR_TAG claimed that the instability of topics is one major shortcoming of this technique.', '']",0
"[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","['dirichlet allocation ( lda ) is a topic modeling technique for textual data [ 5 ] that is widely applied in software engineering  #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ] for different tasks such as requirements engineering [ 15 ], software architecture [ 10 ], source code analysis [ 9 ], defect reports [ 16 ], testing [ 14 ] and to bibliometric analysis of software engineering literature [ 11, 22 ].', 'a survey on topic modelling in software engineering has been conducted [ 24 ] and a book titled "" the art and science of analyzing software data "" [ 4 ] devoted a chapter for lda analysis [ 6 ].', 'many sources give methodological guidance on how to apply lda topic modeling in software engineering  #TAUTHOR_TAG, 3, 19 ].', 'given all this, we think it is fair to say that lda topic modelling is a relevant data analysis technique in empirical software engineering research.', 'the quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [ 13 ], perplexity of measure in the test data [ 13 ], or silhouette coefficient of resulting topics [ 19 ].', 'other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co - occurrences in publicly available texts [ 23 ], or stability which investigates similarity of topics between different runs  #TAUTHOR_TAG.', 'recently,  #TAUTHOR_TAG claimed that the instability of topics is one major shortcoming of this technique.', '']",0
"[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","['dirichlet allocation ( lda ) is a topic modeling technique for textual data [ 5 ] that is widely applied in software engineering  #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ] for different tasks such as requirements engineering [ 15 ], software architecture [ 10 ], source code analysis [ 9 ], defect reports [ 16 ], testing [ 14 ] and to bibliometric analysis of software engineering literature [ 11, 22 ].', 'a survey on topic modelling in software engineering has been conducted [ 24 ] and a book titled "" the art and science of analyzing software data "" [ 4 ] devoted a chapter for lda analysis [ 6 ].', 'many sources give methodological guidance on how to apply lda topic modeling in software engineering  #TAUTHOR_TAG, 3, 19 ].', 'given all this, we think it is fair to say that lda topic modelling is a relevant data analysis technique in empirical software engineering research.', 'the quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [ 13 ], perplexity of measure in the test data [ 13 ], or silhouette coefficient of resulting topics [ 19 ].', 'other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co - occurrences in publicly available texts [ 23 ], or stability which investigates similarity of topics between different runs  #TAUTHOR_TAG.', 'recently,  #TAUTHOR_TAG claimed that the instability of topics is one major shortcoming of this technique.', '']",0
"['19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics']","['( latent dirichlet allocation ) is a soft clustering algorithm that is ideal for text [ 5 ] but also for other purposes such as genetics [ 21 ] where a relationship between a gene and a genotype can be considered similar to a relationship between a word and a document.', 'given a set of documents, lda models from what topics this set of documents may have been created from.', 'as opposed to hard clustering where each document would be assigned to a single topic only, lda soft clustering assigns each document a list of topics and probabilities for the topics.', 'a topic in lda is a collection of words and their probability estimates for each topic.', 'in order to summarize, after running lda we have the following.', '• for all documents m there is a vector θ which is the topic distribution for that document.', '• for all topics k there is a vector [UNK] which is the word distribution for that topic.', 'before topic generation, lda requires that we set the input parameters such as the number of topics k, and hyper priors α and β.', 'past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics such as perplexity [ 13 ], stability  #TAUTHOR_TAG, or coherence [ 23 ].', ""the stability of a topic model can be defined as the model's ability to replicate its solutions [ 8 ]."", 'instability ( the lack of stability ) is caused by the non - deterministic nature of monte - carlo simulation that is part of the lda algorithm  #TAUTHOR_TAG.', 'past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model  #TAUTHOR_TAG, 8, 12 ].', 'we think using the results of a single lda run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.', 'the next section shows a method that can be used to make more informed decisions']",0
"['19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics']","['( latent dirichlet allocation ) is a soft clustering algorithm that is ideal for text [ 5 ] but also for other purposes such as genetics [ 21 ] where a relationship between a gene and a genotype can be considered similar to a relationship between a word and a document.', 'given a set of documents, lda models from what topics this set of documents may have been created from.', 'as opposed to hard clustering where each document would be assigned to a single topic only, lda soft clustering assigns each document a list of topics and probabilities for the topics.', 'a topic in lda is a collection of words and their probability estimates for each topic.', 'in order to summarize, after running lda we have the following.', '• for all documents m there is a vector θ which is the topic distribution for that document.', '• for all topics k there is a vector [UNK] which is the word distribution for that topic.', 'before topic generation, lda requires that we set the input parameters such as the number of topics k, and hyper priors α and β.', 'past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics such as perplexity [ 13 ], stability  #TAUTHOR_TAG, or coherence [ 23 ].', ""the stability of a topic model can be defined as the model's ability to replicate its solutions [ 8 ]."", 'instability ( the lack of stability ) is caused by the non - deterministic nature of monte - carlo simulation that is part of the lda algorithm  #TAUTHOR_TAG.', 'past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model  #TAUTHOR_TAG, 8, 12 ].', 'we think using the results of a single lda run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.', 'the next section shows a method that can be used to make more informed decisions']",0
"['19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics']","['( latent dirichlet allocation ) is a soft clustering algorithm that is ideal for text [ 5 ] but also for other purposes such as genetics [ 21 ] where a relationship between a gene and a genotype can be considered similar to a relationship between a word and a document.', 'given a set of documents, lda models from what topics this set of documents may have been created from.', 'as opposed to hard clustering where each document would be assigned to a single topic only, lda soft clustering assigns each document a list of topics and probabilities for the topics.', 'a topic in lda is a collection of words and their probability estimates for each topic.', 'in order to summarize, after running lda we have the following.', '• for all documents m there is a vector θ which is the topic distribution for that document.', '• for all topics k there is a vector [UNK] which is the word distribution for that topic.', 'before topic generation, lda requires that we set the input parameters such as the number of topics k, and hyper priors α and β.', 'past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics such as perplexity [ 13 ], stability  #TAUTHOR_TAG, or coherence [ 23 ].', ""the stability of a topic model can be defined as the model's ability to replicate its solutions [ 8 ]."", 'instability ( the lack of stability ) is caused by the non - deterministic nature of monte - carlo simulation that is part of the lda algorithm  #TAUTHOR_TAG.', 'past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model  #TAUTHOR_TAG, 8, 12 ].', 'we think using the results of a single lda run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.', 'the next section shows a method that can be used to make more informed decisions']",0
"['19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in']","['genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics']","['( latent dirichlet allocation ) is a soft clustering algorithm that is ideal for text [ 5 ] but also for other purposes such as genetics [ 21 ] where a relationship between a gene and a genotype can be considered similar to a relationship between a word and a document.', 'given a set of documents, lda models from what topics this set of documents may have been created from.', 'as opposed to hard clustering where each document would be assigned to a single topic only, lda soft clustering assigns each document a list of topics and probabilities for the topics.', 'a topic in lda is a collection of words and their probability estimates for each topic.', 'in order to summarize, after running lda we have the following.', '• for all documents m there is a vector θ which is the topic distribution for that document.', '• for all topics k there is a vector [UNK] which is the word distribution for that topic.', 'before topic generation, lda requires that we set the input parameters such as the number of topics k, and hyper priors α and β.', 'past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [ 19 ] or differential evolution  #TAUTHOR_TAG.', 'as pointed out in section 1, what is optimal can be measured with many metrics such as perplexity [ 13 ], stability  #TAUTHOR_TAG, or coherence [ 23 ].', ""the stability of a topic model can be defined as the model's ability to replicate its solutions [ 8 ]."", 'instability ( the lack of stability ) is caused by the non - deterministic nature of monte - carlo simulation that is part of the lda algorithm  #TAUTHOR_TAG.', 'past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model  #TAUTHOR_TAG, 8, 12 ].', 'we think using the results of a single lda run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.', 'the next section shows a method that can be used to make more informed decisions']",0
"['measure jaccard similarity between the top words of any two topics.', 'extended jaccard measures have been used in lda stability task optimization  #TAUTHOR_TAG 12 ].', 'when two topics have all the same top words, the jaccard']","['measure jaccard similarity between the top words of any two topics.', 'extended jaccard measures have been used in lda stability task optimization  #TAUTHOR_TAG 12 ].', 'when two topics have all the same top words, the jaccard']","['to the same top words.', 'another anomaly is that for two topics with no intersecting top ten words, we would get a better spearman correlation value than - 1 ( - 0. 86 ).', 'third, we can measure jaccard similarity between the top words of any two topics.', 'extended jaccard measures have been used in lda stability task optimization  #TAUTHOR_TAG 12 ].', 'when two topics have all the same top words, the jaccard similarity would be 1.', 'on the other hand, the worst case ( when all the top words are different ) would result in a jaccard similarity of 0.', 'the undesirable property of the jaccard similarity is that any variations in ordering would not be reflected in the']","['stability measure.', 'at this point, our results would appear like any lda topic model to the user.', 'however, as we want to give the user transparency to topic stability, we need to add a measure describing topic stability.', 'obviously, user can investigate each cluster in detail but the topic stability measure can help the user to focus on specific clusters.', 'we propose several measures of topic stability, i. e. whether a set of topics are actually about the same content.', 'when two topics contain the same top 10 words in the same order, then we can think that they are exactly about the same content and should result in a maximum score.', 'on the other hand, any deviations from this should result in a lower score.', 'first, silhouette is a well - established measure for cluster validation that considers both how similar each object is to its own cluster ( cohesion ) and how different it is to other clusters ( separation ).', 'it has been used in lda optimization before [ 18, 19 ].', 'the average silhouette is produced by the k - medioids clustering performed earlier.', 'however, the cluster separation is not interesting for the user as the user mainly cares about whether a particular cluster has similar elements, i. e. high stability.', 'furthermore, this measure is based on the absolute values of word probabilities rather than the ranks what are presented to the user.', 'second, to model whether the same top words are present and that they are in the same order, we can use spearman rank correlation between the top words of any two topics.', 'any words that are present in the top word list of one topic, but not the other, are assigned the lowest rank in the other topic.', 'a problem occurs if two topics have the same words but in reverse order, the rank correlation between the topics would be - 1 while one would still consider these two topics somewhat similar due to the same top words.', 'another anomaly is that for two topics with no intersecting top ten words, we would get a better spearman correlation value than - 1 ( - 0. 86 ).', 'third, we can measure jaccard similarity between the top words of any two topics.', 'extended jaccard measures have been used in lda stability task optimization  #TAUTHOR_TAG 12 ].', 'when two topics have all the same top words, the jaccard similarity would be 1.', 'on the other hand, the worst case ( when all the top words are different ) would result in a jaccard similarity of 0.', 'the undesirable property of the jaccard similarity is that any variations in ordering would not be reflected in the']",0
[' #TAUTHOR_TAG and machine learning [ 12 ] point out that lda instability'],[' #TAUTHOR_TAG and machine learning [ 12 ] point out that lda instability'],[' #TAUTHOR_TAG and machine learning [ 12 ] point out that lda instability'],"['work in software engineering  #TAUTHOR_TAG and machine learning [ 12 ] point out that lda instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem.', 'this paper suggests performing replicated runs, clustering the results and measuring the topic stability.', 'these approach are not alternative but additive.', 'our approach can be combined with any lda optimization technique that relies on input parameter optimization.', 'finally, our approach shows topic stability by providing a metric of topic stability and allowing further investigation of the clusters when desired.', 'this paper presents multiple metrics of topic stability in table 1 that are highly correlated with each other in our data set.', 'based on theoretical metric properties ( see section 2. 2. 3 ) we recommend using rbo [ 26 ].', 'in the future, one should empirically establish what p value setting of rbo metric most accurately matches the user expectation on topic stability as in this paper we only used the default ( p = 0. 9 ).', 'we should also study how the topic clusters can be used in the downstream nlp tasks in software engineering.', ""furthermore, to demonstrate our idea we also made other design choices but didn't investigate their impact."", 'for example, we considered only 20 replication runs which might be too little and we only generated 20 lda topics for each run.', 'we also clustered our topics in the word vector space produced by glove as prior work suggested it would produce better results than clustering in word space.', 'all these choices could be challenged.', ""zeller's 2018 icse talk [ 27 ] has warned us about the dangers of adding complexity."", 'our approach adds complexity but eventually hides it behind an rbo metric showing the stability of each topic cluster.', 'if the stability of topics is an issue, users of topics models should be made aware of it but with minimal added complexity as we have tried to do in this paper']",0
"[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","[' #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ]']","['dirichlet allocation ( lda ) is a topic modeling technique for textual data [ 5 ] that is widely applied in software engineering  #TAUTHOR_TAG - 4, 6, 10, 11, 14 - 16, 19, 24, 25 ] for different tasks such as requirements engineering [ 15 ], software architecture [ 10 ], source code analysis [ 9 ], defect reports [ 16 ], testing [ 14 ] and to bibliometric analysis of software engineering literature [ 11, 22 ].', 'a survey on topic modelling in software engineering has been conducted [ 24 ] and a book titled "" the art and science of analyzing software data "" [ 4 ] devoted a chapter for lda analysis [ 6 ].', 'many sources give methodological guidance on how to apply lda topic modeling in software engineering  #TAUTHOR_TAG, 3, 19 ].', 'given all this, we think it is fair to say that lda topic modelling is a relevant data analysis technique in empirical software engineering research.', 'the quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [ 13 ], perplexity of measure in the test data [ 13 ], or silhouette coefficient of resulting topics [ 19 ].', 'other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co - occurrences in publicly available texts [ 23 ], or stability which investigates similarity of topics between different runs  #TAUTHOR_TAG.', 'recently,  #TAUTHOR_TAG claimed that the instability of topics is one major shortcoming of this technique.', '']",7
['.  #TAUTHOR_TAG had around 300. but islam et'],"['research.', 'we are working on readability modeling in bengali, and this dataset will be very helpful.', 'an important limitation of our study is the small corpus size.', ""we only have 30 annotated passages at our disposal, whereas islam et al.  #TAUTHOR_TAG had around 300. but islam et al.'s dataset is not annotated in as fine - grained a fashion as ours."", ""note also that our dataset is larger than both sinha et al.'s 16document dataset [ 21 ], and das and roychoudhury's seven document dataset [ 6 ]."", 'we plan to increase the size of our dataset in future']",['.  #TAUTHOR_TAG had around 300. but islam et'],"['performed an inter - rater agreement study for readability assessment in bengali.', 'this is the first time such an agreement study has been performed.', 'we obtained moderate to fair agreement among seven independent annotators on 30 text passages written by four eminent bengali authors.', 'as a byproduct of this study, we obtained a gold standard human annotated readability dataset for bengali.', 'we plan to release this dataset for future research.', 'we are working on readability modeling in bengali, and this dataset will be very helpful.', 'an important limitation of our study is the small corpus size.', ""we only have 30 annotated passages at our disposal, whereas islam et al.  #TAUTHOR_TAG had around 300. but islam et al.'s dataset is not annotated in as fine - grained a fashion as ours."", ""note also that our dataset is larger than both sinha et al.'s 16document dataset [ 21 ], and das and roychoudhury's seven document dataset [ 6 ]."", 'we plan to increase the size of our dataset in future']",4
"['communities [ 2, 6, 11,  #TAUTHOR_TAG 29, 30, 32,']","['communities [ 2, 6, 11,  #TAUTHOR_TAG 29, 30, 32, 37, 39, 40, 42 ]']","['in the natural language processing and vision communities [ 2, 6, 11,  #TAUTHOR_TAG 29, 30, 32, 37, 39, 40, 42 ]']","['shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities [ 2, 6, 11,  #TAUTHOR_TAG 29, 30, 32, 37, 39, 40, 42 ].', 'to understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross - domain settings [ 12, 13, 29 ] and came up with various domain adaptation techniques [ 6, 11, 28, 39 ].', '']",4
"['[ 7, 10,  #TAUTHOR_TAG 18, 23, 27, 34 ].', 'string kernels represent a way of using information at the character level by measuring the similarity of strings through character n - grams.', 'lodhi']","['[ 7, 10,  #TAUTHOR_TAG 18, 23, 27, 34 ].', 'string kernels represent a way of using information at the character level by measuring the similarity of strings through character n - grams.', 'lodhi']","['[ 7, 10,  #TAUTHOR_TAG 18, 23, 27, 34 ].', 'string kernels represent a way of using information at the character level by measuring the similarity of strings through character n - grams.', 'lodhi']","['recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks [ 7, 10,  #TAUTHOR_TAG 18, 23, 27, 34 ].', 'string kernels represent a way of using information at the character level by measuring the similarity of strings through character n - grams.', 'lodhi et al. [ 27 ] used string kernels for document categorization, obtaining very good results.', 'string kernels were also successfully used in authorship identification [ 34 ].', 'more recently, various combinations of string kernels reached state - of - the - art accuracy rates in native language identification [ 23 ] and arabic dialect identification [ 18 ].', 'interestingly, string kernels have been used in cross - domain settings without any domain adaptation, obtaining impressive results.', 'for instance, ionescu et al. [ 23 ] have employed string kernels in a cross - corpus ( and implicitly cross - domain ) native language identification experiment, improving the state - of - the - art accuracy by a remarkable 32. 3 %.', 'gimenez - perez et al.  #TAUTHOR_TAG have used string kernels for single - source and multi - source polarity classification.', 'remarkably, they obtain state - of - the - art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross - domain setting without any domain adaptation.', 'ionescu et al. [ 18 ] obtained the best performance in the arabic dialect identification shared task of the 2017 vardial evaluation campaign [ 41 ], with an improvement of 4. 6 % over the second - best method.', 'it is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups [ 41 ], or in other words, the training and the test sets are drawn from different distributions.', 'different from all these recent approaches  #TAUTHOR_TAG 18, 23 ], we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross - domain text classification, particularly in english polarity classification']",4
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",4
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",4
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",4
"['3, 12,  #TAUTHOR_TAG 15, 32, 40 ]']","['methods [ 3, 12,  #TAUTHOR_TAG 15, 32, 40 ]']","['3, 12,  #TAUTHOR_TAG 15, 32, 40 ]']",[' #TAUTHOR_TAG'],4
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",5
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",5
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",5
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",5
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",5
"['based on string kernels  #TAUTHOR_TAG, as']","['based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - trad']","['', 'baseline based on string kernels  #TAUTHOR_TAG, as well as sfa [ 32 ], coral [ 40 ] and tr - tradaboost [ 15 ]. the best accuracy rates are highlighted in', '']",3
['best result  #TAUTHOR_TAG'],['best result  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['ubiquitous task in processing electronic medical data is the assignment of standardized codes representing diagnoses and / or procedures to free - text documents such as medical reports.', 'this is a difficult natural language processing task that requires parsing long, heterogeneous documents and selecting a set of appropriate codes from tens of thousands of possibilities - many of which have very few positive training samples.', 'we present a deep learning system that advances the state of the art for the mimic - iii dataset, achieving a new best micro f1 - measure of 55. 85 %, significantly outperforming the previous best result  #TAUTHOR_TAG.', 'we achieve this through a number of enhancements, including two major novel contributions : multiview convolutional channels, which effectively learn to adjust kernel sizes throughout the input ; and attention regularization, mediated by natural - language code descriptions, which helps overcome sparsity for thousands of uncommon codes.', 'these and other modifications are selected to address difficulties inherent to both automated coding specifically and deep learning generally.', 'finally, we investigate our accuracy results in detail to individually measure the impact of these contributions and point the way towards future algorithmic improvements']",4
"['', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['', ' #TAUTHOR_TAG presented a model capable of predicting full codes for both icd - 9 diagnoses and procedures composed of shared embedding and cnn layers between all codes and an individual attention layer for each code.', '']",4
"['studies  #TAUTHOR_TAG.', 'table 3 provides the']","['studies  #TAUTHOR_TAG.', 'table 3 provides the']","['procedure codes for comparability with previous studies  #TAUTHOR_TAG.', 'table 3 provides the evaluation of']","['evaluate the baseline models on the dis set and dis - 50 sets and provide micro f1 scores for diagnosis and procedure codes for comparability with previous studies  #TAUTHOR_TAG.', '']",4
['best result  #TAUTHOR_TAG'],['best result  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['ubiquitous task in processing electronic medical data is the assignment of standardized codes representing diagnoses and / or procedures to free - text documents such as medical reports.', 'this is a difficult natural language processing task that requires parsing long, heterogeneous documents and selecting a set of appropriate codes from tens of thousands of possibilities - many of which have very few positive training samples.', 'we present a deep learning system that advances the state of the art for the mimic - iii dataset, achieving a new best micro f1 - measure of 55. 85 %, significantly outperforming the previous best result  #TAUTHOR_TAG.', 'we achieve this through a number of enhancements, including two major novel contributions : multiview convolutional channels, which effectively learn to adjust kernel sizes throughout the input ; and attention regularization, mediated by natural - language code descriptions, which helps overcome sparsity for thousands of uncommon codes.', 'these and other modifications are selected to address difficulties inherent to both automated coding specifically and deep learning generally.', 'finally, we investigate our accuracy results in detail to individually measure the impact of these contributions and point the way towards future algorithmic improvements']",6
"['', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['', ' #TAUTHOR_TAG presented a model capable of predicting full codes for both icd - 9 diagnoses and procedures composed of shared embedding and cnn layers between all codes and an individual attention layer for each code.', '']",6
"['', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['', ' #TAUTHOR_TAG presented a model capable of predicting full codes for both icd - 9 diagnoses and procedures composed of shared embedding and cnn layers between all codes and an individual attention layer for each code.', '']",0
['##l  #TAUTHOR_TAG'],['caml  #TAUTHOR_TAG'],"['##l  #TAUTHOR_TAG.', 'for flat and hierarchical svms, we follow the approach']","['compare our approach with four baselines : flat and hierarchical svms  #AUTHOR_TAG, leam  #AUTHOR_TAG, and caml  #TAUTHOR_TAG.', 'for flat and hierarchical svms, we follow the approach of  #AUTHOR_TAG, considering 10, 000 tf - idf unigram features, training 8, 929 binary svms for the flat svms and 11, 693 binary svms for hierarchical svms.', 'for the hierarchical svms, we use the icd - 9 - cm hierarchy from bioportal.', '4 for flat svms, a code is considered present if its svm predicts a positive output.', '']",0
"['##1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision']","['icd code prediction is micro f1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision @ n,']","['evaluating icd code prediction is micro f1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision @ n,']","['most widely used metric for evaluating icd code prediction is micro f1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision @ n, and auc of roc as well  #TAUTHOR_TAG.', '']",0
"['##1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision']","['icd code prediction is micro f1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision @ n,']","['evaluating icd code prediction is micro f1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision @ n,']","['most widely used metric for evaluating icd code prediction is micro f1 - score  #TAUTHOR_TAG.', 'new studies have reported results on macro f1 - score, precision @ n, and auc of roc as well  #TAUTHOR_TAG.', '']",0
"['', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['', ' #TAUTHOR_TAG presented a model capable of predicting full codes for both icd - 9 diagnoses and procedures composed of shared embedding and cnn layers between all codes and an individual attention layer for each code.', '']",3
"['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only']","['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only']","['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only the discharge summaries, which allows us to compare our']","['rely on the publicly available mimic - iii dataset  #AUTHOR_TAG for icd - 9 code predictions.', '2 this dataset includes the elec - tronic medical records ( emr ) of inpatient stays in a hospital critical care unit.', 'mimic - iii includes raw notes for each hospital stay in different categories - discharge summary report, discharge summary addendum, radiology note, nursing notes, etc.', 'the number of notes varies between different hospital stays.', 'also, some of the hospital stays do not have discharge summaries ; following previous studies for automated coding, we only consider those that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only the discharge summaries, which allows us to compare our results with previous studies on this corpus  #TAUTHOR_TAG.', 'the dataset includes 8, 929 unique icd codes ( 2, 011 procedures, 6, 918 diagnoses ) for the patients who have discharge summaries.', '3 we follow the train, test, and development splits publicly shared by the recent study on this dataset  #TAUTHOR_TAG.', 'these splits are patient independent.', 'the statistical properties of all the sets are shown in table 1 ; note that there are around three times more tokens for the each hospital admission for the full set compared to the dis set.', 'note that dis - 50 includes far fewer training instances because any instances which do not include any of the 50 most frequent codes are discarded.', 'for preprocessing the text, we convert all characters to lower case and remove tokens which only include numbers.', 'we build the vocabulary from the training set and consider words occurring in fewer than three training samples as out of vocabulary ( oov ).', 'this results in 51, 917 unique words for the dis and dis - 50 set and 72, 891 for the full set']",3
"['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only']","['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only']","['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only the discharge summaries, which allows us to compare our']","['rely on the publicly available mimic - iii dataset  #AUTHOR_TAG for icd - 9 code predictions.', '2 this dataset includes the elec - tronic medical records ( emr ) of inpatient stays in a hospital critical care unit.', 'mimic - iii includes raw notes for each hospital stay in different categories - discharge summary report, discharge summary addendum, radiology note, nursing notes, etc.', 'the number of notes varies between different hospital stays.', 'also, some of the hospital stays do not have discharge summaries ; following previous studies for automated coding, we only consider those that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only the discharge summaries, which allows us to compare our results with previous studies on this corpus  #TAUTHOR_TAG.', 'the dataset includes 8, 929 unique icd codes ( 2, 011 procedures, 6, 918 diagnoses ) for the patients who have discharge summaries.', '3 we follow the train, test, and development splits publicly shared by the recent study on this dataset  #TAUTHOR_TAG.', 'these splits are patient independent.', 'the statistical properties of all the sets are shown in table 1 ; note that there are around three times more tokens for the each hospital admission for the full set compared to the dis set.', 'note that dis - 50 includes far fewer training instances because any instances which do not include any of the 50 most frequent codes are discarded.', 'for preprocessing the text, we convert all characters to lower case and remove tokens which only include numbers.', 'we build the vocabulary from the training set and consider words occurring in fewer than three training samples as out of vocabulary ( oov ).', 'this results in 51, 917 unique words for the dis and dis - 50 set and 72, 891 for the full set']",3
"['studies  #TAUTHOR_TAG.', 'table 3 provides the']","['studies  #TAUTHOR_TAG.', 'table 3 provides the']","['procedure codes for comparability with previous studies  #TAUTHOR_TAG.', 'table 3 provides the evaluation of']","['evaluate the baseline models on the dis set and dis - 50 sets and provide micro f1 scores for diagnosis and procedure codes for comparability with previous studies  #TAUTHOR_TAG.', '']",3
"['approaches  #TAUTHOR_TAG.', 'however, medical codes are ordered']","['approaches  #TAUTHOR_TAG.', 'however, medical codes are ordered']","['previous approaches  #TAUTHOR_TAG.', 'however, medical codes are ordered according to']","['this paper we introduced mvc - rlda, a model for medical code predictions, composed of a stack of embeddings, multi - view cnns with cross channel max pooling shared across all codes, and separate spatial attention pooling for each code.', 'this model has the potential to flexibly capture the relationship between different n - grams and codes.', 'we further enhance this model by using the descriptions of the labels in regularizing the attention weights to mitigate the effect of overfitting, especially for classes with few training examples.', 'we also demonstrate the advantage of using other notes aside from the discharge summaries.', 'our model surpasses the previous state - of - the - art model on the mimic iii dataset, providing more accurate predictions according to numerous metrics.', 'we also presented a detailed analysis of the results to highlight the contributions of our innovations in the achieved result.', 'the simplest among these was to use all available text in addition to the discharge summary, as our approach was to concatenate all relevant notes in each input.', 'it is worth exploring more nuanced approaches for integrating other notes in the input, as all notes may not be similarly important.', 'other modifications may yield further improvements.', 'for instance, we trained the model on all ground - truth codes equally, similarly to previous approaches  #TAUTHOR_TAG.', 'however, medical codes are ordered according to their importance.', 'it is worth exploring approaches which take the rank of labels into account.', 'furthermore, devising models which incorporate the hierarchical knowledge of the codes can be helpful.', 'finally, it will be important to test our model in an aiassist workflow to see how automated predictions can expedite human coding']",3
"['', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['diagnosis codes.', ' #TAUTHOR_TAG presented a model capable of predicting full codes']","['', ' #TAUTHOR_TAG presented a model capable of predicting full codes for both icd - 9 diagnoses and procedures composed of shared embedding and cnn layers between all codes and an individual attention layer for each code.', '']",5
"['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only']","['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only']","['that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only the discharge summaries, which allows us to compare our']","['rely on the publicly available mimic - iii dataset  #AUTHOR_TAG for icd - 9 code predictions.', '2 this dataset includes the elec - tronic medical records ( emr ) of inpatient stays in a hospital critical care unit.', 'mimic - iii includes raw notes for each hospital stay in different categories - discharge summary report, discharge summary addendum, radiology note, nursing notes, etc.', 'the number of notes varies between different hospital stays.', 'also, some of the hospital stays do not have discharge summaries ; following previous studies for automated coding, we only consider those that do  #TAUTHOR_TAG.', 'we have three sets for our experiments : one including only the discharge summaries, which allows us to compare our results with previous studies on this corpus  #TAUTHOR_TAG.', 'the dataset includes 8, 929 unique icd codes ( 2, 011 procedures, 6, 918 diagnoses ) for the patients who have discharge summaries.', '3 we follow the train, test, and development splits publicly shared by the recent study on this dataset  #TAUTHOR_TAG.', 'these splits are patient independent.', 'the statistical properties of all the sets are shown in table 1 ; note that there are around three times more tokens for the each hospital admission for the full set compared to the dis set.', 'note that dis - 50 includes far fewer training instances because any instances which do not include any of the 50 most frequent codes are discarded.', 'for preprocessing the text, we convert all characters to lower case and remove tokens which only include numbers.', 'we build the vocabulary from the training set and consider words occurring in fewer than three training samples as out of vocabulary ( oov ).', 'this results in 51, 917 unique words for the dis and dis - 50 set and 72, 891 for the full set']",5
['##l  #TAUTHOR_TAG'],['caml  #TAUTHOR_TAG'],"['##l  #TAUTHOR_TAG.', 'for flat and hierarchical svms, we follow the approach']","['compare our approach with four baselines : flat and hierarchical svms  #AUTHOR_TAG, leam  #AUTHOR_TAG, and caml  #TAUTHOR_TAG.', 'for flat and hierarchical svms, we follow the approach of  #AUTHOR_TAG, considering 10, 000 tf - idf unigram features, training 8, 929 binary svms for the flat svms and 11, 693 binary svms for hierarchical svms.', 'for the hierarchical svms, we use the icd - 9 - cm hierarchy from bioportal.', '4 for flat svms, a code is considered present if its svm predicts a positive output.', '']",5
['##l  #TAUTHOR_TAG'],['caml  #TAUTHOR_TAG'],"['##l  #TAUTHOR_TAG.', 'for flat and hierarchical svms, we follow the approach']","['compare our approach with four baselines : flat and hierarchical svms  #AUTHOR_TAG, leam  #AUTHOR_TAG, and caml  #TAUTHOR_TAG.', 'for flat and hierarchical svms, we follow the approach of  #AUTHOR_TAG, considering 10, 000 tf - idf unigram features, training 8, 929 binary svms for the flat svms and 11, 693 binary svms for hierarchical svms.', 'for the hierarchical svms, we use the icd - 9 - cm hierarchy from bioportal.', '4 for flat svms, a code is considered present if its svm predicts a positive output.', '']",5
"[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural']","['of neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural network is']","['transliteration is defined as phonetic transformation of names across languages  #AUTHOR_TAG.', 'transliteration of named entities is the essential part of many multilingual applications, such as machine translation  #AUTHOR_TAG and cross - language information retrieval  #AUTHOR_TAG.', 'recent studies pay a great attention to the task of neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural network is responsible for reading a source sentence and generates its translation.', 'from a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i. e., arg max y p ( y | x ).', 'the whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus.', 'transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task  #AUTHOR_TAG.', 'based on successful studies on neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, in this paper, we proposed a character - based encoderdecoder model which learn to transliterate endto - end.', 'in the opposite side of classical models which contains different components, the proposed model is trained end - to - end, so it able to apply to any language pairs without tuning for a spacific one']",0
"[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural']","['of neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural network is']","['transliteration is defined as phonetic transformation of names across languages  #AUTHOR_TAG.', 'transliteration of named entities is the essential part of many multilingual applications, such as machine translation  #AUTHOR_TAG and cross - language information retrieval  #AUTHOR_TAG.', 'recent studies pay a great attention to the task of neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural network is responsible for reading a source sentence and generates its translation.', 'from a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i. e., arg max y p ( y | x ).', 'the whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus.', 'transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task  #AUTHOR_TAG.', 'based on successful studies on neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, in this paper, we proposed a character - based encoderdecoder model which learn to transliterate endto - end.', 'in the opposite side of classical models which contains different components, the proposed model is trained end - to - end, so it able to apply to any language pairs without tuning for a spacific one']",0
"[', called  #TAUTHOR_TAG upon which we']","['underlying framework, called  #TAUTHOR_TAG upon which we']",['called  #TAUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate'],"[', we describe briefly the underlying framework, called  #TAUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate end - to - end.', 'the enoder is a character - based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.', 'this network reads the source name x = ( x 1,..., x t ) and encodes it into a sequence of hidden states h = ( h 1, · · ·, h t ) :', 'each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.', '']",0
"[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural']","['of neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural network is']","['transliteration is defined as phonetic transformation of names across languages  #AUTHOR_TAG.', 'transliteration of named entities is the essential part of many multilingual applications, such as machine translation  #AUTHOR_TAG and cross - language information retrieval  #AUTHOR_TAG.', 'recent studies pay a great attention to the task of neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'in neural machine translation, a single neural network is responsible for reading a source sentence and generates its translation.', 'from a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i. e., arg max y p ( y | x ).', 'the whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus.', 'transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task  #AUTHOR_TAG.', 'based on successful studies on neural machine translation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, in this paper, we proposed a character - based encoderdecoder model which learn to transliterate endto - end.', 'in the opposite side of classical models which contains different components, the proposed model is trained end - to - end, so it able to apply to any language pairs without tuning for a spacific one']",5
"[', called  #TAUTHOR_TAG upon which we']","['underlying framework, called  #TAUTHOR_TAG upon which we']",['called  #TAUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate'],"[', we describe briefly the underlying framework, called  #TAUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate end - to - end.', 'the enoder is a character - based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.', 'this network reads the source name x = ( x 1,..., x t ) and encodes it into a sequence of hidden states h = ( h 1, · · ·, h t ) :', 'each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.', '']",5
['of  #TAUTHOR_TAG in'],['of  #TAUTHOR_TAG in'],['conducted a set of experiments to show the effectiveness of  #TAUTHOR_TAG in'],"['conducted a set of experiments to show the effectiveness of  #TAUTHOR_TAG in the task of machine transliteration using standard benchmark datasets provided by news 2015 - 16 shared task.', 'table 1 shows different datasets in our experiments.', 'each dataset covers different levels of difficulty and training set size.', 'the proposed model has been applied on.', 'each dataset without tuning the algorithm for each specific language pairs.', ""also, we don't apply any preprocessing on the source or target language in order to evaluate the effectiveness of the proposed model in a fair situation."", ""' taskid'is a unique identifier in the following experiments."", 'we leveraged a character - based encoderdecoder model  #AUTHOR_TAG with soft attention mechanism  #AUTHOR_TAG b ).', 'in this model, input sequences in both source and target languages have been represented as characters.', 'using characters instead of words leads to longer sequences, so gated recurrent units  #AUTHOR_TAG a ) have been used for the encoder network to model long term dependencies.', 'the encoder has 128 hidden units for each direction ( forward and backward ), and the decoder has 128 hidden units with soft attention mechanism  #AUTHOR_TAG b ).', 'we train the model using stochastic gradient descent with adam  #AUTHOR_TAG.', 'each update is computed using a minibatch of 128 sequence pairs.', 'the norm of the gradient is clipped with a threshold 1  #AUTHOR_TAG.', 'also, beamsearch has been used to approximately find the most likely transliteration given a source sequence  #AUTHOR_TAG.', 'table 2 shows the effectiveness of the proposed model on different datasets using standard measures.', 'the proposed neural machine transliteration model has been compared to the baseline method provided by news 2016 organizers.', 'baseline results are based on a machine translation implementation at the character level using moses  #AUTHOR_TAG.', 'experimental results shows that the proposed model is significantly better than the robust baseline using different metrics.', 'figure 1 shows the learning curve of the pro - table 2 : the effectiveness of neural machine transliteration is compared with the robust baseline  #AUTHOR_TAG provided by news 2016 shared task on transliteration of named entities.', 'posed model on different datasets.', 'it is clear that in most datasets, the trained model is capable of robust transliteration after a few number of iterations.', 'as shown in table 1, each dataset has different number of training set and also different number of characters in the source and target language.', ""for example, when transliterating from english to chinese ( taskid ='en - ch') and english to hebrew, the target names contains 548 and 37 different tokens respectively."", 'since we leverage a same model for different datasets without tuning the model for each dataset, differences']",5
['based on successful studies in sequence to sequence learning  #TAUTHOR_TAG and neural machine'],['this paper we proposed neural machine transliteration based on successful studies in sequence to sequence learning  #TAUTHOR_TAG and neural machine'],"['this paper we proposed neural machine transliteration based on successful studies in sequence to sequence learning  #TAUTHOR_TAG and neural machine translation  #AUTHOR_TAG costa - jussa and  #AUTHOR_TAG a ).', 'neural machine transliteration']","['this paper we proposed neural machine transliteration based on successful studies in sequence to sequence learning  #TAUTHOR_TAG and neural machine translation  #AUTHOR_TAG costa - jussa and  #AUTHOR_TAG a ).', 'neural machine transliteration typically consists of two components, the first of which encodes a source name sequence x and the second decodes to a target name sequence y. different parts of the proposed model jointly trained using stochastic gradient descent to minimize the log - likelihood.', 'experiments on different datasets using benchmark measures revealed that the proposed model is able to achieve significantly higher transliteration quality over traditional statistical models  #AUTHOR_TAG.', 'in this paper we did not concentrate on improving the model for achieving state - of - the - art results, so applying hyperparameter optimization  #AUTHOR_TAG, multi - task sequence to sequence learning  #AUTHOR_TAG and multiway transliteration  #AUTHOR_TAG are quite promising for future works']",5
"['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['phrase - based translation ( hiero )  #AUTHOR_TAG uses a lexicalized synchronous context - free grammar ( scfg ) extracted from word and phrase alignments of a bitext.', 'decoding for hiero is typically done with cky - style decoding with time complexity o ( n 3 ) for source input with n words.', 'computing the language model score for each hypothesis within cky decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside - out from sub - spans  #AUTHOR_TAG.', 'lr - decoding algorithms exist for phrasebased  #AUTHOR_TAG and syntax - based  #AUTHOR_TAG models and also for hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed left - toright ( lr ) decoding for hiero ( lr - hiero henceforth ) which uses beam search and runs in o ( n 2 b ) in practice where n is the length of source sentence and b is the size of beam  #AUTHOR_TAG.', 'to simplify target generation, scfg rules are constrained to be prefix - lexicalized on target side aka griebach normal form ( gnf ).', 'throughout this paper we abuse the notation for simplicity and use the term gnf grammars for such scfgs.', '']",0
"['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['phrase - based translation ( hiero )  #AUTHOR_TAG uses a lexicalized synchronous context - free grammar ( scfg ) extracted from word and phrase alignments of a bitext.', 'decoding for hiero is typically done with cky - style decoding with time complexity o ( n 3 ) for source input with n words.', 'computing the language model score for each hypothesis within cky decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside - out from sub - spans  #AUTHOR_TAG.', 'lr - decoding algorithms exist for phrasebased  #AUTHOR_TAG and syntax - based  #AUTHOR_TAG models and also for hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed left - toright ( lr ) decoding for hiero ( lr - hiero henceforth ) which uses beam search and runs in o ( n 2 b ) in practice where n is the length of source sentence and b is the size of beam  #AUTHOR_TAG.', 'to simplify target generation, scfg rules are constrained to be prefix - lexicalized on target side aka griebach normal form ( gnf ).', 'throughout this paper we abuse the notation for simplicity and use the term gnf grammars for such scfgs.', '']",0
"['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['phrase - based translation ( hiero )  #AUTHOR_TAG uses a lexicalized synchronous context - free grammar ( scfg ) extracted from word and phrase alignments of a bitext.', 'decoding for hiero is typically done with cky - style decoding with time complexity o ( n 3 ) for source input with n words.', 'computing the language model score for each hypothesis within cky decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside - out from sub - spans  #AUTHOR_TAG.', 'lr - decoding algorithms exist for phrasebased  #AUTHOR_TAG and syntax - based  #AUTHOR_TAG models and also for hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed left - toright ( lr ) decoding for hiero ( lr - hiero henceforth ) which uses beam search and runs in o ( n 2 b ) in practice where n is the length of source sentence and b is the size of beam  #AUTHOR_TAG.', 'to simplify target generation, scfg rules are constrained to be prefix - lexicalized on target side aka griebach normal form ( gnf ).', 'throughout this paper we abuse the notation for simplicity and use the term gnf grammars for such scfgs.', '']",0
"['for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudo']","['the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']","['the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']","['- hiero uses a constrained lexicalized scfg which we call a gnf grammar : x → γ, b β where γ is a string of non - terminal and terminal symbols, b is a string of terminal symbols and β is a possibly empty sequence of non - terminals.', 'this ensures that as each rule is used in a derivation, add h to hyplist 29 :', 'return hyplist the target string is generated from left to right.', 'the rules are obtained from a word and phrase aligned bitext using the rule extraction algorithm in  #AUTHOR_TAG.', 'lr - hiero decoding uses a top - down depth - first search, which strictly grows the hypotheses in target surface ordering.', 'search on the source side follows an earley - style search  #AUTHOR_TAG, the dot jumps around on the source side of the rules based on the order of nonterminals on the target side.', 'this search is integrated with beam search or cube pruning to find the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c )']","['1. 5 bleu ) but using queue diversity ( qd = 15 ) we fill this gap. we use the same', 'qd ( = 15 ) in next rows for zh - en. for cs - en and de - en we use regular cube pruning ( qd = 1 ), as', 'it works as well as beam search ( compare rows 4 and 2 ). we measure the benefit of the new modified rules from section 3 :', '( ab ) : adding modifications for rules type ( a ) and ( b ) ; ( abc ) :', 'modification of all rules. we can see that for all language pairs ( ab ) constantly improves', 'performance of lrhiero, significantly better than  #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c ) does not show any improvement due to spurious ambiguity created by 5 we report results on cs - en and de - en in  #TAUTHOR_TAG. row 4 is the same translation system', 'as row 3  #TAUTHOR_TAG. we achieve better results than  #TAUTHOR_TAG type ( c ) rules. figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets. all of the baselines', 'use the same wrapper to kenlm  #AUTHOR_TAG to query the language model, and we have instrumented', 'the wrapper to count the statistics. in  #TAUTHOR_TAG we discuss that lr - hiero with beam search  #AUTHOR_TAG does not perform at the same level of state - of - the - art hier', '##o ( more lm calls and less translation quality ). as we can see in this figure, adding', 'new modified rules slightly increases the number of language model queries on cs - en and de - en so that  #TAUTHOR_TAG still works 2 to', '3 times faster than hiero. on zh - en,  #TAUTHOR_TAG applies queue diversity ( qd =', '15 ) which reduces search errors and improves translation quality but increases the number of hypothesis generation as', 'well.  #TAUTHOR_TAG with our modifications works substantially faster than lr - hiero while obtain significantly better translation quality on zh - en. comparing table 2a', 'with figure 2b we can see that overall our modifications to lr - hiero decoder significantly improves the bleu scores compared to previous lr decoders for hiero. we obtain comparable results to cky - hiero for cs - en and de - en and remarkably improve results on zh - en, while at the same time making 2 to 3 times less lm calls', 'on cs - en and de - en compared to ckyhiero']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c )']","['1. 5 bleu ) but using queue diversity ( qd = 15 ) we fill this gap. we use the same', 'qd ( = 15 ) in next rows for zh - en. for cs - en and de - en we use regular cube pruning ( qd = 1 ), as', 'it works as well as beam search ( compare rows 4 and 2 ). we measure the benefit of the new modified rules from section 3 :', '( ab ) : adding modifications for rules type ( a ) and ( b ) ; ( abc ) :', 'modification of all rules. we can see that for all language pairs ( ab ) constantly improves', 'performance of lrhiero, significantly better than  #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c ) does not show any improvement due to spurious ambiguity created by 5 we report results on cs - en and de - en in  #TAUTHOR_TAG. row 4 is the same translation system', 'as row 3  #TAUTHOR_TAG. we achieve better results than  #TAUTHOR_TAG type ( c ) rules. figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets. all of the baselines', 'use the same wrapper to kenlm  #AUTHOR_TAG to query the language model, and we have instrumented', 'the wrapper to count the statistics. in  #TAUTHOR_TAG we discuss that lr - hiero with beam search  #AUTHOR_TAG does not perform at the same level of state - of - the - art hier', '##o ( more lm calls and less translation quality ). as we can see in this figure, adding', 'new modified rules slightly increases the number of language model queries on cs - en and de - en so that  #TAUTHOR_TAG still works 2 to', '3 times faster than hiero. on zh - en,  #TAUTHOR_TAG applies queue diversity ( qd =', '15 ) which reduces search errors and improves translation quality but increases the number of hypothesis generation as', 'well.  #TAUTHOR_TAG with our modifications works substantially faster than lr - hiero while obtain significantly better translation quality on zh - en. comparing table 2a', 'with figure 2b we can see that overall our modifications to lr - hiero decoder significantly improves the bleu scores compared to previous lr decoders for hiero. we obtain comparable results to cky - hiero for cs - en and de - en and remarkably improve results on zh - en, while at the same time making 2 to 3 times less lm calls', 'on cs - en and de - en compared to ckyhiero']",0
"['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed']","['phrase - based translation ( hiero )  #AUTHOR_TAG uses a lexicalized synchronous context - free grammar ( scfg ) extracted from word and phrase alignments of a bitext.', 'decoding for hiero is typically done with cky - style decoding with time complexity o ( n 3 ) for source input with n words.', 'computing the language model score for each hypothesis within cky decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside - out from sub - spans  #AUTHOR_TAG.', 'lr - decoding algorithms exist for phrasebased  #AUTHOR_TAG and syntax - based  #AUTHOR_TAG models and also for hierarchical phrasebased models  #TAUTHOR_TAG, which is our focus in this paper.', ' #AUTHOR_TAG first proposed left - toright ( lr ) decoding for hiero ( lr - hiero henceforth ) which uses beam search and runs in o ( n 2 b ) in practice where n is the length of source sentence and b is the size of beam  #AUTHOR_TAG.', 'to simplify target generation, scfg rules are constrained to be prefix - lexicalized on target side aka griebach normal form ( gnf ).', 'throughout this paper we abuse the notation for simplicity and use the term gnf grammars for such scfgs.', '']",6
"['for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudo']","['the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']","['the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']","['- hiero uses a constrained lexicalized scfg which we call a gnf grammar : x → γ, b β where γ is a string of non - terminal and terminal symbols, b is a string of terminal symbols and β is a possibly empty sequence of non - terminals.', 'this ensures that as each rule is used in a derivation, add h to hyplist 29 :', 'return hyplist the target string is generated from left to right.', 'the rules are obtained from a word and phrase aligned bitext using the rule extraction algorithm in  #AUTHOR_TAG.', 'lr - hiero decoding uses a top - down depth - first search, which strictly grows the hypotheses in target surface ordering.', 'search on the source side follows an earley - style search  #AUTHOR_TAG, the dot jumps around on the source side of the rules based on the order of nonterminals on the target side.', 'this search is integrated with beam search or cube pruning to find the k - best translations.', 'algorithm 1 shows the pseudocode for lrhiero decoding with cube pruning  #AUTHOR_TAG ( cp ).', 'lr - hiero with cp was introduced in  #TAUTHOR_TAG.', 'in this pseudocode, we have introduced the notion of queue diversity ( explained below ).', 'however to understand our change we need to understand the algorithm in more detail']",6
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c )']","['1. 5 bleu ) but using queue diversity ( qd = 15 ) we fill this gap. we use the same', 'qd ( = 15 ) in next rows for zh - en. for cs - en and de - en we use regular cube pruning ( qd = 1 ), as', 'it works as well as beam search ( compare rows 4 and 2 ). we measure the benefit of the new modified rules from section 3 :', '( ab ) : adding modifications for rules type ( a ) and ( b ) ; ( abc ) :', 'modification of all rules. we can see that for all language pairs ( ab ) constantly improves', 'performance of lrhiero, significantly better than  #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c ) does not show any improvement due to spurious ambiguity created by 5 we report results on cs - en and de - en in  #TAUTHOR_TAG. row 4 is the same translation system', 'as row 3  #TAUTHOR_TAG. we achieve better results than  #TAUTHOR_TAG type ( c ) rules. figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets. all of the baselines', 'use the same wrapper to kenlm  #AUTHOR_TAG to query the language model, and we have instrumented', 'the wrapper to count the statistics. in  #TAUTHOR_TAG we discuss that lr - hiero with beam search  #AUTHOR_TAG does not perform at the same level of state - of - the - art hier', '##o ( more lm calls and less translation quality ). as we can see in this figure, adding', 'new modified rules slightly increases the number of language model queries on cs - en and de - en so that  #TAUTHOR_TAG still works 2 to', '3 times faster than hiero. on zh - en,  #TAUTHOR_TAG applies queue diversity ( qd =', '15 ) which reduces search errors and improves translation quality but increases the number of hypothesis generation as', 'well.  #TAUTHOR_TAG with our modifications works substantially faster than lr - hiero while obtain significantly better translation quality on zh - en. comparing table 2a', 'with figure 2b we can see that overall our modifications to lr - hiero decoder significantly improves the bleu scores compared to previous lr decoders for hiero. we obtain comparable results to cky - hiero for cs - en and de - en and remarkably improve results on zh - en, while at the same time making 2 to 3 times less lm calls', 'on cs - en and de - en compared to ckyhiero']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c )']","['1. 5 bleu ) but using queue diversity ( qd = 15 ) we fill this gap. we use the same', 'qd ( = 15 ) in next rows for zh - en. for cs - en and de - en we use regular cube pruning ( qd = 1 ), as', 'it works as well as beam search ( compare rows 4 and 2 ). we measure the benefit of the new modified rules from section 3 :', '( ab ) : adding modifications for rules type ( a ) and ( b ) ; ( abc ) :', 'modification of all rules. we can see that for all language pairs ( ab ) constantly improves', 'performance of lrhiero, significantly better than  #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c ) does not show any improvement due to spurious ambiguity created by 5 we report results on cs - en and de - en in  #TAUTHOR_TAG. row 4 is the same translation system', 'as row 3  #TAUTHOR_TAG. we achieve better results than  #TAUTHOR_TAG type ( c ) rules. figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets. all of the baselines', 'use the same wrapper to kenlm  #AUTHOR_TAG to query the language model, and we have instrumented', 'the wrapper to count the statistics. in  #TAUTHOR_TAG we discuss that lr - hiero with beam search  #AUTHOR_TAG does not perform at the same level of state - of - the - art hier', '##o ( more lm calls and less translation quality ). as we can see in this figure, adding', 'new modified rules slightly increases the number of language model queries on cs - en and de - en so that  #TAUTHOR_TAG still works 2 to', '3 times faster than hiero. on zh - en,  #TAUTHOR_TAG applies queue diversity ( qd =', '15 ) which reduces search errors and improves translation quality but increases the number of hypothesis generation as', 'well.  #TAUTHOR_TAG with our modifications works substantially faster than lr - hiero while obtain significantly better translation quality on zh - en. comparing table 2a', 'with figure 2b we can see that overall our modifications to lr - hiero decoder significantly improves the bleu scores compared to previous lr decoders for hiero. we obtain comparable results to cky - hiero for cs - en and de - en and remarkably improve results on zh - en, while at the same time making 2 to 3 times less lm calls', 'on cs - en and de - en compared to ckyhiero']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c )']","['1. 5 bleu ) but using queue diversity ( qd = 15 ) we fill this gap. we use the same', 'qd ( = 15 ) in next rows for zh - en. for cs - en and de - en we use regular cube pruning ( qd = 1 ), as', 'it works as well as beam search ( compare rows 4 and 2 ). we measure the benefit of the new modified rules from section 3 :', '( ab ) : adding modifications for rules type ( a ) and ( b ) ; ( abc ) :', 'modification of all rules. we can see that for all language pairs ( ab ) constantly improves', 'performance of lrhiero, significantly better than  #TAUTHOR_TAG and lr - hiero ( p - value < 0. 05 ) on cs - en and', 'zh - en, evaluated by multeval  #AUTHOR_TAG. but modifying rule type ( c ) does not show any improvement due to spurious ambiguity created by 5 we report results on cs - en and de - en in  #TAUTHOR_TAG. row 4 is the same translation system', 'as row 3  #TAUTHOR_TAG. we achieve better results than  #TAUTHOR_TAG type ( c ) rules. figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets. all of the baselines', 'use the same wrapper to kenlm  #AUTHOR_TAG to query the language model, and we have instrumented', 'the wrapper to count the statistics. in  #TAUTHOR_TAG we discuss that lr - hiero with beam search  #AUTHOR_TAG does not perform at the same level of state - of - the - art hier', '##o ( more lm calls and less translation quality ). as we can see in this figure, adding', 'new modified rules slightly increases the number of language model queries on cs - en and de - en so that  #TAUTHOR_TAG still works 2 to', '3 times faster than hiero. on zh - en,  #TAUTHOR_TAG applies queue diversity ( qd =', '15 ) which reduces search errors and improves translation quality but increases the number of hypothesis generation as', 'well.  #TAUTHOR_TAG with our modifications works substantially faster than lr - hiero while obtain significantly better translation quality on zh - en. comparing table 2a', 'with figure 2b we can see that overall our modifications to lr - hiero decoder significantly improves the bleu scores compared to previous lr decoders for hiero. we obtain comparable results to cky - hiero for cs - en and de - en and remarkably improve results on zh - en, while at the same time making 2 to 3 times less lm calls', 'on cs - en and de - en compared to ckyhiero']",1
"[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our']","['of the recent work on exploiting coreference relations in machine translation focused on improving the translation of anaphoric pronouns  #AUTHOR_TAG novak et al., 2015 ;  #AUTHOR_TAG, disregarding other types of coreference relations, one of the reasons being the lack of annotated parallel corpora as well as the variability in the annotated data.', 'however, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages.', 'more importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme.', 'in our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains.', 'in particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method.', 'our approach to the annotation projection builds upon the approach recently introduced by  #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our goal is slightly different : we are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages.', 'therefore, in contrast to  #TAUTHOR_TAG, we use automatic source annotations produced by two state - of - the - art coreference systems, and we combine the output of our projection method for two source languages ( english and german ) to obtain target annotations for a third language ( russian ).', 'through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method.', 'the paper is organized as follows : section 2 presents an overview of the related work and section 3 describes the experimental setup.', 'in section 4, we give a detailed error analysis and discuss the results of our experiment.', '']",3
['of  #TAUTHOR_TAG as the basis'],['of  #TAUTHOR_TAG as the basis'],"['automatically produced annotations.', 'we use the english - german - russian unannotated corpus of  #TAUTHOR_TAG as the basis']","['our experiment, we propose a fully automatic projection setup : first, we perform coreference resolution on the source language data and then we implement the single - and multi - source approaches to transfer the automatically produced annotations.', 'we use the english - german - russian unannotated corpus of  #TAUTHOR_TAG as the basis for our experiment, which contains texts in two genres - newswire texts ( 229 sentences per language ) and short stories ( 184 sentences per language ).', 'furthermore, we use manual annotations present in the corpus as the gold standard for our evaluation.', 'it should be noted that the manual annotations were performed according to the parallel coreference annotation guidelines of  #AUTHOR_TAG that are in general compatible with the annotation of the the ontonotes corpus  #AUTHOR_TAG and are therefore suitable for our evaluation']",3
['as described in  #TAUTHOR_TAG'],['as described in  #TAUTHOR_TAG'],"['transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming']","['our experiment, we implement a direct projection method for coreference as described in  #AUTHOR_TAG.', 'our method works as follows : for each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one.', 'since the corpus was already sentence - and word - aligned 2, we use the available alignments to transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming from different sources and ( b ) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3.', 'in our experiment, we apply the following strategies from  #TAUTHOR_TAG :', ""1. setting 1 ('add') : disjoint chains from one source language are added to all the chains projected from the other source language ; 2. setting 2 ('unify - intersect') : the intersection of mentions for overlapping chains is selected."", ""3. setting 3 ('unify - concatenate') : chains that overlap are treated as one chain starting from a certain percentage of overlap."", 'for both single - and multi - source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages.', '2 sentence alignment was performed using hunalign  #AUTHOR_TAG ; word alignments were computed with giza + +  #AUTHOR_TAG on a parallel newswire corpus  #AUTHOR_TAG.', '3 computed as dice coefficient']",3
['as described in  #TAUTHOR_TAG'],['as described in  #TAUTHOR_TAG'],"['transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming']","['our experiment, we implement a direct projection method for coreference as described in  #AUTHOR_TAG.', 'our method works as follows : for each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one.', 'since the corpus was already sentence - and word - aligned 2, we use the available alignments to transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming from different sources and ( b ) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3.', 'in our experiment, we apply the following strategies from  #TAUTHOR_TAG :', ""1. setting 1 ('add') : disjoint chains from one source language are added to all the chains projected from the other source language ; 2. setting 2 ('unify - intersect') : the intersection of mentions for overlapping chains is selected."", ""3. setting 3 ('unify - concatenate') : chains that overlap are treated as one chain starting from a certain percentage of overlap."", 'for both single - and multi - source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages.', '2 sentence alignment was performed using hunalign  #AUTHOR_TAG ; word alignments were computed with giza + +  #AUTHOR_TAG on a parallel newswire corpus  #AUTHOR_TAG.', '3 computed as dice coefficient']",3
"['the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by']","['the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by intersecting the overlapping chains ( setting 2 ) and']","['the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by']","['evaluate the projection results, we computed the standard coreference metrics - muc  #AUTHOR_TAG, b - cubed  #AUTHOR_TAG and ceaf  #AUTHOR_TAG - and their average for each of the approaches ( table 3 ).', 'as one can see from the table, the quality of projections from english to russian outperforms the quality of projections from german to russian by 6. 5 points f1.', 'moreover, while precision number are quite similar, projections from english exhibit higher recall numbers.', 'as for the multi - source settings, we were able to achieve the highest f1 of 36. 2 by combining disjoint chains ( setting 1 ), which is 1. 9 point higher than the best single - source projection scores and constitutes almost 62 % of the quality of the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by intersecting the overlapping chains ( setting 2 ) and the highest recall by concatenating them ( setting 3 ).', 'finally, we evaluate the annotations coming from english and german against each other, in order to estimate their comparability and the percentage of overlap.', 'interestingly, we achieve 52. 0 f1, with precision being slightly higher than recall ( 53. 9 vs. 50. 2 ), which shows the dissimilarity between the two projections']",3
"['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection']","['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection']","['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection accuracy']","['# % # % markables 757 82. 7 596 57. 6 chains 182 100. 0 227 84. 7 table 4 : transferred chains and markables', 'since we do not have access to any gold alignment data, we estimate the quality of the word alignments by computing the number of unaligned tokens.', 'not surprisingly, we see a higher percentage of unaligned words for german - russian than for english - russian : 17. 03 % vs. 14. 96 % respectively, which supports our hypothesis regarding the difference in the alignment quality for the two pairs.', 'furthermore, we computed the distribution of unaligned words : the highest percentage of unaligned tokens disregarding punctuation marks are prepositions ; pronouns constitute only 3 % and 5 % of all unaligned words for the alignments between english - russian and german - russian respectively.', 'however, these numbers do not constitute more than 5 % of the overall number of pronouns in the corpus.', ""following the work of  #TAUTHOR_TAG, we analyse the projection accuracy for common nouns ('nc'), named entities ('np') and pronouns ('p') separately 4 : table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type."", 'our results conform to the results of  #TAUTHOR_TAG : for both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi - token common and proper names.', '']",3
"[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our']","['of the recent work on exploiting coreference relations in machine translation focused on improving the translation of anaphoric pronouns  #AUTHOR_TAG novak et al., 2015 ;  #AUTHOR_TAG, disregarding other types of coreference relations, one of the reasons being the lack of annotated parallel corpora as well as the variability in the annotated data.', 'however, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages.', 'more importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme.', 'in our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains.', 'in particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method.', 'our approach to the annotation projection builds upon the approach recently introduced by  #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our goal is slightly different : we are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages.', 'therefore, in contrast to  #TAUTHOR_TAG, we use automatic source annotations produced by two state - of - the - art coreference systems, and we combine the output of our projection method for two source languages ( english and german ) to obtain target annotations for a third language ( russian ).', 'through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method.', 'the paper is organized as follows : section 2 presents an overview of the related work and section 3 describes the experimental setup.', 'in section 4, we give a detailed error analysis and discuss the results of our experiment.', '']",5
['of  #TAUTHOR_TAG as the basis'],['of  #TAUTHOR_TAG as the basis'],"['automatically produced annotations.', 'we use the english - german - russian unannotated corpus of  #TAUTHOR_TAG as the basis']","['our experiment, we propose a fully automatic projection setup : first, we perform coreference resolution on the source language data and then we implement the single - and multi - source approaches to transfer the automatically produced annotations.', 'we use the english - german - russian unannotated corpus of  #TAUTHOR_TAG as the basis for our experiment, which contains texts in two genres - newswire texts ( 229 sentences per language ) and short stories ( 184 sentences per language ).', 'furthermore, we use manual annotations present in the corpus as the gold standard for our evaluation.', 'it should be noted that the manual annotations were performed according to the parallel coreference annotation guidelines of  #AUTHOR_TAG that are in general compatible with the annotation of the the ontonotes corpus  #AUTHOR_TAG and are therefore suitable for our evaluation']",5
['as described in  #TAUTHOR_TAG'],['as described in  #TAUTHOR_TAG'],"['transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming']","['our experiment, we implement a direct projection method for coreference as described in  #AUTHOR_TAG.', 'our method works as follows : for each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one.', 'since the corpus was already sentence - and word - aligned 2, we use the available alignments to transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming from different sources and ( b ) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3.', 'in our experiment, we apply the following strategies from  #TAUTHOR_TAG :', ""1. setting 1 ('add') : disjoint chains from one source language are added to all the chains projected from the other source language ; 2. setting 2 ('unify - intersect') : the intersection of mentions for overlapping chains is selected."", ""3. setting 3 ('unify - concatenate') : chains that overlap are treated as one chain starting from a certain percentage of overlap."", 'for both single - and multi - source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages.', '2 sentence alignment was performed using hunalign  #AUTHOR_TAG ; word alignments were computed with giza + +  #AUTHOR_TAG on a parallel newswire corpus  #AUTHOR_TAG.', '3 computed as dice coefficient']",5
['as described in  #TAUTHOR_TAG'],['as described in  #TAUTHOR_TAG'],"['transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming']","['our experiment, we implement a direct projection method for coreference as described in  #AUTHOR_TAG.', 'our method works as follows : for each markable on the source side, we automatically select all the corresponding tokens on the target side aligned to it, and we then take the span between the first and the last word as the new target markable, which has the same coreference chain number as the source one.', 'since the corpus was already sentence - and word - aligned 2, we use the available alignments to transfer the annotations.', 'thereafter, we re - implement the multi - source approach as described in  #TAUTHOR_TAG.', 'in particular, they ( a ) looked at disjoint chains coming from different sources and ( b ) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3.', 'in our experiment, we apply the following strategies from  #TAUTHOR_TAG :', ""1. setting 1 ('add') : disjoint chains from one source language are added to all the chains projected from the other source language ; 2. setting 2 ('unify - intersect') : the intersection of mentions for overlapping chains is selected."", ""3. setting 3 ('unify - concatenate') : chains that overlap are treated as one chain starting from a certain percentage of overlap."", 'for both single - and multi - source approaches, we deliberately rely solely on word alignment information to project the annotations, in order to keep our approach easily transferable to other languages.', '2 sentence alignment was performed using hunalign  #AUTHOR_TAG ; word alignments were computed with giza + +  #AUTHOR_TAG on a parallel newswire corpus  #AUTHOR_TAG.', '3 computed as dice coefficient']",5
"['the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by']","['the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by intersecting the overlapping chains ( setting 2 ) and']","['the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by']","['evaluate the projection results, we computed the standard coreference metrics - muc  #AUTHOR_TAG, b - cubed  #AUTHOR_TAG and ceaf  #AUTHOR_TAG - and their average for each of the approaches ( table 3 ).', 'as one can see from the table, the quality of projections from english to russian outperforms the quality of projections from german to russian by 6. 5 points f1.', 'moreover, while precision number are quite similar, projections from english exhibit higher recall numbers.', 'as for the multi - source settings, we were able to achieve the highest f1 of 36. 2 by combining disjoint chains ( setting 1 ), which is 1. 9 point higher than the best single - source projection scores and constitutes almost 62 % of the quality of the projection of gold standard annotations reported in  #TAUTHOR_TAG.', 'we were able to achieve the highest precision scores by intersecting the overlapping chains ( setting 2 ) and the highest recall by concatenating them ( setting 3 ).', 'finally, we evaluate the annotations coming from english and german against each other, in order to estimate their comparability and the percentage of overlap.', 'interestingly, we achieve 52. 0 f1, with precision being slightly higher than recall ( 53. 9 vs. 50. 2 ), which shows the dissimilarity between the two projections']",5
"['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection']","['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection']","['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection accuracy']","['# % # % markables 757 82. 7 596 57. 6 chains 182 100. 0 227 84. 7 table 4 : transferred chains and markables', 'since we do not have access to any gold alignment data, we estimate the quality of the word alignments by computing the number of unaligned tokens.', 'not surprisingly, we see a higher percentage of unaligned words for german - russian than for english - russian : 17. 03 % vs. 14. 96 % respectively, which supports our hypothesis regarding the difference in the alignment quality for the two pairs.', 'furthermore, we computed the distribution of unaligned words : the highest percentage of unaligned tokens disregarding punctuation marks are prepositions ; pronouns constitute only 3 % and 5 % of all unaligned words for the alignments between english - russian and german - russian respectively.', 'however, these numbers do not constitute more than 5 % of the overall number of pronouns in the corpus.', ""following the work of  #TAUTHOR_TAG, we analyse the projection accuracy for common nouns ('nc'), named entities ('np') and pronouns ('p') separately 4 : table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type."", 'our results conform to the results of  #TAUTHOR_TAG : for both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi - token common and proper names.', '']",5
"[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our']","['of the recent work on exploiting coreference relations in machine translation focused on improving the translation of anaphoric pronouns  #AUTHOR_TAG novak et al., 2015 ;  #AUTHOR_TAG, disregarding other types of coreference relations, one of the reasons being the lack of annotated parallel corpora as well as the variability in the annotated data.', 'however, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages.', 'more importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme.', 'in our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains.', 'in particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method.', 'our approach to the annotation projection builds upon the approach recently introduced by  #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our goal is slightly different : we are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages.', 'therefore, in contrast to  #TAUTHOR_TAG, we use automatic source annotations produced by two state - of - the - art coreference systems, and we combine the output of our projection method for two source languages ( english and german ) to obtain target annotations for a third language ( russian ).', 'through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method.', 'the paper is organized as follows : section 2 presents an overview of the related work and section 3 describes the experimental setup.', 'in section 4, we give a detailed error analysis and discuss the results of our experiment.', '']",4
"['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection']","['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection']","['of pronouns in the corpus.', 'following the work of  #TAUTHOR_TAG, we analyse the projection accuracy']","['# % # % markables 757 82. 7 596 57. 6 chains 182 100. 0 227 84. 7 table 4 : transferred chains and markables', 'since we do not have access to any gold alignment data, we estimate the quality of the word alignments by computing the number of unaligned tokens.', 'not surprisingly, we see a higher percentage of unaligned words for german - russian than for english - russian : 17. 03 % vs. 14. 96 % respectively, which supports our hypothesis regarding the difference in the alignment quality for the two pairs.', 'furthermore, we computed the distribution of unaligned words : the highest percentage of unaligned tokens disregarding punctuation marks are prepositions ; pronouns constitute only 3 % and 5 % of all unaligned words for the alignments between english - russian and german - russian respectively.', 'however, these numbers do not constitute more than 5 % of the overall number of pronouns in the corpus.', ""following the work of  #TAUTHOR_TAG, we analyse the projection accuracy for common nouns ('nc'), named entities ('np') and pronouns ('p') separately 4 : table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type."", 'our results conform to the results of  #TAUTHOR_TAG : for both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi - token common and proper names.', '']",4
"[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to']","[' #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our']","['of the recent work on exploiting coreference relations in machine translation focused on improving the translation of anaphoric pronouns  #AUTHOR_TAG novak et al., 2015 ;  #AUTHOR_TAG, disregarding other types of coreference relations, one of the reasons being the lack of annotated parallel corpora as well as the variability in the annotated data.', 'however, this could be alleviated by exploiting annotation projection across parallel corpora to create more linguistically annotated resources for new languages.', 'more importantly, applying annotation projection using several source languages would support the creation of corpora less biased towards the peculiarities of a single source annotation scheme.', 'in our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains.', 'in particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method.', 'our approach to the annotation projection builds upon the approach recently introduced by  #TAUTHOR_TAG, who experimented with projecting manually annotated coreference chains from two source languages to the target language.', 'however, our goal is slightly different : we are interested in developing a fully automatic pipeline, which would support the automatic creation of parallel annotated corpora in new languages.', 'therefore, in contrast to  #TAUTHOR_TAG, we use automatic source annotations produced by two state - of - the - art coreference systems, and we combine the output of our projection method for two source languages ( english and german ) to obtain target annotations for a third language ( russian ).', 'through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method.', 'the paper is organized as follows : section 2 presents an overview of the related work and section 3 describes the experimental setup.', 'in section 4, we give a detailed error analysis and discuss the results of our experiment.', '']",6
"['of  #AUTHOR_TAG  #AUTHOR_TAG.', 'thereafter,  #TAUTHOR_TAG proposed a multi - source method for annotation projection :']","['of  #AUTHOR_TAG  #AUTHOR_TAG.', 'thereafter,  #TAUTHOR_TAG proposed a multi - source method for annotation projection :']","['to automatically obtain annotated data.', 'it was first introduced in the work of  #AUTHOR_TAG  #AUTHOR_TAG.', 'thereafter,  #TAUTHOR_TAG proposed a multi - source method for annotation projection : they used a manually annotated']","['projection is a method that allows for automatically transferring annotations from a well - studied ( source ) language to a low - resource ( target ) language in a parallel corpus in order to automatically obtain annotated data.', 'it was first introduced in the work of  #AUTHOR_TAG  #AUTHOR_TAG.', 'thereafter,  #TAUTHOR_TAG proposed a multi - source method for annotation projection : they used a manually annotated trilingual coreference corpus and two source languages ( english - german, english - russian ) to transfer annotations to the target language ( russian and german, respectively ).', 'although their approach showed promising results, it was based on transferring manually produced annotations, which are typically not available for other languages and, more importantly, can not be acquired large - scale due to the complexity of the annotation task']",0
"['extend  #TAUTHOR_TAG by incorporating pre - training for ten additional languages, and compare the']","['extend  #TAUTHOR_TAG by incorporating pre - training for ten additional languages, and compare the']","['extend  #TAUTHOR_TAG by incorporating pre - training for ten additional languages, and compare the benefits']","['extend  #TAUTHOR_TAG by incorporating pre - training for ten additional languages, and compare the benefits of no pre - training, elmo, and bert  #AUTHOR_TAG.', 'pre - training is effective across all languages evaluated, and bert outperforms elmo in large part due to the benefits of increased model capacity.', 'our parser obtains new state - of - the - art results for 11 languages, including english ( 95. 8 f1 ) and chinese ( 91. 8 f1 )']",6
"['of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['a range of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations are helpful for constituency parsing.', 'however,  #TAUTHOR_TAG results only considered the lstm - based elmo representations, and only for the english language.', 'we now extend  #TAUTHOR_TAG work to show that using only self - attention also works by substituting bert  #AUTHOR_TAG.', 'we further demonstrate that pre - training and self - attention are effective across languages by applying our parsing architecture to ten additional languages.', 'our parser code and trained models for 11 languages are publicly available.', '']",6
"['of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['a range of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations are helpful for constituency parsing.', 'however,  #TAUTHOR_TAG results only considered the lstm - based elmo representations, and only for the english language.', 'we now extend  #TAUTHOR_TAG work to show that using only self - attention also works by substituting bert  #AUTHOR_TAG.', 'we further demonstrate that pre - training and self - attention are effective across languages by applying our parsing architecture to ten additional languages.', 'our parser code and trained models for 11 languages are publicly available.', '']",0
['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors'],['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors'],['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors corresponding'],[' #TAUTHOR_TAG'],0
"['of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['a range of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations']","['has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG showed that such representations are helpful for constituency parsing.', 'however,  #TAUTHOR_TAG results only considered the lstm - based elmo representations, and only for the english language.', 'we now extend  #TAUTHOR_TAG work to show that using only self - attention also works by substituting bert  #AUTHOR_TAG.', 'we further demonstrate that pre - training and self - attention are effective across languages by applying our parsing architecture to ten additional languages.', 'our parser code and trained models for 11 languages are publicly available.', '']",1
['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors'],['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors'],['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors corresponding'],[' #TAUTHOR_TAG'],5
['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors'],['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors'],['as described in  #TAUTHOR_TAG accepts as input a sequence of vectors corresponding'],[' #TAUTHOR_TAG'],3
"['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary']","['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary']","['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary application of the  #TAUTHOR_TAG annotation has been to news articles.', 'in this study, we tested']","['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary application of the  #TAUTHOR_TAG annotation has been to news articles.', 'in this study, we tested whether the  #TAUTHOR_TAG guidelines can be adapted to a different genre.', 'we annotated discourse connectives and  #TAUTHOR_TAG arguments in one 4, 937 - token full - text biomedical article.', 'two linguist annotators showed an agreement of 85 % after simple conventions were added.', 'for the remaining 15 % cases, we found that biomedical domain - specific knowledge is needed to capture the linguistic cues that can be used to resolve inter - annotator disagreement.', 'we found that the two annotators were able to reach an agreement after discussion.', 'thus our experiments suggest that the  #TAUTHOR_TAG annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain - specific linguistic cues']",1
[' #TAUTHOR_TAG annotates the argument'],"[' #TAUTHOR_TAG annotates the argument structure, semantics, and attribution of discourse connectives and their arguments.', 'the current release of pdtb - 2. 0 contains the annotations of']","['1993 ), have played an important role in text - mining.', 'the  #TAUTHOR_TAG annotates the argument structure, semantics, and attribution of discourse connectives and their arguments.', 'the current release of pdtb - 2']","['scale annotated corpora, e. g., the penn treebank ( ptb ) project ( marcus et al. 1993 ), have played an important role in text - mining.', 'the  #TAUTHOR_TAG annotates the argument structure, semantics, and attribution of discourse connectives and their arguments.', 'the current release of pdtb - 2. 0 contains the annotations of 1, 808 wall street journal articles ( ~ 1 million words ) from the penn treebank ( marcus et al. 1993 ) ii distribution and a total of 40, 600 discourse connective tokens ( prasad et al. 2008b ).', 'this work examines whether the  #TAUTHOR_TAG annotation guidelines can be adapted to a different genre, the biomedical literature']",1
"['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary']","['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary']","['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary application of the  #TAUTHOR_TAG annotation has been to news articles.', 'in this study, we tested']","['goal of the  #TAUTHOR_TAG project is to develop a large - scale corpus, annotated with coherence relations marked by discourse connectives.', 'currently, the primary application of the  #TAUTHOR_TAG annotation has been to news articles.', 'in this study, we tested whether the  #TAUTHOR_TAG guidelines can be adapted to a different genre.', 'we annotated discourse connectives and  #TAUTHOR_TAG arguments in one 4, 937 - token full - text biomedical article.', 'two linguist annotators showed an agreement of 85 % after simple conventions were added.', 'for the remaining 15 % cases, we found that biomedical domain - specific knowledge is needed to capture the linguistic cues that can be used to resolve inter - annotator disagreement.', 'we found that the two annotators were able to reach an agreement after discussion.', 'thus our experiments suggest that the  #TAUTHOR_TAG annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain - specific linguistic cues']",6
"['the  #TAUTHOR_TAG annotation manual ( prasad et al. 2008b ), we conducted a pilot annotation of discourse connectivity in biomedical text.', 'as an initial step,']","['the  #TAUTHOR_TAG annotation manual ( prasad et al. 2008b ), we conducted a pilot annotation of discourse connectivity in biomedical text.', 'as an initial step,']","['the  #TAUTHOR_TAG annotation manual ( prasad et al. 2008b ), we conducted a pilot annotation of discourse connectivity in biomedical text.', 'as an initial step,']","['the  #TAUTHOR_TAG annotation manual ( prasad et al. 2008b ), we conducted a pilot annotation of discourse connectivity in biomedical text.', 'as an initial step, we only annotated the three most important components of a discourse relation ; namely, a discourse connective and its two arguments ; we did not annotate attribution.', 'two linguist annotators independently annotated one full - text biomedical article ( verpy et al. 1999 ) that we randomly selected.', 'the article is 4, 937 tokens long.', 'when the annotation work was completed, we measured the inter - annotator agreement, following the  #TAUTHOR_TAG exact match criterion ( miltsakaki et al. 2004 ).', 'according to this criterion, a discourse relation is in disagreement if there is disagreement on any textspan ( i. e., the discourse connective or any of its two arguments ).', 'in addition, we also measured the agreement in the components ( i. e., discourse connectives and the arguments ).', 'we discussed the annotation results and made suggestions to adapt the  #TAUTHOR_TAG guidelines to biomedical text']",6
"['of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines']","['of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines']","['the completion of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines']","['the completion of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines to address the characteristics of biomedical text :', 'i. citation references are to be annotated as a part of an argument because the inclusion will benefit many text - mining tasks including identifying the semantic relations among citations.', '']",6
"['of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines']","['of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines']","['the completion of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines']","['the completion of the pilot annotation and the discussion, we decided to add the following conventions to the  #TAUTHOR_TAG annotation guidelines to address the characteristics of biomedical text :', 'i. citation references are to be annotated as a part of an argument because the inclusion will benefit many text - mining tasks including identifying the semantic relations among citations.', '']",6
"['.', 'the  #TAUTHOR_TAG also reported a']","['', 'the  #TAUTHOR_TAG also reported a']","['.', 'the  #TAUTHOR_TAG also reported']","['', 'the  #TAUTHOR_TAG also reported a higher level of agreement in annotating arg2 than in annotating arg1 ( miltsakaki et al. 2004 ).', 'we manually analyzed the cases with disagreement.', 'we found the disagreements are nearly all related to the annotation of citation references, supplementary clauses, and other conventions.', 'when a few conventions for these cases were added, the inter - annotator agreement went up to 85 %.', 'we also found that different interpretation of a relation and its arguments by annotators plays an important role for the remaining 15 % inconsistency, and domain - specific knowledge is necessary to resolve such cases']",4
['games  #TAUTHOR_TAG used'],[') and novel algorithms ( such as cooperative / adversarial games  #TAUTHOR_TAG used'],['adversarial games  #TAUTHOR_TAG used'],[' #TAUTHOR_TAG'],7
['games  #TAUTHOR_TAG used'],[') and novel algorithms ( such as cooperative / adversarial games  #TAUTHOR_TAG used'],['adversarial games  #TAUTHOR_TAG used'],[' #TAUTHOR_TAG'],7
['.  #TAUTHOR_TAG ; ) try to'],[' #TAUTHOR_TAG ; ) try to'],['.  #TAUTHOR_TAG ; ) try to'],"['recent years, the research community has noticed the great success of neural networks in computer vision  #AUTHOR_TAG, speech recognition and natural language processing  #AUTHOR_TAG tasks.', 'however, the potential of neural networks has not been fully investigated in the ir field.', 'although a significant number of studies ( e. g.  #TAUTHOR_TAG ; ) try to apply neural networks in ir, there have been few studies reporting the performance that is comparable to state - of - the - art ir models.', 'these approaches rely on the general idea that neural network can provide a low - dimensional and semantics - rich representation for both queries and documents.', 'such a representation can bridge lexical and semantic gaps in traditional ir models.', '']",1
['.  #TAUTHOR_TAG ; ) try to'],[' #TAUTHOR_TAG ; ) try to'],['.  #TAUTHOR_TAG ; ) try to'],"['recent years, the research community has noticed the great success of neural networks in computer vision  #AUTHOR_TAG, speech recognition and natural language processing  #AUTHOR_TAG tasks.', 'however, the potential of neural networks has not been fully investigated in the ir field.', 'although a significant number of studies ( e. g.  #TAUTHOR_TAG ; ) try to apply neural networks in ir, there have been few studies reporting the performance that is comparable to state - of - the - art ir models.', 'these approaches rely on the general idea that neural network can provide a low - dimensional and semantics - rich representation for both queries and documents.', 'such a representation can bridge lexical and semantic gaps in traditional ir models.', '']",0
"[' #TAUTHOR_TAG use', 'bm']","[' #TAUTHOR_TAG use', 'bm']","['representation learning.  #TAUTHOR_TAG use', 'bm']","['comparable corpora the embeddings that can be used for both monolingual ir and cross - lingual ir.', 'supervised approaches use query - document relevance information to learn', 'the representation that is optimized end - to - end for the task at hand', '. with click - through data,  #AUTHOR_TAG develop dssm, a feed forward neural network with a word hashing phrase as the first layer to predict the click probability given a query string and a', 'document title. dssm is extended in  #AUTHOR_TAG a ;  #AUTHOR_TAG b ) by incorporating convolutional neural network and max - pooling layers to extract the most salient local features.', 'since the dssm related methods make implicit but strong assumptions about click', '##ed data, try to relax the assumptions in their model.  #AUTHOR_TAG develop the drmm model that takes the histogram - based features representing interactions between', 'queries and documents as input into neural networks. drmm is one of the first neural ir models to', 'show improvement over traditional ir models. aim to simultaneously learn local and distributional representation to capture both lexical matching and semantic matching in ir. following', 'the discussion in section 1, we note that click - through data are not always available in massive amount outside of industrial labs. more recent', 'works propose to use unsupervised ir models to pseudo label query - document pairs that provide weak supervision for representation learning.  #TAUTHOR_TAG use', '']",0
"['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size and number of hidden layers are respectively selected from { 64, 128, 256, 512, 1024 } and { 1, 2, 3, 4 }. the values of α, β in equation 3 are chosen from { 0. 001, 0. 01, 0. 1, 1, 10, 100, 1000 }. we select the initial learning rate from { 10 −3, 10 −4, 5 * 10 −4, 10 −5, 5 * 10 −5 }.', 'the batch size for learning is selected from { 64, 128, 256, 512 }. these model hyper - parameters are tuned on the validation set ( 20 % of the training queries used for validation ).', '']",0
"[' #TAUTHOR_TAG use', 'bm']","[' #TAUTHOR_TAG use', 'bm']","['representation learning.  #TAUTHOR_TAG use', 'bm']","['comparable corpora the embeddings that can be used for both monolingual ir and cross - lingual ir.', 'supervised approaches use query - document relevance information to learn', 'the representation that is optimized end - to - end for the task at hand', '. with click - through data,  #AUTHOR_TAG develop dssm, a feed forward neural network with a word hashing phrase as the first layer to predict the click probability given a query string and a', 'document title. dssm is extended in  #AUTHOR_TAG a ;  #AUTHOR_TAG b ) by incorporating convolutional neural network and max - pooling layers to extract the most salient local features.', 'since the dssm related methods make implicit but strong assumptions about click', '##ed data, try to relax the assumptions in their model.  #AUTHOR_TAG develop the drmm model that takes the histogram - based features representing interactions between', 'queries and documents as input into neural networks. drmm is one of the first neural ir models to', 'show improvement over traditional ir models. aim to simultaneously learn local and distributional representation to capture both lexical matching and semantic matching in ir. following', 'the discussion in section 1, we note that click - through data are not always available in massive amount outside of industrial labs. more recent', 'works propose to use unsupervised ir models to pseudo label query - document pairs that provide weak supervision for representation learning.  #TAUTHOR_TAG use', '']",3
"['pairs ).', 'the actual probability in this paper is estimated in a similar way as in  #TAUTHOR_TAG, which is :', 'where s denotes the relevance scores']","['pairs ).', 'the actual probability in this paper is estimated in a similar way as in  #TAUTHOR_TAG, which is :', 'where s denotes the relevance scores']","['document pairs ).', 'the actual probability in this paper is estimated in a similar way as in  #TAUTHOR_TAG, which is :', 'where s denotes the relevance scores']","['', 'where the score function is computed with the pairwise ranking model, and the parameter σ is used to determine the shape of the sigmoid.', 'the supervised training objective l s on a triplet of query - document pair ( q, d 1, d 2 ) can then be defined as the cross entropy loss, which is :', 'where p ( d 1 q d 2 ) is the actual probability that d 1 is ranked higher than d 2 according to annotations ( i. e. pseudo - labels of query - document pairs ).', 'the actual probability in this paper is estimated in a similar way as in  #TAUTHOR_TAG, which is :', 'where s denotes the relevance scores obtained from training instances.', 'in the training process, the positive sample d 1 for the query q can be chosen as the most relevant documents according to annotated relevance scores.', 'the negative sample d 2 is selected randomly from the document collection']",3
"['in recent studies  #TAUTHOR_TAG.', 'the details of these collections']","['in recent studies  #TAUTHOR_TAG.', 'the details of these collections']","['in recent studies  #TAUTHOR_TAG.', 'the details of these collections']","['ir experiments are carried out against standard trec collections consisting of one robust track and one web track, which represent different sizes and genres of heterogeneous text collections.', 'these collections have been broadly used in recent studies  #TAUTHOR_TAG.', 'the details of these collections and corresponding queries are given in table 1.', 'the robust dataset is used in the standard form without change.', 'the clueweb - 09 - cat - b collection ( or clueweb for short ) is filtered to the set of documents with spam scores in the 60 - th percentile with waterloo fusion spam scores 1.', '']",3
"['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size and number of hidden layers are respectively selected from { 64, 128, 256, 512, 1024 } and { 1, 2, 3, 4 }. the values of α, β in equation 3 are chosen from { 0. 001, 0. 01, 0. 1, 1, 10, 100, 1000 }. we select the initial learning rate from { 10 −3, 10 −4, 5 * 10 −4, 10 −5, 5 * 10 −5 }.', 'the batch size for learning is selected from { 64, 128, 256, 512 }. these model hyper - parameters are tuned on the validation set ( 20 % of the training queries used for validation ).', '']",3
"['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size and number of hidden layers are respectively selected from { 64, 128, 256, 512, 1024 } and { 1, 2, 3, 4 }. the values of α, β in equation 3 are chosen from { 0. 001, 0. 01, 0. 1, 1, 10, 100, 1000 }. we select the initial learning rate from { 10 −3, 10 −4, 5 * 10 −4, 10 −5, 5 * 10 −5 }.', 'the batch size for learning is selected from { 64, 128, 256, 512 }. these model hyper - parameters are tuned on the validation set ( 20 % of the training queries used for validation ).', '']",3
"['in recent studies  #TAUTHOR_TAG.', 'the details of these collections']","['in recent studies  #TAUTHOR_TAG.', 'the details of these collections']","['in recent studies  #TAUTHOR_TAG.', 'the details of these collections']","['ir experiments are carried out against standard trec collections consisting of one robust track and one web track, which represent different sizes and genres of heterogeneous text collections.', 'these collections have been broadly used in recent studies  #TAUTHOR_TAG.', 'the details of these collections and corresponding queries are given in table 1.', 'the robust dataset is used in the standard form without change.', 'the clueweb - 09 - cat - b collection ( or clueweb for short ) is filtered to the set of documents with spam scores in the 60 - th percentile with waterloo fusion spam scores 1.', '']",4
"['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size']","['set the hyper - parameters of our model by following similar tasks such as  #TAUTHOR_TAG.', 'the size and number of hidden layers are respectively selected from { 64, 128, 256, 512, 1024 } and { 1, 2, 3, 4 }. the values of α, β in equation 3 are chosen from { 0. 001, 0. 01, 0. 1, 1, 10, 100, 1000 }. we select the initial learning rate from { 10 −3, 10 −4, 5 * 10 −4, 10 −5, 5 * 10 −5 }.', 'the batch size for learning is selected from { 64, 128, 256, 512 }. these model hyper - parameters are tuned on the validation set ( 20 % of the training queries used for validation ).', '']",5
['is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from'],"['across different domains / industries such as telecom, healthcare, finance etc.', 'soda is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from']","['is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from the source to the new target collection while being cognizant of similarity between the two collections.', 'it has been extensively evaluated by business professionals in a user - trial and on a benchmark dataset.', 'figure 1 illustrates the architecture of']","['', 'from our conversations with business professionals, we derived the need of a practical solution which would help them scale up across hundreds of collections and domains without the overhead of annotating data and building models from scratch every time for a new collection.', 'in this paper, we demonstrate service oriented domain adaptation ( soda ) which offers social media analytics as - a - service to the users.', 'specifically, it provides sentiment categorization as - aservice that allows users to efficiently analyze comments from any new collection without the over - head of manual annotations or re - training models.', 'it thus enables faster wide - scale analysis within and across different domains / industries such as telecom, healthcare, finance etc.', 'soda is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from the source to the new target collection while being cognizant of similarity between the two collections.', 'it has been extensively evaluated by business professionals in a user - trial and on a benchmark dataset.', 'figure 1 illustrates the architecture of soda comprising three primary modules, 1 ) similarity, 2 ) domain adaptation, and 3 ) active learning.', 'the first two modules use unlabeled data from the new collection while the optional third module helps in creating labeled data for enhanced classification performance.', 'these modules are explained']",5
['from existing literature  #TAUTHOR_TAG which suggest that if'],['from existing literature  #TAUTHOR_TAG which suggest that if'],"['the source and target collections.', 'this is based on the observations from existing literature  #TAUTHOR_TAG which suggest that if the source']","['social media analytics, especially for sentiment categorization, there exist numerous collections about different products or services where labeled data is available and thus can be used to adapt to a new unlabeled collection.', 'given a target collection, the key question is to identify the best possible source collection to adapt from.', 'the similarity module in soda identifies the best adaptable source collection based on the similarity between the source and target collections.', 'this is based on the observations from existing literature  #TAUTHOR_TAG which suggest that if the source and target collections are similar, the adaptation performance tends to be better than if the two collections are dissimilar.', 'the similarity module in soda is capable of computing different kinds of lexical, syntactic, and semantic similarities between unlabeled target and labeled source collections.', 'for this demonstration on sentiment categorization from social media data, it measures cosine similarity between the comments in each collection and computes sim as the similarity score']",5
['( refer  #TAUTHOR_TAG for more details )'],['( refer  #TAUTHOR_TAG for more details )'],['( refer  #TAUTHOR_TAG for more details )'],"['', 'algorithm 1 summarizes our approach ( refer  #TAUTHOR_TAG for more details )']",5
['is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from'],"['across different domains / industries such as telecom, healthcare, finance etc.', 'soda is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from']","['is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from the source to the new target collection while being cognizant of similarity between the two collections.', 'it has been extensively evaluated by business professionals in a user - trial and on a benchmark dataset.', 'figure 1 illustrates the architecture of']","['', 'from our conversations with business professionals, we derived the need of a practical solution which would help them scale up across hundreds of collections and domains without the overhead of annotating data and building models from scratch every time for a new collection.', 'in this paper, we demonstrate service oriented domain adaptation ( soda ) which offers social media analytics as - a - service to the users.', 'specifically, it provides sentiment categorization as - aservice that allows users to efficiently analyze comments from any new collection without the over - head of manual annotations or re - training models.', 'it thus enables faster wide - scale analysis within and across different domains / industries such as telecom, healthcare, finance etc.', 'soda is based on an iterative ensemble based adaptation technique  #TAUTHOR_TAG which gradually transfers knowledge from the source to the new target collection while being cognizant of similarity between the two collections.', 'it has been extensively evaluated by business professionals in a user - trial and on a benchmark dataset.', 'figure 1 illustrates the architecture of soda comprising three primary modules, 1 ) similarity, 2 ) domain adaptation, and 3 ) active learning.', 'the first two modules use unlabeled data from the new collection while the optional third module helps in creating labeled data for enhanced classification performance.', 'these modules are explained']",0
['from existing literature  #TAUTHOR_TAG which suggest that if'],['from existing literature  #TAUTHOR_TAG which suggest that if'],"['the source and target collections.', 'this is based on the observations from existing literature  #TAUTHOR_TAG which suggest that if the source']","['social media analytics, especially for sentiment categorization, there exist numerous collections about different products or services where labeled data is available and thus can be used to adapt to a new unlabeled collection.', 'given a target collection, the key question is to identify the best possible source collection to adapt from.', 'the similarity module in soda identifies the best adaptable source collection based on the similarity between the source and target collections.', 'this is based on the observations from existing literature  #TAUTHOR_TAG which suggest that if the source and target collections are similar, the adaptation performance tends to be better than if the two collections are dissimilar.', 'the similarity module in soda is capable of computing different kinds of lexical, syntactic, and semantic similarities between unlabeled target and labeled source collections.', 'for this demonstration on sentiment categorization from social media data, it measures cosine similarity between the comments in each collection and computes sim as the similarity score']",0
['( refer  #TAUTHOR_TAG for more details )'],['( refer  #TAUTHOR_TAG for more details )'],['( refer  #TAUTHOR_TAG for more details )'],"['', 'algorithm 1 summarizes our approach ( refer  #TAUTHOR_TAG for more details )']",0
"[' #TAUTHOR_TAG 24 ] was proposed, which uses']","[' #TAUTHOR_TAG 24 ] was proposed, which uses']","['- attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence']","['works proposed partially - or purely - convolutional ctc models [ 8 ] [ 9 ] [ 10 ] [ 11 ] and convolution - heavy encoder - decoder models [ 16 ] for asr.', 'however, convolutional models must be significantly deeper to retrieve the same temporal receptive field [ 23 ].', 'recently, the mechanism of self - attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.', 'its use in both encoder - decoder and feedforward contexts has led to faster training and state - of - the - art results in translation ( via the transformer  #TAUTHOR_TAG, sentiment analysis [ 25 ], and other tasks.', '']",0
"[' #TAUTHOR_TAG 24 ] was proposed, which uses']","[' #TAUTHOR_TAG 24 ] was proposed, which uses']","['- attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence']","['works proposed partially - or purely - convolutional ctc models [ 8 ] [ 9 ] [ 10 ] [ 11 ] and convolution - heavy encoder - decoder models [ 16 ] for asr.', 'however, convolutional models must be significantly deeper to retrieve the same temporal receptive field [ 23 ].', 'recently, the mechanism of self - attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.', 'its use in both encoder - decoder and feedforward contexts has led to faster training and state - of - the - art results in translation ( via the transformer  #TAUTHOR_TAG, sentiment analysis [ 25 ], and other tasks.', '']",0
"['- product, self - attention  #TAUTHOR_TAG.', 'for']","['performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', 'for']","['- product, self - attention  #TAUTHOR_TAG.', 'for']","['', 'the first sublayer performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', '']",0
"['- product, self - attention  #TAUTHOR_TAG.', 'for']","['performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', 'for']","['- product, self - attention  #TAUTHOR_TAG.', 'for']","['', 'the first sublayer performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', '']",0
"['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only']","['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only [ 21 ], which forgoes position encodings ; additive [ 19 ],']","['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only [ 21 ], which forg']","['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only [ 21 ], which forgoes position encodings ; additive [ 19 ], which takes demb = dh and adds the encoding to the embedding ; and concatenative, where one takes demb = 40 and concatenates it to the embedding.', 'the latter was found necessary for self - attentional las [ 27 ], as additive encodings did not give convergence.', 'however, the monotonicity of ctc is a further positional inductive bias, which may enable the success of content - only and additive encodings']",0
"['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['take ( nlayers, dh, nheads, dff ) = ( 10, 512, 8, 2048 ), giving ∼30m parameters.', 'this is on par with models on wsj ( 10 - 30m ) [ 4, 5, 9 ] and an order of magnitude below models on librispeech ( 100 - 250m ) [ 8, 23 ].', 'we use mxnet [ 37 ] for modeling and kaldi / eesen [ 7, 38 ] for data preparation and decoding.', ""our self - attention code is based on gluonnlp's implementation."", 'at train time, utterances are sorted by length : we exclude those longer than 1800 frames ( 1 % of each training set ).', 'we take a window of 25ms, a hop of 10ms, and concatenate cepstral mean - variance normalized features with temporal first - and second - order differences.', '1 we downsample by a factor of k = 3 ( this also gave an ideal t / k ≈ dh for our data ; see table 1 ).', 'we perform nesterov - accelerated gradient descent on batches of 20 utterances.', 'as self - attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', 'let n denote the global step number of the batch ( across epochs ) ; the learning rate is given by', '1 rescaling so that these differences also have var.', '≈ 1 helped wsj training.', '[ 17 ] 11. 5 - 9. 0 - enc - dec ( 4 - 1 ) [ 17 ] 12. 0 - 8. 2 - enc - dec + ctc ( 4 - 1 ) [ 17 ] 11. 3 - 7. 4 - enc - dec ( 4 - 1 ) [ 39 ] - - 6. 4 9. 3 ctc / asg ( gated cnn ) [ 40 ] 6. 9 9. 5 4. 9 6. 6 enc - dec ( 2, 1, 3 - 1 table 3 : ctc phoneme models with wfst decoding on wsj.', 'where we take λ = 400 and nwarmup as a hyperparameter.', 'however, such a decay led to early stagnation in validation accuracy, so we later divide the learning rate by 10 and run at the decayed rate for 20 epochs.', 'we do this twice, then take the epoch with the best validation score.', 'xavier initialization gave validation accuracies of zero for the first few epochs, suggesting room for improvement.', 'like']",0
"['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in']","['now replace recurrent and convolutional layers for ctc with self - attention [ 24 ].', 'our proposed framework ( figure 1a ) is built around self - attention layers, as used in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section 2. 3.', 'the other stages are downsampling, which reduces input length t via methods like those in section 2. 4 ; embedding, which learns a dh - dim. embedding that also describes token position ( section 2. 5 ) ; and projection, where each final representation is mapped framewise to logits over the intermediate alphabet l.', ""the first implements self - attention, where the success of attention in ctc and encoder - decoder models [ 14, 31 ] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position."", 'hence, the full receptive field is immediately available at the cost of o ( t 2 ) inner products ( table 1 ), enabling richer representations in fewer layers']",3
"['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is']","['path length table 1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is no. of hidden units, and k is filter / context width.', 'we also see inspiration from convolutional blocks : residual connections, layer normalization, and tied dense layers with relu for representation learning.', 'in particular, multi - head attention is akin to having a number of infinitely - wide filters whose weights adapt to the content ( allowing fewer "" filters "" to suffice ).', 'one can also assign interpretations ; for example, [ 27 ] argue their las self - attention heads are differentiated phoneme detectors.', 'further inductive biases like filter widths and causality could be expressed through time - restricted self - attention [ 26 ] and directed self - attention [ 25 ], respectively']",3
"['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in']","['now replace recurrent and convolutional layers for ctc with self - attention [ 24 ].', 'our proposed framework ( figure 1a ) is built around self - attention layers, as used in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section 2. 3.', 'the other stages are downsampling, which reduces input length t via methods like those in section 2. 4 ; embedding, which learns a dh - dim. embedding that also describes token position ( section 2. 5 ) ; and projection, where each final representation is mapped framewise to logits over the intermediate alphabet l.', ""the first implements self - attention, where the success of attention in ctc and encoder - decoder models [ 14, 31 ] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position."", 'hence, the full receptive field is immediately available at the cost of o ( t 2 ) inner products ( table 1 ), enabling richer representations in fewer layers']",5
"['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is']","['path length table 1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is no. of hidden units, and k is filter / context width.', 'we also see inspiration from convolutional blocks : residual connections, layer normalization, and tied dense layers with relu for representation learning.', 'in particular, multi - head attention is akin to having a number of infinitely - wide filters whose weights adapt to the content ( allowing fewer "" filters "" to suffice ).', 'one can also assign interpretations ; for example, [ 27 ] argue their las self - attention heads are differentiated phoneme detectors.', 'further inductive biases like filter widths and causality could be expressed through time - restricted self - attention [ 26 ] and directed self - attention [ 25 ], respectively']",5
,,,,3
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",3
,,,,5
"['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work']","['speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for']","['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply']","['multimodal encoder maps images and their corresponding captions to a common embedding space.', 'the idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space.', 'our model consists of two parts ; an image encoder and a sentence encoder as depicted in figure 1.', 'the approach is based on our own text - based model described in [ 8 ] and on the speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply a 1 - dimensional convolutional layer to the acoustic input features.', 'the convolution has a stride of size 2, kernel size 6 and 64 output channels.', 'this is the only layer where the model differs from the text - based model, which features a character embedding layer instead of a convolutional layer.', '']",5
"['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an']","['', 'vice versa where r @ n is the percentage of test items for which the correct image or', 'caption was in the top n results. we compare our models to [ 12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character - based', 'model is similar to the model we use here and was trained on the original flickr8k text captions', '( see [ 8 ] for a full description ). both our mfcc and mbn', 'based model significantly outperform previous spoken captionto - image methods on the flickr8k dataset. the largest improvement is the mbn model which', 'outperforms the results reported in  #TAUTHOR_TAG by as much as 23. 2 percentage points on r @ 10. the mfcc model also improves on previous results but scores significantly lower than the mbn model across the board, improving as much', 'as 12. 3 percentage points over previous work. there is a large performance gap between the text - caption to image retrieval results and the spoken - caption to image results, showing there is still a lot of room for improvement']",5
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",5
,,,,0
"['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work']","['speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for']","['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply']","['multimodal encoder maps images and their corresponding captions to a common embedding space.', 'the idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space.', 'our model consists of two parts ; an image encoder and a sentence encoder as depicted in figure 1.', 'the approach is based on our own text - based model described in [ 8 ] and on the speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply a 1 - dimensional convolutional layer to the acoustic input features.', 'the convolution has a stride of size 2, kernel size 6 and 64 output channels.', 'this is the only layer where the model differs from the text - based model, which features a character embedding layer instead of a convolutional layer.', '']",0
"['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an']","['', 'vice versa where r @ n is the percentage of test items for which the correct image or', 'caption was in the top n results. we compare our models to [ 12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character - based', 'model is similar to the model we use here and was trained on the original flickr8k text captions', '( see [ 8 ] for a full description ). both our mfcc and mbn', 'based model significantly outperform previous spoken captionto - image methods on the flickr8k dataset. the largest improvement is the mbn model which', 'outperforms the results reported in  #TAUTHOR_TAG by as much as 23. 2 percentage points on r @ 10. the mfcc model also improves on previous results but scores significantly lower than the mbn model across the board, improving as much', 'as 12. 3 percentage points over previous work. there is a large performance gap between the text - caption to image retrieval results and the spoken - caption to image results, showing there is still a lot of room for improvement']",0
"['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an']","['', 'vice versa where r @ n is the percentage of test items for which the correct image or', 'caption was in the top n results. we compare our models to [ 12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character - based', 'model is similar to the model we use here and was trained on the original flickr8k text captions', '( see [ 8 ] for a full description ). both our mfcc and mbn', 'based model significantly outperform previous spoken captionto - image methods on the flickr8k dataset. the largest improvement is the mbn model which', 'outperforms the results reported in  #TAUTHOR_TAG by as much as 23. 2 percentage points on r @ 10. the mfcc model also improves on previous results but scores significantly lower than the mbn model across the board, improving as much', 'as 12. 3 percentage points over previous work. there is a large performance gap between the text - caption to image retrieval results and the spoken - caption to image results, showing there is still a lot of room for improvement']",0
"['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an']","['', 'vice versa where r @ n is the percentage of test items for which the correct image or', 'caption was in the top n results. we compare our models to [ 12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character - based', 'model is similar to the model we use here and was trained on the original flickr8k text captions', '( see [ 8 ] for a full description ). both our mfcc and mbn', 'based model significantly outperform previous spoken captionto - image methods on the flickr8k dataset. the largest improvement is the mbn model which', 'outperforms the results reported in  #TAUTHOR_TAG by as much as 23. 2 percentage points on r @ 10. the mfcc model also improves on previous results but scores significantly lower than the mbn model across the board, improving as much', 'as 12. 3 percentage points over previous work. there is a large performance gap between the text - caption to image retrieval results and the spoken - caption to image results, showing there is still a lot of room for improvement']",0
"['8,  #TAUTHOR_TAG,']","['mismatched pairs ; rather than using all the other samples in the mini - batch as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG,']","['8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples (']","['[ 8 ], the model is trained to embed the images and captions such that the cosine similarity between image and caption pairs is larger ( by a certain margin ) than the similarity be - tween mismatching pairs.', 'this so called hinge loss l as a function of the network parameters θ is given by :', 'where the other caption - image pairs in the batch serve to create mismatched pairs ( c, i ′ ) and ( c ′, i ).', 'we take the cosine similarity cos ( x, y ) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin α.', 'we use importance sampling to select the mismatched pairs ; rather than using all the other samples in the mini - batch as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples ( i. e. mismatched pairs with high cosine similarity ).', 'while [ 10 ] used only the single hardest example in the batch for text - captions, we found that this did not work for the spoken captions.', 'instead we found that using the hardest 25 percent worked well.', 'the networks are trained using adam [ 25 ] with a cyclic learning rate schedule based on [ 26 ].', 'the learning rate schedule varies the learning rate smoothly between a minimum and maximum bound which were set to 10 −6 and 2 × 10 −4 respectively.', 'the learning rate schedule causes the network to visit several local minima during training, allowing us to use snapshot ensembling [ 27 ].', 'by saving the network parameters at each local minimum, we can ensemble the embeddings of multiple networks at no extra cost.', 'we use a margin α = 0. 2 for the loss function.', 'we train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch.', 'for ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings.', 'the main differences with the approaches described in [ 13,  #TAUTHOR_TAG are the use of multi - layered grus, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention']",4
"['8,  #TAUTHOR_TAG,']","['mismatched pairs ; rather than using all the other samples in the mini - batch as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG,']","['8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples (']","['[ 8 ], the model is trained to embed the images and captions such that the cosine similarity between image and caption pairs is larger ( by a certain margin ) than the similarity be - tween mismatching pairs.', 'this so called hinge loss l as a function of the network parameters θ is given by :', 'where the other caption - image pairs in the batch serve to create mismatched pairs ( c, i ′ ) and ( c ′, i ).', 'we take the cosine similarity cos ( x, y ) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin α.', 'we use importance sampling to select the mismatched pairs ; rather than using all the other samples in the mini - batch as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples ( i. e. mismatched pairs with high cosine similarity ).', 'while [ 10 ] used only the single hardest example in the batch for text - captions, we found that this did not work for the spoken captions.', 'instead we found that using the hardest 25 percent worked well.', 'the networks are trained using adam [ 25 ] with a cyclic learning rate schedule based on [ 26 ].', 'the learning rate schedule varies the learning rate smoothly between a minimum and maximum bound which were set to 10 −6 and 2 × 10 −4 respectively.', 'the learning rate schedule causes the network to visit several local minima during training, allowing us to use snapshot ensembling [ 27 ].', 'by saving the network parameters at each local minimum, we can ensemble the embeddings of multiple networks at no extra cost.', 'we use a margin α = 0. 2 for the loss function.', 'we train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch.', 'for ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings.', 'the main differences with the approaches described in [ 13,  #TAUTHOR_TAG are the use of multi - layered grus, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention']",4
"['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an']","['', 'vice versa where r @ n is the percentage of test items for which the correct image or', 'caption was in the top n results. we compare our models to [ 12 ] and  #TAUTHOR_TAG', ', and include our own character - based model for comparison. [ 12 ] is a convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character - based', 'model is similar to the model we use here and was trained on the original flickr8k text captions', '( see [ 8 ] for a full description ). both our mfcc and mbn', 'based model significantly outperform previous spoken captionto - image methods on the flickr8k dataset. the largest improvement is the mbn model which', 'outperforms the results reported in  #TAUTHOR_TAG by as much as 23. 2 percentage points on r @ 10. the mfcc model also improves on previous results but scores significantly lower than the mbn model across the board, improving as much', 'as 12. 3 percentage points over previous work. there is a large performance gap between the text - caption to image retrieval results and the spoken - caption to image results, showing there is still a lot of room for improvement']",4
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",4
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",2
"['7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or']","['during training.', 'end - to - end speech models often have millions of parameters [ 7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or']","['7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or']","['has proven crucial to improving the generalization performance of many machine learning models.', 'in particular, regularization is crucial when the model is highly flexible ( e. g. deep neural networks ) and likely to overfit on the training data.', 'data augmentation is an efficient and effective way of doing regularization that introduces very small ( or no ) overhead during training.', 'it has shown to consistently improve performance in various pattern recognition tasks [ 1, 2, 3, 4, 5 ].', 'dropout [ 6 ] is another powerful way of doing regularization for training deep neural networks, it intends to reduce the co - adaptation amongst hidden units by randomly zero - ing out inputs to the hidden layer during training.', 'end - to - end speech models often have millions of parameters [ 7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or applied to them.', 'we investigate the effectiveness of data augmentation and dropout for regularizing end - to - end speech models.', 'in particular, we augment the raw audio data by changing the tempo and pitch independently.', 'the volume and temporal alignment of the audio signals are also randomly perturbed, with additional random noises added.', 'to further regularize the model, we employ dropout to each input layer of the network.', 'with these regularization techniques, we obtained over 20 % relative performance on the wall street journal ( wsj ) dataset and librispeech dataset']",0
"['augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been']","['augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been']","['augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been applied to acoustic models in']","['augmentation in speech recognition has been applied before.', 'gales et al. [ 12 ] use hidden markov models to generate synthetic data to enable 1 - vs - 1 training of svms.', 'feature level augmentation has also demonstrated effectiveness [ 5, 3, 4 ].', 'ko et al. [ 2 ] performed audio level speed perturbation that also lead to performance improvements.', 'background noise is used for augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been applied to acoustic models in [ 13, 14, 15 ] and demonstrated promising performance.', 'for end - to - end speech models, hannun et al. [ 8 ] applied dropout to the output layer of the network.', 'however, to the best of our knowledge, our work is the first to apply it to other layers in end - to - end speech models']",0
"['7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or']","['during training.', 'end - to - end speech models often have millions of parameters [ 7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or']","['7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or']","['has proven crucial to improving the generalization performance of many machine learning models.', 'in particular, regularization is crucial when the model is highly flexible ( e. g. deep neural networks ) and likely to overfit on the training data.', 'data augmentation is an efficient and effective way of doing regularization that introduces very small ( or no ) overhead during training.', 'it has shown to consistently improve performance in various pattern recognition tasks [ 1, 2, 3, 4, 5 ].', 'dropout [ 6 ] is another powerful way of doing regularization for training deep neural networks, it intends to reduce the co - adaptation amongst hidden units by randomly zero - ing out inputs to the hidden layer during training.', 'end - to - end speech models often have millions of parameters [ 7, 8,  #TAUTHOR_TAG 10, 11 ].', 'however, data augmentation and dropout have not been extensively studied or applied to them.', 'we investigate the effectiveness of data augmentation and dropout for regularizing end - to - end speech models.', 'in particular, we augment the raw audio data by changing the tempo and pitch independently.', 'the volume and temporal alignment of the audio signals are also randomly perturbed, with additional random noises added.', 'to further regularize the model, we employ dropout to each input layer of the network.', 'with these regularization techniques, we obtained over 20 % relative performance on the wall street journal ( wsj ) dataset and librispeech dataset']",1
"['augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been']","['augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been']","['augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been applied to acoustic models in']","['augmentation in speech recognition has been applied before.', 'gales et al. [ 12 ] use hidden markov models to generate synthetic data to enable 1 - vs - 1 training of svms.', 'feature level augmentation has also demonstrated effectiveness [ 5, 3, 4 ].', 'ko et al. [ 2 ] performed audio level speed perturbation that also lead to performance improvements.', 'background noise is used for augmentation in [ 8,  #TAUTHOR_TAG to improve performance on noisy speech.', 'apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio.', 'dropout has been applied to speech recognition before.', 'it has been applied to acoustic models in [ 13, 14, 15 ] and demonstrated promising performance.', 'for end - to - end speech models, hannun et al. [ 8 ] applied dropout to the output layer of the network.', 'however, to the best of our knowledge, our work is the first to apply it to other layers in end - to - end speech models']",3
"['ds2 )  #TAUTHOR_TAG.', 'while we have']","['deep speech 2 ( ds2 )  #TAUTHOR_TAG.', 'while we have']","['ds2 )  #TAUTHOR_TAG.', 'while we have made some modifications']","['end - to - end model structure used in this work is very similar to the model architecture of deep speech 2 ( ds2 )  #TAUTHOR_TAG.', 'while we have made some modifications at the front - end time - frequency convolution ( i. e. 2 - d convolution ) layers, the core structure of recurrent layers is the same.', '']",3
"['.  #TAUTHOR_TAG on librispeech dataset, although our model is only trained only on the provided training set.', 'this demonstrates the effectiveness of the proposed regularization methods']","['with amodei et al.  #TAUTHOR_TAG on librispeech dataset, although our model is only trained only on the provided training set.', 'this demonstrates the effectiveness of the proposed regularization methods']","['.  #TAUTHOR_TAG on librispeech dataset, although our model is only trained only on the provided training set.', 'this demonstrates the effectiveness of the proposed regularization methods']","['##al 92 bahdanau et al. [ 10 ] 9. 30 % graves and jaitly [ 27 ] 8. 20 % miao et al. [ 7 ] 7. 34 % ours 6. 42 % ours ( extended 3 - gram ) 6. 26 % amodei et al. [ 9 ] 3. 60 % 5. 33 % 13. 25 % table 4.', 'word error rate comparison with other end - to - end methods on librispeech dataset.', 'with the data released with the corpus.', 'our results on both wsj and librispeech are competitive to existing methods.', 'we would like to note that our model achieved comparable results with amodei et al.  #TAUTHOR_TAG on librispeech dataset, although our model is only trained only on the provided training set.', 'this demonstrates the effectiveness of the proposed regularization methods for training end - to - end speech models']",3
['( e. g.  #TAUTHOR_TAG also suggest'],"['( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['', 'apart from the empirical work in tilburg and antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low - frequency events, is harmful to generalization accuracy.', 'after reviewing this empirical work briefly, i will report on new results ( work in progress in collaboration with van den bosch and zavrel ), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks : tagging, grapheme - to - phoneme conversion, and pp - attachment.', 'the results show that forgetting individual training items, however "" improbable\'they may be, is indeed harmful.', 'furthermore, they show that combining lazy learning with training set editing techniques ( based on typicality and other regularity criteria ) also leads to worse generalization results.', 'i will conclude that forgetting, either by abstracting from the training data or by editing exceptional training items in lazy learning is']",0
['( e. g.  #TAUTHOR_TAG also suggest'],"['( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['', 'apart from the empirical work in tilburg and antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low - frequency events, is harmful to generalization accuracy.', 'after reviewing this empirical work briefly, i will report on new results ( work in progress in collaboration with van den bosch and zavrel ), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks : tagging, grapheme - to - phoneme conversion, and pp - attachment.', 'the results show that forgetting individual training items, however "" improbable\'they may be, is indeed harmful.', 'furthermore, they show that combining lazy learning with training set editing techniques ( based on typicality and other regularity criteria ) also leads to worse generalization results.', 'i will conclude that forgetting, either by abstracting from the training data or by editing exceptional training items in lazy learning is']",0
['( e. g.  #TAUTHOR_TAG also suggest'],"['( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['', 'apart from the empirical work in tilburg and antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low - frequency events, is harmful to generalization accuracy.', 'after reviewing this empirical work briefly, i will report on new results ( work in progress in collaboration with van den bosch and zavrel ), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks : tagging, grapheme - to - phoneme conversion, and pp - attachment.', 'the results show that forgetting individual training items, however "" improbable\'they may be, is indeed harmful.', 'furthermore, they show that combining lazy learning with training set editing techniques ( based on typicality and other regularity criteria ) also leads to worse generalization results.', 'i will conclude that forgetting, either by abstracting from the training data or by editing exceptional training items in lazy learning is']",3
['( e. g.  #TAUTHOR_TAG also suggest'],"['( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary']","['', 'apart from the empirical work in tilburg and antwerp, a number of recent studies on statistical natural language processing ( e. g.  #TAUTHOR_TAG also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low - frequency events, is harmful to generalization accuracy.', 'after reviewing this empirical work briefly, i will report on new results ( work in progress in collaboration with van den bosch and zavrel ), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks : tagging, grapheme - to - phoneme conversion, and pp - attachment.', 'the results show that forgetting individual training items, however "" improbable\'they may be, is indeed harmful.', 'furthermore, they show that combining lazy learning with training set editing techniques ( based on typicality and other regularity criteria ) also leads to worse generalization results.', 'i will conclude that forgetting, either by abstracting from the training data or by editing exceptional training items in lazy learning is']",6
"['approaches  #TAUTHOR_TAG,']","['approaches  #TAUTHOR_TAG,']","['more recent approaches  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],3
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG and train monolingual embeddings with word2vec, cbow, and negative sampling  #AUTHOR_TAG a ) on a 2. 8 billion word corpus for english ( ukwac + wikipedia + bnc ), a 1. 6 billion word corpus for italian ( itwac ), a 0. 9 billion word corpus for german ( sdewac ), and a 2. 8 billion word corpus for finnish ( common crawl ).', 'seed dictionaries following  #TAUTHOR_TAG, we use dictionaries of 5, 000 words, 25 words, and a numeral dictionary consisting of words matching the [ 0 - 9 ] + regular expression in both vocabularies.', '10 in line with, we additionally use a dictionary of identically spelled strings in both vocabularies.', 'implementation details similar to  #TAUTHOR_TAG, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations.', 'unless stated otherwise, we induce a dictionary of 200, 000 source and 200, 000 target words as in previous work  #AUTHOR_TAG c ;  #AUTHOR_TAG.', 'for optimal 1 : 1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.', 'if using a rank constraint, we restrict the matching in the estep to the top 40, 000 words in both languages.', '11 finding an optimal alignment on the 200, 000 × 200, 000 graph takes about 25 minutes on cpu ; 12 with a rank constraint, matching takes around three minutes.', 'baselines we compare our approach with and without the rank constraint to the original bilingual mapping approach by  #AUTHOR_TAG c ).', 'in addition, we compare with  #AUTHOR_TAG and  #AUTHOR_TAG who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.', 'finally, we compare with  #AUTHOR_TAG who add dimension - wise mean centering to  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG c ) and  #TAUTHOR_TAG are special cases of our famework and comparisons to these approaches thus act as an ablation study.', ' #AUTHOR_TAG c ) does not employ orthogonal procrustes, but rather allows the learned matrix ω to range freely.', 'likewise, as discussed in § 5,  #TAUTHOR_TAG make use of a viterbi em style algorithm with a different prior over edge sets.', '']",3
['of time as  #TAUTHOR_TAG'],['of time as  #TAUTHOR_TAG'],['of time as  #TAUTHOR_TAG'],"['show results for bilingual dictionary induction in table 1 and for cross - lingual word similarity in table 2.', 'our method with a 1 : 1 prior outperforms all baselines on english - german and english - italian.', '14 interestingly, the 1 : 1 prior by itself fails on english - finnish with a 25 word and numerals seed lexicon.', 'we hypothesize that the prior imposes too strong of a constraint to find a good solution for a distant language pair from a poor initialization.', 'with a better - but still weakly supervised - starting point using identical strings, our approach finds a good solution.', 'alternatively, we can mitigate this deficiency effectively using a rank constraint, which allows our model to converge to good solutions even with a 25 word or numerals seed lexicon.', 'the rank constraint gen - 11 we validated both values with identical strings using the 5, 000 word lexicon as validation set on english - italian.', '12 training takes a similar amount of time as  #TAUTHOR_TAG due to faster convergence.', '13 other recent improvements such as symmetric reweighting  #AUTHOR_TAG are orthogonal to our method, which is why we do not explicitly compare to them here.', '14 note that results are not directly comparable to  #AUTHOR_TAG due to the use of embeddings trained on different monolingual corpora ( wacky vs. wikipedia ).', 'erally performs similarly or boosts performance, while being about 8 times faster.', 'all approaches do better with identical strings compared to numerals, indicating that the former may be generally suitable as a default weakly - supervised seed lexicon']",3
['terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],"['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG for english - german with a 5, 000', 'seed lexicon and the full vocabulary in table 3. 17 hubs are fewer and occur less often with our method, demonstrating that the prior -', 'to some en - tr en - bn en - hi et - fi  #TAUTHOR_TAG extent - aids with resolving hubness. interestingly, compared to, hubs seem to occur less often and are more meaningful in current cross -', ""lingual word embedding models. 18 for instance, the neighbors of'gleichgultigkeit'all relate to"", ""indifference and words appearing close to'luis'or'jorge'are spanish names. this suggests that the prior"", 'might also be beneficial in other ways, e. g. by enforcing more reliable translation pairs for subsequent iterations', '. low - resource languages cross - lingual embeddings are particularly promising for low - resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks ( besides the english - finnish language pair ). we perform experiments with', 'our method with and without a rank constraint and  #TAUTHOR_TAG for three truly lowresource language pairs, english - { turkish, bengali, hind', '##i }. we additionally conduct an experiment for estonian - finnish, similarly to. for all languages, we use fasttext embeddings  #AUTHOR_TAG trained on wikipedia, the evaluation dictionaries provided by  #AUTHOR_TAG, and', 'a seed lexicon based on identical strings to reflect a realistic use case. we note that english', 'does not share scripts with bengali and hindi, making this even more challenging. we show results in table 4. surprisingly, the method by  #TAUTHOR_TAG a similar self - learning method that uses word embeddings, with an implicit one - to - many alignment based on nearest neighbor queries. vulic and  #AUTHOR_TAG proposed a more strict', 'one - to - many alignment based on symmetric translation pairs, which is also used by  #AUTHOR_TAG. our method bridges the gap between early latent variable and', 'word embedding - based approaches and explicitly allows us to reason over its prior. hub', '##ness problem the hubness problem is an intrinsic problem in high - dimensional vector spaces ( radovanovic et al', '., 2010 ). first observed it for cross - lingual embedding spaces and proposed to address it by re - ranking neighbor lists. proposed a max - marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the', 'softmax  #AUTHOR_TAG or scaling the similarity values  #AUTHOR_TAG']",3
"['approaches  #TAUTHOR_TAG,']","['approaches  #TAUTHOR_TAG,']","['more recent approaches  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],6
"['? the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']","['the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']","['the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']","['in the context of viterbi em, it means the max over a will decompose additively s', 'thus, we can simply find a component - wise as follows :', "" #AUTHOR_TAG's m - step the m - step remains unchanged from the exposition in § 3 with the exception that we fit ω given matrices s a and t a formed from a one - to - many alignment a, rather than a matching m."", 'why a reinterpretation? the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']",6
"[' #TAUTHOR_TAG.', 'our model combines the prior over bipartite matchings inspired']","[' #TAUTHOR_TAG.', 'our model combines the prior over bipartite matchings inspired']","[' #TAUTHOR_TAG.', 'our model combines the prior over bipartite matchings inspired']","['have presented a novel latent - variable model for bilingual lexicon induction, building on the work of  #TAUTHOR_TAG.', 'our model combines the prior over bipartite matchings inspired by  #AUTHOR_TAG and the discriminative, rather than generative, approach inspired by irvine and callison  #AUTHOR_TAG.', 'we show empirical gains on six language pairs and theoretically and empirically demonstrate the application of the bipartite matching prior to solving the hubness problem']",6
"['approaches  #TAUTHOR_TAG,']","['approaches  #TAUTHOR_TAG,']","['more recent approaches  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],4
"['approaches  #TAUTHOR_TAG,']","['approaches  #TAUTHOR_TAG,']","['more recent approaches  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],4
"['? the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']","['the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']","['the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']","['in the context of viterbi em, it means the max over a will decompose additively s', 'thus, we can simply find a component - wise as follows :', "" #AUTHOR_TAG's m - step the m - step remains unchanged from the exposition in § 3 with the exception that we fit ω given matrices s a and t a formed from a one - to - many alignment a, rather than a matching m."", 'why a reinterpretation? the reinterpretation of  #TAUTHOR_TAG as a probabilistic model yields a clear analytical comparison between our method and theirs.', 'the only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce']",4
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['cross - lingual word similarity, our approach yields the best performance on wordsim - 353 and rg - 65 for english - german and is only outperformed by  #TAUTHOR_TAG on english - italian wordsim - 353']",4
['terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],"['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG for english - german with a 5, 000', 'seed lexicon and the full vocabulary in table 3. 17 hubs are fewer and occur less often with our method, demonstrating that the prior -', 'to some en - tr en - bn en - hi et - fi  #TAUTHOR_TAG extent - aids with resolving hubness. interestingly, compared to, hubs seem to occur less often and are more meaningful in current cross -', ""lingual word embedding models. 18 for instance, the neighbors of'gleichgultigkeit'all relate to"", ""indifference and words appearing close to'luis'or'jorge'are spanish names. this suggests that the prior"", 'might also be beneficial in other ways, e. g. by enforcing more reliable translation pairs for subsequent iterations', '. low - resource languages cross - lingual embeddings are particularly promising for low - resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks ( besides the english - finnish language pair ). we perform experiments with', 'our method with and without a rank constraint and  #TAUTHOR_TAG for three truly lowresource language pairs, english - { turkish, bengali, hind', '##i }. we additionally conduct an experiment for estonian - finnish, similarly to. for all languages, we use fasttext embeddings  #AUTHOR_TAG trained on wikipedia, the evaluation dictionaries provided by  #AUTHOR_TAG, and', 'a seed lexicon based on identical strings to reflect a realistic use case. we note that english', 'does not share scripts with bengali and hindi, making this even more challenging. we show results in table 4. surprisingly, the method by  #TAUTHOR_TAG a similar self - learning method that uses word embeddings, with an implicit one - to - many alignment based on nearest neighbor queries. vulic and  #AUTHOR_TAG proposed a more strict', 'one - to - many alignment based on symmetric translation pairs, which is also used by  #AUTHOR_TAG. our method bridges the gap between early latent variable and', 'word embedding - based approaches and explicitly allows us to reason over its prior. hub', '##ness problem the hubness problem is an intrinsic problem in high - dimensional vector spaces ( radovanovic et al', '., 2010 ). first observed it for cross - lingual embedding spaces and proposed to address it by re - ranking neighbor lists. proposed a max - marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the', 'softmax  #AUTHOR_TAG or scaling the similarity values  #AUTHOR_TAG']",4
[' #TAUTHOR_TAG has achieved some'],[' #TAUTHOR_TAG has achieved some'],"['assumption ( ii ), previous work  #TAUTHOR_TAG has achieved some success using an orthogonal transformation ; recently, however, demonstrated that monolingual embedding spaces']","['the previous section, we have formulated the induction of a bilingual lexicon as the search for an edge set e, which we treat as a latent variable that we marginalize out in equation ( 2 ).', 'specifically, we assume that e is a partial matching.', 'thus, for every ( i, j ) ∈ m, we have t i ∼ n ( ω s j, i ), that is, the embedding for v trg ( i ) is assumed to have been drawn from a gaussian centered around the embedding for v src ( j ), after an orthogonal transformation.', 'this gives rise to two modeling assumptions, which we make explicit : ( i ) there exists a single source for every word in the target lexicon and that source cannot be used more than once.', '4 ( ii ) there exists an orthogonal transformation, after which the embedding spaces are more or less equivalent.', 'assumption ( i ) may be true for related languages, but is likely false for morphologically rich languages that have a many - to - many relationship between the words in their respective lexicons.', 'we propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in § 6.', 'in addition, we experiment with priors that express different matchings in § 7.', 'as for assumption ( ii ), previous work  #TAUTHOR_TAG has achieved some success using an orthogonal transformation ; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy.', 'nevertheless, we will show that imbuing our model with these assumptions helps empirically in § 6, giving them practical utility']",0
"[' #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step']","[' #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step optimizes two objectives independently.', 'first, making use of the result in']","['. 1, we search for a matrix ω ∈ r d×d.', 'we additionally enforce the constraint that ω is a real orthogonal matrix, i. e., ω ω = i. previous work  #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step optimizes two objectives independently.', 'first, making use of the result in']","[', we will describe the m - step.', 'given an optimal matching m computed in § 4. 1, we search for a matrix ω ∈ r d×d.', 'we additionally enforce the constraint that ω is a real orthogonal matrix, i. e., ω ω = i. previous work  #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step optimizes two objectives independently.', 'first, making use of the result in equation ( 6 ), we optimize the following :', 'with respect to ω subject to ω ω = i. ( note we may ignore the constant c during the optimization. ) second, we optimize the objective', 'with respect to the mean parameter [UNK], which is simply an average.', 'note, again, we may ignore the constant d during optimization.', 'optimizing equation ( 8 ) with respect to ω is known as the orthogonal procrustes problem ( schonemann, 1966 ;  #AUTHOR_TAG and has a closed form solution that exploits the singular value decomposition  #AUTHOR_TAG.', 'namely, we compute u σv = t m s m.', 'then, we directly arrive at the optimum : ω = u v.', 'optimizing equation ( 9 ) can also been done in closed form ; the point which minimizes distance to the data points ( thereby maximizing the log - probability ) is the centroid : [UNK] = 1 / | utrg | · i∈utrg t i']",0
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG and train monolingual embeddings with word2vec, cbow, and negative sampling  #AUTHOR_TAG a ) on a 2. 8 billion word corpus for english ( ukwac + wikipedia + bnc ), a 1. 6 billion word corpus for italian ( itwac ), a 0. 9 billion word corpus for german ( sdewac ), and a 2. 8 billion word corpus for finnish ( common crawl ).', 'seed dictionaries following  #TAUTHOR_TAG, we use dictionaries of 5, 000 words, 25 words, and a numeral dictionary consisting of words matching the [ 0 - 9 ] + regular expression in both vocabularies.', '10 in line with, we additionally use a dictionary of identically spelled strings in both vocabularies.', 'implementation details similar to  #TAUTHOR_TAG, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations.', 'unless stated otherwise, we induce a dictionary of 200, 000 source and 200, 000 target words as in previous work  #AUTHOR_TAG c ;  #AUTHOR_TAG.', 'for optimal 1 : 1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.', 'if using a rank constraint, we restrict the matching in the estep to the top 40, 000 words in both languages.', '11 finding an optimal alignment on the 200, 000 × 200, 000 graph takes about 25 minutes on cpu ; 12 with a rank constraint, matching takes around three minutes.', 'baselines we compare our approach with and without the rank constraint to the original bilingual mapping approach by  #AUTHOR_TAG c ).', 'in addition, we compare with  #AUTHOR_TAG and  #AUTHOR_TAG who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.', 'finally, we compare with  #AUTHOR_TAG who add dimension - wise mean centering to  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG c ) and  #TAUTHOR_TAG are special cases of our famework and comparisons to these approaches thus act as an ablation study.', ' #AUTHOR_TAG c ) does not employ orthogonal procrustes, but rather allows the learned matrix ω to range freely.', 'likewise, as discussed in § 5,  #TAUTHOR_TAG make use of a viterbi em style algorithm with a different prior over edge sets.', '']",0
[' #TAUTHOR_TAG has achieved some'],[' #TAUTHOR_TAG has achieved some'],"['assumption ( ii ), previous work  #TAUTHOR_TAG has achieved some success using an orthogonal transformation ; recently, however, demonstrated that monolingual embedding spaces']","['the previous section, we have formulated the induction of a bilingual lexicon as the search for an edge set e, which we treat as a latent variable that we marginalize out in equation ( 2 ).', 'specifically, we assume that e is a partial matching.', 'thus, for every ( i, j ) ∈ m, we have t i ∼ n ( ω s j, i ), that is, the embedding for v trg ( i ) is assumed to have been drawn from a gaussian centered around the embedding for v src ( j ), after an orthogonal transformation.', 'this gives rise to two modeling assumptions, which we make explicit : ( i ) there exists a single source for every word in the target lexicon and that source cannot be used more than once.', '4 ( ii ) there exists an orthogonal transformation, after which the embedding spaces are more or less equivalent.', 'assumption ( i ) may be true for related languages, but is likely false for morphologically rich languages that have a many - to - many relationship between the words in their respective lexicons.', 'we propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in § 6.', 'in addition, we experiment with priors that express different matchings in § 7.', 'as for assumption ( ii ), previous work  #TAUTHOR_TAG has achieved some success using an orthogonal transformation ; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy.', 'nevertheless, we will show that imbuing our model with these assumptions helps empirically in § 6, giving them practical utility']",5
"[' #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step']","[' #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step optimizes two objectives independently.', 'first, making use of the result in']","['. 1, we search for a matrix ω ∈ r d×d.', 'we additionally enforce the constraint that ω is a real orthogonal matrix, i. e., ω ω = i. previous work  #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step optimizes two objectives independently.', 'first, making use of the result in']","[', we will describe the m - step.', 'given an optimal matching m computed in § 4. 1, we search for a matrix ω ∈ r d×d.', 'we additionally enforce the constraint that ω is a real orthogonal matrix, i. e., ω ω = i. previous work  #TAUTHOR_TAG found that the orthogonality constraint leads to noticeable improvements.', 'our m - step optimizes two objectives independently.', 'first, making use of the result in equation ( 6 ), we optimize the following :', 'with respect to ω subject to ω ω = i. ( note we may ignore the constant c during the optimization. ) second, we optimize the objective', 'with respect to the mean parameter [UNK], which is simply an average.', 'note, again, we may ignore the constant d during optimization.', 'optimizing equation ( 8 ) with respect to ω is known as the orthogonal procrustes problem ( schonemann, 1966 ;  #AUTHOR_TAG and has a closed form solution that exploits the singular value decomposition  #AUTHOR_TAG.', 'namely, we compute u σv = t m s m.', 'then, we directly arrive at the optimum : ω = u v.', 'optimizing equation ( 9 ) can also been done in closed form ; the point which minimizes distance to the data points ( thereby maximizing the log - probability ) is the centroid : [UNK] = 1 / | utrg | · i∈utrg t i']",5
"['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG, our strongest baseline in § 6, may also be interpreted as a latent - variable model in the spirit of our exposition in § 3.', 'indeed, we only need to change the edge - set prior p ( m ) to allow for edge sets other than those that are matchings.', 'specifically, a matching enforces a one - to - one alignment between types in the respective lexicons.', ' #AUTHOR_TAG, on the other hand, allow for one - to - many alignments.', '2011 ;  #AUTHOR_TAG may have been a computationally more effective manner to deal with the latent matchings.', ""we show how this corresponds to an alignment distribution that is equivalent to ibm model 1  #AUTHOR_TAG, and that  #TAUTHOR_TAG's selftraining method is actually a form of viterbi em."", ""to formalize  #TAUTHOR_TAG's contribution as a latent - variable model, we lay down some more notation."", 'let a = { 1,..., n src + 1 } ntrg, where we define ( n src + 1 ) to be none, a distinguished symbol indicating unalignment.', '']",5
"['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG, our strongest baseline in § 6, may also be interpreted as a latent - variable model in the spirit of our exposition in § 3.', 'indeed, we only need to change the edge - set prior p ( m ) to allow for edge sets other than those that are matchings.', 'specifically, a matching enforces a one - to - one alignment between types in the respective lexicons.', ' #AUTHOR_TAG, on the other hand, allow for one - to - many alignments.', '2011 ;  #AUTHOR_TAG may have been a computationally more effective manner to deal with the latent matchings.', ""we show how this corresponds to an alignment distribution that is equivalent to ibm model 1  #AUTHOR_TAG, and that  #TAUTHOR_TAG's selftraining method is actually a form of viterbi em."", ""to formalize  #TAUTHOR_TAG's contribution as a latent - variable model, we lay down some more notation."", 'let a = { 1,..., n src + 1 } ntrg, where we define ( n src + 1 ) to be none, a distinguished symbol indicating unalignment.', '']",5
"['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG,']","['self - training method of  #TAUTHOR_TAG, our strongest baseline in § 6, may also be interpreted as a latent - variable model in the spirit of our exposition in § 3.', 'indeed, we only need to change the edge - set prior p ( m ) to allow for edge sets other than those that are matchings.', 'specifically, a matching enforces a one - to - one alignment between types in the respective lexicons.', ' #AUTHOR_TAG, on the other hand, allow for one - to - many alignments.', '2011 ;  #AUTHOR_TAG may have been a computationally more effective manner to deal with the latent matchings.', ""we show how this corresponds to an alignment distribution that is equivalent to ibm model 1  #AUTHOR_TAG, and that  #TAUTHOR_TAG's selftraining method is actually a form of viterbi em."", ""to formalize  #TAUTHOR_TAG's contribution as a latent - variable model, we lay down some more notation."", 'let a = { 1,..., n src + 1 } ntrg, where we define ( n src + 1 ) to be none, a distinguished symbol indicating unalignment.', '']",5
"['english - finnish datasets by  #TAUTHOR_TAG.', 'for cross - lingual word similarity, we use the rg - 65 and wordsim']","['english - finnish datasets by  #TAUTHOR_TAG.', 'for cross - lingual word similarity, we use the rg - 65 and wordsim - 353 cross - lingual datasets for english - german and the wordsim - 353 cross - lingual dataset']","['english - finnish datasets by  #TAUTHOR_TAG.', 'for cross - lingual word similarity, we use the rg - 65 and wordsim']","['for bilingual dictionary induction, we use the english - italian dataset by and the english - german and english - finnish datasets by  #TAUTHOR_TAG.', 'for cross - lingual word similarity, we use the rg - 65 and wordsim - 353 cross - lingual datasets for english - german and the wordsim - 353 cross - lingual dataset for englishitalian by camacho -  #AUTHOR_TAG']",5
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG and train monolingual embeddings with word2vec, cbow, and negative sampling  #AUTHOR_TAG a ) on a 2. 8 billion word corpus for english ( ukwac + wikipedia + bnc ), a 1. 6 billion word corpus for italian ( itwac ), a 0. 9 billion word corpus for german ( sdewac ), and a 2. 8 billion word corpus for finnish ( common crawl ).', 'seed dictionaries following  #TAUTHOR_TAG, we use dictionaries of 5, 000 words, 25 words, and a numeral dictionary consisting of words matching the [ 0 - 9 ] + regular expression in both vocabularies.', '10 in line with, we additionally use a dictionary of identically spelled strings in both vocabularies.', 'implementation details similar to  #TAUTHOR_TAG, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations.', 'unless stated otherwise, we induce a dictionary of 200, 000 source and 200, 000 target words as in previous work  #AUTHOR_TAG c ;  #AUTHOR_TAG.', 'for optimal 1 : 1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.', 'if using a rank constraint, we restrict the matching in the estep to the top 40, 000 words in both languages.', '11 finding an optimal alignment on the 200, 000 × 200, 000 graph takes about 25 minutes on cpu ; 12 with a rank constraint, matching takes around three minutes.', 'baselines we compare our approach with and without the rank constraint to the original bilingual mapping approach by  #AUTHOR_TAG c ).', 'in addition, we compare with  #AUTHOR_TAG and  #AUTHOR_TAG who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.', 'finally, we compare with  #AUTHOR_TAG who add dimension - wise mean centering to  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG c ) and  #TAUTHOR_TAG are special cases of our famework and comparisons to these approaches thus act as an ablation study.', ' #AUTHOR_TAG c ) does not employ orthogonal procrustes, but rather allows the learned matrix ω to range freely.', 'likewise, as discussed in § 5,  #TAUTHOR_TAG make use of a viterbi em style algorithm with a different prior over edge sets.', '']",5
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG and train monolingual embeddings with word2vec, cbow, and negative sampling  #AUTHOR_TAG a ) on a 2. 8 billion word corpus for english ( ukwac + wikipedia + bnc ), a 1. 6 billion word corpus for italian ( itwac ), a 0. 9 billion word corpus for german ( sdewac ), and a 2. 8 billion word corpus for finnish ( common crawl ).', 'seed dictionaries following  #TAUTHOR_TAG, we use dictionaries of 5, 000 words, 25 words, and a numeral dictionary consisting of words matching the [ 0 - 9 ] + regular expression in both vocabularies.', '10 in line with, we additionally use a dictionary of identically spelled strings in both vocabularies.', 'implementation details similar to  #TAUTHOR_TAG, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations.', 'unless stated otherwise, we induce a dictionary of 200, 000 source and 200, 000 target words as in previous work  #AUTHOR_TAG c ;  #AUTHOR_TAG.', 'for optimal 1 : 1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.', 'if using a rank constraint, we restrict the matching in the estep to the top 40, 000 words in both languages.', '11 finding an optimal alignment on the 200, 000 × 200, 000 graph takes about 25 minutes on cpu ; 12 with a rank constraint, matching takes around three minutes.', 'baselines we compare our approach with and without the rank constraint to the original bilingual mapping approach by  #AUTHOR_TAG c ).', 'in addition, we compare with  #AUTHOR_TAG and  #AUTHOR_TAG who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.', 'finally, we compare with  #AUTHOR_TAG who add dimension - wise mean centering to  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG c ) and  #TAUTHOR_TAG are special cases of our famework and comparisons to these approaches thus act as an ablation study.', ' #AUTHOR_TAG c ) does not employ orthogonal procrustes, but rather allows the learned matrix ω to range freely.', 'likewise, as discussed in § 5,  #TAUTHOR_TAG make use of a viterbi em style algorithm with a different prior over edge sets.', '']",5
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG and train monolingual embeddings with word2vec, cbow, and negative sampling  #AUTHOR_TAG a ) on a 2. 8 billion word corpus for english ( ukwac + wikipedia + bnc ), a 1. 6 billion word corpus for italian ( itwac ), a 0. 9 billion word corpus for german ( sdewac ), and a 2. 8 billion word corpus for finnish ( common crawl ).', 'seed dictionaries following  #TAUTHOR_TAG, we use dictionaries of 5, 000 words, 25 words, and a numeral dictionary consisting of words matching the [ 0 - 9 ] + regular expression in both vocabularies.', '10 in line with, we additionally use a dictionary of identically spelled strings in both vocabularies.', 'implementation details similar to  #TAUTHOR_TAG, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations.', 'unless stated otherwise, we induce a dictionary of 200, 000 source and 200, 000 target words as in previous work  #AUTHOR_TAG c ;  #AUTHOR_TAG.', 'for optimal 1 : 1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.', 'if using a rank constraint, we restrict the matching in the estep to the top 40, 000 words in both languages.', '11 finding an optimal alignment on the 200, 000 × 200, 000 graph takes about 25 minutes on cpu ; 12 with a rank constraint, matching takes around three minutes.', 'baselines we compare our approach with and without the rank constraint to the original bilingual mapping approach by  #AUTHOR_TAG c ).', 'in addition, we compare with  #AUTHOR_TAG and  #AUTHOR_TAG who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.', 'finally, we compare with  #AUTHOR_TAG who add dimension - wise mean centering to  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG c ) and  #TAUTHOR_TAG are special cases of our famework and comparisons to these approaches thus act as an ablation study.', ' #AUTHOR_TAG c ) does not employ orthogonal procrustes, but rather allows the learned matrix ω to range freely.', 'likewise, as discussed in § 5,  #TAUTHOR_TAG make use of a viterbi em style algorithm with a different prior over edge sets.', '']",5
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG and train monolingual embeddings with word2vec, cbow, and negative sampling  #AUTHOR_TAG a ) on a 2. 8 billion word corpus for english ( ukwac + wikipedia + bnc ), a 1. 6 billion word corpus for italian ( itwac ), a 0. 9 billion word corpus for german ( sdewac ), and a 2. 8 billion word corpus for finnish ( common crawl ).', 'seed dictionaries following  #TAUTHOR_TAG, we use dictionaries of 5, 000 words, 25 words, and a numeral dictionary consisting of words matching the [ 0 - 9 ] + regular expression in both vocabularies.', '10 in line with, we additionally use a dictionary of identically spelled strings in both vocabularies.', 'implementation details similar to  #TAUTHOR_TAG, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations.', 'unless stated otherwise, we induce a dictionary of 200, 000 source and 200, 000 target words as in previous work  #AUTHOR_TAG c ;  #AUTHOR_TAG.', 'for optimal 1 : 1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.', 'if using a rank constraint, we restrict the matching in the estep to the top 40, 000 words in both languages.', '11 finding an optimal alignment on the 200, 000 × 200, 000 graph takes about 25 minutes on cpu ; 12 with a rank constraint, matching takes around three minutes.', 'baselines we compare our approach with and without the rank constraint to the original bilingual mapping approach by  #AUTHOR_TAG c ).', 'in addition, we compare with  #AUTHOR_TAG and  #AUTHOR_TAG who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.', 'finally, we compare with  #AUTHOR_TAG who add dimension - wise mean centering to  #AUTHOR_TAG, and  #TAUTHOR_TAG.', ' #AUTHOR_TAG c ) and  #TAUTHOR_TAG are special cases of our famework and comparisons to these approaches thus act as an ablation study.', ' #AUTHOR_TAG c ) does not employ orthogonal procrustes, but rather allows the learned matrix ω to range freely.', 'likewise, as discussed in § 5,  #TAUTHOR_TAG make use of a viterbi em style algorithm with a different prior over edge sets.', '']",5
['terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],"['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG for english - german with a 5, 000', 'seed lexicon and the full vocabulary in table 3. 17 hubs are fewer and occur less often with our method, demonstrating that the prior -', 'to some en - tr en - bn en - hi et - fi  #TAUTHOR_TAG extent - aids with resolving hubness. interestingly, compared to, hubs seem to occur less often and are more meaningful in current cross -', ""lingual word embedding models. 18 for instance, the neighbors of'gleichgultigkeit'all relate to"", ""indifference and words appearing close to'luis'or'jorge'are spanish names. this suggests that the prior"", 'might also be beneficial in other ways, e. g. by enforcing more reliable translation pairs for subsequent iterations', '. low - resource languages cross - lingual embeddings are particularly promising for low - resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks ( besides the english - finnish language pair ). we perform experiments with', 'our method with and without a rank constraint and  #TAUTHOR_TAG for three truly lowresource language pairs, english - { turkish, bengali, hind', '##i }. we additionally conduct an experiment for estonian - finnish, similarly to. for all languages, we use fasttext embeddings  #AUTHOR_TAG trained on wikipedia, the evaluation dictionaries provided by  #AUTHOR_TAG, and', 'a seed lexicon based on identical strings to reflect a realistic use case. we note that english', 'does not share scripts with bengali and hindi, making this even more challenging. we show results in table 4. surprisingly, the method by  #TAUTHOR_TAG a similar self - learning method that uses word embeddings, with an implicit one - to - many alignment based on nearest neighbor queries. vulic and  #AUTHOR_TAG proposed a more strict', 'one - to - many alignment based on symmetric translation pairs, which is also used by  #AUTHOR_TAG. our method bridges the gap between early latent variable and', 'word embedding - based approaches and explicitly allows us to reason over its prior. hub', '##ness problem the hubness problem is an intrinsic problem in high - dimensional vector spaces ( radovanovic et al', '., 2010 ). first observed it for cross - lingual embedding spaces and proposed to address it by re - ranking neighbor lists. proposed a max - marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the', 'softmax  #AUTHOR_TAG or scaling the similarity values  #AUTHOR_TAG']",5
['terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG'],"['k = 20 and use the words in the evaluation dictionary as query terms. we show the target language words with the highest hubness using our method and  #TAUTHOR_TAG for english - german with a 5, 000', 'seed lexicon and the full vocabulary in table 3. 17 hubs are fewer and occur less often with our method, demonstrating that the prior -', 'to some en - tr en - bn en - hi et - fi  #TAUTHOR_TAG extent - aids with resolving hubness. interestingly, compared to, hubs seem to occur less often and are more meaningful in current cross -', ""lingual word embedding models. 18 for instance, the neighbors of'gleichgultigkeit'all relate to"", ""indifference and words appearing close to'luis'or'jorge'are spanish names. this suggests that the prior"", 'might also be beneficial in other ways, e. g. by enforcing more reliable translation pairs for subsequent iterations', '. low - resource languages cross - lingual embeddings are particularly promising for low - resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks ( besides the english - finnish language pair ). we perform experiments with', 'our method with and without a rank constraint and  #TAUTHOR_TAG for three truly lowresource language pairs, english - { turkish, bengali, hind', '##i }. we additionally conduct an experiment for estonian - finnish, similarly to. for all languages, we use fasttext embeddings  #AUTHOR_TAG trained on wikipedia, the evaluation dictionaries provided by  #AUTHOR_TAG, and', 'a seed lexicon based on identical strings to reflect a realistic use case. we note that english', 'does not share scripts with bengali and hindi, making this even more challenging. we show results in table 4. surprisingly, the method by  #TAUTHOR_TAG a similar self - learning method that uses word embeddings, with an implicit one - to - many alignment based on nearest neighbor queries. vulic and  #AUTHOR_TAG proposed a more strict', 'one - to - many alignment based on symmetric translation pairs, which is also used by  #AUTHOR_TAG. our method bridges the gap between early latent variable and', 'word embedding - based approaches and explicitly allows us to reason over its prior. hub', '##ness problem the hubness problem is an intrinsic problem in high - dimensional vector spaces ( radovanovic et al', '., 2010 ). first observed it for cross - lingual embedding spaces and proposed to address it by re - ranking neighbor lists. proposed a max - marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the', 'softmax  #AUTHOR_TAG or scaling the similarity values  #AUTHOR_TAG']",5
"['close results  #TAUTHOR_TAG. before model combination,']","['then run an', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', 'prediction']","['this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination,']","['', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', ""prediction results from different machine learn - ing models based on the results on the training set to decrease the number of models combined and improve the results. a criteria that we use is to include results that are better than the best rr model '"", 's results. in general, the combined model is better than the best model in the set and stacking achieves better results than mix. we tokenize and truecase all of the corpora', ""using moses' #AUTHOR_TAG processing tools. 2 lms are built using kenlm  #AUTHOR_TAG. the comparison of results on the"", '']",5
"['close results  #TAUTHOR_TAG. before model combination,']","['then run an', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', 'prediction']","['this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination,']","['', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', ""prediction results from different machine learn - ing models based on the results on the training set to decrease the number of models combined and improve the results. a criteria that we use is to include results that are better than the best rr model '"", 's results. in general, the combined model is better than the best model in the set and stacking achieves better results than mix. we tokenize and truecase all of the corpora', ""using moses' #AUTHOR_TAG processing tools. 2 lms are built using kenlm  #AUTHOR_TAG. the comparison of results on the"", '']",5
"['close results  #TAUTHOR_TAG. before model combination,']","['then run an', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', 'prediction']","['this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination,']","['', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', ""prediction results from different machine learn - ing models based on the results on the training set to decrease the number of models combined and improve the results. a criteria that we use is to include results that are better than the best rr model '"", 's results. in general, the combined model is better than the best model in the set and stacking achieves better results than mix. we tokenize and truecase all of the corpora', ""using moses' #AUTHOR_TAG processing tools. 2 lms are built using kenlm  #AUTHOR_TAG. the comparison of results on the"", '']",5
[')  #TAUTHOR_TAG for word - and phrase'],"[')  #TAUTHOR_TAG for word - and phrase - level translation', 'performance']",['softmax layer. we use global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd )  #TAUTHOR_TAG for word - and phrase'],"['datasets selected by rtm models to provide context for the training and test sets improve as can be seen in the data statistics of parfda instance selection ( bicici, 2019 ). figure 1 depicts rtms and explains the model building process. rtms use', 'parfda for instance selection and machine translation performance prediction system ( mtpps ) for obtaining the features, which includes', 'additional features from word alignment and also from glmd for word - level prediction. we use ridge regression, kernel ridge regression, k - nearest neighors, support vector regression, adaboost  #AUTHOR_TAG, gradient tree boosting, gaussian process regress', '##or, extremely randomized trees  #AUTHOR_TAG, and multi - layer perceptron  #AUTHOR_TAG as learning models in combination with feature selection ( fs )  #AUTHOR_TAG and partial least squares ( pls )  #AUTHOR_TAG where most', 'of these models can be found in scikit - learn. 1 we experiment with : • including the statistics of the', 'binary tags obtained as features extracted from word - level tag predictions for sentence - level prediction, • using knn to estimate the noise level', 'for figure 1 : rtm depiction : parfda selects interpretants close to the training and test data using parallel corpus in bilingual settings and monolingual corpus in the target language or just the monolingual target corpus in monolingual settings ; an mt', '##pps use interpretants and training data to generate training features and another use interpretants and test data to generate test features in the same feature space ; learning and prediction takes place taking these features as input. svr, which obtains accuracy with 5 % error compared with estimates obtained with known noise level  #AUTHOR_TAG', 'and set = σ / 2.  #AUTHOR_TAG used a hybrid stacking', 'model to combine the word - level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. the neural network architecture they used is also hybrid with different types of layers :', 'input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. we use global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd )  #TAUTHOR_TAG for word - and phrase - level translation', '']",5
[')  #TAUTHOR_TAG for word - and phrase'],"[')  #TAUTHOR_TAG for word - and phrase - level translation', 'performance']",['softmax layer. we use global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd )  #TAUTHOR_TAG for word - and phrase'],"['datasets selected by rtm models to provide context for the training and test sets improve as can be seen in the data statistics of parfda instance selection ( bicici, 2019 ). figure 1 depicts rtms and explains the model building process. rtms use', 'parfda for instance selection and machine translation performance prediction system ( mtpps ) for obtaining the features, which includes', 'additional features from word alignment and also from glmd for word - level prediction. we use ridge regression, kernel ridge regression, k - nearest neighors, support vector regression, adaboost  #AUTHOR_TAG, gradient tree boosting, gaussian process regress', '##or, extremely randomized trees  #AUTHOR_TAG, and multi - layer perceptron  #AUTHOR_TAG as learning models in combination with feature selection ( fs )  #AUTHOR_TAG and partial least squares ( pls )  #AUTHOR_TAG where most', 'of these models can be found in scikit - learn. 1 we experiment with : • including the statistics of the', 'binary tags obtained as features extracted from word - level tag predictions for sentence - level prediction, • using knn to estimate the noise level', 'for figure 1 : rtm depiction : parfda selects interpretants close to the training and test data using parallel corpus in bilingual settings and monolingual corpus in the target language or just the monolingual target corpus in monolingual settings ; an mt', '##pps use interpretants and training data to generate training features and another use interpretants and test data to generate test features in the same feature space ; learning and prediction takes place taking these features as input. svr, which obtains accuracy with 5 % error compared with estimates obtained with known noise level  #AUTHOR_TAG', 'and set = σ / 2.  #AUTHOR_TAG used a hybrid stacking', 'model to combine the word - level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. the neural network architecture they used is also hybrid with different types of layers :', 'input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. we use global linear models ( glm )  #AUTHOR_TAG with dynamic learning ( glmd )  #TAUTHOR_TAG for word - and phrase - level translation', '']",5
[' #TAUTHOR_TAG to'],[' #TAUTHOR_TAG to'],['use prediction averaging  #TAUTHOR_TAG to'],"['use prediction averaging  #TAUTHOR_TAG to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions, y with evaluation metrics indexed by j ∈ j and weights with w :', 'we assume independent predictions and use p i / ( 1 − p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble ( kuncheva and rodriguez, 2014 ).', 'we only use the mix prediction if we obtain better results on the training set.', 'we select the best model using r and mix the results using r, rae, mraer, and maer.', 'we filter out those results with higher than 1 relative evaluation metric scores.', '']",5
"['close results  #TAUTHOR_TAG. before model combination,']","['then run an', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', 'prediction']","['this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination,']","['', 'rtm model. this conversion decreases the number of features and obtains close results  #TAUTHOR_TAG. before model combination, we further filter', ""prediction results from different machine learn - ing models based on the results on the training set to decrease the number of models combined and improve the results. a criteria that we use is to include results that are better than the best rr model '"", 's results. in general, the combined model is better than the best model in the set and stacking achieves better results than mix. we tokenize and truecase all of the corpora', ""using moses' #AUTHOR_TAG processing tools. 2 lms are built using kenlm  #AUTHOR_TAG. the comparison of results on the"", '']",3
[' #TAUTHOR_TAG to'],[' #TAUTHOR_TAG to'],['use prediction averaging  #TAUTHOR_TAG to'],"['use prediction averaging  #TAUTHOR_TAG to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions, y with evaluation metrics indexed by j ∈ j and weights with w :', 'we assume independent predictions and use p i / ( 1 − p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble ( kuncheva and rodriguez, 2014 ).', 'we only use the mix prediction if we obtain better results on the training set.', 'we select the best model using r and mix the results using r, rae, mraer, and maer.', 'we filter out those results with higher than 1 relative evaluation metric scores.', '']",3
"['', 'it has been recently studied in two distinct settings : ( 1 )  #TAUTHOR_TAG addressed the']","['formality level.', 'it has been recently studied in two distinct settings : ( 1 )  #TAUTHOR_TAG addressed the']","['', 'it has been recently studied in two distinct settings : ( 1 )  #TAUTHOR_TAG addressed the task of formal']","['', 'it has been recently studied in two distinct settings : ( 1 )  #TAUTHOR_TAG addressed the task of formality transfer ( ft ) where given an informal sentence in english, systems are asked to output a formal equivalent, or vice - versa ; ( 2 ) introduced the task of formalitysensitive machine translation ( fsmt ), where given a sentence in french and a desired formality level ( approximating the intended audience of the translation ), systems are asked to produce an english translation of the desired formality level.', 'while ft and fsmt can both be framed as machine translation ( mt ), appropriate training examples are much harder to obtain than for traditional machine translation tasks.', 'ft requires sentence pairs that express the same meaning in two different styles, which rarely occur naturally and are therefore only available in small quantities.', '']",0
"['nmt models  #TAUTHOR_TAG.', 'in this']","['nmt models  #TAUTHOR_TAG.', 'in this work, we leverage this corpus']","['nmt models  #TAUTHOR_TAG.', 'in this work, we leverage this corpus']","['transfer can naturally be framed as a sequence to sequence translation problem given sentence pairs that are paraphrases in two distinct styles.', 'these parallel style corpora are constructed by creatively collecting existing texts of varying styles, and are therefore rare and much smaller than machine translation parallel corpora.', ""for instance,  #AUTHOR_TAG scrape modern translations of shakespeare's plays and use a phrase - based mt ( pbmt ) system to paraphrase shakespearean english into / from modern english."", ' #AUTHOR_TAG improve performance on this dataset using neural translation model with pointers to enable copy actions.', 'the availability of parallel standard and simple wikipedia ( and sometimes additional human rewrites ) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax - based mt  #AUTHOR_TAG, phrase - based mt  #AUTHOR_TAG to neural mt  #AUTHOR_TAG trained via reinforcement learning  #AUTHOR_TAG.', 'naturally occurring examples of parallel formal - informal sentences are harder to find.', 'prior work relied on synthetic examples generated based on lists of words of known formality  #AUTHOR_TAG.', ""this state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, gyafc ( grammarly's yahoo answers formality corpus )."", '110k informal sentences were collected from yahoo answers and they were rewritten in a formal style via crowd - sourcing, which made it possible to benchmark style transfer systems based on both pbmt and nmt models  #TAUTHOR_TAG.', 'in this work, we leverage this corpus to enable multi - task ft and fsmt.', 'recent work also explores how to perform style transfer without parallel data.', 'however, this line of work considers transformations that alter the original meaning ( e. g., changes in sentiment or topic ), while we view style transfer as meaning - preserving.', 'an auto - encoder is used to encode a sequence to a latent representation which is then decoded to get the style transferred output sequence  #AUTHOR_TAG.', 'style in machine translation has received little attention in recent mt architectures.', "" #AUTHOR_TAG improve rule - based mt by using extra - linguistic information such as speaker's role and gender."", ' #AUTHOR_TAG and  #AUTHOR_TAG equate style with domain, and train conversational mt systems by selecting in - domain ( i. e. conversation - like ) training data.', ' #AUTHOR_TAG and  #AUTHOR_TAG take an adaptation approach to personalize mt with gender - specific or speaker - specific data.', 'other work has focused on specific realizations of stylistic variations, such as t - v pronoun selection for translation into german  #AUTHOR_TAG a ) or controlling voice  #AUTHOR_TAG.', 'in contrast, we adopt the broader range of style']",0
['used independent neural machine translation models for'],['used independent neural machine translation models for'],['used independent neural machine translation models for each formality transfer direction ( informal'],"['used independent neural machine translation models for each formality transfer direction ( informal→formal and formal→informal ).', 'inspired by the bi - directional nmt for low - resource languages, we propose a unified model that can handle either direction - we concatenate the parallel data from the two directions of formality transfer and attach a tag to the beginning of each source sentence denoting the desired target formality level i. e. < f > for transferring to formal and < i > for transferring to informal.', 'this enables our ft model to learn to transfer to the correct style via attending to the tag in the source embedding.', 'we train an nmt model on this combined dataset.', 'since both the source and target sentences come from the same language, we encourage their representations to lie in the same distributional vector space by ( 1 ) building a shared byte - pair encoding ( bpe ) model on source and target data  #AUTHOR_TAG b ) and ( 2 ) tying source and target word embeddings  #AUTHOR_TAG']",0
"['with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that ble']","['fsmt.', 'we report case - sensitive bleu with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that bleu correlates']","['fsmt.', 'we report case - sensitive bleu with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that ble']","['evaluate both ft and fsmt tasks using bleu  #AUTHOR_TAG, which compares the model output with four reference target - style rewrites for ft and a single reference translation for fsmt.', 'we report case - sensitive bleu with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that bleu correlates well with the overall system ranking assigned by humans.', 'for fsmt, bleu is an imperfect metric as it conflates mismatches due to translation errors and due to correct style variations.', 'we therefore turn to human evaluation to isolate formality differences from translation quality']",0
['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbm'],['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation'],['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation model'],"['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation model trained on the gyafc corpus using a training regime consisting of self - training, data sub - selection and a large language model.', 'nmt baseline uses opennmt - py  #AUTHOR_TAG.', ' #TAUTHOR_TAG use a pre - processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re - casing.', 'word embeddings pre - trained on yahoo answers are also used.', ""nmt combined is rao and tetreault's best performing nmt model trained on the rule - processed gyafc corpus, with additional forward and backward translations produced by the pbmt model""]",0
['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbm'],['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation'],['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation model'],"['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation model trained on the gyafc corpus using a training regime consisting of self - training, data sub - selection and a large language model.', 'nmt baseline uses opennmt - py  #AUTHOR_TAG.', ' #TAUTHOR_TAG use a pre - processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re - casing.', 'word embeddings pre - trained on yahoo answers are also used.', ""nmt combined is rao and tetreault's best performing nmt model trained on the rule - processed gyafc corpus, with additional forward and backward translations produced by the pbmt model""]",0
"['nmt models  #TAUTHOR_TAG.', 'in this']","['nmt models  #TAUTHOR_TAG.', 'in this work, we leverage this corpus']","['nmt models  #TAUTHOR_TAG.', 'in this work, we leverage this corpus']","['transfer can naturally be framed as a sequence to sequence translation problem given sentence pairs that are paraphrases in two distinct styles.', 'these parallel style corpora are constructed by creatively collecting existing texts of varying styles, and are therefore rare and much smaller than machine translation parallel corpora.', ""for instance,  #AUTHOR_TAG scrape modern translations of shakespeare's plays and use a phrase - based mt ( pbmt ) system to paraphrase shakespearean english into / from modern english."", ' #AUTHOR_TAG improve performance on this dataset using neural translation model with pointers to enable copy actions.', 'the availability of parallel standard and simple wikipedia ( and sometimes additional human rewrites ) makes text simplification a popular style transfer task, typically addressed using machine translation models ranging from syntax - based mt  #AUTHOR_TAG, phrase - based mt  #AUTHOR_TAG to neural mt  #AUTHOR_TAG trained via reinforcement learning  #AUTHOR_TAG.', 'naturally occurring examples of parallel formal - informal sentences are harder to find.', 'prior work relied on synthetic examples generated based on lists of words of known formality  #AUTHOR_TAG.', ""this state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, gyafc ( grammarly's yahoo answers formality corpus )."", '110k informal sentences were collected from yahoo answers and they were rewritten in a formal style via crowd - sourcing, which made it possible to benchmark style transfer systems based on both pbmt and nmt models  #TAUTHOR_TAG.', 'in this work, we leverage this corpus to enable multi - task ft and fsmt.', 'recent work also explores how to perform style transfer without parallel data.', 'however, this line of work considers transformations that alter the original meaning ( e. g., changes in sentiment or topic ), while we view style transfer as meaning - preserving.', 'an auto - encoder is used to encode a sequence to a latent representation which is then decoded to get the style transferred output sequence  #AUTHOR_TAG.', 'style in machine translation has received little attention in recent mt architectures.', "" #AUTHOR_TAG improve rule - based mt by using extra - linguistic information such as speaker's role and gender."", ' #AUTHOR_TAG and  #AUTHOR_TAG equate style with domain, and train conversational mt systems by selecting in - domain ( i. e. conversation - like ) training data.', ' #AUTHOR_TAG and  #AUTHOR_TAG take an adaptation approach to personalize mt with gender - specific or speaker - specific data.', 'other work has focused on specific realizations of stylistic variations, such as t - v pronoun selection for translation into german  #AUTHOR_TAG a ) or controlling voice  #AUTHOR_TAG.', 'in contrast, we adopt the broader range of style']",5
['used independent neural machine translation models for'],['used independent neural machine translation models for'],['used independent neural machine translation models for each formality transfer direction ( informal'],"['used independent neural machine translation models for each formality transfer direction ( informal→formal and formal→informal ).', 'inspired by the bi - directional nmt for low - resource languages, we propose a unified model that can handle either direction - we concatenate the parallel data from the two directions of formality transfer and attach a tag to the beginning of each source sentence denoting the desired target formality level i. e. < f > for transferring to formal and < i > for transferring to informal.', 'this enables our ft model to learn to transfer to the correct style via attending to the tag in the source embedding.', 'we train an nmt model on this combined dataset.', 'since both the source and target sentences come from the same language, we encourage their representations to lie in the same distributional vector space by ( 1 ) building a shared byte - pair encoding ( bpe ) model on source and target data  #AUTHOR_TAG b ) and ( 2 ) tying source and target word embeddings  #AUTHOR_TAG']",5
"['corpus introduced by  #TAUTHOR_TAG as our ft data.', 'this corpus consists of 110']","['corpus introduced by  #TAUTHOR_TAG as our ft data.', 'this corpus consists of 110k informal sentences']","['introduced by  #TAUTHOR_TAG as our ft data.', 'this corpus consists of 110k informal sentences']","['data : we use the gyafc corpus introduced by  #TAUTHOR_TAG as our ft data.', 'this corpus consists of 110k informal sentences from two domains of yahoo answers ( entertainment and music ( e & m ) and family and relationships ( f & r ) ) paired with their formal rewrites by humans.', 'the train split consists of 100k informal - formal sentence pairs whereas the dev / test sets consist of roughly 5k source - style sentences paired with four reference target - style human rewrites for both transfer directions.', 'fsmt data : we evaluate the fsmt models on a large - scale french to english ( fr - en ) translation task.', 'examples are drawn from opensubtitles2016  #AUTHOR_TAG which consists of movie and television subtitles and is thus more similar to the gyafc corpus compared to news or parliament proceedings.', 'this is a noisy dataset where aligned french and english sentences often do not have the same meaning, so we use a bilingual semantic similarity detector to select 20, 005, 000 least divergent examples from ∼27. 5m deduplicated sentence pairs in the original set  #AUTHOR_TAG.', '']",5
"['with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that ble']","['fsmt.', 'we report case - sensitive bleu with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that bleu correlates']","['fsmt.', 'we report case - sensitive bleu with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that ble']","['evaluate both ft and fsmt tasks using bleu  #AUTHOR_TAG, which compares the model output with four reference target - style rewrites for ft and a single reference translation for fsmt.', 'we report case - sensitive bleu with standard wmt tokenization.', '2 for ft,  #TAUTHOR_TAG show that bleu correlates well with the overall system ranking assigned by humans.', 'for fsmt, bleu is an imperfect metric as it conflates mismatches due to translation errors and due to correct style variations.', 'we therefore turn to human evaluation to isolate formality differences from translation quality']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we assess model outputs on three criteria : formality, fluency and meaning preservation.', 'since the goal of our evaluation is to compare models, our evaluation scheme asks workers to compare sentence pairs on these three criteria instead of rating each sentence in isolation.', 'we collect human judgments using crowdflower on 300 samples of each model outputs.', 'for ft, we compare the top performing nmt benchmark model in  #TAUTHOR_TAG with our best ft model.', 'for fsmt, we compare outputs from three representative models : nmt - constraint, multitask - random and pbmt - random.', '3 formality.', 'for ft, we want to measure the amount of style variation introduced by a model.', '']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we assess model outputs on three criteria : formality, fluency and meaning preservation.', 'since the goal of our evaluation is to compare models, our evaluation scheme asks workers to compare sentence pairs on these three criteria instead of rating each sentence in isolation.', 'we collect human judgments using crowdflower on 300 samples of each model outputs.', 'for ft, we compare the top performing nmt benchmark model in  #TAUTHOR_TAG with our best ft model.', 'for fsmt, we compare outputs from three representative models : nmt - constraint, multitask - random and pbmt - random.', '3 formality.', 'for ft, we want to measure the amount of style variation introduced by a model.', '']",5
['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbm'],['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation'],['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation model'],"['formality transfer experiments 6. 1 baseline models from  #TAUTHOR_TAG pbmt is a phrase - based machine translation model trained on the gyafc corpus using a training regime consisting of self - training, data sub - selection and a large language model.', 'nmt baseline uses opennmt - py  #AUTHOR_TAG.', ' #TAUTHOR_TAG use a pre - processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re - casing.', 'word embeddings pre - trained on yahoo answers are also used.', ""nmt combined is rao and tetreault's best performing nmt model trained on the rule - processed gyafc corpus, with additional forward and backward translations produced by the pbmt model""]",5
"['the best baseline model ( nmt - combined ) from  #TAUTHOR_TAG.', 'table 3 shows some samples representative of']","['the best baseline model ( nmt - combined ) from  #TAUTHOR_TAG.', 'table 3 shows some samples representative of']","['the best baseline model ( nmt - combined ) from  #TAUTHOR_TAG.', 'table 3 shows some samples representative of']","['manually inspect 100 randomly selected samples from our evaluation set and compare the target - style output of our best model ( multitask - tag - style ) with that of the best baseline model ( nmt - combined ) from  #TAUTHOR_TAG.', 'table 3 shows some samples representative of the trends we find for informal→formal ( 3a ) and formal→informal ( 3b ) tasks.', '']",5
"['scores than those of  #TAUTHOR_TAG, even without using rule - processed source training data and pre']","['bleu scores than those of  #TAUTHOR_TAG, even without using rule - processed source training data and pretrained word embeddings.', 'we']","['bleu scores than those of  #TAUTHOR_TAG, even without using rule - processed source training data and pre']","['evaluation.', 'as shown in table 1, our nmt baselines yield surprisingly better bleu scores than those of  #TAUTHOR_TAG, even without using rule - processed source training data and pretrained word embeddings.', 'we attribute the difference to the more optimized nmt toolkit we use.', 'initial bi - directional models outperforms uni - directional models.', 'this matches the behavior of bidirectional nmt in low - resource settings studied by - we work with a relatively small amount of training data ( ∼50k ), and ft models benefit from doubling the size of training data without being confused by mixing two transfer directions.', 'for the same reason, increasing the training data by combining two domains together improves performance further.', 'ensemble decoding is a consistently effective technique used by nmt and it enhances our nmt - based ft models as expected.', 'incorporating the bilingual parallel data by multi - task learning yields further improvement.', 'the target side of bilingual data is selected based on the closeness to the gyafc corpus, so we hypothesize that the higher quality comes from better target language modeling by training on more english text.', 'table 2 : human evaluation of formality difference and meaning preservation.', '']",4
"[' #TAUTHOR_TAG.', 'the joint model interestingly']","[' #TAUTHOR_TAG.', 'the joint model interestingly']","['formal and informal styles in both directions, compared to prior work  #TAUTHOR_TAG.', 'the joint model interestingly']","['explored the use of multi - task learning to jointly perform monolingual ft and bilingual fsmt.', 'using french - english translation and english style transfer data, we showed that the joint model is able to learn from both style transfer parallel examples and translation parallel examples.', 'on the ft task, the joint model significantly improves the quality of transfer between formal and informal styles in both directions, compared to prior work  #TAUTHOR_TAG.', 'the joint model interestingly also learns to perform fsmt without being explicitly trained on style - annotated translation examples.', 'on the fsmt task, our model outperforms previously proposed phrase - based mt model, and performs on par with a neural model with side - constraints which requires more involved data selection.', 'these results show the promise of multi - task learning for controlling style in language generation applications.', 'in future work, we plan to investigate other multi - task architectures and objective functions that better capture the desired output properties, in order to help address current weaknesses such as meaning errors revealed by manual analysis']",4
"['to latent structures  #TAUTHOR_TAG representations ) of infinite heights.', 'to resolve such an issue,']","['to latent structures  #TAUTHOR_TAG representations ) of infinite heights.', 'to resolve such an issue,']","['address this limitation, we allow pattern x to be included when building our new discriminative semantic parsing model.', 'however, as mentioned above, doing so will lead to latent structures  #TAUTHOR_TAG representations ) of infinite heights.', 'to resolve such an issue,']","['address this limitation, we allow pattern x to be included when building our new discriminative semantic parsing model.', 'however, as mentioned above, doing so will lead to latent structures  #TAUTHOR_TAG representations ) of infinite heights.', 'to resolve such an issue, we instead add an additional constraint - limiting the height of a semantic representation to a fixed constant c, where c is larger than the maximum height of all the trees appearing in the training set.', 'table 1 summarizes the list of patterns that our model considers.', 'this is essentially the same as those considered by the hybrid tree model.', 'our new objective function is as follows :', 'where m refers to the set of all possible semantic trees whose heights are less than or equal to c, and h ( n, m ) refers to the set of possible  #TAUTHOR_TAG representations where the pattern x is allowed.', 'the main challenge now becomes the computation of the denominator term in equation 2, as the set m is still very large.', 'to properly handle all such semantic trees in an efficient way, we introduce a constrained semantic forest ( csf ) representation of m here.', 'such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. by contrast, it was not possible in our previous  #TAUTHOR_TAG model to introduce such a compact representation over all possible semantic trees.', ""in our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible relaxed hybrid trees containing n."", 'setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. we then constructed the ( exponentially many )  #TAUTHOR_TAG representations.', 'this allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in  #TAUTHOR_TAG.', 'optimization of the model parameters were done by using l - bfgs  #AUTHOR_TAG, where the gradients were computed efficiently using an analogous dynamic programming algorithm']",5
"['system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['- s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['bridtree + system  #AUTHOR_TAG is based on the generative hybrid tree model', 'augmented with a discriminative re - ranking stage where certain global features are', 'used. ubl - s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice, we set c ( the maximum height of', 'a semantic representation ) to 20 in our experi - ments, which we determined based on the heights', 'of the semantic trees that appear in the training data. results showed that our system consistently yielded higher results than all the previous systems,', 'including our state - of - the - art  #TAUTHOR_TAG, when all the features are used ),', 'in terms of both accuracy score and f 1 - measure. we would like to highlight two potential advantages of our new model over the old  #TAUTHOR_TAG model.', 'first, our model is able to handle certain sentence - semantics pairs which could not be handled by  #TAUTHOR_TAG during both training and evaluation as discussed', 'in section 3. 1. second, our model considers the additional pattern x and therefore has the', 'capability to capture more accurate dependencies between the words and semantic units. we', 'note that in our experiments we used a small subset of the features used by our  #TAUTHOR_TAG work. specifically, we did not use any long - distance features, and also did not use any character - level features. as we', 'have mentioned in  #TAUTHOR_TAG, table 4 ). here in this work, we', 'only used simple unigram features ( concatenation of a semantic', 'unit and', 'an individual word that appears directly below that unit in the joint representation ), pattern features ( concatenation of a semantic unit and the pattern below that unit ) as well as transition features ( concatenation of two semantic units that', 'form a parent - child relationship ) described in  #TAUTHOR_TAG. while additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. we summarized in table 3 the number of features used in', 'both the previous  #TAUTHOR_TAG system and our system across four different languages. it can be seen that', 'our system only required about 2 - 3 % of the table 3 : number of features involved for both the  #TAUTHOR_TAG system and our new system using constrained semantic forests, across', 'four different languages. features used in the previous system. we also note that the training time for our model is longer than that of', 'the  #TAUTHOR_TAG model since the space for h ( n, m ) is now much larger than the space', 'for h ( n, m ). in practice, to make the overall training process faster, we implemented a parallel', 'version of the original  #TAUTHOR_TAG algorithm']",5
"['system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['- s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['bridtree + system  #AUTHOR_TAG is based on the generative hybrid tree model', 'augmented with a discriminative re - ranking stage where certain global features are', 'used. ubl - s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice, we set c ( the maximum height of', 'a semantic representation ) to 20 in our experi - ments, which we determined based on the heights', 'of the semantic trees that appear in the training data. results showed that our system consistently yielded higher results than all the previous systems,', 'including our state - of - the - art  #TAUTHOR_TAG, when all the features are used ),', 'in terms of both accuracy score and f 1 - measure. we would like to highlight two potential advantages of our new model over the old  #TAUTHOR_TAG model.', 'first, our model is able to handle certain sentence - semantics pairs which could not be handled by  #TAUTHOR_TAG during both training and evaluation as discussed', 'in section 3. 1. second, our model considers the additional pattern x and therefore has the', 'capability to capture more accurate dependencies between the words and semantic units. we', 'note that in our experiments we used a small subset of the features used by our  #TAUTHOR_TAG work. specifically, we did not use any long - distance features, and also did not use any character - level features. as we', 'have mentioned in  #TAUTHOR_TAG, table 4 ). here in this work, we', 'only used simple unigram features ( concatenation of a semantic', 'unit and', 'an individual word that appears directly below that unit in the joint representation ), pattern features ( concatenation of a semantic unit and the pattern below that unit ) as well as transition features ( concatenation of two semantic units that', 'form a parent - child relationship ) described in  #TAUTHOR_TAG. while additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. we summarized in table 3 the number of features used in', 'both the previous  #TAUTHOR_TAG system and our system across four different languages. it can be seen that', 'our system only required about 2 - 3 % of the table 3 : number of features involved for both the  #TAUTHOR_TAG system and our new system using constrained semantic forests, across', 'four different languages. features used in the previous system. we also note that the training time for our model is longer than that of', 'the  #TAUTHOR_TAG model since the space for h ( n, m ) is now much larger than the space', 'for h ( n, m ). in practice, to make the overall training process faster, we implemented a parallel', 'version of the original  #TAUTHOR_TAG algorithm']",5
"['system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['- s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['bridtree + system  #AUTHOR_TAG is based on the generative hybrid tree model', 'augmented with a discriminative re - ranking stage where certain global features are', 'used. ubl - s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice, we set c ( the maximum height of', 'a semantic representation ) to 20 in our experi - ments, which we determined based on the heights', 'of the semantic trees that appear in the training data. results showed that our system consistently yielded higher results than all the previous systems,', 'including our state - of - the - art  #TAUTHOR_TAG, when all the features are used ),', 'in terms of both accuracy score and f 1 - measure. we would like to highlight two potential advantages of our new model over the old  #TAUTHOR_TAG model.', 'first, our model is able to handle certain sentence - semantics pairs which could not be handled by  #TAUTHOR_TAG during both training and evaluation as discussed', 'in section 3. 1. second, our model considers the additional pattern x and therefore has the', 'capability to capture more accurate dependencies between the words and semantic units. we', 'note that in our experiments we used a small subset of the features used by our  #TAUTHOR_TAG work. specifically, we did not use any long - distance features, and also did not use any character - level features. as we', 'have mentioned in  #TAUTHOR_TAG, table 4 ). here in this work, we', 'only used simple unigram features ( concatenation of a semantic', 'unit and', 'an individual word that appears directly below that unit in the joint representation ), pattern features ( concatenation of a semantic unit and the pattern below that unit ) as well as transition features ( concatenation of two semantic units that', 'form a parent - child relationship ) described in  #TAUTHOR_TAG. while additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. we summarized in table 3 the number of features used in', 'both the previous  #TAUTHOR_TAG system and our system across four different languages. it can be seen that', 'our system only required about 2 - 3 % of the table 3 : number of features involved for both the  #TAUTHOR_TAG system and our new system using constrained semantic forests, across', 'four different languages. features used in the previous system. we also note that the training time for our model is longer than that of', 'the  #TAUTHOR_TAG model since the space for h ( n, m ) is now much larger than the space', 'for h ( n, m ). in practice, to make the overall training process faster, we implemented a parallel', 'version of the original  #TAUTHOR_TAG algorithm']",5
"['system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['- s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice,']","['bridtree + system  #AUTHOR_TAG is based on the generative hybrid tree model', 'augmented with a discriminative re - ranking stage where certain global features are', 'used. ubl - s  #AUTHOR_TAG is a ccg - based semantic parsing system. treetrans  #AUTHOR_TAG is the system', 'based on tree transducers.  #TAUTHOR_TAG. in practice, we set c ( the maximum height of', 'a semantic representation ) to 20 in our experi - ments, which we determined based on the heights', 'of the semantic trees that appear in the training data. results showed that our system consistently yielded higher results than all the previous systems,', 'including our state - of - the - art  #TAUTHOR_TAG, when all the features are used ),', 'in terms of both accuracy score and f 1 - measure. we would like to highlight two potential advantages of our new model over the old  #TAUTHOR_TAG model.', 'first, our model is able to handle certain sentence - semantics pairs which could not be handled by  #TAUTHOR_TAG during both training and evaluation as discussed', 'in section 3. 1. second, our model considers the additional pattern x and therefore has the', 'capability to capture more accurate dependencies between the words and semantic units. we', 'note that in our experiments we used a small subset of the features used by our  #TAUTHOR_TAG work. specifically, we did not use any long - distance features, and also did not use any character - level features. as we', 'have mentioned in  #TAUTHOR_TAG, table 4 ). here in this work, we', 'only used simple unigram features ( concatenation of a semantic', 'unit and', 'an individual word that appears directly below that unit in the joint representation ), pattern features ( concatenation of a semantic unit and the pattern below that unit ) as well as transition features ( concatenation of two semantic units that', 'form a parent - child relationship ) described in  #TAUTHOR_TAG. while additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable. we summarized in table 3 the number of features used in', 'both the previous  #TAUTHOR_TAG system and our system across four different languages. it can be seen that', 'our system only required about 2 - 3 % of the table 3 : number of features involved for both the  #TAUTHOR_TAG system and our new system using constrained semantic forests, across', 'four different languages. features used in the previous system. we also note that the training time for our model is longer than that of', 'the  #TAUTHOR_TAG model since the space for h ( n, m ) is now much larger than the space', 'for h ( n, m ). in practice, to make the overall training process faster, we implemented a parallel', 'version of the original  #TAUTHOR_TAG algorithm']",5
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",0
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",0
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",4
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",4
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",1
['by  #TAUTHOR_TAG to'],['a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to'],['taken by  #TAUTHOR_TAG to'],"['are often called universals to represent their cross - lingual nature  #AUTHOR_TAG. for example, used the multext - east  #AUTHOR_TAG corpus to evaluate', 'their multi - lingual pos induction system, because it uses the same tagset for multiple languages. when corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and', 'treebank specific fine - grained tagsets to a predefined universal set. this was the approach taken by  #TAUTHOR_TAG to evaluate their cross - lingu', '##al pos projection system for six different languages. to facilitate future research and to standardize best - practices, we propose', 'a tagset that consists of twelve universal pos categories. while there might be some controversy', 'about what the exact tagset should be, we feel that these twelve categories cover the most frequent part - of - speech that exist in most languages. in addition to', 'the tagset, we also develop a mapping from fine - grained pos tags for 25 different treebanks to this universal set. as a result, when combined with the original', 'treebank data, this universal tagset and mapping produce a dataset consisting of common parts - ofspeech for 22', '']",6
"['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG,']","['their experiments, naseem et al. also used a set of universal categories, however, with some differences to the tagset presented here.', 'their tagset does not have punctuation and catch - all categories, but includes a category for auxiliaries.', 'the auxiliary category helps define a syntactic rule that attaches verbs to an auxiliary head, which is beneficial for certain languages.', 'however, since this rule is reversed for other languages, we omit it in our tagset.', 'additionally, they also used refined categories in the form of conll treebank tags.', 'in our experiments, we did not make use of refined categories, as the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG, so that we can make use of their automatically projected pos tags.', 'for all languages, we used the treebanks released as a part of the conll - x  #AUTHOR_TAG shared task.', 'we only considered sentences of length 10 or less, after the removal of punctuations.', 'we performed bayesian inference on the whole treebank and report dependency attachment accuracy.', 'table 2 shows directed dependency accuracies for the dmv and pgi models using fine - grained gold pos tags.', 'for the usr model, it reports results on gold universal pos tags ( usr - g ) and automatically induced universal pos tags ( usr - i ).', 'the usr - i model falls short of the usr - g model, but has the advantage that it does not require any labeled data from the target language.', 'quite impressively, it does better than dmv for all languages, and is competitive with pgi, even though those models have access to fine - grained gold pos tags']",5
"['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG,']","['their experiments, naseem et al. also used a set of universal categories, however, with some differences to the tagset presented here.', 'their tagset does not have punctuation and catch - all categories, but includes a category for auxiliaries.', 'the auxiliary category helps define a syntactic rule that attaches verbs to an auxiliary head, which is beneficial for certain languages.', 'however, since this rule is reversed for other languages, we omit it in our tagset.', 'additionally, they also used refined categories in the form of conll treebank tags.', 'in our experiments, we did not make use of refined categories, as the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG, so that we can make use of their automatically projected pos tags.', 'for all languages, we used the treebanks released as a part of the conll - x  #AUTHOR_TAG shared task.', 'we only considered sentences of length 10 or less, after the removal of punctuations.', 'we performed bayesian inference on the whole treebank and report dependency attachment accuracy.', 'table 2 shows directed dependency accuracies for the dmv and pgi models using fine - grained gold pos tags.', 'for the usr model, it reports results on gold universal pos tags ( usr - g ) and automatically induced universal pos tags ( usr - i ).', 'the usr - i model falls short of the usr - g model, but has the advantage that it does not require any labeled data from the target language.', 'quite impressively, it does better than dmv for all languages, and is competitive with pgi, even though those models have access to fine - grained gold pos tags']",5
"['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the']","['the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG,']","['their experiments, naseem et al. also used a set of universal categories, however, with some differences to the tagset presented here.', 'their tagset does not have punctuation and catch - all categories, but includes a category for auxiliaries.', 'the auxiliary category helps define a syntactic rule that attaches verbs to an auxiliary head, which is beneficial for certain languages.', 'however, since this rule is reversed for other languages, we omit it in our tagset.', 'additionally, they also used refined categories in the form of conll treebank tags.', 'in our experiments, we did not make use of refined categories, as the pos tags induced by  #TAUTHOR_TAG were all coarse.', 'we present results on the same eight indoeuropean languages as  #TAUTHOR_TAG, so that we can make use of their automatically projected pos tags.', 'for all languages, we used the treebanks released as a part of the conll - x  #AUTHOR_TAG shared task.', 'we only considered sentences of length 10 or less, after the removal of punctuations.', 'we performed bayesian inference on the whole treebank and report dependency attachment accuracy.', 'table 2 shows directed dependency accuracies for the dmv and pgi models using fine - grained gold pos tags.', 'for the usr model, it reports results on gold universal pos tags ( usr - g ) and automatically induced universal pos tags ( usr - i ).', 'the usr - i model falls short of the usr - g model, but has the advantage that it does not require any labeled data from the target language.', 'quite impressively, it does better than dmv for all languages, and is competitive with pgi, even though those models have access to fine - grained gold pos tags']",3
"['', 'many techniques have been proposed to learn such embeddings  #TAUTHOR_TAG.', 'while']","['techniques on large text corpora.', 'many techniques have been proposed to learn such embeddings  #TAUTHOR_TAG.', 'while']","['learned using unsupervised learning techniques on large text corpora.', 'many techniques have been proposed to learn such embeddings  #TAUTHOR_TAG.', 'while']","['word representations, commonly referred to as word embeddings, represent words as vectors in a low - dimensional space.', 'the goal of this deep representation of words is to capture syntactic and semantic relationships between words.', 'these word embeddings have been proven to be very useful in various nlp applications, particularly those employing deep learning.', 'word embeddings are typically learned using unsupervised learning techniques on large text corpora.', 'many techniques have been proposed to learn such embeddings  #TAUTHOR_TAG.', 'while most of the work has focused on english word embeddings, few attempts have been carried out to learn word embeddings for other languages, mostly using the above mentioned techniques.', 'in this paper, we focus on arabic word embeddings.', 'particularly, we provide a thorough evaluation of the quality of four arabic word embeddings that have been generated by previous work  #AUTHOR_TAG al -  #AUTHOR_TAG.', 'we use both intrinsic and extrinsic evaluation methods to evaluate the different embeddings.', 'for the intrinsic evaluation, we build a benchmark consisting of over 115, 000 word analogy questions for the arabic language.', 'unlike previous attempts to evaluate arabic embeddings, which relied on translating existing english benchmarks, our benchmark is the first specifically built for the arabic language and is publicly available for future work in this area 1. translating an english benchmark is not the best strategy to evaluate arabic embeddings for the following reasons.', '']",0
"[' #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models']","[' #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models']","['extrinsic evalu -  #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models']","['is a wealth of research on evaluating unsupervised word embeddings, which can be can be broadly divided into intrinsic and extrinsic evalu -  #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models for other tasks, such as semantic role labeling and part - of - speech tagging  #AUTHOR_TAG, or noun - phrase chunking and sentiment analysis  #AUTHOR_TAG.', 'however, all of these tasks and benchmarks are build for english and thus cannot be used to assess the quality of arabic word embeddings, which is the main focus here.', 'to the best of our knowledge, only a handful of recent studies attempted evaluating arabic word embeddings.', 'zahran et al.  #AUTHOR_TAG translated the english benchmark in  #TAUTHOR_TAG and used it to evaluate different embedding techniques when applied on a large arabic corpus.', 'however, as the authors themselves point out, translating an english benchmark is not the best strategy to evaluate arabic embeddings.', 'zahran et al. also consider extrinsic evaluation on two nlp tasks, namely query expansion for ir and short answer grading.', 'dahou et al.  #AUTHOR_TAG used the analogy questions from  #AUTHOR_TAG after correcting some arabic spelling mistakes resulting from the translation and after adding new analogy questions to make up for the inadequacy of the english questions for the arabic language.', '']",0
"[' #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models']","[' #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models']","['extrinsic evalu -  #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models']","['is a wealth of research on evaluating unsupervised word embeddings, which can be can be broadly divided into intrinsic and extrinsic evalu -  #TAUTHOR_TAG.', 'extrinsic evaluations assess the quality of the embeddings as features in models for other tasks, such as semantic role labeling and part - of - speech tagging  #AUTHOR_TAG, or noun - phrase chunking and sentiment analysis  #AUTHOR_TAG.', 'however, all of these tasks and benchmarks are build for english and thus cannot be used to assess the quality of arabic word embeddings, which is the main focus here.', 'to the best of our knowledge, only a handful of recent studies attempted evaluating arabic word embeddings.', 'zahran et al.  #AUTHOR_TAG translated the english benchmark in  #TAUTHOR_TAG and used it to evaluate different embedding techniques when applied on a large arabic corpus.', 'however, as the authors themselves point out, translating an english benchmark is not the best strategy to evaluate arabic embeddings.', 'zahran et al. also consider extrinsic evaluation on two nlp tasks, namely query expansion for ir and short answer grading.', 'dahou et al.  #AUTHOR_TAG used the analogy questions from  #AUTHOR_TAG after correcting some arabic spelling mistakes resulting from the translation and after adding new analogy questions to make up for the inadequacy of the english questions for the arabic language.', '']",0
['as mentioned in  #TAUTHOR_TAG'],['as mentioned in  #TAUTHOR_TAG'],['as mentioned in  #TAUTHOR_TAG. for'],"['', 'by using more than two word pairs per question. this', 'provides a more accurate representation of a relation as mentioned in  #TAUTHOR_TAG. for each relation,', 'we generate a question per word pair consisting of the word pair plus 10 random word pairs from the same relation. thus, each question would consist of 11 word pairs ( a', '']",5
"[') model  #TAUTHOR_TAG and glove  #AUTHOR_TAG.', 'the fourth word embeddings we evaluate in this paper is the arabic part of the polyglot word embeddings,']","['the continuous bagof - words ( cbow ) model  #TAUTHOR_TAG and glove  #AUTHOR_TAG.', 'the fourth word embeddings we evaluate in this paper is the arabic part of the polyglot word embeddings,']","[') model  #TAUTHOR_TAG and glove  #AUTHOR_TAG.', 'the fourth word embeddings we evaluate in this paper is the arabic part of the polyglot word embeddings,']","['compare four different arabic word embeddings that have been generated by previous work.', 'the first three are based on a large corpus of arabic documents constructed by zahran et al.  #AUTHOR_TAG, which consists of 2, 340, 895 words.', 'using this corpus, the authors generated three different word embeddings using three different techniques, namely the continuous bagof - words ( cbow ) model  #TAUTHOR_TAG and glove  #AUTHOR_TAG.', 'the fourth word embeddings we evaluate in this paper is the arabic part of the polyglot word embeddings, which was trained on the arabic wikipedia by al - rfou et al and consists of over 100, 000 words ( al -  #AUTHOR_TAG.', 'to the best of our knowledge, these are the only available word embeddings that have been constructed for the arabic language']",5
['as mentioned in  #TAUTHOR_TAG'],['as mentioned in  #TAUTHOR_TAG'],['as mentioned in  #TAUTHOR_TAG. for'],"['', 'by using more than two word pairs per question. this', 'provides a more accurate representation of a relation as mentioned in  #TAUTHOR_TAG. for each relation,', 'we generate a question per word pair consisting of the word pair plus 10 random word pairs from the same relation. thus, each question would consist of 11 word pairs ( a', '']",3
"['linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the']","['of exploiting visual and / or linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the']","['of exploiting visual and / or linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the ability of']","['', 'however, whether or not these models are actually learning to address the tasks they are designed for is questionable.', 'for example,  #AUTHOR_TAG showed that ic models do not understand images sufficiently, as reflected by the generated captions.', 'as a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and / or linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the ability of v2l models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.', ""this is done by replacing a word in mscoco  #AUTHOR_TAG captions with a'foiled'word that is semantically similar or related to the original word ( substituting dog with cat ), thus rendering the image caption unfaithful to the image content, while yet linguistically valid."", ' #AUTHOR_TAG b ) report poor performance for v2l models in classifying captions as foiled ( or not ).', 'they suggested that their models ( using image embeddings as input ) are very poor at encoding structured visuallinguistic information to spot the mismatch between a foiled caption and the corresponding content depicted in the image.', 'in this paper, we focus on the foiled captions classification task ( section 2 ), and propose the use of explicit object detections as salient image cues for solving the task.', 'in contrast to methods from previous work that make use of word based information extracted from captions  #AUTHOR_TAG, we use explicit object category information directly extracted from the images.', 'more specifically, we use an interpretable bag of objects as image representation for the classifier.', ""our hypothesis is that, to truly'understand'the image, v2l models should exploit information about objects and their relations in the image and not just global, low - level image embeddings as used by most v2l models."", 'our main contributions are : 3.', 'an analysis of the models, providing insights into the reasons for their strong performance ( section 5 ).', 'our results reveal that the foil dataset has a very strong linguistic bias, and that the proposed simple object - based models are capable of finding salient patterns to solve the task']",0
"['linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the']","['of exploiting visual and / or linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the']","['of exploiting visual and / or linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the ability of']","['', 'however, whether or not these models are actually learning to address the tasks they are designed for is questionable.', 'for example,  #AUTHOR_TAG showed that ic models do not understand images sufficiently, as reflected by the generated captions.', 'as a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and / or linguistic information  #TAUTHOR_TAG.', 'foil  #TAUTHOR_TAG is one such dataset.', 'it was proposed to evaluate the ability of v2l models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.', ""this is done by replacing a word in mscoco  #AUTHOR_TAG captions with a'foiled'word that is semantically similar or related to the original word ( substituting dog with cat ), thus rendering the image caption unfaithful to the image content, while yet linguistically valid."", ' #AUTHOR_TAG b ) report poor performance for v2l models in classifying captions as foiled ( or not ).', 'they suggested that their models ( using image embeddings as input ) are very poor at encoding structured visuallinguistic information to spot the mismatch between a foiled caption and the corresponding content depicted in the image.', 'in this paper, we focus on the foiled captions classification task ( section 2 ), and propose the use of explicit object detections as salient image cues for solving the task.', 'in contrast to methods from previous work that make use of word based information extracted from captions  #AUTHOR_TAG, we use explicit object category information directly extracted from the images.', 'more specifically, we use an interpretable bag of objects as image representation for the classifier.', ""our hypothesis is that, to truly'understand'the image, v2l models should exploit information about objects and their relations in the image and not just global, low - level image embeddings as used by most v2l models."", 'our main contributions are : 3.', 'an analysis of the models, providing insights into the reasons for their strong performance ( section 5 ).', 'our results reveal that the foil dataset has a very strong linguistic bias, and that the proposed simple object - based models are capable of finding salient patterns to solve the task']",0
"['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and a foiled caption where a word from the original caption is swapped such that it no longer describes the image accurately.', ""there are several sets of'foiled captions'where words from specific parts of speech are swapped :"", '• foiled noun : in this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.', 'the foiled noun is obtained from list of object annotations from mscoco  #AUTHOR_TAG and nouns are constrained to the same supercategory ;', '• foiled verb : here, verb is foiled with a similar verb.', 'the similar verb is extracted using external resources ;', '• foiled adjective and adverb : adjectives and adverbs are replaced with similar adjectives and adverbs.', 'here, the notion of similarity again is obtained from external resources ;', '• foiled preposition : prepositions are directly replaced with functionally similar prepositions.', 'the verb, adjective, adverb and preposition subsets were obtained using a slightly different methodology ( see  #AUTHOR_TAG a ) ) than that used for nouns  #TAUTHOR_TAG.', 'therefore, we evaluate these two groups separately']",0
"['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and a foiled caption where a word from the original caption is swapped such that it no longer describes the image accurately.', ""there are several sets of'foiled captions'where words from specific parts of speech are swapped :"", '• foiled noun : in this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.', 'the foiled noun is obtained from list of object annotations from mscoco  #AUTHOR_TAG and nouns are constrained to the same supercategory ;', '• foiled verb : here, verb is foiled with a similar verb.', 'the similar verb is extracted using external resources ;', '• foiled adjective and adverb : adjectives and adverbs are replaced with similar adjectives and adverbs.', 'here, the notion of similarity again is obtained from external resources ;', '• foiled preposition : prepositions are directly replaced with functionally similar prepositions.', 'the verb, adjective, adverb and preposition subsets were obtained using a slightly different methodology ( see  #AUTHOR_TAG a ) ) than that used for nouns  #TAUTHOR_TAG.', 'therefore, we evaluate these two groups separately']",5
"['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and a foiled caption where a word from the original caption is swapped such that it no longer describes the image accurately.', ""there are several sets of'foiled captions'where words from specific parts of speech are swapped :"", '• foiled noun : in this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.', 'the foiled noun is obtained from list of object annotations from mscoco  #AUTHOR_TAG and nouns are constrained to the same supercategory ;', '• foiled verb : here, verb is foiled with a similar verb.', 'the similar verb is extracted using external resources ;', '• foiled adjective and adverb : adjectives and adverbs are replaced with similar adjectives and adverbs.', 'here, the notion of similarity again is obtained from external resources ;', '• foiled preposition : prepositions are directly replaced with functionally similar prepositions.', 'the verb, adjective, adverb and preposition subsets were obtained using a slightly different methodology ( see  #AUTHOR_TAG a ) ) than that used for nouns  #TAUTHOR_TAG.', 'therefore, we evaluate these two groups separately']",5
['nouns from  #TAUTHOR_TAG 1 and the datasets'],['nouns from  #TAUTHOR_TAG 1 and the datasets'],['nouns from  #TAUTHOR_TAG 1 and the datasets'],"[': we use the dataset for nouns from  #TAUTHOR_TAG 1 and the datasets for other parts of speech from  #AUTHOR_TAG a ) 2.', 'statistics about the dataset are given in table 1.', 'the evaluation metric is accuracy per class and the average ( overall ) accuracy over the two classes']",5
"['- generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['data - generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['- generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['', 'bag of objects information are the best performing models across classifiers.', 'we also note that the performance is better than human performance.', 'we hypothesize the following reasons for this : ( a ) human responses were crowd - sourced, which could have resulted in some noisy annotations ; ( b ) our gold object - based features closely resembles the information used for data - generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close to the performance of gold.', 'the performance of models using simple bag of words ( bow ) sentence representations and an mlp is better than that of models that use lstms.', 'also, the accuracy of the bag of objects model with frequency counts is higher than with the binary mention vector, which only encodes the presence of objects.', 'the multimodal lstm ( mm - lstm ) has a slightly better performance than lstm classifiers.', 'in all cases, we observe that the performance is on par with human - level accuracy.', '']",5
"['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and']","['this section we describe the foiled caption classification task and dataset.', 'we combine the tasks and data from  #TAUTHOR_TAG and  #AUTHOR_TAG a ).', 'given an image and a caption, in both cases the task is to learn a model that can distinguish between a real caption that describes the image, and a foiled caption where a word from the original caption is swapped such that it no longer describes the image accurately.', ""there are several sets of'foiled captions'where words from specific parts of speech are swapped :"", '• foiled noun : in this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.', 'the foiled noun is obtained from list of object annotations from mscoco  #AUTHOR_TAG and nouns are constrained to the same supercategory ;', '• foiled verb : here, verb is foiled with a similar verb.', 'the similar verb is extracted using external resources ;', '• foiled adjective and adverb : adjectives and adverbs are replaced with similar adjectives and adverbs.', 'here, the notion of similarity again is obtained from external resources ;', '• foiled preposition : prepositions are directly replaced with functionally similar prepositions.', 'the verb, adjective, adverb and preposition subsets were obtained using a slightly different methodology ( see  #AUTHOR_TAG a ) ) than that used for nouns  #TAUTHOR_TAG.', 'therefore, we evaluate these two groups separately']",6
"['- generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['data - generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['- generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['', 'bag of objects information are the best performing models across classifiers.', 'we also note that the performance is better than human performance.', 'we hypothesize the following reasons for this : ( a ) human responses were crowd - sourced, which could have resulted in some noisy annotations ; ( b ) our gold object - based features closely resembles the information used for data - generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close to the performance of gold.', 'the performance of models using simple bag of words ( bow ) sentence representations and an mlp is better than that of models that use lstms.', 'also, the accuracy of the bag of objects model with frequency counts is higher than with the binary mention vector, which only encodes the presence of objects.', 'the multimodal lstm ( mm - lstm ) has a slightly better performance than lstm classifiers.', 'in all cases, we observe that the performance is on par with human - level accuracy.', '']",3
"['- generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['data - generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['- generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close']","['', 'bag of objects information are the best performing models across classifiers.', 'we also note that the performance is better than human performance.', 'we hypothesize the following reasons for this : ( a ) human responses were crowd - sourced, which could have resulted in some noisy annotations ; ( b ) our gold object - based features closely resembles the information used for data - generation as described in  #TAUTHOR_TAG for the foil noun dataset.', 'the models using predicted bag of objects from a detector are very close to the performance of gold.', 'the performance of models using simple bag of words ( bow ) sentence representations and an mlp is better than that of models that use lstms.', 'also, the accuracy of the bag of objects model with frequency counts is higher than with the binary mention vector, which only encodes the presence of objects.', 'the multimodal lstm ( mm - lstm ) has a slightly better performance than lstm classifiers.', 'in all cases, we observe that the performance is on par with human - level accuracy.', '']",4
"['in  #TAUTHOR_TAG, even for equivalent models.', 'we note, however, that while']","['in  #TAUTHOR_TAG, even for equivalent models.', 'we note, however, that while']","['in  #TAUTHOR_TAG, even for equivalent models.', 'we note, however, that while']","['', 'on the other hand, text - only models achieve a very high accuracy.', 'this is a central finding, suggesting that foiled captions are easy to detect even without image information.', 'we also observe that the performance of bow improves by adding object frequency image information, but not cnn image embeddings.', 'we posit that this is because there is a tighter correspondence between the bag of objects and bag of word models.', 'in the case of lstms, adding either image information helps slightly.', 'the accuracy of our models is substantially higher than that reported in  #TAUTHOR_TAG, even for equivalent models.', 'we note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of bow based models are lower than the performance of lstm based models.', 'the anomaly of improved performance of bow based models seems heavily pronounced in the nouns dataset.', 'thus, we further analyze our model in the next section to shed light on whether the high performance is due to the models or the dataset itself.', 'table 4 : ablation study on foil ( nouns )']",4
"['- translation iterations  #TAUTHOR_TAG, or large -']","['initial back - translation iterations  #TAUTHOR_TAG, or large - scale pre - training through masked denoising,']","['the initial back - translation iterations  #TAUTHOR_TAG, or large - scale pre - training through masked denoising,']","['- translation  #AUTHOR_TAG allows to naturally exploit monolingual corpora in neural machine translation ( nmt ) by using a reverse model to generate a synthetic parallel corpus.', 'despite its simplicity, this technique has become a key component in state - of - the - art nmt systems.', 'for instance, the majority of wmt19 submissions, including the best performing systems, made extensive use of it  #AUTHOR_TAG.', 'while the synthetic parallel corpus generated through back - translation is typically combined with real parallel corpora, iterative or online variants of this technique also play a central role in unsupervised machine translation  #AUTHOR_TAG a ( artetxe et al.,, b, 2019  #AUTHOR_TAG a, b ;  #AUTHOR_TAG.', 'in iterative back - translation, both nmt models are jointly trained using synthetic parallel data generated on - the - fly with the reverse model, alternating between both translation directions iteratively.', 'while this enables fully unsupervised training without parallel corpora, some initialization mechanism is still required so the models can start producing sound translations and provide a meaningful training signal to each other.', 'for that purpose, state - of - the - art approaches rely on either a separately trained unsupervised statistical machine translation ( smt ) system, which is used for warmup during the initial back - translation iterations  #TAUTHOR_TAG, or large - scale pre - training through masked denoising, which is used to initialize the weights of the underlying encoder - decoder  #AUTHOR_TAG.', 'in this paper, we aim to understand the role that the initialization mechanism plays in iterative backtranslation.', 'for that purpose, we mimic the experimental settings of  #TAUTHOR_TAG themselves, supervised nmt and smt systems trained on both small and large parallel corpora, and a commercial rule - based machine translation ( rbmt ) system.', 'despite the fundamentally different nature of these systems, our analysis reveals that iterative back - translation has a strong tendency to converge to a similar solution.', 'given the relatively small impact of the initial system, we conclude that fu - ture research on unsupervised machine translation should focus more on improving the iterative backtranslation mechanism itself']",0
['- translation was also explored by  #AUTHOR_TAG and  #TAUTHOR_TAG in'],"['iteration of back - translation.', 'iterative back - translation was also explored by  #AUTHOR_TAG and  #TAUTHOR_TAG in']","['of back - translation.', 'iterative back - translation was also explored by  #AUTHOR_TAG and  #TAUTHOR_TAG in']","['proposed by  #AUTHOR_TAG, backtranslation has been widely adopted by the machine translation community  #AUTHOR_TAG, yet its behavior is still not fully understood.', 'several authors have studied the optimal balance between real and synthetic parallel data, concluding that using too much synthetic data can be harmful  #AUTHOR_TAG.', 'in addition to that,  #AUTHOR_TAG observe that back - translation is most helpful for tokens with a high prediction loss, and use this insight to design a better selection method for monolingual data.', 'at the same time, show that random sampling provides a stronger training signal than beam search or greedy decoding.', 'closer to our work, the impact of the system used for backtranslation has also been explored by some authors  #AUTHOR_TAG, although the iterative back - translation variant, which allows to jointly train both systems so they can help each other, was not considered, and synthetic data was always combined with real parallel data.', 'while all the previous authors use a fixed system to generate synthetic parallel corpora,  #AUTHOR_TAG propose performing a second iteration of back - translation.', 'iterative back - translation was also explored by  #AUTHOR_TAG and  #TAUTHOR_TAG in the context of unsupervised machine translation, relying on an unsupervised smt system  #AUTHOR_TAG b ;  #AUTHOR_TAG a ) for warmup.', 'early work in unsupervised nmt also incorporated the idea of on - the - fly backtranslation, which was combined with denoising autoencoding and a shared encoder initialized through unsupervised cross - lingual embeddings  #AUTHOR_TAG b ;  #AUTHOR_TAG a ).', 'more recently, several authors have performed large - scale unsupervised pre - training through masked denoising to initialize the full model, which is then trained through iterative back - translation  #AUTHOR_TAG.', 'finally, iterative back - translation is also connected to the reconstruction loss in dual learning  #AUTHOR_TAG, which incorporates an additional language modeling loss and also requires a warm start']",0
"['- translation iterations  #TAUTHOR_TAG, or large -']","['initial back - translation iterations  #TAUTHOR_TAG, or large - scale pre - training through masked denoising,']","['the initial back - translation iterations  #TAUTHOR_TAG, or large - scale pre - training through masked denoising,']","['- translation  #AUTHOR_TAG allows to naturally exploit monolingual corpora in neural machine translation ( nmt ) by using a reverse model to generate a synthetic parallel corpus.', 'despite its simplicity, this technique has become a key component in state - of - the - art nmt systems.', 'for instance, the majority of wmt19 submissions, including the best performing systems, made extensive use of it  #AUTHOR_TAG.', 'while the synthetic parallel corpus generated through back - translation is typically combined with real parallel corpora, iterative or online variants of this technique also play a central role in unsupervised machine translation  #AUTHOR_TAG a ( artetxe et al.,, b, 2019  #AUTHOR_TAG a, b ;  #AUTHOR_TAG.', 'in iterative back - translation, both nmt models are jointly trained using synthetic parallel data generated on - the - fly with the reverse model, alternating between both translation directions iteratively.', 'while this enables fully unsupervised training without parallel corpora, some initialization mechanism is still required so the models can start producing sound translations and provide a meaningful training signal to each other.', 'for that purpose, state - of - the - art approaches rely on either a separately trained unsupervised statistical machine translation ( smt ) system, which is used for warmup during the initial back - translation iterations  #TAUTHOR_TAG, or large - scale pre - training through masked denoising, which is used to initialize the weights of the underlying encoder - decoder  #AUTHOR_TAG.', 'in this paper, we aim to understand the role that the initialization mechanism plays in iterative backtranslation.', 'for that purpose, we mimic the experimental settings of  #TAUTHOR_TAG themselves, supervised nmt and smt systems trained on both small and large parallel corpora, and a commercial rule - based machine translation ( rbmt ) system.', 'despite the fundamentally different nature of these systems, our analysis reveals that iterative back - translation has a strong tendency to converge to a similar solution.', 'given the relatively small impact of the initial system, we conclude that fu - ture research on unsupervised machine translation should focus more on improving the iterative backtranslation mechanism itself']",5
"['by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative']","['by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this']","['next describe the iterative back - translation implementation used in our experiments, which was proposed by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this particular implementation.', 'the method in question trains two nmt systems in opposite directions following an iterative process where,']","['next describe the iterative back - translation implementation used in our experiments, which was proposed by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this particular implementation.', 'the method in question trains two nmt systems in opposite directions following an iterative process where, at every iteration, each model is updated by performing a single pass over a set of n synthetic parallel sentences generated through back - translation.', 'after iteration a, the synthetic parallel corpus is entirely generated by the reverse nmt model.', 'however, so as to ensure that the nmt models produce sound translations and provide meaningful training signal to each other, the first a warmup iterations progressively transition from a separate initial system to the reverse nmt model itself.', 'more concretely, iteration t uses n init = n · max ( 0, 1 − t / a ) back - translated sentences from the reverse initial system, and the remaining n − n initial sentences are generated by the reverse nmt model.', 'in the latter case, half of the translations use random sampling, which produces more varied translations, whereas the other half are generated through greedy decoding, which produces more fluent and predictable translations.', 'following  #TAUTHOR_TAG, we set n = 1, 000, 000 and a = 30, and perform a total of 60 such iterations.', 'both nmt models use the big transformer implementation from fairseq 1, training with a total batch size of 20, 000 tokens with the exact same hyperparameters as.', 'at test time, we use beam search decoding with a beam size of 5']",5
"['by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative']","['by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this']","['next describe the iterative back - translation implementation used in our experiments, which was proposed by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this particular implementation.', 'the method in question trains two nmt systems in opposite directions following an iterative process where,']","['next describe the iterative back - translation implementation used in our experiments, which was proposed by  #TAUTHOR_TAG.', 'note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this particular implementation.', 'the method in question trains two nmt systems in opposite directions following an iterative process where, at every iteration, each model is updated by performing a single pass over a set of n synthetic parallel sentences generated through back - translation.', 'after iteration a, the synthetic parallel corpus is entirely generated by the reverse nmt model.', 'however, so as to ensure that the nmt models produce sound translations and provide meaningful training signal to each other, the first a warmup iterations progressively transition from a separate initial system to the reverse nmt model itself.', 'more concretely, iteration t uses n init = n · max ( 0, 1 − t / a ) back - translated sentences from the reverse initial system, and the remaining n − n initial sentences are generated by the reverse nmt model.', 'in the latter case, half of the translations use random sampling, which produces more varied translations, whereas the other half are generated through greedy decoding, which produces more fluent and predictable translations.', 'following  #TAUTHOR_TAG, we set n = 1, 000, 000 and a = 30, and perform a total of 60 such iterations.', 'both nmt models use the big transformer implementation from fairseq 1, training with a total batch size of 20, 000 tokens with the exact same hyperparameters as.', 'at test time, we use beam search decoding with a beam size of 5']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['as to better understand the role of initialization in iterative back - translation, we train different english - german models using the following initial systems for warmup :', '• rbmt : we use the commercial lucy lt translator  #AUTHOR_TAG, a tra - 1 https : / / github. com / pytorch / fairseq ditional transfer - based rbmt system combining human crafted computational grammars and monolingual and bilingual lexicons.', '• supervised nmt : we use the fairseq implementation of the big transformer model using the same hyperparameters as.', 'we train two separate models : one using the concatenation of all parallel corpora from wmt 2014, and another one using a random subset of 100, 000 sentences.', 'in both cases, we use early stopping according to the cross - entropy in newstest2013.', '• supervised smt : we use the moses  #AUTHOR_TAG implementation of phrase - based smt  #AUTHOR_TAG with default hyperparameters, using fastalign  #AUTHOR_TAG for word alignment.', 'we train two separate models using the same parallel corpus splits as for nmt.', 'in both cases, we use a 5 - gram language model trained with kenlm  #AUTHOR_TAG on news crawl 2007 - 2013, and apply mert tuning  #AUTHOR_TAG over newstest2013.', '• unsupervised : we use the unsupervised smt system proposed by  #TAUTHOR_TAG, which induces an initial phrase - table using cross - lingual word embedding mappings, combines it with an n - gram language model, and further improves the resulting model through unsupervised tuning and joint refinement.', 'for each initial system, we train a separate nmt model through iterative back - translation as described in section 2.', 'for that purpose, we use the news crawl 2007 - 2013 monolingual corpus as distributed in the wmt 2014 shared task.', '2 preprocessing is done using standard moses tools, and involves punctuation normalization, tokenization with aggressive hyphen splitting, and truecasing.', 'we evaluate in newstest2014 using tokenized bleu, and compare the performance of the different final systems after iterative back - translation and the initial systems used in their warmup.', '3 however, this only provides a measure of the quality of the different systems, but not the similarity of the translations they produce.', 'so as to quantify how similar the translations of two systems are, we compute their corresponding bleu scores taking one of them as the reference.', 'this way, we report the average similarity of each final system with the rest of final systems, and analogously for the initial ones.', 'finally, we also compute the similarity between each initial system and']",5
"[' #TAUTHOR_TAG, a new corpus was constructed from']","[' #TAUTHOR_TAG, a new corpus was constructed from a pun of the day website. it systematically explained']","[' #TAUTHOR_TAG, a new corpus was constructed from']","['humorous instances. three humor - specific stylistic features, including alliteration, antonymy, and', 'adult slang were utilized together with content - based features to build classifiers. in a recent', 'work  #TAUTHOR_TAG, a new corpus was constructed from a pun of the day website. it systematically explained and computed latent semantic structure features based on the following four aspects : ( a ) incongruity, ( b ) ambiguity, ( c ) interpersonal effect, and ( d', "") phonetic style. in addition, word2vec  #AUTHOR_TAG distributed representations were utilized in the model building. beyond lexical cues from text inputs, other research has also utilized speakers '"", 'acoustic cues  #AUTHOR_TAG b ). these studies have typically used audio tracks from tv shows and their corresponding', ""captions in order to categorize characters'speaking turns as humorous or non - humorous. utterances prior to canned laughter that was manually inserted into the shows were treated as humorous"", ', while other utterances were treated as negative cases. convolutional neural networks ( cnns', '']",0
"[' #TAUTHOR_TAG, a new corpus was constructed from']","[' #TAUTHOR_TAG, a new corpus was constructed from a pun of the day website. it systematically explained']","[' #TAUTHOR_TAG, a new corpus was constructed from']","['humorous instances. three humor - specific stylistic features, including alliteration, antonymy, and', 'adult slang were utilized together with content - based features to build classifiers. in a recent', 'work  #TAUTHOR_TAG, a new corpus was constructed from a pun of the day website. it systematically explained and computed latent semantic structure features based on the following four aspects : ( a ) incongruity, ( b ) ambiguity, ( c ) interpersonal effect, and ( d', "") phonetic style. in addition, word2vec  #AUTHOR_TAG distributed representations were utilized in the model building. beyond lexical cues from text inputs, other research has also utilized speakers '"", 'acoustic cues  #AUTHOR_TAG b ). these studies have typically used audio tracks from tv shows and their corresponding', ""captions in order to categorize characters'speaking turns as humorous or non - humorous. utterances prior to canned laughter that was manually inserted into the shows were treated as humorous"", ', while other utterances were treated as negative cases. convolutional neural networks ( cnns', '']",0
"['.', 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of hum']","['defined as non - humorous sentences.', 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of humorous and']","['', 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of hum']","['talks 3 are recordings from ted conferences and other special ted programs.', 'in the present study, we focused on the transcripts of the talks.', ""most transcripts of the talks contain the markup'( laughter ) ', which represents where audiences laughed aloud during the talks."", 'this special markup was used to determine utterance labels.', 'we collected 1, 192 ted talk transcripts 4.', 'an example transcription is given in figure 1.', 'the collected transcripts were split into sentences using the stanford corenlp tool  #AUTHOR_TAG.', ""in this study, sentences containing or immediately followed by'( laughter )'were used as humorous sentences, as shown in figure 1 ; all other sentences were defined as non - humorous sentences."", 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of humorous and non - humorous sentences.', 'to minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby ( the context window was 7 sentences in this study ).', ""for example, in figure 1, a negative instance ( corresponding to'sent - 2') was selected from the nearby sentences ranging from'sent - 7'and'sent + 7 '""]",0
"['model, as suggested in  #TAUTHOR_TAG. in particular,']","['model, as suggested in  #TAUTHOR_TAG. in particular,']","['model, as suggested in  #TAUTHOR_TAG. in particular, precision has been greatly increased', '']","['', '##thub. com / educationaltestingservice / skll 8 https : / / github. com / fchollet / keras 9 the implementation will be released with the paper on', 'the pun data, the cnn model shows consistent improved performance over the conventional model, as suggested in  #TAUTHOR_TAG. in particular, precision has been greatly increased', '']",0
"[' #TAUTHOR_TAG, a new corpus was constructed from']","[' #TAUTHOR_TAG, a new corpus was constructed from a pun of the day website. it systematically explained']","[' #TAUTHOR_TAG, a new corpus was constructed from']","['humorous instances. three humor - specific stylistic features, including alliteration, antonymy, and', 'adult slang were utilized together with content - based features to build classifiers. in a recent', 'work  #TAUTHOR_TAG, a new corpus was constructed from a pun of the day website. it systematically explained and computed latent semantic structure features based on the following four aspects : ( a ) incongruity, ( b ) ambiguity, ( c ) interpersonal effect, and ( d', "") phonetic style. in addition, word2vec  #AUTHOR_TAG distributed representations were utilized in the model building. beyond lexical cues from text inputs, other research has also utilized speakers '"", 'acoustic cues  #AUTHOR_TAG b ). these studies have typically used audio tracks from tv shows and their corresponding', ""captions in order to categorize characters'speaking turns as humorous or non - humorous. utterances prior to canned laughter that was manually inserted into the shows were treated as humorous"", ', while other utterances were treated as negative cases. convolutional neural networks ( cnns', '']",1
"['.', 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of hum']","['defined as non - humorous sentences.', 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of humorous and']","['', 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of hum']","['talks 3 are recordings from ted conferences and other special ted programs.', 'in the present study, we focused on the transcripts of the talks.', ""most transcripts of the talks contain the markup'( laughter ) ', which represents where audiences laughed aloud during the talks."", 'this special markup was used to determine utterance labels.', 'we collected 1, 192 ted talk transcripts 4.', 'an example transcription is given in figure 1.', 'the collected transcripts were split into sentences using the stanford corenlp tool  #AUTHOR_TAG.', ""in this study, sentences containing or immediately followed by'( laughter )'were used as humorous sentences, as shown in figure 1 ; all other sentences were defined as non - humorous sentences."", 'following  #TAUTHOR_TAG, we selected the same sizes ( n = 4726 ) of humorous and non - humorous sentences.', 'to minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby ( the context window was 7 sentences in this study ).', ""for example, in figure 1, a negative instance ( corresponding to'sent - 2') was selected from the nearby sentences ranging from'sent - 7'and'sent + 7 '""]",5
"[' #TAUTHOR_TAG, we applied random forest  #AUTHOR_TAG to']","[' #TAUTHOR_TAG, we applied random forest  #AUTHOR_TAG to']","[' #TAUTHOR_TAG, we applied random forest  #AUTHOR_TAG to do humor recognition by using the following two groups of features.', 'the first group are latent semantic structural features covering the following 4 categories 5 : incongruity ( 2 ), ambiguity ( 6 ), interpersonal effect ( 4 ), and phonetic pattern ( 4 ).', '']","[' #TAUTHOR_TAG, we applied random forest  #AUTHOR_TAG to do humor recognition by using the following two groups of features.', 'the first group are latent semantic structural features covering the following 4 categories 5 : incongruity ( 2 ), ambiguity ( 6 ), interpersonal effect ( 4 ), and phonetic pattern ( 4 ).', ""the second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence ( found by using a k - nearest neighbors ( knn ) method ), and each sentence's averaged word2vec representations ( n = 300 )""]",5
"['model, as suggested in  #TAUTHOR_TAG. in particular,']","['model, as suggested in  #TAUTHOR_TAG. in particular,']","['model, as suggested in  #TAUTHOR_TAG. in particular, precision has been greatly increased', '']","['', '##thub. com / educationaltestingservice / skll 8 https : / / github. com / fchollet / keras 9 the implementation will be released with the paper on', 'the pun data, the cnn model shows consistent improved performance over the conventional model, as suggested in  #TAUTHOR_TAG. in particular, precision has been greatly increased', '']",3
"['model, as suggested in  #TAUTHOR_TAG. in particular,']","['model, as suggested in  #TAUTHOR_TAG. in particular,']","['model, as suggested in  #TAUTHOR_TAG. in particular, precision has been greatly increased', '']","['', '##thub. com / educationaltestingservice / skll 8 https : / / github. com / fchollet / keras 9 the implementation will be released with the paper on', 'the pun data, the cnn model shows consistent improved performance over the conventional model, as suggested in  #TAUTHOR_TAG. in particular, precision has been greatly increased', '']",3
"['with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power -']","['with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']","['s law with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']","['', ""= 1 [ 8 ]. in contrast, our argument only requires n to be large enough. third, its assumptions are far reaching : compression allows one to shed light on the origins of three linguistic laws at the same time : zipf's law for word frequencies, zipf's law of abbreviation and menzerath's law with the"", 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']",0
"['with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power -']","['with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']","['s law with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']","['', ""= 1 [ 8 ]. in contrast, our argument only requires n to be large enough. third, its assumptions are far reaching : compression allows one to shed light on the origins of three linguistic laws at the same time : zipf's law for word frequencies, zipf's law of abbreviation and menzerath's law with the"", 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']",0
"['with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power -']","['with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']","['s law with the', 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']","['', ""= 1 [ 8 ]. in contrast, our argument only requires n to be large enough. third, its assumptions are far reaching : compression allows one to shed light on the origins of three linguistic laws at the same time : zipf's law for word frequencies, zipf's law of abbreviation and menzerath's law with the"", 'unifying principle of compression [ 3,  #TAUTHOR_TAG 5 ]. there are many', 'ways of explaining the origins of power - law -', '']",0
"['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion -']","[', they tend to overfit the training data. this is not surprising because these models do', 'not explicitly use much syntactic information, which is more general than lexical information', '. in this paper, we aim to study how syntactic information can be incorporated', 'into neural network models for sentence compression to improve their', 'domain adaptability. we hope to train a model that performs well on both in - domain and out - ofdomain data. to this end, we extend the deletionbased lstm model for sentence', 'compression by  #TAUTHOR_TAG. although deletion - based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion - based sentence compression for', 'the following reason. abstractive sentence compression allows new words to be used in a compressed sentence, i. e., words that do not occur in the input sentence. oftentimes', 'these new words serve as paraphrases of some words or phrases in the source sentence. but to generate such paraphrases, the model needs to have seen them in the training data. because we', '']",1
['original method by  #TAUTHOR_TAG'],['original method by  #TAUTHOR_TAG'],"['the original method by  #TAUTHOR_TAG cannot impose any length constraint on the compressed sentences.', 'this is']","['the method above has explicitly incorporated some syntactic information into the bi - lstm model, the syntactic information is used in a soft manner through the learned model weights.', 'we hypothesize that there are also hard constraints we can impose on the compressed sentences.', 'for example, the method above as well as the original method by  #TAUTHOR_TAG cannot impose any length constraint on the compressed sentences.', 'this is because the labels y 1, y 2,.', '.., y n are not jointly predicted.', 'we propose to use integer linear programming ( ilp ) to find an optimal combination of the labels y 1, y 2,..., y n for a sentence, subject to some constraints.', 'specifically, the ilp problem consists of two parts : the objective function, and the constraints']",1
"['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion -']","[', they tend to overfit the training data. this is not surprising because these models do', 'not explicitly use much syntactic information, which is more general than lexical information', '. in this paper, we aim to study how syntactic information can be incorporated', 'into neural network models for sentence compression to improve their', 'domain adaptability. we hope to train a model that performs well on both in - domain and out - ofdomain data. to this end, we extend the deletionbased lstm model for sentence', 'compression by  #TAUTHOR_TAG. although deletion - based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion - based sentence compression for', 'the following reason. abstractive sentence compression allows new words to be used in a compressed sentence, i. e., words that do not occur in the input sentence. oftentimes', 'these new words serve as paraphrases of some words or phrases in the source sentence. but to generate such paraphrases, the model needs to have seen them in the training data. because we', '']",6
"['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion -']","[', they tend to overfit the training data. this is not surprising because these models do', 'not explicitly use much syntactic information, which is more general than lexical information', '. in this paper, we aim to study how syntactic information can be incorporated', 'into neural network models for sentence compression to improve their', 'domain adaptability. we hope to train a model that performs well on both in - domain and out - ofdomain data. to this end, we extend the deletionbased lstm model for sentence', 'compression by  #TAUTHOR_TAG. although deletion - based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion - based sentence compression for', 'the following reason. abstractive sentence compression allows new words to be used in a compressed sentence, i. e., words that do not occur in the input sentence. oftentimes', 'these new words serve as paraphrases of some words or phrases in the source sentence. but to generate such paraphrases, the model needs to have seen them in the training data. because we', '']",6
"['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion']","['', 'compression by  #TAUTHOR_TAG. although deletion -']","[', they tend to overfit the training data. this is not surprising because these models do', 'not explicitly use much syntactic information, which is more general than lexical information', '. in this paper, we aim to study how syntactic information can be incorporated', 'into neural network models for sentence compression to improve their', 'domain adaptability. we hope to train a model that performs well on both in - domain and out - ofdomain data. to this end, we extend the deletionbased lstm model for sentence', 'compression by  #TAUTHOR_TAG. although deletion - based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion - based sentence compression for', 'the following reason. abstractive sentence compression allows new words to be used in a compressed sentence, i. e., words that do not occur in the input sentence. oftentimes', 'these new words serve as paraphrases of some words or phrases in the source sentence. but to generate such paraphrases, the model needs to have seen them in the training data. because we', '']",5
"['that we focus on deletion - based sentence compression.', 'our problem setup is the same as that by  #TAUTHOR_TAG']","['that we focus on deletion - based sentence compression.', 'our problem setup is the same as that by  #TAUTHOR_TAG']","['that we focus on deletion - based sentence compression.', 'our problem setup is the same as that by  #TAUTHOR_TAG.', 'let us use']","['that we focus on deletion - based sentence compression.', 'our problem setup is the same as that by  #TAUTHOR_TAG.', 'let us use s = ( w 1, w 2,..., w n ) to denote an input sentence, which consists of a sequence of words.', '']",5
"['on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a']","['on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a']","['on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a d - dimensional embedding vector.', 'for']","['first introduce our base model, which uses lstm to perform sequence labeling.', 'this base model is largely based on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a d - dimensional embedding vector.', 'for input sentence s, let us use ( w 1, w 2,..., w n ) to denote the sequence of the word embedding vectors, where', 'we use a standard bi - directional lstm model to process these embedding vectors sequentially from both directions to obtain a sequence of hidden vectors ( h 1, h 2,..., h n ), where h i ∈ r h.', 'we omit the details of the bi - lstm and refer the interested readers to the work by  #AUTHOR_TAG for further explanation.', 'following  #TAUTHOR_TAG, our bi - lstm has three layers, as shown in figure 2.', 'we then use the hidden vectors to predict the label sequence.', 'specifically, label y i is predicted from h i as follows :', 'where w ∈ r 2×h and b ∈ r 2 are a weight matrix and a weight vector to be learned.', 'there are some differences between our base model and the lstm model by  #TAUTHOR_TAG.', '']",5
"['on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a']","['on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a']","['on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a d - dimensional embedding vector.', 'for']","['first introduce our base model, which uses lstm to perform sequence labeling.', 'this base model is largely based on the model by  #TAUTHOR_TAG with some differences, which will be explained below.', 'we assume that each word in the vocabulary has a d - dimensional embedding vector.', 'for input sentence s, let us use ( w 1, w 2,..., w n ) to denote the sequence of the word embedding vectors, where', 'we use a standard bi - directional lstm model to process these embedding vectors sequentially from both directions to obtain a sequence of hidden vectors ( h 1, h 2,..., h n ), where h i ∈ r h.', 'we omit the details of the bi - lstm and refer the interested readers to the work by  #AUTHOR_TAG for further explanation.', 'following  #TAUTHOR_TAG, our bi - lstm has three layers, as shown in figure 2.', 'we then use the hidden vectors to predict the label sequence.', 'specifically, label y i is predicted from h i as follows :', 'where w ∈ r 2×h and b ∈ r 2 are a weight matrix and a weight vector to be learned.', 'there are some differences between our base model and the lstm model by  #TAUTHOR_TAG.', '']",5
['by  #TAUTHOR_TAG. we report both the'],"['by  #TAUTHOR_TAG. we report both the performance they achieved', 'using close']",['by  #TAUTHOR_TAG. we report both the'],"['', '##tm model. bilstm + synfeat + ilp : in this setting, on top of bilstm +', 'synfeat, we solve the ilp problem as described in section 2. 4 to predict the final label sequence y. in the experiments, our model was trained using the adam', ' #AUTHOR_TAG algorithm with a learning rate initialized at 0. 001. the dimension of the hidden layers of bi - lstm is 100', '. word embeddings are initialized from glove 100 - dimensional pre - trained embeddings  #AUTHOR_TAG. pos and dependency embeddings are randomly initialized with 40 - dimensional vectors. the', 'embeddings are all updated during training. dropping probability for dropout layers between stacked lstm layers is 0. 5. the batch size is set as 30. for the ilp part, λ is set to 0. 5, β and γ', 'are turned by the validation data and finally they are set to 0. 7 and 0. 2, respectively. we utilize an open source ilp solver 4 in our method. we compare our methods with a few baselines :', 'lstm : this is the basic lstm - based deletion method proposed by  #TAUTHOR_TAG. we report both the performance they achieved', 'using close to two million training sentence pairs and the performance of our re', '- implementation of their model trained on the 8, 000 sentence pairs. lstm', '+ : this is advanced version of the model proposed by  #TAUTHOR_TAG,', 'where the authors incorporated some dependency parse tree', 'information into the lstm model and used the prediction on the previous word to', 'help the prediction on the current word.', 'traditional ilp : this is the ilp - based method proposed by  #AUTHOR_TAG. this method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input', 'sentences 5. abstractive seq2seq : this is an abstract', '##ive sequence - to - sequence model trained on 3. 8 million gigaword title - article pairs as described in section 1']",5
['by  #TAUTHOR_TAG. we report both the'],"['by  #TAUTHOR_TAG. we report both the performance they achieved', 'using close']",['by  #TAUTHOR_TAG. we report both the'],"['', '##tm model. bilstm + synfeat + ilp : in this setting, on top of bilstm +', 'synfeat, we solve the ilp problem as described in section 2. 4 to predict the final label sequence y. in the experiments, our model was trained using the adam', ' #AUTHOR_TAG algorithm with a learning rate initialized at 0. 001. the dimension of the hidden layers of bi - lstm is 100', '. word embeddings are initialized from glove 100 - dimensional pre - trained embeddings  #AUTHOR_TAG. pos and dependency embeddings are randomly initialized with 40 - dimensional vectors. the', 'embeddings are all updated during training. dropping probability for dropout layers between stacked lstm layers is 0. 5. the batch size is set as 30. for the ilp part, λ is set to 0. 5, β and γ', 'are turned by the validation data and finally they are set to 0. 7 and 0. 2, respectively. we utilize an open source ilp solver 4 in our method. we compare our methods with a few baselines :', 'lstm : this is the basic lstm - based deletion method proposed by  #TAUTHOR_TAG. we report both the performance they achieved', 'using close to two million training sentence pairs and the performance of our re', '- implementation of their model trained on the 8, 000 sentence pairs. lstm', '+ : this is advanced version of the model proposed by  #TAUTHOR_TAG,', 'where the authors incorporated some dependency parse tree', 'information into the lstm model and used the prediction on the previous word to', 'help the prediction on the current word.', 'traditional ilp : this is the ilp - based method proposed by  #AUTHOR_TAG. this method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input', 'sentences 5. abstractive seq2seq : this is an abstract', '##ive sequence - to - sequence model trained on 3. 8 million gigaword title - article pairs as described in section 1']",5
"['practice as  #TAUTHOR_TAG.', 'we then use']","['practice as  #TAUTHOR_TAG.', 'we then use']","['as the test set, following the same practice as  #TAUTHOR_TAG.', 'we then use']","['the two datasets google news and bnc news that have the ground truth compressed sentences, we can perform automatic evaluation.', 'we first split the google news dataset into a training set, a validation set and a test set.', 'we took the first 1, 000 sentence pairs from google news as the test set, following the same practice as  #TAUTHOR_TAG.', 'we then use 8, 000 of the remaining sentence pairs for training and the other 1, 000 sentence pairs for validation.', 'for the nbc news dataset, we use it only as a test set, applying the sentence compression models trained from the 8, 000 sentence pairs from google news.', '']",5
"[', we adopt the manual evaluation procedure by  #TAUTHOR_TAG to compare']","['readable, we adopt the manual evaluation procedure by  #TAUTHOR_TAG to compare']",['we adopt the manual evaluation procedure by  #TAUTHOR_TAG to compare'],"['evaluation above does not look at the readability of the compressed sentences.', 'in order to evaluate whether sentences generated by our method are readable, we adopt the manual evaluation procedure by  #TAUTHOR_TAG to compare our method with lstm + and traditional ilp in terms of readability and informativeness.', 'we asked two raters to score a randomly selected set of 100 sentences from the research papers dataset.', 'the compressed sentences were randomly ordered and presented to the human raters to avoid any bias.', 'the raters were asked to score the sentences on a five - point scale in terms of both readability and informativeness.', 'we show the average scores of the three methods we compare in table 3.', 'we can see that our bilstm + synfeat + ilp method clearly outperforms the two baseline methods in the manual evaluation.', 'we also show a small sample of input sentences from the research papers dataset and the automatically compressed sentences by different methods in table 2.', 'as we can see from the table, a general weakness of the lstm + method is that the compressed sentences may not be grammatical.', 'in comparison, our method does better in terms of preserving grammaticality']",5
['compression by  #TAUTHOR_TAG. there has also been much'],"['is based on the deletion - based lstm model', 'for sentence compression by  #TAUTHOR_TAG. there has also been much']","['is based on the deletion - based lstm model', 'for sentence compression by  #TAUTHOR_TAG. there has also been much interest in applying sequence - to - sequence models', 'for abstractive sentence compression  #AUTHOR_TAG. as we pointed out in section 1, in a cross - domain setting, abstract']","['', 'separately, berg  #AUTHOR_TAG jointly model the two processes in one ilp problem. bigrams', 'and subtrees are represented by some features, and feature are learned on some training data. the ilp problem maximizes the coverage of weighted bigrams and deleted subtrees of the summary. in recent years, neural', 'network models, especially sequence - to - sequence models, have been applied to sentence compression. our work is based on the deletion - based lstm model', 'for sentence compression by  #TAUTHOR_TAG. there has also been much interest in applying sequence - to - sequence models', 'for abstractive sentence compression  #AUTHOR_TAG. as we pointed out in section 1, in a cross - domain setting, abstractive sentence compression', 'may not be suitable']",5
"['the methods of  #AUTHOR_TAG,  #TAUTHOR_TAG hearst (, 1997, and a new technique utilizing an orthonormal']","['the methods of  #AUTHOR_TAG,  #TAUTHOR_TAG hearst (, 1997, and a new technique utilizing an orthonormal']","['', 'the only existing work on topic segmentation in dialogue,  #AUTHOR_TAG, segments recorded speech between multiple persons using both lexical cohesion and dis - tinctive textual and acoustic markers.', 'the present work differs from  #AUTHOR_TAG in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue.', 'in this study we apply the methods of  #AUTHOR_TAG,  #TAUTHOR_TAG hearst (, 1997, and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.', '']","['', 'the only existing work on topic segmentation in dialogue,  #AUTHOR_TAG, segments recorded speech between multiple persons using both lexical cohesion and dis - tinctive textual and acoustic markers.', 'the present work differs from  #AUTHOR_TAG in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue.', 'in this study we apply the methods of  #AUTHOR_TAG,  #TAUTHOR_TAG hearst (, 1997, and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.', 'all three are vector space methods that measure lexical cohesion to determine topic shifts.', 'our results show that the new using an orthonormal basis significantly outperforms the other methods.', '']",5
"['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to']","['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to']","['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to']","['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to be determined for dialogue.', ' #AUTHOR_TAG recommends using the average paragraph size as the window size.', ""using the development corpus's average topic length of 16 utterances as a reference point, f - measures were calculated for the combinations of window size and text unit size in table 1."", 'the optimal combination of parameters ( fmeasure =. 17 ) is a unit size of 16 words and a window size of 16 units.', ""this combination matches  #TAUTHOR_TAG's heuristic of choosing the window size to be the average paragraph length."", 'on the test set, this combination of parameters yielded an f - measure of. 14 as opposed to the fmeasure for monologue reported by  #AUTHOR_TAG,. 70.', 'for dialogue, the algorithm is 20 % as effective as it is for monologue.', 'it is unclear, however, exactly what part of the algorithm contributes to this poor performance.', 'the two most obvious possibilities are the segmentation criterion, i. e. depth scores, or the standard vector space method.', 'to further explore these possibilities, the hearst method was augmented with lsa.', 'again, the unit size and window size had to be calculated.', 'as with foltz, the unit size was taken to be the utterance.', 'the window size was determined by computing f - measures on the development corpus for all sizes between 1 and 16.', 'the optimal window size is 9, f - measure =. 22.', 'given the smaller number of test cases, 22, this f - measure of. 22 is not significantly different from. 17.', 'however, the foltz method is significantly higher than both of these, p <. 10']",5
"['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to']","['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to']","['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to']","['jtexttile software was used to implement  #TAUTHOR_TAG on dialogue.', 'as with  #AUTHOR_TAG, a text unit and window size had to be determined for dialogue.', ' #AUTHOR_TAG recommends using the average paragraph size as the window size.', ""using the development corpus's average topic length of 16 utterances as a reference point, f - measures were calculated for the combinations of window size and text unit size in table 1."", 'the optimal combination of parameters ( fmeasure =. 17 ) is a unit size of 16 words and a window size of 16 units.', ""this combination matches  #TAUTHOR_TAG's heuristic of choosing the window size to be the average paragraph length."", 'on the test set, this combination of parameters yielded an f - measure of. 14 as opposed to the fmeasure for monologue reported by  #AUTHOR_TAG,. 70.', 'for dialogue, the algorithm is 20 % as effective as it is for monologue.', 'it is unclear, however, exactly what part of the algorithm contributes to this poor performance.', 'the two most obvious possibilities are the segmentation criterion, i. e. depth scores, or the standard vector space method.', 'to further explore these possibilities, the hearst method was augmented with lsa.', 'again, the unit size and window size had to be calculated.', 'as with foltz, the unit size was taken to be the utterance.', 'the window size was determined by computing f - measures on the development corpus for all sizes between 1 and 16.', 'the optimal window size is 9, f - measure =. 22.', 'given the smaller number of test cases, 22, this f - measure of. 22 is not significantly different from. 17.', 'however, the foltz method is significantly higher than both of these, p <. 10']",5
"['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.', 'an implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG use vector space methods discussed below to represent and compare units of text.', 'the comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.', 'however,  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG differ on how text units are defined and on how to interpret the results of a comparison.', ""the text unit's definition in  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG is generally task dependent, depending on what size gives the best results."", 'for example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because lsa is most correlated with comprehension at that level.', 'however, when using lsa to segment text,  #AUTHOR_TAG use the paragraph as the unit, to "" smooth out "" the local changes in cohesion and become more sensitive to more global changes of cohesion.', 'hearst likewise chooses a large unit, 6 token - sequences of 20 tokens  #TAUTHOR_TAG, but varies these parameters dependent on the characteristics of the text to be segmented, e. g. paragraph size.', 'under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.', 'as stated previously, these comparisons reflect the cohesion between units of text.', 'in order to use these comparisons to segment text, however, one must have a criterion in place.', ' #AUTHOR_TAG, noting mean cosines of. 16 for boundaries and. 43 for non - boundaries, choose a threshold criterion of. 15, which is two standard deviations below the boundary mean of. 43.', 'using lsa and this criterion,  #AUTHOR_TAG detected chapter boundaries with an f - measure of. 33 ( see manning and schutze ( 1999 ) for a definition of fmeasure ).', ' #AUTHOR_TAG hearst (, 1997 in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.', 'a depth score is computed as the difference between a given vector comparison and its surrounding peaks, i. e. the local maxima of vector comparisons on either side of the given vector comparison.', 'the greater the difference between a given comparison and its surrounding peaks, the higher the depth score.', 'once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken']",0
"['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.', 'an implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG use vector space methods discussed below to represent and compare units of text.', 'the comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.', 'however,  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG differ on how text units are defined and on how to interpret the results of a comparison.', ""the text unit's definition in  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG is generally task dependent, depending on what size gives the best results."", 'for example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because lsa is most correlated with comprehension at that level.', 'however, when using lsa to segment text,  #AUTHOR_TAG use the paragraph as the unit, to "" smooth out "" the local changes in cohesion and become more sensitive to more global changes of cohesion.', 'hearst likewise chooses a large unit, 6 token - sequences of 20 tokens  #TAUTHOR_TAG, but varies these parameters dependent on the characteristics of the text to be segmented, e. g. paragraph size.', 'under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.', 'as stated previously, these comparisons reflect the cohesion between units of text.', 'in order to use these comparisons to segment text, however, one must have a criterion in place.', ' #AUTHOR_TAG, noting mean cosines of. 16 for boundaries and. 43 for non - boundaries, choose a threshold criterion of. 15, which is two standard deviations below the boundary mean of. 43.', 'using lsa and this criterion,  #AUTHOR_TAG detected chapter boundaries with an f - measure of. 33 ( see manning and schutze ( 1999 ) for a definition of fmeasure ).', ' #AUTHOR_TAG hearst (, 1997 in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.', 'a depth score is computed as the difference between a given vector comparison and its surrounding peaks, i. e. the local maxima of vector comparisons on either side of the given vector comparison.', 'the greater the difference between a given comparison and its surrounding peaks, the higher the depth score.', 'once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken']",0
"['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.', 'an implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG use vector space methods discussed below to represent and compare units of text.', 'the comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.', 'however,  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG differ on how text units are defined and on how to interpret the results of a comparison.', ""the text unit's definition in  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG is generally task dependent, depending on what size gives the best results."", 'for example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because lsa is most correlated with comprehension at that level.', 'however, when using lsa to segment text,  #AUTHOR_TAG use the paragraph as the unit, to "" smooth out "" the local changes in cohesion and become more sensitive to more global changes of cohesion.', 'hearst likewise chooses a large unit, 6 token - sequences of 20 tokens  #TAUTHOR_TAG, but varies these parameters dependent on the characteristics of the text to be segmented, e. g. paragraph size.', 'under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.', 'as stated previously, these comparisons reflect the cohesion between units of text.', 'in order to use these comparisons to segment text, however, one must have a criterion in place.', ' #AUTHOR_TAG, noting mean cosines of. 16 for boundaries and. 43 for non - boundaries, choose a threshold criterion of. 15, which is two standard deviations below the boundary mean of. 43.', 'using lsa and this criterion,  #AUTHOR_TAG detected chapter boundaries with an f - measure of. 33 ( see manning and schutze ( 1999 ) for a definition of fmeasure ).', ' #AUTHOR_TAG hearst (, 1997 in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.', 'a depth score is computed as the difference between a given vector comparison and its surrounding peaks, i. e. the local maxima of vector comparisons on either side of the given vector comparison.', 'the greater the difference between a given comparison and its surrounding peaks, the higher the depth score.', 'once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken']",0
"['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997']","['the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.', 'an implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.', 'both  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG use vector space methods discussed below to represent and compare units of text.', 'the comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.', 'however,  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG differ on how text units are defined and on how to interpret the results of a comparison.', ""the text unit's definition in  #TAUTHOR_TAG hearst (, 1997 and  #AUTHOR_TAG is generally task dependent, depending on what size gives the best results."", 'for example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because lsa is most correlated with comprehension at that level.', 'however, when using lsa to segment text,  #AUTHOR_TAG use the paragraph as the unit, to "" smooth out "" the local changes in cohesion and become more sensitive to more global changes of cohesion.', 'hearst likewise chooses a large unit, 6 token - sequences of 20 tokens  #TAUTHOR_TAG, but varies these parameters dependent on the characteristics of the text to be segmented, e. g. paragraph size.', 'under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.', 'as stated previously, these comparisons reflect the cohesion between units of text.', 'in order to use these comparisons to segment text, however, one must have a criterion in place.', ' #AUTHOR_TAG, noting mean cosines of. 16 for boundaries and. 43 for non - boundaries, choose a threshold criterion of. 15, which is two standard deviations below the boundary mean of. 43.', 'using lsa and this criterion,  #AUTHOR_TAG detected chapter boundaries with an f - measure of. 33 ( see manning and schutze ( 1999 ) for a definition of fmeasure ).', ' #AUTHOR_TAG hearst (, 1997 in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.', 'a depth score is computed as the difference between a given vector comparison and its surrounding peaks, i. e. the local maxima of vector comparisons on either side of the given vector comparison.', 'the greater the difference between a given comparison and its surrounding peaks, the higher the depth score.', 'once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken']",0
"['window.', ' #AUTHOR_TAG hearst (, 1997 was replicated using the jtexttile  #AUTHOR_TAG java software.', 'a variant of  #TAUTHOR_TAG hearst (, 1997 was created by using lsa instead of']","['window.', ' #AUTHOR_TAG hearst (, 1997 was replicated using the jtexttile  #AUTHOR_TAG java software.', 'a variant of  #TAUTHOR_TAG hearst (, 1997 was created by using lsa instead of']","['calculated the cosine of each window.', ' #AUTHOR_TAG hearst (, 1997 was replicated using the jtexttile  #AUTHOR_TAG java software.', 'a variant of  #TAUTHOR_TAG hearst (, 1997 was created by using lsa instead of the standard vector space method.', 'the orthonormal basis method also used a moving window ; however, in contrast']","[""task domain is a subset of conversations from human - human computer mediated tutoring sessions on newton's three laws of motion, in which tutor and tutee engaged in a chat room - style conversation."", 'the benefits of this task domain are twofold.', 'firstly, the conversations are already transcribed.', 'additionally, tutors were instructed to introduce problems using a fixed set of scripted problem statements.', 'therefore each topic shift corresponds to a distinct problem introduced by the tutor.', 'clearly this problem would be trivial for a cue phrase based approach, which could learn the finite set of problem introductions.', 'however, the current lexical approach does not have this luxury : words in the problem statements recur throughout the following dialogue.', 'human to human computer mediated physics tutoring transcripts first were removed of all markup, translated to lower case, and each utterance was broken into a separate paragraph.', 'an lsa space was made with these paragraphs alone, approximately one megabyte of text.', 'the conversations were then randomly assigned to training ( 21 conversations ) and testing ( 22 conversations ).', 'the average number of utterances per topic, 16 utterances, and the average number of words per utterance, 32 words, were calculated to determine the parameters of the segmentation methods.', 'for example, a moving window size greater than 16 utterances implies that, in the majority of occurrences, the moving window straddles three topics as opposed to the desired two.', 'to replicate  #AUTHOR_TAG, software was written in java that created a moving window of varying sizes on the input text, and the software retrieved the lsa vector and calculated the cosine of each window.', ' #AUTHOR_TAG hearst (, 1997 was replicated using the jtexttile  #AUTHOR_TAG java software.', 'a variant of  #TAUTHOR_TAG hearst (, 1997 was created by using lsa instead of the standard vector space method.', 'the orthonormal basis method also used a moving window ; however, in contrast to the previous methods, the window is not treated just as a large block of text.', 'instead, the window consists of two orthonormal bases, one on either side of an utterance.', 'that is, a region of utterances above the test utterance is projected, utterance by utterance, into an orthonormal basis, and likewise a region of utterances below the test utterance is projected into another orthonormal basis.', 'then the test utterance is projected into each orthonormal basis, yielding measures of "" relevance "" and "" informativity "" with respect to each.', 'next the elements that make up each orthonormal basis are aggregated into a block, and a cosine is calculated between the test utterance and the blocks on either side, producing a total of six']",4
[' #TAUTHOR_TAG hear'],[' #TAUTHOR_TAG hear'],"["" #TAUTHOR_TAG hearst (, 1997 )'s segmentation criterion, i. e. depth"", 'scores, do not translate well to dialogue. perhaps the assignment of segment boundaries based on the relative difference between', 'a candidate score and its surrounding peaks is highly sensitive to cohesion', 'gaps created by conversational implicatures. on the other hand the differences between']","['', 'is more difficult to explain. these two methods differ by their segmentation criterion and by their training ( foltz is a regression model and hearst is not )', "". it may be that  #TAUTHOR_TAG hearst (, 1997 )'s segmentation criterion, i. e. depth"", 'scores, do not translate well to dialogue. perhaps the assignment of segment boundaries based on the relative difference between', 'a candidate score and its surrounding peaks is highly sensitive to cohesion', 'gaps created by conversational implicatures. on the other hand the differences between these two methods may be entirely attributable to the amount of training they received. one way to separate', 'the contributions of the segmentation criterion and training would be to create a logistic model using the hearst + lsa method and to compare this to foltz. the increased', '']",4
[' #TAUTHOR_TAG hear'],[' #TAUTHOR_TAG hear'],"["" #TAUTHOR_TAG hearst (, 1997 )'s segmentation criterion, i. e. depth"", 'scores, do not translate well to dialogue. perhaps the assignment of segment boundaries based on the relative difference between', 'a candidate score and its surrounding peaks is highly sensitive to cohesion', 'gaps created by conversational implicatures. on the other hand the differences between']","['', 'is more difficult to explain. these two methods differ by their segmentation criterion and by their training ( foltz is a regression model and hearst is not )', "". it may be that  #TAUTHOR_TAG hearst (, 1997 )'s segmentation criterion, i. e. depth"", 'scores, do not translate well to dialogue. perhaps the assignment of segment boundaries based on the relative difference between', 'a candidate score and its surrounding peaks is highly sensitive to cohesion', 'gaps created by conversational implicatures. on the other hand the differences between these two methods may be entirely attributable to the amount of training they received. one way to separate', 'the contributions of the segmentation criterion and training would be to create a logistic model using the hearst + lsa method and to compare this to foltz. the increased', '']",7
"['tasks  #TAUTHOR_TAG.', 'if we can']","['tasks  #TAUTHOR_TAG.', 'if we can']","[' #TAUTHOR_TAG.', 'if we can learn a general - purpose embedding for textual relations,']","['', 'textual relation  #AUTHOR_TAG, defined as the shortest path between two entities in the dependency parse tree of a sentence, has been widely shown to be the main bearer of relational information in text and proved effective in relation extraction tasks  #TAUTHOR_TAG.', '']",0
"['tasks  #TAUTHOR_TAG.', 'if we can']","['tasks  #TAUTHOR_TAG.', 'if we can']","[' #TAUTHOR_TAG.', 'if we can learn a general - purpose embedding for textual relations,']","['', 'textual relation  #AUTHOR_TAG, defined as the shortest path between two entities in the dependency parse tree of a sentence, has been widely shown to be the main bearer of relational information in text and proved effective in relation extraction tasks  #TAUTHOR_TAG.', '']",0
"['have been studied by a number of works  #AUTHOR_TAG.', ' #TAUTHOR_TAG use global']","['have been studied by a number of works  #AUTHOR_TAG.', ' #TAUTHOR_TAG use global co - occurrence statistics of 1 https : / / github. com / czyssrs / gloreplus textual and kb relations to effectively']","['have been studied by a number of works  #AUTHOR_TAG.', ' #TAUTHOR_TAG use global co - occurrence statistics of 1']","['supervision methods  #AUTHOR_TAG for relation extraction have been studied by a number of works  #AUTHOR_TAG.', ' #TAUTHOR_TAG use global co - occurrence statistics of 1 https : / / github. com / czyssrs / gloreplus textual and kb relations to effectively combat the wrong labeling problem. but the global statistics in their work is limited to nyt dataset, capturing domain - specific distributions.', 'another line of research that relates to ours is the universal schema  #AUTHOR_TAG for relation extraction, kb completion, as well as its extensions  #AUTHOR_TAG.', 'wrong labeling problem still exists since their embedding is learned based on individual relation facts.', 'in contrast, we use the global cooccurrence statistics as explicit supervision signal']",0
"['', 'following  #TAUTHOR_TAG, we']","['are unique.', 'following  #TAUTHOR_TAG, we']","['', 'following  #TAUTHOR_TAG, we']","['', 'following  #TAUTHOR_TAG, we then collect the global co - occurrence statistics of textual and kb relations.', '']",5
"['the checkpoint with minimum validation loss for the result.', 'we also compare with using vanilla rnn in glore  #TAUTHOR_TAG.', 'denote the embedding']","['the checkpoint with minimum validation loss for the result.', 'we also compare with using vanilla rnn in glore  #TAUTHOR_TAG.', 'denote the embedding']","['the checkpoint with minimum validation loss for the result.', 'we also compare with using vanilla rnn in glore  #TAUTHOR_TAG.', 'denote the embedding']","['', 'the training objective is to minimize the cross - entropy loss :', 'where', 'w and b are trainable parameters.', 'we use the filtered relation graph in § 3. 1 as our training data.', 'to guarantee that the model generalizes to unseen textual relations, we take 5 % of the training data as validation set.', 'word embeddings are initialized with the glove  #AUTHOR_TAG vectors 3.', 'dependency relation embeddings are initialized randomly.', 'for the transformer model, we use 6 layers and 6 attention heads for each layer.', 'we use the adam optimizer  #AUTHOR_TAG with parameter settings suggested by the original transformer paper  #AUTHOR_TAG.', 'we train a maximum number of 200 epochs and take the checkpoint with minimum validation loss for the result.', 'we also compare with using vanilla rnn in glore  #TAUTHOR_TAG.', 'denote the embedding trained with tranformer as glore + +, standing for both new data and different model, and with rnn as glore +, standing for new data.', 'we observe that, in the early stage of training, the validation loss of rnn decreases faster than transformer.', 'however, it starts to overfit soon']",5
"[' #TAUTHOR_TAG, we aim at augment']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augment']","['experiment on the popular new york times ( nyt ) relation extraction dataset  #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing relation extractors with the textual relation embeddings.', '']",5
"[' #TAUTHOR_TAG, we aim at augment']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augment']","['experiment on the popular new york times ( nyt ) relation extraction dataset  #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing relation extractors with the textual relation embeddings.', '']",5
"[' #TAUTHOR_TAG, we aim at augment']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augment']","['experiment on the popular new york times ( nyt ) relation extraction dataset  #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing relation extractors with the textual relation embeddings.', '']",3
"[' #TAUTHOR_TAG, we aim at augment']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing']","[' #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augment']","['experiment on the popular new york times ( nyt ) relation extraction dataset  #AUTHOR_TAG.', 'following glore  #TAUTHOR_TAG, we aim at augmenting existing relation extractors with the textual relation embeddings.', '']",1
"['type of information are trainable and randomly initialized before training following  #TAUTHOR_TAG.', '']","['type of information are trainable and randomly initialized before training following  #TAUTHOR_TAG.', '[ ; ] denotes']","['##able parameters.', 'the word embeddings for each type of information are trainable and randomly initialized before training following  #TAUTHOR_TAG.', '']","['', 'w a and b a are trainable parameters.', 'the word embeddings for each type of information are trainable and randomly initialized before training following  #TAUTHOR_TAG.', '[ ; ] denotes the vector concatenation.', 'then, we use a lstm decoder with attention and conditional copy to model the conditional probability p ( y t | y < t, s ).', 'the base model first use attention mechanism  #AUTHOR_TAG to find relevant records from the input tables and represent them as context vector.', ""please note that the base model doesn't utilize the structure of three tables and normalize the attention weight α t, i, j across every records in every tables."", ""then it combines the context vector with decoder's hidden state d t and form a new attentional hidden state d t which is used to generate words from vocabulary p gen ( y t | y < t, s ) = sof tmax ( w ddt + b d ) also the conditional copy mechanism is adopted in base model."", 'it introduces a variable z t to decide whether to copy from tables or generate from vocabulary.', 'the probability to copy from table is p ( z t = 1 | y < t, s ) = sigmoid ( w e · d t + b e ).', 'then it decomposes the conditional probability of generating t th word p ( y t | y < t, s ), given the tables s and previously generated words y < t, as follows']",3
['used in  #TAUTHOR_TAG which'],['used in  #TAUTHOR_TAG which'],['one used in  #TAUTHOR_TAG which outputted'],"['', 'heads and layers mentioned above were for both record -', 'level encoder and rowlevel encoder respectively. the self - attention ( sa ) cell we used, as described in', 'section 3, achieved better overall performance in terms of f1 % of cs, co and bleu among the hierarchical encoders. also we implemented a template system same as the one used in  #TAUTHOR_TAG which outputted eight sentences : an', ""introductory sentence ( two teams'points and who win ), six top players'statistics ( ranked by"", ""their points ) and a conclusion sentence. we refer the readers to  #TAUTHOR_TAG's paper for more detailed information on"", ""templates. the gold reference's result is also included in table 1. overall, our model performs better than other neural models on both development and test set in"", '']",3
"['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for']","['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for']","['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for each example, it provides three tables as described in section 2. 1 which consists of 628 records in total with a long game summary.', 'the average length of game summary is 337. 1.', 'in this paper, we followed the data split introduced in']","['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for each example, it provides three tables as described in section 2. 1 which consists of 628 records in total with a long game summary.', 'the average length of game summary is 337. 1.', 'in this paper, we followed the data split introduced in']",5
"['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for']","['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for']","['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for each example, it provides three tables as described in section 2. 1 which consists of 628 records in total with a long game summary.', 'the average length of game summary is 337. 1.', 'in this paper, we followed the data split introduced in']","['conducted experiments on rotowire  #TAUTHOR_TAG.', 'for each example, it provides three tables as described in section 2. 1 which consists of 628 records in total with a long game summary.', 'the average length of game summary is 337. 1.', 'in this paper, we followed the data split introduced in']",5
['used in  #TAUTHOR_TAG which'],['used in  #TAUTHOR_TAG which'],['one used in  #TAUTHOR_TAG which outputted'],"['', 'heads and layers mentioned above were for both record -', 'level encoder and rowlevel encoder respectively. the self - attention ( sa ) cell we used, as described in', 'section 3, achieved better overall performance in terms of f1 % of cs, co and bleu among the hierarchical encoders. also we implemented a template system same as the one used in  #TAUTHOR_TAG which outputted eight sentences : an', ""introductory sentence ( two teams'points and who win ), six top players'statistics ( ranked by"", ""their points ) and a conclusion sentence. we refer the readers to  #TAUTHOR_TAG's paper for more detailed information on"", ""templates. the gold reference's result is also included in table 1. overall, our model performs better than other neural models on both development and test set in"", '']",5
['used in  #TAUTHOR_TAG which'],['used in  #TAUTHOR_TAG which'],['one used in  #TAUTHOR_TAG which outputted'],"['', 'heads and layers mentioned above were for both record -', 'level encoder and rowlevel encoder respectively. the self - attention ( sa ) cell we used, as described in', 'section 3, achieved better overall performance in terms of f1 % of cs, co and bleu among the hierarchical encoders. also we implemented a template system same as the one used in  #TAUTHOR_TAG which outputted eight sentences : an', ""introductory sentence ( two teams'points and who win ), six top players'statistics ( ranked by"", ""their points ) and a conclusion sentence. we refer the readers to  #TAUTHOR_TAG's paper for more detailed information on"", ""templates. the gold reference's result is also included in table 1. overall, our model performs better than other neural models on both development and test set in"", '']",5
['using  #TAUTHOR_TAG'],"[""using  #TAUTHOR_TAG's""]","['on test set.', 'results were obtained using  #TAUTHOR_TAG']","[', column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance table 2 : automatic evaluation results on test set.', ""results were obtained using  #TAUTHOR_TAG's trained extractive evaluation models with relexicalization  #AUTHOR_TAG."", '']",5
['using  #TAUTHOR_TAG'],"[""using  #TAUTHOR_TAG's""]","['on test set.', 'results were obtained using  #TAUTHOR_TAG']","[', column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance table 2 : automatic evaluation results on test set.', ""results were obtained using  #TAUTHOR_TAG's trained extractive evaluation models with relexicalization  #AUTHOR_TAG."", '']",5
['used in  #TAUTHOR_TAG which'],['used in  #TAUTHOR_TAG which'],['one used in  #TAUTHOR_TAG which outputted'],"['', 'heads and layers mentioned above were for both record -', 'level encoder and rowlevel encoder respectively. the self - attention ( sa ) cell we used, as described in', 'section 3, achieved better overall performance in terms of f1 % of cs, co and bleu among the hierarchical encoders. also we implemented a template system same as the one used in  #TAUTHOR_TAG which outputted eight sentences : an', ""introductory sentence ( two teams'points and who win ), six top players'statistics ( ranked by"", ""their points ) and a conclusion sentence. we refer the readers to  #TAUTHOR_TAG's paper for more detailed information on"", ""templates. the gold reference's result is also included in table 1. overall, our model performs better than other neural models on both development and test set in"", '']",7
['cc  #TAUTHOR_TAG'],['cc  #TAUTHOR_TAG'],"['full model with gold texts, template - based system, cc  #TAUTHOR_TAG']","['', 'it turns out that 56. 7 % summaries of the sampled summaries need history information.', 'following human evaluation settings in  #AUTHOR_TAG, we conducted the following human evaluation experiments at the same scale.', 'the second experiment is to assess whether the improvement on relation generation metric reported in automatic evaluation is supported by human evaluation.', 'we compared our full model with gold texts, template - based system, cc  #TAUTHOR_TAG and ncp + cc ( ncp )  #AUTHOR_TAG.', 'we randomly sampled 30 examples from test set.', ""then, we randomly sampled 4 sentences from each model's output for each example."", 'we provided the raters of those sampled sentences with the corresponding nba game statistics.', '']",4
"['on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', 'to the original sentence.']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['', 'the probability distribution of the vocabulary corresponding to the masked word. here, we give an example shown in figure 1 to illustrate', 'the generation process using bert. for complex words\'composed\'and\'verses\'in the sentence "" john composed these verses. "", the top three substitution', 'candidates of the two complex words generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', '']",0
"['on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', 'to the original sentence.']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['', 'the probability distribution of the vocabulary corresponding to the masked word. here, we give an example shown in figure 1 to illustrate', 'the generation process using bert. for complex words\'composed\'and\'verses\'in the sentence "" john composed these verses. "", the top three substitution', 'candidates of the two complex words generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', '']",0
"['on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', 'to the original sentence.']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['', 'the probability distribution of the vocabulary corresponding to the masked word. here, we give an example shown in figure 1 to illustrate', 'the generation process using bert. for complex words\'composed\'and\'verses\'in the sentence "" john composed these verses. "", the top three substitution', 'candidates of the two complex words generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', '']",0
"['on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', 'to the original sentence.']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['', 'the probability distribution of the vocabulary corresponding to the masked word. here, we give an example shown in figure 1 to illustrate', 'the generation process using bert. for complex words\'composed\'and\'verses\'in the sentence "" john composed these verses. "", the top three substitution', 'candidates of the two complex words generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', '']",0
"['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task.', 'the popular lexical simplification ( ls ) approaches are rule - based, which each rule contain a complex word and its simple synonyms  #TAUTHOR_TAG.', 'in order to construct rules, rule - based systems usually identified synonyms from wordnet for a predefined set of complex words, and selected the "" simplest "" from these synonyms based on the frequency of word [  #AUTHOR_TAG ] or length of word [  #AUTHOR_TAG ].', 'however, rule - based systems need a lot of human involvement to manually define these rules, and it is impossible to give all possible simplification rules.', ""as complex and simplified parallel corpora are available, especially, the'ordinary'english wikipedia ( ew ) in combination with the'simple'english wikipedia ( sew ), the paradigm shift of ls systems is from knowledge - based to data - driven simplification [  #AUTHOR_TAG ]."", ' #AUTHOR_TAG identified lexical simplifications from the edit history of sew.', 'they utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes.', ' #AUTHOR_TAG considered every pair of the distinct word in the ew and sew to be a possible simplification pair, and filtered part of them based on morphological variants and wordnet.', '']",0
"['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task.', 'the popular lexical simplification ( ls ) approaches are rule - based, which each rule contain a complex word and its simple synonyms  #TAUTHOR_TAG.', 'in order to construct rules, rule - based systems usually identified synonyms from wordnet for a predefined set of complex words, and selected the "" simplest "" from these synonyms based on the frequency of word [  #AUTHOR_TAG ] or length of word [  #AUTHOR_TAG ].', 'however, rule - based systems need a lot of human involvement to manually define these rules, and it is impossible to give all possible simplification rules.', ""as complex and simplified parallel corpora are available, especially, the'ordinary'english wikipedia ( ew ) in combination with the'simple'english wikipedia ( sew ), the paradigm shift of ls systems is from knowledge - based to data - driven simplification [  #AUTHOR_TAG ]."", ' #AUTHOR_TAG identified lexical simplifications from the edit history of sew.', 'they utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes.', ' #AUTHOR_TAG considered every pair of the distinct word in the ew and sew to be a possible simplification pair, and filtered part of them based on morphological variants and wordnet.', '']",0
"['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task.', 'the popular lexical simplification ( ls ) approaches are rule - based, which each rule contain a complex word and its simple synonyms  #TAUTHOR_TAG.', 'in order to construct rules, rule - based systems usually identified synonyms from wordnet for a predefined set of complex words, and selected the "" simplest "" from these synonyms based on the frequency of word [  #AUTHOR_TAG ] or length of word [  #AUTHOR_TAG ].', 'however, rule - based systems need a lot of human involvement to manually define these rules, and it is impossible to give all possible simplification rules.', ""as complex and simplified parallel corpora are available, especially, the'ordinary'english wikipedia ( ew ) in combination with the'simple'english wikipedia ( sew ), the paradigm shift of ls systems is from knowledge - based to data - driven simplification [  #AUTHOR_TAG ]."", ' #AUTHOR_TAG identified lexical simplifications from the edit history of sew.', 'they utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes.', ' #AUTHOR_TAG considered every pair of the distinct word in the ew and sew to be a possible simplification pair, and filtered part of them based on morphological variants and wordnet.', '']",0
"['is the simplest  #TAUTHOR_TAG.', 'we rank the candidate substitutions based on']","['is the simplest  #TAUTHOR_TAG.', 'we rank the candidate substitutions based on']","['is the simplest  #TAUTHOR_TAG.', 'we rank the candidate substitutions based on the following features.', '']","['substitution ranking of the lexical simplification pipeline is to decide which of the candidate substitutions that fit the context of a complex word is the simplest  #TAUTHOR_TAG.', 'we rank the candidate substitutions based on the following features.', 'each of the features captures one aspect of the suitability of the candidate word to replace the complex word.', 'bert prediction.', 'on this step of simplification candidate generation, we obtain the probability distribution of the vocabulary corresponding to the mask word p ( · | s, s \\ { w } ).', 'the higher the probability, the more relevant the candidate for the original sentence.', '']",0
"['on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', 'to the original sentence.']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['', 'the probability distribution of the vocabulary corresponding to the masked word. here, we give an example shown in figure 1 to illustrate', 'the generation process using bert. for complex words\'composed\'and\'verses\'in the sentence "" john composed these verses. "", the top three substitution', 'candidates of the two complex words generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', '']",5
"['6  #TAUTHOR_TAG.', '']","['owns a pretty good coverage of gold simplifications.', '( 2 ) benchls 6  #TAUTHOR_TAG.', '']","['was annotated by 50 turkers, this dataset owns a pretty good coverage of gold simplifications.', '( 2 ) benchls 6  #TAUTHOR_TAG.', 'it is composed, 2017b ].', '']","['.', 'we use three widely used lexical simplification datasets to do experiments.', '( 1 ) lexmturk 5 [  #AUTHOR_TAG ].', 'it is composed of 500 instances for english.', 'each instance contains one sentence from wikipedia, a target complex word and 50 substitutions annotated by 50 amazon mechanical "" turkers "".', 'since each complex word of each instance was annotated by 50 turkers, this dataset owns a pretty good coverage of gold simplifications.', '( 2 ) benchls 6  #TAUTHOR_TAG.', 'it is composed, 2017b ].', 'it is composed of 239 instances for english, which is a filtered version of benchls.', 'these instances of benchls are dropped : 1 ) the target complex word in the instance was not regarded as the complex word by a non - native english speaker, and 2 ) any candidate in the instance was regarded as a complex word by a non - native speaker.', 'notice that, because these datasets already offer target words regarded complex by human annotators, we do not address complex word identification task in our evaluations.', 'comparison systems.', 'we choose the following eight baselines to evaluation : devlin [  #AUTHOR_TAG ], biran [  #AUTHOR_TAG ], yamamoto [  #AUTHOR_TAG horn [  #AUTHOR_TAG ], glavas [ glavas andstajner, 2015 ], simpleppdb [ pavlick and callison -  #AUTHOR_TAG ], paetzold - ca  #TAUTHOR_TAG.', 'the substitution generation strategies of these baselines generate candidates from wordnet, ew and sew parallel corpus, merriam dictionary, ew and sew parallel corpus, word embeddings, context - aware word embeddings, combining the newsela parallel corpus and contextaware word embeddings']",5
"['6  #TAUTHOR_TAG.', '']","['owns a pretty good coverage of gold simplifications.', '( 2 ) benchls 6  #TAUTHOR_TAG.', '']","['was annotated by 50 turkers, this dataset owns a pretty good coverage of gold simplifications.', '( 2 ) benchls 6  #TAUTHOR_TAG.', 'it is composed, 2017b ].', '']","['.', 'we use three widely used lexical simplification datasets to do experiments.', '( 1 ) lexmturk 5 [  #AUTHOR_TAG ].', 'it is composed of 500 instances for english.', 'each instance contains one sentence from wikipedia, a target complex word and 50 substitutions annotated by 50 amazon mechanical "" turkers "".', 'since each complex word of each instance was annotated by 50 turkers, this dataset owns a pretty good coverage of gold simplifications.', '( 2 ) benchls 6  #TAUTHOR_TAG.', 'it is composed, 2017b ].', 'it is composed of 239 instances for english, which is a filtered version of benchls.', 'these instances of benchls are dropped : 1 ) the target complex word in the instance was not regarded as the complex word by a non - native english speaker, and 2 ) any candidate in the instance was regarded as a complex word by a non - native speaker.', 'notice that, because these datasets already offer target words regarded complex by human annotators, we do not address complex word identification task in our evaluations.', 'comparison systems.', 'we choose the following eight baselines to evaluation : devlin [  #AUTHOR_TAG ], biran [  #AUTHOR_TAG ], yamamoto [  #AUTHOR_TAG horn [  #AUTHOR_TAG ], glavas [ glavas andstajner, 2015 ], simpleppdb [ pavlick and callison -  #AUTHOR_TAG ], paetzold - ca  #TAUTHOR_TAG.', 'the substitution generation strategies of these baselines generate candidates from wordnet, ew and sew parallel corpus, merriam dictionary, ew and sew parallel corpus, word embeddings, context - aware word embeddings, combining the newsela parallel corpus and contextaware word embeddings']",5
"['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', 'precision :']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['following three widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']",5
"['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', 'precision :']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['following three widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']",5
"['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', 'precision :']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['following three widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']",5
"['following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']","['following two well - known metrics used by these work [  #TAUTHOR_TAG.', 'precision :']","['this section, we evaluate the performance of various complete ls systems.', 'we adopt the following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']","['this section, we evaluate the performance of various complete ls systems.', 'we adopt the following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']",5
"['following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']","['following two well - known metrics used by these work [  #TAUTHOR_TAG.', 'precision :']","['this section, we evaluate the performance of various complete ls systems.', 'we adopt the following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']","['this section, we evaluate the performance of various complete ls systems.', 'we adopt the following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']",5
"['on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', 'to the original sentence.']","['generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only']","['', 'the probability distribution of the vocabulary corresponding to the masked word. here, we give an example shown in figure 1 to illustrate', 'the generation process using bert. for complex words\'composed\'and\'verses\'in the sentence "" john composed these verses. "", the top three substitution', 'candidates of the two complex words generated by the ls systems based on word embeddings [ glavas andstajner, 2015 ;  #TAUTHOR_TAG are only related with the complex words itself without without paying attention', '']",4
"['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task.', 'the popular lexical simplification ( ls ) approaches are rule - based, which each rule contain a complex word and its simple synonyms  #TAUTHOR_TAG.', 'in order to construct rules, rule - based systems usually identified synonyms from wordnet for a predefined set of complex words, and selected the "" simplest "" from these synonyms based on the frequency of word [  #AUTHOR_TAG ] or length of word [  #AUTHOR_TAG ].', 'however, rule - based systems need a lot of human involvement to manually define these rules, and it is impossible to give all possible simplification rules.', ""as complex and simplified parallel corpora are available, especially, the'ordinary'english wikipedia ( ew ) in combination with the'simple'english wikipedia ( sew ), the paradigm shift of ls systems is from knowledge - based to data - driven simplification [  #AUTHOR_TAG ]."", ' #AUTHOR_TAG identified lexical simplifications from the edit history of sew.', 'they utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes.', ' #AUTHOR_TAG considered every pair of the distinct word in the ew and sew to be a possible simplification pair, and filtered part of them based on morphological variants and wordnet.', '']",4
"['simplification algorithm bert - ls is shown in algorithm 1.', 'in this paper, we are not focused on identifying complex words  #TAUTHOR_TAG, which is a separate task.', 'for']","['simplification algorithm bert - ls is shown in algorithm 1.', 'in this paper, we are not focused on identifying complex words  #TAUTHOR_TAG, which is a separate task.', 'for']","['overall simplification algorithm bert - ls is shown in algorithm 1.', 'in this paper, we are not focused on identifying complex words  #TAUTHOR_TAG, which is a separate task.', 'for each complex word,']","['overall simplification algorithm bert - ls is shown in algorithm 1.', 'in this paper, we are not focused on identifying complex words  #TAUTHOR_TAG, which is a separate task.', 'for each complex word, we first get simplification candidates using bert after preprocessing the original sequence ( lines 1 - 4 ).', 'afterward, bert - ls computes various rankings for each of the simplification candidates using each of the features, and then scores each candidate by averaging all its rankings ( lines 5 - 13 ).', 'we choose the top candidate with the highest average rank over all features as the simplification replacement ( line 15 )']",4
"['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', 'precision :']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['following three widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']",4
"['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', 'precision :']","['widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']","['following three widely used metrics are used for evaluation  #TAUTHOR_TAG.', '']",4
"['following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']","['following two well - known metrics used by these work [  #TAUTHOR_TAG.', 'precision :']","['this section, we evaluate the performance of various complete ls systems.', 'we adopt the following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']","['this section, we evaluate the performance of various complete ls systems.', 'we adopt the following two well - known metrics used by these work [  #TAUTHOR_TAG.', '']",4
"['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping']","['simplification ( ls ) contains identifying complex words and finding the best candidate substitution for these complex words [  #TAUTHOR_TAG.', 'the best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task.', 'the popular lexical simplification ( ls ) approaches are rule - based, which each rule contain a complex word and its simple synonyms  #TAUTHOR_TAG.', 'in order to construct rules, rule - based systems usually identified synonyms from wordnet for a predefined set of complex words, and selected the "" simplest "" from these synonyms based on the frequency of word [  #AUTHOR_TAG ] or length of word [  #AUTHOR_TAG ].', 'however, rule - based systems need a lot of human involvement to manually define these rules, and it is impossible to give all possible simplification rules.', ""as complex and simplified parallel corpora are available, especially, the'ordinary'english wikipedia ( ew ) in combination with the'simple'english wikipedia ( sew ), the paradigm shift of ls systems is from knowledge - based to data - driven simplification [  #AUTHOR_TAG ]."", ' #AUTHOR_TAG identified lexical simplifications from the edit history of sew.', 'they utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes.', ' #AUTHOR_TAG considered every pair of the distinct word in the ew and sew to be a possible simplification pair, and filtered part of them based on morphological variants and wordnet.', '']",1
['el  #TAUTHOR_TAG'],['el  #TAUTHOR_TAG'],"['el  #TAUTHOR_TAG. however, as', '']",[' #TAUTHOR_TAG'],0
['el  #TAUTHOR_TAG'],['el  #TAUTHOR_TAG'],"['el  #TAUTHOR_TAG. however, as', '']",[' #TAUTHOR_TAG'],0
['el  #TAUTHOR_TAG'],['el  #TAUTHOR_TAG'],"['el  #TAUTHOR_TAG. however, as', '']",[' #TAUTHOR_TAG'],0
['sum function  #TAUTHOR_TAG'],['sum function  #TAUTHOR_TAG'],"['the hidden vector sequence, that is then transformed by a non - linear function g and pooled by the sum function  #TAUTHOR_TAG.', 'following the previous work on cnn  #AUTHOR_TAG a ; 2015b ) ), we utilize']","['x be some context word sequence of the entity mentions or target candidates ( i. e, x ∈ { s i, c i,', '.', 'in order to obtain the distributed representation for x, we first transform each word x i ∈ x into a real - valued, h - dimensional vector w i using the word embedding table e  #AUTHOR_TAG :', '.', 'this essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed - length sequence of vectors w = ( w 1, w 2,..., w n ) of length n.', 'in the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non - linear function g and pooled by the sum function  #TAUTHOR_TAG.', 'following the previous work on cnn  #AUTHOR_TAG a ; 2015b ) ), we utilize the set l of multiple window sizes to parameterize the convolution operation.', 'each window size l ∈ l corresponds to a convolution matrix m l ∈ r v×lh of dimensionality v. eventually, the concatenation vectorx of the resulting vectors for each window size in l would be used as the distributed representation for', 'where is the concatenation operation over the window set l and w i : ( i + l−1 ) is the concatenation vector of the given word vectors.', 'for convenience, lets i, c i,', 'i and b * i obtained by the convolution procedure above, respectively.', 'note that we apply the same set of convolution parameters for each type of text granularity in the source document d as well as in the target entity side.', 'the vector representations of the context would then be fed into the next components to compute the features for el']",0
"['local ( m i, p ij ) from  #TAUTHOR_TAG, the']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el']","['employ the local similarities φ local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el.', 'in particular :', 'in this formula, w sparse and w cn n are the weights for the feature vectors f sparse and w cn n respectively.', 'f sparse ( m i, p ij ) is the sparse feature vector obtained from  #AUTHOR_TAG.', 'this vector captures various linguistic properties and statistics that have been discovered in the previous studies for el.', 'the representative features include the anchor text counts from wikipedia, the string match indications with the title of the wikipedia candidate pages, or the information about the shape of the queries for candidate generations  #TAUTHOR_TAG.', ', on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij.', 'in particular :', 'the intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for el  #TAUTHOR_TAG']",0
"['local ( m i, p ij ) from  #TAUTHOR_TAG, the']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el']","['employ the local similarities φ local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el.', 'in particular :', 'in this formula, w sparse and w cn n are the weights for the feature vectors f sparse and w cn n respectively.', 'f sparse ( m i, p ij ) is the sparse feature vector obtained from  #AUTHOR_TAG.', 'this vector captures various linguistic properties and statistics that have been discovered in the previous studies for el.', 'the representative features include the anchor text counts from wikipedia, the string match indications with the title of the wikipedia candidate pages, or the information about the shape of the queries for candidate generations  #TAUTHOR_TAG.', ', on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij.', 'in particular :', 'the intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for el  #TAUTHOR_TAG']",0
"['local ( m i, p ij ) from  #TAUTHOR_TAG, the']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el']","['employ the local similarities φ local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el.', 'in particular :', 'in this formula, w sparse and w cn n are the weights for the feature vectors f sparse and w cn n respectively.', 'f sparse ( m i, p ij ) is the sparse feature vector obtained from  #AUTHOR_TAG.', 'this vector captures various linguistic properties and statistics that have been discovered in the previous studies for el.', 'the representative features include the anchor text counts from wikipedia, the string match indications with the title of the wikipedia candidate pages, or the information about the shape of the queries for candidate generations  #TAUTHOR_TAG.', ', on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij.', 'in particular :', 'the intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for el  #TAUTHOR_TAG']",0
[' #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data'],[' #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data'],"['##ings on the whole english wikipedia dump using the word2vec toolkit  #AUTHOR_TAG.', 'the training parameters are set to the default values in this toolkit.', 'the dimensionality of the word embeddings is 300.', 'note that every parameter and resource in this work is either taken from the previous work  #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data']","['all the experiments below, in the cnn models to learn the distributed representations for the inputs, we use window sizes in the set l = { 2, 3, 4, 5 } for the convolution operation with the dimensionality v = 200 for each window size 5.', 'the non - linear function for transformation is g = tanh.', 'we employ the english wikipedia dump from june 2016 as our reference knowledge base.', 'regarding the input contexts for the entity mentions and the target candidates, we utilize the window size of 10 for the immediate context c i, and only extract the first 100 words in the documents for d i and b ij.', 'finally, we pre - train the word embedings on the whole english wikipedia dump using the word2vec toolkit  #AUTHOR_TAG.', 'the training parameters are set to the default values in this toolkit.', 'the dimensionality of the word embeddings is 300.', 'note that every parameter and resource in this work is either taken from the previous work  #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data']",0
"['', 'these systems include the neural network model in  #TAUTHOR_TAG,']","['proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG,']","['section compares the proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG, the joint model']","['section compares the proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG, the joint model for entity analysis in  #AUTHOR_TAG and the aida - light system with two - stage mapping in  #AUTHOR_TAG b ) 6.', 'table 2 shows the performance of the systems on the test sets with the reference knowledge base of the june 2016 wikipedia dump.', 'we also include the performance of the systems on the december 2014 wikipedia dump that was used and provided by  #TAUTHOR_TAG for further and compatible comparison']",0
"['', 'these systems include the neural network model in  #TAUTHOR_TAG,']","['proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG,']","['section compares the proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG, the joint model']","['section compares the proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG, the joint model for entity analysis in  #AUTHOR_TAG and the aida - light system with two - stage mapping in  #AUTHOR_TAG b ) 6.', 'table 2 shows the performance of the systems on the test sets with the reference knowledge base of the june 2016 wikipedia dump.', 'we also include the performance of the systems on the december 2014 wikipedia dump that was used and provided by  #TAUTHOR_TAG for further and compatible comparison']",0
['local approach  #TAUTHOR_TAG'],['local approach  #TAUTHOR_TAG'],['to domain shifts than the local approach  #TAUTHOR_TAG'],"['', 'these invariants serve as the connectors between different domains and help to transfer the knowledge from one domain to the others.', 'for el, we hypothesize that the global coherence is an effective domain - independent feature that would help to improve the crossdomain performance of the models.', 'the intuition is that the entities mentioned in a document of any domains should be related to each other.', 'eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach  #TAUTHOR_TAG']",0
"['with the neural network el model in  #TAUTHOR_TAG,']","['3 compares global - rnn with the neural network el model in  #TAUTHOR_TAG,']","['with the neural network el model in  #TAUTHOR_TAG,']","['use the ace dataset to evaluate the cross - domain performance of the models.', 'ace involves documents in 6 different domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and webblogs ( wl ).', 'following the common practice of domain adaptation research on this dataset  #AUTHOR_TAG c ;  #AUTHOR_TAG, we use news ( the union of bn and nw ) as the source domain and bc, cts, wl, un as four different target domains.', 'we take half of bc as the development set and use the remaining data for testing.', 'we note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles  #AUTHOR_TAG.', 'table 3 compares global - rnn with the neural network el model in  #TAUTHOR_TAG, the best reported model on the ace dataset in the literature 8.', 'in this table, the models are trained on the source domain news, and evaluated on news itself ( in - domain performance ) ( via 5 - fold cross validation ) as well as on the 4 target domains bc, cts, wl, un ( out - of - domain performance the first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in - domain performance.', 'in particular, the performance gap between the in - domain performance and the the worst out - of - domain performance ( on the domain wl ) is up to 10 %, thus indicating the mismatches between the source and the target domains for el.', 'second and most importantly, global - rnn is consistently better than the model with only local features in  #TAUTHOR_TAG over all the target domains ( although it is less pronounced in the cts domain ).', 'this demonstrates the cross - domain robustness of the proposed model and confirms our hypothesis about the domain - independence of the global coherence features for el']",0
"['with the neural network el model in  #TAUTHOR_TAG,']","['3 compares global - rnn with the neural network el model in  #TAUTHOR_TAG,']","['with the neural network el model in  #TAUTHOR_TAG,']","['use the ace dataset to evaluate the cross - domain performance of the models.', 'ace involves documents in 6 different domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and webblogs ( wl ).', 'following the common practice of domain adaptation research on this dataset  #AUTHOR_TAG c ;  #AUTHOR_TAG, we use news ( the union of bn and nw ) as the source domain and bc, cts, wl, un as four different target domains.', 'we take half of bc as the development set and use the remaining data for testing.', 'we note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles  #AUTHOR_TAG.', 'table 3 compares global - rnn with the neural network el model in  #TAUTHOR_TAG, the best reported model on the ace dataset in the literature 8.', 'in this table, the models are trained on the source domain news, and evaluated on news itself ( in - domain performance ) ( via 5 - fold cross validation ) as well as on the 4 target domains bc, cts, wl, un ( out - of - domain performance the first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in - domain performance.', 'in particular, the performance gap between the in - domain performance and the the worst out - of - domain performance ( on the domain wl ) is up to 10 %, thus indicating the mismatches between the source and the target domains for el.', 'second and most importantly, global - rnn is consistently better than the model with only local features in  #TAUTHOR_TAG over all the target domains ( although it is less pronounced in the cts domain ).', 'this demonstrates the cross - domain robustness of the proposed model and confirms our hypothesis about the domain - independence of the global coherence features for el']",0
"[' #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse']","[' #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse']","[' #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse features to']","['linking or disambiguation has been studied extensively in nlp research, falling broadly into two major approaches : local and global disambiguation.', 'both approaches share the goal of measuring the similarities between the entity mentions and the target candidates in the reference kb.', 'the local paradigm focuses on the internal structures of each separate mention - entity pair, covering the name string comparisons between the surfaces of the entity mentions and target candidates, entity popularity or entity type and so on  #AUTHOR_TAG.', 'in contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence.', 'various techniques have been exploited for capturing such semantic consistency, including wikipedia category agreement  #AUTHOR_TAG, wikipedia link - based measures  #AUTHOR_TAG, point - wise mutual information measures, integer linear programming  #AUTHOR_TAG, pagerank  #AUTHOR_TAG, stacked generalization  #AUTHOR_TAG a ), to name a few.', 'the entity linking techniques and systems have been actively evaluated at the nist - organized text analysis conference  #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse features to improve the performance.', 'however, none of these work utilize recurrent neural networks to capture the coherence features as we do in this work']",0
['el  #TAUTHOR_TAG'],['el  #TAUTHOR_TAG'],"['el  #TAUTHOR_TAG. however, as', '']",[' #TAUTHOR_TAG'],1
['local approach  #TAUTHOR_TAG'],['local approach  #TAUTHOR_TAG'],['to domain shifts than the local approach  #TAUTHOR_TAG'],"['', 'these invariants serve as the connectors between different domains and help to transfer the knowledge from one domain to the others.', 'for el, we hypothesize that the global coherence is an effective domain - independent feature that would help to improve the crossdomain performance of the models.', 'the intuition is that the entities mentioned in a document of any domains should be related to each other.', 'eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach  #TAUTHOR_TAG']",1
['el  #TAUTHOR_TAG'],['el  #TAUTHOR_TAG'],"['el  #TAUTHOR_TAG. however, as', '']",[' #TAUTHOR_TAG'],3
[' #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data'],[' #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data'],"['##ings on the whole english wikipedia dump using the word2vec toolkit  #AUTHOR_TAG.', 'the training parameters are set to the default values in this toolkit.', 'the dimensionality of the word embeddings is 300.', 'note that every parameter and resource in this work is either taken from the previous work  #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data']","['all the experiments below, in the cnn models to learn the distributed representations for the inputs, we use window sizes in the set l = { 2, 3, 4, 5 } for the convolution operation with the dimensionality v = 200 for each window size 5.', 'the non - linear function for transformation is g = tanh.', 'we employ the english wikipedia dump from june 2016 as our reference knowledge base.', 'regarding the input contexts for the entity mentions and the target candidates, we utilize the window size of 10 for the immediate context c i, and only extract the first 100 words in the documents for d i and b ij.', 'finally, we pre - train the word embedings on the whole english wikipedia dump using the word2vec toolkit  #AUTHOR_TAG.', 'the training parameters are set to the default values in this toolkit.', 'the dimensionality of the word embeddings is 300.', 'note that every parameter and resource in this work is either taken from the previous work  #AUTHOR_TAG b ;  #TAUTHOR_TAG or selected by the development data']",3
"['', 'these systems include the neural network model in  #TAUTHOR_TAG,']","['proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG,']","['section compares the proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG, the joint model']","['section compares the proposed system ( called global - rnn ) with the state - of - the - art models on our four datasets.', 'these systems include the neural network model in  #TAUTHOR_TAG, the joint model for entity analysis in  #AUTHOR_TAG and the aida - light system with two - stage mapping in  #AUTHOR_TAG b ) 6.', 'table 2 shows the performance of the systems on the test sets with the reference knowledge base of the june 2016 wikipedia dump.', 'we also include the performance of the systems on the december 2014 wikipedia dump that was used and provided by  #TAUTHOR_TAG for further and compatible comparison']",3
['sum function  #TAUTHOR_TAG'],['sum function  #TAUTHOR_TAG'],"['the hidden vector sequence, that is then transformed by a non - linear function g and pooled by the sum function  #TAUTHOR_TAG.', 'following the previous work on cnn  #AUTHOR_TAG a ; 2015b ) ), we utilize']","['x be some context word sequence of the entity mentions or target candidates ( i. e, x ∈ { s i, c i,', '.', 'in order to obtain the distributed representation for x, we first transform each word x i ∈ x into a real - valued, h - dimensional vector w i using the word embedding table e  #AUTHOR_TAG :', '.', 'this essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed - length sequence of vectors w = ( w 1, w 2,..., w n ) of length n.', 'in the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non - linear function g and pooled by the sum function  #TAUTHOR_TAG.', 'following the previous work on cnn  #AUTHOR_TAG a ; 2015b ) ), we utilize the set l of multiple window sizes to parameterize the convolution operation.', 'each window size l ∈ l corresponds to a convolution matrix m l ∈ r v×lh of dimensionality v. eventually, the concatenation vectorx of the resulting vectors for each window size in l would be used as the distributed representation for', 'where is the concatenation operation over the window set l and w i : ( i + l−1 ) is the concatenation vector of the given word vectors.', 'for convenience, lets i, c i,', 'i and b * i obtained by the convolution procedure above, respectively.', 'note that we apply the same set of convolution parameters for each type of text granularity in the source document d as well as in the target entity side.', 'the vector representations of the context would then be fed into the next components to compute the features for el']",5
"['local ( m i, p ij ) from  #TAUTHOR_TAG, the']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural']","['local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el']","['employ the local similarities φ local ( m i, p ij ) from  #TAUTHOR_TAG, the state - of - the - art neural network model for el.', 'in particular :', 'in this formula, w sparse and w cn n are the weights for the feature vectors f sparse and w cn n respectively.', 'f sparse ( m i, p ij ) is the sparse feature vector obtained from  #AUTHOR_TAG.', 'this vector captures various linguistic properties and statistics that have been discovered in the previous studies for el.', 'the representative features include the anchor text counts from wikipedia, the string match indications with the title of the wikipedia candidate pages, or the information about the shape of the queries for candidate generations  #TAUTHOR_TAG.', ', on the other hand, involves the cosine similarities between the representation vectors at multiple granularities of m i and p ij.', 'in particular :', 'the intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for el  #TAUTHOR_TAG']",5
"[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from']","[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from']","[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from']","[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from the 2005 evaluation of nist.', 'it is also used in  #AUTHOR_TAG and  #AUTHOR_TAG.', 'ii ) conll - yago  #AUTHOR_TAG : this corpus is originally from the conll 2003 shared task of named entity recognition for english.', 'iii ) wp  #AUTHOR_TAG : this dataset consists of short snippets from wikipedia.', 'iv ) wiki : this dataset contains 10, 000 randomly sampled wikipedia articles.', 'the task is to disambiguate the links in each article 4.', 'for all the datasets, we use the standard data splits ( for training data, test data and development data ) as the previous works for comparable comparison  #TAUTHOR_TAG']",5
"[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from']","[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from']","[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from']","[' #TAUTHOR_TAG, we evaluate the models on 4 different entity linking datasets : i ) ace  #AUTHOR_TAG : this corpus is from the 2005 evaluation of nist.', 'it is also used in  #AUTHOR_TAG and  #AUTHOR_TAG.', 'ii ) conll - yago  #AUTHOR_TAG : this corpus is originally from the conll 2003 shared task of named entity recognition for english.', 'iii ) wp  #AUTHOR_TAG : this dataset consists of short snippets from wikipedia.', 'iv ) wiki : this dataset contains 10, 000 randomly sampled wikipedia articles.', 'the task is to disambiguate the links in each article 4.', 'for all the datasets, we use the standard data splits ( for training data, test data and development data ) as the previous works for comparable comparison  #TAUTHOR_TAG']",5
"['with the neural network el model in  #TAUTHOR_TAG,']","['3 compares global - rnn with the neural network el model in  #TAUTHOR_TAG,']","['with the neural network el model in  #TAUTHOR_TAG,']","['use the ace dataset to evaluate the cross - domain performance of the models.', 'ace involves documents in 6 different domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and webblogs ( wl ).', 'following the common practice of domain adaptation research on this dataset  #AUTHOR_TAG c ;  #AUTHOR_TAG, we use news ( the union of bn and nw ) as the source domain and bc, cts, wl, un as four different target domains.', 'we take half of bc as the development set and use the remaining data for testing.', 'we note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles  #AUTHOR_TAG.', 'table 3 compares global - rnn with the neural network el model in  #TAUTHOR_TAG, the best reported model on the ace dataset in the literature 8.', 'in this table, the models are trained on the source domain news, and evaluated on news itself ( in - domain performance ) ( via 5 - fold cross validation ) as well as on the 4 target domains bc, cts, wl, un ( out - of - domain performance the first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in - domain performance.', 'in particular, the performance gap between the in - domain performance and the the worst out - of - domain performance ( on the domain wl ) is up to 10 %, thus indicating the mismatches between the source and the target domains for el.', 'second and most importantly, global - rnn is consistently better than the model with only local features in  #TAUTHOR_TAG over all the target domains ( although it is less pronounced in the cts domain ).', 'this demonstrates the cross - domain robustness of the proposed model and confirms our hypothesis about the domain - independence of the global coherence features for el']",4
"[' #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse']","[' #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse']","[' #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse features to']","['linking or disambiguation has been studied extensively in nlp research, falling broadly into two major approaches : local and global disambiguation.', 'both approaches share the goal of measuring the similarities between the entity mentions and the target candidates in the reference kb.', 'the local paradigm focuses on the internal structures of each separate mention - entity pair, covering the name string comparisons between the surfaces of the entity mentions and target candidates, entity popularity or entity type and so on  #AUTHOR_TAG.', 'in contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence.', 'various techniques have been exploited for capturing such semantic consistency, including wikipedia category agreement  #AUTHOR_TAG, wikipedia link - based measures  #AUTHOR_TAG, point - wise mutual information measures, integer linear programming  #AUTHOR_TAG, pagerank  #AUTHOR_TAG, stacked generalization  #AUTHOR_TAG a ), to name a few.', 'the entity linking techniques and systems have been actively evaluated at the nist - organized text analysis conference  #AUTHOR_TAG.', 'neural networks are applied to entity linking very recently.', ' #AUTHOR_TAG b ) learn enttiy representation via stacked denoising auto - encoders.', ' #AUTHOR_TAG employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while  #TAUTHOR_TAG combine cnn - based representations with sparse features to improve the performance.', 'however, none of these work utilize recurrent neural networks to capture the coherence features as we do in this work']",4
"[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles']","['.', 'we evaluate our model on the publicly available data set provided by  #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles taken from the new york times.', 'each topic is represented by ten terms with the highest probability.', 'they are also associated with 20 candidate image labels and their human ratings between 0 ( lowest ) and 3 ( highest ) denoting the appropriateness of these images for the topic.', 'that results into a total of 6k images and their associated textual metadata which are considered as captions.', 'the task is to choose the image with the highest rating from the set of the 20 candidates for a given topic.', 'negative examples sampling.', 'the 20 candidate image labels per topic are collected by  #TAUTHOR_TAG using an information retrieval engine ( google ).', '']",5
"[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles']","['.', 'we evaluate our model on the publicly available data set provided by  #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles taken from the new york times.', 'each topic is represented by ten terms with the highest probability.', 'they are also associated with 20 candidate image labels and their human ratings between 0 ( lowest ) and 3 ( highest ) denoting the appropriateness of these images for the topic.', 'that results into a total of 6k images and their associated textual metadata which are considered as captions.', 'the task is to choose the image with the highest rating from the set of the 20 candidates for a given topic.', 'negative examples sampling.', 'the 20 candidate image labels per topic are collected by  #TAUTHOR_TAG using an information retrieval engine ( google ).', '']",5
['uses personalized pagerank  #TAUTHOR_TAG'],['uses personalized pagerank  #TAUTHOR_TAG'],['uses personalized pagerank  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
['uses personalized pagerank  #TAUTHOR_TAG'],['uses personalized pagerank  #TAUTHOR_TAG'],['uses personalized pagerank  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],6
['5  #TAUTHOR_TAG 2. 24 - - - the'],['ndcg - 5  #TAUTHOR_TAG 2. 24 - - - the'],['1 ndcg - 3 ndcg - 5  #TAUTHOR_TAG 2. 24 - - -'],"['- 1 aver. rating ndcg - 1 ndcg - 3 ndcg - 5  #TAUTHOR_TAG 2. 24 - - - the dnn ( topic + caption ) model that uses only textual information, obtains a top - 1 average performance of 1. 94.', 'incorporating visual information ( vgg ) improves it to 2. 12 ( dnn ( topic + caption + vgg ) ).', 'an interesting finding is that using only the visual information ( dnn ( topic + vgg ) ) achieves better results ( 2. 04 ) compared to using only text.', 'this demonstrates that images contain less noisy information compared to their captions for this particular task.', 'the dnn models also provide a better ranking for the image candidates.', 'the ndcg scores for the majority of the dnn methods are higher than the other methods.', 'dnn ( topic + caption + vgg ) consistently obtains the best ndcg scores, 0. 79, 0. 80 and 0. 81 respectively.', 'figure 2 shows two topics and the top - 3 images selected by the dnn ( topic + caption + vgg ) model from the candidate set.', 'the labels selected for the topic # 288 are all very relevant to a surgical operation.', 'on the other hand, the images selected for topic # 99 are irrelevant to wedding photography.', 'the main problem is that the set of candidate images do not contain any relevant ones.', 'however, in this situation our model can identify other images that might be good labels which do not belong in the original set of candidates']",4
['method of  #TAUTHOR_TAG and a relevant method originally utilized'],['method of  #TAUTHOR_TAG and a relevant method originally utilized'],['proposed approach significantly outperforms the state - of - the - art method of  #TAUTHOR_TAG and a relevant method originally utilized'],"['presented a deep neural network that jointly models textual and visual information for the task of topic labeling with images.', 'our model is generic and works for any unseen pair of topic and image.', 'our evaluation results show that our proposed approach significantly outperforms the state - of - the - art method of  #TAUTHOR_TAG and a relevant method originally utilized for image annotation proposed by  #AUTHOR_TAG']",4
"['part of speech.', 'this is an extension and further validation of the results achieved by  #TAUTHOR_TAG.', ""sudoku's three submissions are incremental in the use of the two aforementioned constraints."", 'run1 has no constraints']","['part of speech.', 'this is an extension and further validation of the results achieved by  #TAUTHOR_TAG.', ""sudoku's three submissions are incremental in the use of the two aforementioned constraints."", 'run1 has no constraints']","['part of speech.', 'this is an extension and further validation of the results achieved by  #TAUTHOR_TAG.', ""sudoku's three submissions are incremental in the use of the two aforementioned constraints."", 'run1 has no constraints']","[""##oku's submissions to semeval task 13 treats word sense disambiguation and entity linking as a deterministic problem that exploits two key attributes of open - class words as constraints - their degree of polysemy and their part of speech."", 'this is an extension and further validation of the results achieved by  #TAUTHOR_TAG.', ""sudoku's three submissions are incremental in the use of the two aforementioned constraints."", 'run1 has no constraints and disambiguates all lemmas in one pass.', 'run2 disambiguates lemmas at increasing degrees of polysemy, leaving the most polysemous until last.', 'run3 is identical to run2, with the additional constraint of disambiguating all named entities and nouns first before other types of open - class words ( verbs, adjectives, and adverbs ).', 'over all - domains, for english run2 and run3 were placed second and third.', 'for spanish run2, run3, and run1 were placed first, second, and third respectively.', 'for italian run1 was placed first with run2 and run3 placed second equal']",6
['this paper  #TAUTHOR_TAG'],['this paper  #TAUTHOR_TAG'],['this paper  #TAUTHOR_TAG'],"['a decade ago,  #AUTHOR_TAG suggested the promising potential for wsd that could exploit the interdependencies between senses in an interactive manner.', 'in other words, this would be a wsd system which allows the disambiguation of word a to directly influence the consecutive disambiguation of word b. this is analogous to treating wsd as a deterministic problem, much like the sudoku puzzle in which the final solution is reached by adhering to a set of pre - determined constraints.', 'conventional approaches to wsd often overlook the potential to exploit sense interdependencies, and simply disambiguate all senses in one pass based on a context window ( e. g. a sentence or document ).', 'for this task the author proposes an iterative approach which makes several passes based on a set of constraints.', 'for a more formal distinction between the conventional and iterative approach to wsd, please refer to this paper  #TAUTHOR_TAG table 1 : parts of speech disambiguated ( as percentages ) for each semeval task ( denoted by its year ).', 'in - degree centrality as implemented in  #TAUTHOR_TAG observes f - score improvement ( f + ∆f ) by applying the iterative approach.', 'the author found in the investigations of his thesis  #AUTHOR_TAG that the iterative approach performed best on the semeval 2013 multilingual wsd task  #AUTHOR_TAG, as opposed to earlier tasks such as senseval 2004 english all words wsd task  #AUTHOR_TAG and the semeval 2010 all words wsd task on a specific domain  #AUTHOR_TAG.', 'while these earlier tasks also experienced improvement, f - scores remained lower overall.', 'table 1 depicted above are distributions for each domain and language, detailing the probability ( y - axis ) of specific parts of speech at increasing degrees of polysemy ( x - axis ).', 'these distributions were produced from the gold keys ( or synsets ) of the test documents by querying babelnet for the polysemy of each word.', 'each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored.', 'lastly the difference in f - score between the conventional run1 and the iterative run2 and run3 is listed beside each distribution.', 'firstly wsd tasks before 2013 generally relied on only a lexicon, such as wordnet  #AUTHOR_TAG or an alternative equivalent, whereas semeval 2013 task 12 wsd and this task  #AUTHOR_TAG included entity linking ( el ) using the encyclopaedia wikipedia via babelnet  #AUTHOR_TAG.', 'secondly, as shown by  #TAUTHOR_TAG with a simple linear regression, the iterative approach increases wsd performance for documents that have a higher degree of document monosemy - the percentage']",7
['this paper  #TAUTHOR_TAG'],['this paper  #TAUTHOR_TAG'],['this paper  #TAUTHOR_TAG'],"['a decade ago,  #AUTHOR_TAG suggested the promising potential for wsd that could exploit the interdependencies between senses in an interactive manner.', 'in other words, this would be a wsd system which allows the disambiguation of word a to directly influence the consecutive disambiguation of word b. this is analogous to treating wsd as a deterministic problem, much like the sudoku puzzle in which the final solution is reached by adhering to a set of pre - determined constraints.', 'conventional approaches to wsd often overlook the potential to exploit sense interdependencies, and simply disambiguate all senses in one pass based on a context window ( e. g. a sentence or document ).', 'for this task the author proposes an iterative approach which makes several passes based on a set of constraints.', 'for a more formal distinction between the conventional and iterative approach to wsd, please refer to this paper  #TAUTHOR_TAG table 1 : parts of speech disambiguated ( as percentages ) for each semeval task ( denoted by its year ).', 'in - degree centrality as implemented in  #TAUTHOR_TAG observes f - score improvement ( f + ∆f ) by applying the iterative approach.', 'the author found in the investigations of his thesis  #AUTHOR_TAG that the iterative approach performed best on the semeval 2013 multilingual wsd task  #AUTHOR_TAG, as opposed to earlier tasks such as senseval 2004 english all words wsd task  #AUTHOR_TAG and the semeval 2010 all words wsd task on a specific domain  #AUTHOR_TAG.', 'while these earlier tasks also experienced improvement, f - scores remained lower overall.', 'table 1 depicted above are distributions for each domain and language, detailing the probability ( y - axis ) of specific parts of speech at increasing degrees of polysemy ( x - axis ).', 'these distributions were produced from the gold keys ( or synsets ) of the test documents by querying babelnet for the polysemy of each word.', 'each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored.', 'lastly the difference in f - score between the conventional run1 and the iterative run2 and run3 is listed beside each distribution.', 'firstly wsd tasks before 2013 generally relied on only a lexicon, such as wordnet  #AUTHOR_TAG or an alternative equivalent, whereas semeval 2013 task 12 wsd and this task  #AUTHOR_TAG included entity linking ( el ) using the encyclopaedia wikipedia via babelnet  #AUTHOR_TAG.', 'secondly, as shown by  #TAUTHOR_TAG with a simple linear regression, the iterative approach increases wsd performance for documents that have a higher degree of document monosemy - the percentage']",0
['this paper  #TAUTHOR_TAG'],['this paper  #TAUTHOR_TAG'],['this paper  #TAUTHOR_TAG'],"['a decade ago,  #AUTHOR_TAG suggested the promising potential for wsd that could exploit the interdependencies between senses in an interactive manner.', 'in other words, this would be a wsd system which allows the disambiguation of word a to directly influence the consecutive disambiguation of word b. this is analogous to treating wsd as a deterministic problem, much like the sudoku puzzle in which the final solution is reached by adhering to a set of pre - determined constraints.', 'conventional approaches to wsd often overlook the potential to exploit sense interdependencies, and simply disambiguate all senses in one pass based on a context window ( e. g. a sentence or document ).', 'for this task the author proposes an iterative approach which makes several passes based on a set of constraints.', 'for a more formal distinction between the conventional and iterative approach to wsd, please refer to this paper  #TAUTHOR_TAG table 1 : parts of speech disambiguated ( as percentages ) for each semeval task ( denoted by its year ).', 'in - degree centrality as implemented in  #TAUTHOR_TAG observes f - score improvement ( f + ∆f ) by applying the iterative approach.', 'the author found in the investigations of his thesis  #AUTHOR_TAG that the iterative approach performed best on the semeval 2013 multilingual wsd task  #AUTHOR_TAG, as opposed to earlier tasks such as senseval 2004 english all words wsd task  #AUTHOR_TAG and the semeval 2010 all words wsd task on a specific domain  #AUTHOR_TAG.', 'while these earlier tasks also experienced improvement, f - scores remained lower overall.', 'table 1 depicted above are distributions for each domain and language, detailing the probability ( y - axis ) of specific parts of speech at increasing degrees of polysemy ( x - axis ).', 'these distributions were produced from the gold keys ( or synsets ) of the test documents by querying babelnet for the polysemy of each word.', 'each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored.', 'lastly the difference in f - score between the conventional run1 and the iterative run2 and run3 is listed beside each distribution.', 'firstly wsd tasks before 2013 generally relied on only a lexicon, such as wordnet  #AUTHOR_TAG or an alternative equivalent, whereas semeval 2013 task 12 wsd and this task  #AUTHOR_TAG included entity linking ( el ) using the encyclopaedia wikipedia via babelnet  #AUTHOR_TAG.', 'secondly, as shown by  #TAUTHOR_TAG with a simple linear regression, the iterative approach increases wsd performance for documents that have a higher degree of document monosemy - the percentage']",0
"['##no constraints are applied.', 'formalised in  #TAUTHOR_TAG, this run can act as a baseline to gauge any improvement for run2']","['conventional approachno constraints are applied.', 'formalised in  #TAUTHOR_TAG, this run can act as a baseline to gauge any improvement for run2']","['- 1 ) is the conventional approachno constraints are applied.', 'formalised in  #TAUTHOR_TAG, this run can act as a baseline to gauge any improvement for run2']","['##1 ( sudoku - 1 ) is the conventional approachno constraints are applied.', 'formalised in  #TAUTHOR_TAG, this run can act as a baseline to gauge any improvement for run2 and run3 that apply the iterative approach.', 'run2 ( sudoku - 2 ) has the constraint of words being disambiguated in order of increasing polysemy, leaving the most polysemous to last.', 'run3 ( sudoku - 3 ) is an untested and unpublished version of the iterative approach.', ""it includes run2's constraint plus a second constraint - that all nouns and named entities must be disambiguated before other parts of speech."", 'for each run, a semantic subgraph is constructed from babelnet ( version 2. 5. 1 ).', '']",0
[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG'],0
['with rewriting  #TAUTHOR_TAG'],['with rewriting  #TAUTHOR_TAG'],"['with rewriting  #TAUTHOR_TAG.', 'our model improves sentence rewriting with bert as an extractor and summary - level rewards']","['', 'some notable examples include the use of inconsistency loss  #AUTHOR_TAG, key phrase extraction  #AUTHOR_TAG, and sentence extraction with rewriting  #TAUTHOR_TAG.', 'our model improves sentence rewriting with bert as an extractor and summary - level rewards to optimize the extractor.', 'reinforcement learning has been shown to be effective to directly optimize a non - differentiable objective in language generation including text summarization  #AUTHOR_TAG.', ' #AUTHOR_TAG use actor - critic methods for language generation, using reward shaping  #AUTHOR_TAG to solve the sparsity of training signals.', 'inspired by this, we generalize it to sentence extraction to give per step reward preserving optimality']",0
[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG have attempted to'],[' #TAUTHOR_TAG'],6
[' #TAUTHOR_TAG which consists of two parts : a neural'],[' #TAUTHOR_TAG which consists of two parts : a neural'],"['this paper, we focus on single - document multisentence summarization and propose a neural abstractive model based on the sentence rewriting framework  #TAUTHOR_TAG which consists of two parts : a neural network']","['this paper, we focus on single - document multisentence summarization and propose a neural abstractive model based on the sentence rewriting framework  #TAUTHOR_TAG which consists of two parts : a neural network for the extractor and another network for the abstractor.', 'the extractor network is designed to extract salient sentences from a source article.', '']",6
"[', we convey the equations of  #TAUTHOR_TAG to']","['already.', 'as the decoder structure is almost the same with the previous work, we convey the equations of  #TAUTHOR_TAG to']","['already.', 'as the decoder structure is almost the same with the previous work, we convey the equations of  #TAUTHOR_TAG to']","['use lstm pointer network  #AUTHOR_TAG as the decoder to select the extracted sentences based on the above sentence representations.', 'the decoder extracts sentences recurrently, producing a distribution over all of the remaining sentence representations excluding those already selected.', 'since we use the sequential model which selects one sentence at a time step, our decoder can consider the previously selected sentences.', 'this property is needed to avoid selecting sentences that have overlapping information with the sentences extracted already.', 'as the decoder structure is almost the same with the previous work, we convey the equations of  #TAUTHOR_TAG to avoid confusion, with minor modifications to agree with our notations.', 'formally, the extraction probability is calculated as :', 'where e t is the output of the glimpse operation :', 'in equation 3, z t is the hidden state of the lstm decoder at time t ( shown in green in figure 1 ).', 'all the w and v are trainable parameters']",5
"['work  #TAUTHOR_TAG.', 'however, there is no gold label for extract']","['work  #TAUTHOR_TAG.', 'however, there is no gold label for extractive']","['( ce ) loss like previous work  #TAUTHOR_TAG.', 'however, there is no gold label for extract']","['##or pre - training starting from a poor random policy makes it difficult to train the extractor agent to converge towards the optimal policy.', 'thus, we pre - train the network using cross entropy ( ce ) loss like previous work  #TAUTHOR_TAG.', 'however, there is no gold label for extractive summarization in most of the summarization datasets.', '']",5
"['', 'following  #TAUTHOR_TAG, we use the advantage actor critic  #AUTHOR_TAG method to train.', 'we add a critic network']","['', 'following  #TAUTHOR_TAG, we use the advantage actor critic  #AUTHOR_TAG method to train.', 'we add a critic network']","['', 'following  #TAUTHOR_TAG, we use the advantage actor critic  #AUTHOR_TAG method to train.', 'we add a critic network']","['', 'this encourages the model to extract additional sentences only when they are expected to increase the final return.', 'following  #TAUTHOR_TAG, we use the advantage actor critic  #AUTHOR_TAG method to train.', 'we add a critic network to estimate a value function v t ( d, s 1, · · ·, s t−1 ), which then is used to compute advantage of each action ( we will omit the current state ( d, s 1, · · ·, s t−1 ) to simplify ) :', 'where q t ( s i ) is the expected future reward for selecting s i at the current step t. we maximize this advantage with the policy gradient with the', 'where θ π is the trainable parameters of the actor network ( original extractor ).', 'and the critic is trained to minimize the square loss :', 'where θ ψ is the trainable parameters of the critic network']",5
"['did  #TAUTHOR_TAG.', 'on nyt50']","['did  #TAUTHOR_TAG.', 'on nyt50 dataset, following  #AUTHOR_TAG']","['did  #TAUTHOR_TAG.', 'on nyt50 dataset, following  #AUTHOR_TAG']","['evaluate the performance of our method using different variants of rouge metric computed with respect to the gold summaries.', 'on the cnn / daily mail and duc - 2002 dataset, we use standard rouge - 1, rouge - 2, and rouge - l  #AUTHOR_TAG on full length f 1 with stemming as previous work did  #TAUTHOR_TAG.', 'on nyt50 dataset, following  #AUTHOR_TAG and  #AUTHOR_TAG, we used the limited length rouge recall metric, truncating the generated summary to the length of the ground truth summary.', 'table 1 shows the experimental results on cnn / daily mail dataset, with extractive models in the top block and abstractive models in the bottom block.', 'for comparison, we list the performance of many recent approaches with ours']",5
[' #TAUTHOR_TAG are almost tie'],['in sentence rewrite framework  #TAUTHOR_TAG are almost tie'],[' #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited'],"['and extractors from previous work in sentence rewrite framework  #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited performances of their full model', 'are due to their extractor networks. our extractor network with bert ( bert - ext ), as a single model, outperforms those', 'models with large margins. adding reinforcement learning ( bert - ext + rl ) gives higher performance, which is competitive with other extractive approaches using pre', '']",5
[' #TAUTHOR_TAG are almost tie'],['in sentence rewrite framework  #TAUTHOR_TAG are almost tie'],[' #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited'],"['and extractors from previous work in sentence rewrite framework  #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited performances of their full model', 'are due to their extractor networks. our extractor network with bert ( bert - ext ), as a single model, outperforms those', 'models with large margins. adding reinforcement learning ( bert - ext + rl ) gives higher performance, which is competitive with other extractive approaches using pre', '']",5
['rewrite  #TAUTHOR_TAG'],['rewrite  #TAUTHOR_TAG'],['readability total sentence rewrite  #TAUTHOR_TAG 56 59 115 bertsum  #AUTHOR_TAG 58 60 118 bert - ext + abs + rl + rerank ( ours ) 66 61 127'],"['readability total sentence rewrite  #TAUTHOR_TAG 56 59 115 bertsum  #AUTHOR_TAG 58 60 118 bert - ext + abs + rl + rerank ( ours ) 66 61 127 which has the highest summary - level rouge - l score, from all the possible combinations of sentences.', 'due to time constraints, we limited the maximum number of sentences to 5.', 'this method corresponds to our final return in rl training.', 'table 3 shows the summary - level rouge scores of previously explained methods.', 'we see considerable gaps between sentence - matching and greedy search, while the scores of greedy search are close to those of combination search. note that since we limited the number of sentences for combination search, the exact scores for it would be higher.', 'the scores can be interpreted to be upper bounds for corresponding training methods.', 'this result supports our training strategy ; pretraining with greedy search and final optimization with the combinatorial return.', 'additionally, we experiment to verify the contribution of our training method.', 'we train the same model with different training signals ; sentencelevel reward from  #TAUTHOR_TAG and combinatorial reward from ours.', 'the results are shown in table 4.', 'both with and without reranking, the models trained with the combinatorial reward consistently outperform those trained with the sentence - level reward.', '']",5
['rewrite  #TAUTHOR_TAG'],['rewrite  #TAUTHOR_TAG'],['readability total sentence rewrite  #TAUTHOR_TAG 56 59 115 bertsum  #AUTHOR_TAG 58 60 118 bert - ext + abs + rl + rerank ( ours ) 66 61 127'],"['readability total sentence rewrite  #TAUTHOR_TAG 56 59 115 bertsum  #AUTHOR_TAG 58 60 118 bert - ext + abs + rl + rerank ( ours ) 66 61 127 which has the highest summary - level rouge - l score, from all the possible combinations of sentences.', 'due to time constraints, we limited the maximum number of sentences to 5.', 'this method corresponds to our final return in rl training.', 'table 3 shows the summary - level rouge scores of previously explained methods.', 'we see considerable gaps between sentence - matching and greedy search, while the scores of greedy search are close to those of combination search. note that since we limited the number of sentences for combination search, the exact scores for it would be higher.', 'the scores can be interpreted to be upper bounds for corresponding training methods.', 'this result supports our training strategy ; pretraining with greedy search and final optimization with the combinatorial return.', 'additionally, we experiment to verify the contribution of our training method.', 'we train the same model with different training signals ; sentencelevel reward from  #TAUTHOR_TAG and combinatorial reward from ours.', 'the results are shown in table 4.', 'both with and without reranking, the models trained with the combinatorial reward consistently outperform those trained with the sentence - level reward.', '']",5
['proposed in  #TAUTHOR_TAG'],['proposed in  #TAUTHOR_TAG'],['proposed in  #TAUTHOR_TAG'],"['abstractor network approximates f, which compresses and paraphrases an extracted document sentence to a concise summary sentence.', 'we use the standard attention based sequence - tosequence ( seq2seq ) model  #AUTHOR_TAG with the copying mechanism  #AUTHOR_TAG for handling out - of - vocabulary ( oov ) words.', 'our abstractor is practically identical to the one proposed in  #TAUTHOR_TAG']",3
[' #TAUTHOR_TAG are almost tie'],['in sentence rewrite framework  #TAUTHOR_TAG are almost tie'],[' #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited'],"['and extractors from previous work in sentence rewrite framework  #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited performances of their full model', 'are due to their extractor networks. our extractor network with bert ( bert - ext ), as a single model, outperforms those', 'models with large margins. adding reinforcement learning ( bert - ext + rl ) gives higher performance, which is competitive with other extractive approaches using pre', '']",3
[' #TAUTHOR_TAG are almost tie'],['in sentence rewrite framework  #TAUTHOR_TAG are almost tie'],[' #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited'],"['and extractors from previous work in sentence rewrite framework  #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited performances of their full model', 'are due to their extractor networks. our extractor network with bert ( bert - ext ), as a single model, outperforms those', 'models with large margins. adding reinforcement learning ( bert - ext + rl ) gives higher performance, which is competitive with other extractive approaches using pre', '']",4
[' #TAUTHOR_TAG are almost tie'],['in sentence rewrite framework  #TAUTHOR_TAG are almost tie'],[' #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited'],"['and extractors from previous work in sentence rewrite framework  #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited performances of their full model', 'are due to their extractor networks. our extractor network with bert ( bert - ext ), as a single model, outperforms those', 'models with large margins. adding reinforcement learning ( bert - ext + rl ) gives higher performance, which is competitive with other extractive approaches using pre', '']",4
[' #TAUTHOR_TAG are almost tie'],['in sentence rewrite framework  #TAUTHOR_TAG are almost tie'],[' #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited'],"['and extractors from previous work in sentence rewrite framework  #TAUTHOR_TAG are almost tie. we can easily conjecture that the limited performances of their full model', 'are due to their extractor networks. our extractor network with bert ( bert - ext ), as a single model, outperforms those', 'models with large margins. adding reinforcement learning ( bert - ext + rl ) gives higher performance, which is competitive with other extractive approaches using pre', '']",4
['rewrite  #TAUTHOR_TAG'],['rewrite  #TAUTHOR_TAG'],['readability total sentence rewrite  #TAUTHOR_TAG 56 59 115 bertsum  #AUTHOR_TAG 58 60 118 bert - ext + abs + rl + rerank ( ours ) 66 61 127'],"['readability total sentence rewrite  #TAUTHOR_TAG 56 59 115 bertsum  #AUTHOR_TAG 58 60 118 bert - ext + abs + rl + rerank ( ours ) 66 61 127 which has the highest summary - level rouge - l score, from all the possible combinations of sentences.', 'due to time constraints, we limited the maximum number of sentences to 5.', 'this method corresponds to our final return in rl training.', 'table 3 shows the summary - level rouge scores of previously explained methods.', 'we see considerable gaps between sentence - matching and greedy search, while the scores of greedy search are close to those of combination search. note that since we limited the number of sentences for combination search, the exact scores for it would be higher.', 'the scores can be interpreted to be upper bounds for corresponding training methods.', 'this result supports our training strategy ; pretraining with greedy search and final optimization with the combinatorial return.', 'additionally, we experiment to verify the contribution of our training method.', 'we train the same model with different training signals ; sentencelevel reward from  #TAUTHOR_TAG and combinatorial reward from ours.', 'the results are shown in table 4.', 'both with and without reranking, the models trained with the combinatorial reward consistently outperform those trained with the sentence - level reward.', '']",4
"['', ' #TAUTHOR_TAG']","['for qe.', ' #TAUTHOR_TAG']","['', ' #TAUTHOR_TAG']","['introduce the recurrent neural network based quality estimation ( qe ) model for predicting the sentence, word and phrase - level translation qualities, without relying on manual efforts to find qe related features.', 'existing qe researches have been usually focused on finding desirable qe related features to use machine learning algorithms.', 'recently, however, there have been efforts to apply neural networks to qe and these neural approaches have shown potential for qe.', ' #AUTHOR_TAG use continuous space language model features for sentence - level qe and word embedding features for word - level qe, in combination with other features produced by quest + +.', ' #AUTHOR_TAG apply neural networks using pre - trained alignments and word lookup - table to word - level qe, which achieve the excellent performance by using the combination of baseline features at word level.', ""however, these are not'pure'neural approaches for qe."", ' #TAUTHOR_TAG apply neural machine translation ( nmt ) models, based on recurrent neural network, to sentence - level qe.', 'this is the first try of using nmt models for the translation quality estimation.', 'this recurrent neural network based quality estimation model is a pure neural approach for qe and achieves a competitive performance in sentence - level qe ( english - spanish ).', 'in this paper, we extend the recurrent neural network based quality estimation model to word and phrase level.', 'also, we apply this model to sentence, word and phrase - level qe shared task ( english - german ) of wmt16']",0
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],0
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],0
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],0
"['machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', 'w s is the']","['machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', 'w s is the']","['machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', 'w s is']","['rnn based sentence - level qe model ( figure 2 ), hter ( human - targeted translation edit rate )  #AUTHOR_TAG in [ 0, 1 ] for target sentence is predicted by using a logistic sigmoid func - a recurrent neural networks approach for estimating the quality of machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', '']",0
"['machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', 'w s is the']","['machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', 'w s is the']","['machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', 'w s is']","['rnn based sentence - level qe model ( figure 2 ), hter ( human - targeted translation edit rate )  #AUTHOR_TAG in [ 0, 1 ] for target sentence is predicted by using a logistic sigmoid func - a recurrent neural networks approach for estimating the quality of machine translation output  #TAUTHOR_TAG tion such that', '( 1 )', '']",0
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],5
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],5
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],6
['estimation model  #TAUTHOR_TAG consists of'],['estimation model  #TAUTHOR_TAG consists of'],[') based quality estimation model  #TAUTHOR_TAG consists of two parts : two bidirectional rnns on the source and target sentences in the first part'],[' #TAUTHOR_TAG'],6
['than the baseline reported by  #TAUTHOR_TAG for the task'],['than the baseline reported by  #TAUTHOR_TAG for the task'],['than the baseline reported by  #TAUTHOR_TAG for the task'],"['describe ubc - nlp contribution to iest - 2018, focused at learning implicit emotion in twitter data.', 'among the 30 participating teams, our system ranked the 4th ( with 69. 3 % f - score ).', 'post competition, we were able to score slightly higher than the 3rd ranking system ( reaching 70. 7 % ).', 'our system is trained on top of a pre - trained language model ( lm ), fine - tuned on the data provided by the task organizers.', 'our best results are acquired by an average of an ensemble of language models.', 'we also offer an analysis of system performance and the impact of training data size on the task.', 'for example, we show that training our best model for only one epoch with < 40 % of the data enables better performance than the baseline reported by  #TAUTHOR_TAG for the task']",4
['regression model introduced by  #TAUTHOR_TAG as a baseline for the competition ('],['regression model introduced by  #TAUTHOR_TAG as a baseline for the competition ( f - score ='],['our models achieve sizable gains over the logistic regression model introduced by  #TAUTHOR_TAG as a baseline for the competition ('],"['', 'as the table shows, all our models achieve sizable gains over the logistic regression model introduced by  #TAUTHOR_TAG as a baseline for the competition ( f - score = 60 % ).', 'even though our models trained based on fasttext and elmo each has a single hidden layer, which is not that deep, these at least 1. 5 % higher than the logistic regression model.', 'we also observe that elmo embeddings, which are acquired from language models rather than optimized from sequences of tokens, achieves higher performance than fasttext embeddings.', 'this is not surprising, and aligns with the results reported by  #AUTHOR_TAG']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],4
"['.', ' #TAUTHOR_TAG propose yet a fourth method']","['happy that "" to collect emotion data.', ' #TAUTHOR_TAG propose yet a fourth method']","['m happy that "" to collect emotion data.', ' #TAUTHOR_TAG propose yet a fourth method']","['is essential in human experience and communication, lending special significance to natural language processing systems aimed at learning it.', 'emotion detection systems can be applied in a host of domains, including health and well - being, user profiling, education, and marketing.', 'there is a small, yet growing, body of nlp literature on emotion.', 'early works focused on creating and manually labeling datasets.', 'the semeval 2007 affective text task  #AUTHOR_TAG and  #AUTHOR_TAG are two examples that target the news and blog domains respectively.', 'in these works, data were labeled for the 6 basic emotions of ekman  #AUTHOR_TAG.', 'more recent works exploit distant supervision  #AUTHOR_TAG to automatically acquire emotion data for training systems.', 'more specifically, a number of works use hashtags like # happy and # sad, especially occurring finally in twitter data, as a proxy of emotion  #AUTHOR_TAG.', 'abdul -  #AUTHOR_TAG report state - of - the - art results using a large dataset acquired with hashtags.', 'other works exploit emojis to capture emotion carrying data  #AUTHOR_TAG.', ' #AUTHOR_TAG introduce a third effective approach that leverages firstperson seed phrases like "" i\'m happy that "" to collect emotion data.', ' #TAUTHOR_TAG propose yet a fourth method for collecting emotion data that depends on the existence of the expression "" emotion - word + one of the following words ( when, that or because ) "" in a tweet, regardless of the position of the emotion word.', 'in the "" implicit emotion "" shared task 1, participants were provided data representing the 6 emotions in the set ( anger, disgust, fear, joy, sad, surprise ).', 'the trigger word was removed from each tweet.', 'to illustrate, the task is to predict the emotion in a tweet like "" boys who like starbucks make me [ # triggerword # ] because we can go on cute coffee dates "" ( with the triggered word labeled as joy ).', 'in this paper, we describe our system submitted as part of the competition.', 'overall, our submission ranked the 4th out of the 30 participating teams.', 'with further experiments, we were able to acquire better results, which would rank our model at top 3 ( 70. 7 % f - score ).', 'the rest of the paper is organized as follows : section 2 describes the data.', 'section 3 offers a description of the methods employed in our work.', 'section 3 is where we present our results, and we perform an analysis of these results in section 5.', 'we list negative experiments in section 6 and conclude in section 7']",0
"['deadline of system submission.', 'the full details of the dataset can be found in  #TAUTHOR_TAG.', 'we now describe our methods in the nesxt section']","['deadline of system submission.', 'the full details of the dataset can be found in  #TAUTHOR_TAG.', 'we now describe our methods in the nesxt section']","['system development, while the test set was released one week before the deadline of system submission.', 'the full details of the dataset can be found in  #TAUTHOR_TAG.', 'we now describe our methods in the nesxt section']","['use the twitter dataset released by the organizers of the "" implicit emotion "" task, as described in the previous section.', 'the data are partitioned into 153, 383 tweets for training, 9591 tweets for validation, and 28, 757 data points for testing.', 'the training and validation sets were provided early for system development, while the test set was released one week before the deadline of system submission.', 'the full details of the dataset can be found in  #TAUTHOR_TAG.', 'we now describe our methods in the nesxt section']",0
"['', 'as an additional baseline, we compare to  #TAUTHOR_TAG who propose a model based on']","['baseline models.', 'as an additional baseline, we compare to  #TAUTHOR_TAG who propose a model based on']","['baseline models.', 'as an additional baseline, we compare to  #TAUTHOR_TAG who propose a model based on logistic regression with a bag of word unigrams ( bow )']","['develop a host of models based on deep neural networks, using some of these models as our baseline models.', 'as an additional baseline, we compare to  #TAUTHOR_TAG who propose a model based on logistic regression with a bag of word unigrams ( bow ).', 'all our deep learning models are based on variations of recurrent neural networks ( rnns ), which have proved useful for several nlp tasks.', 'rnns are able to capture sequential dependencies especially in time series data, of which language can be seen as an example.', 'one weakness of rnns, however, lies in gradient either vanishing or exploding during training.', 'longshort term memory ( lstm ) networks were developed to target this problem, and hence we employ these in our work.', 'we also use a bidirectional version ( bilstm ) where the vector of representation is built as a concatenation of two vectors, one that runs from left - to - right and another running from right - to - left.', 'ultimately, we generate a fixed - size representation for a given tweet using the last hidden state for the fwd and bwd lstm.', 'our systems can be categorized as follows : ( 1 ) systems tuning simple pre - trained embeddings, ( 2 ) systems tuning embeddings from language models, and ( 3 ) systems directly tuning language models.', 'we treat # 1 and # 2 as baseline systems, while our best models are based on # 3']",5
['to the locality of headwords of constituents ( collins 1997  #TAUTHOR_TAG'],"['to the locality of headwords of constituents ( collins 1997  #TAUTHOR_TAG eisner 1997 ),']","['example by restricting the statistical dependencies to the locality of headwords of constituents ( collins 1997  #TAUTHOR_TAG eisner 1997 ),']","['of the main goals in statistical natural language parsing is to find the minimal set of statistical dependencies ( between words and syntactic structures ) which achieves maximal parse accuracy.', 'many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents ( collins 1997  #TAUTHOR_TAG eisner 1997 ), leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies.', ""the data oriented parsing model, on the other hand, takes a rather extreme view on this issue : it does not single out a narrowly predefined set of structures as the statistically significant ones ; given an annotated corpus, all fragments ( i. e., subtrees ) seen in that corpus, regardless of size and lexicalization, are in principle taken to form a grammar ( see bod 1992  #AUTHOR_TAG bod & kaplan 1998 ; bonnema et al. 1997 ; cormons 1999 ; goodman 1996  #AUTHOR_TAG kaplan 1996 ; de pauw 2000 ; scha 1990 ; sima'an 1995  #AUTHOR_TAG way 1999 )."", 'the set of subtrees that is used is thus very large and extremely redundant.', '']",0
"['by only one product of probabilities - - see  #AUTHOR_TAG  #TAUTHOR_TAG,  #AUTHOR_TAG charniak (, 2000 and  #AUTHOR_TAG ( dempster']","['by only one product of probabilities - - see  #AUTHOR_TAG  #TAUTHOR_TAG,  #AUTHOR_TAG charniak (, 2000 and  #AUTHOR_TAG ( dempster']","['compute the probability of a tree by only one product of probabilities - - see  #AUTHOR_TAG  #TAUTHOR_TAG,  #AUTHOR_TAG charniak (, 2000 and  #AUTHOR_TAG ( dempster']","['', 'the probability of a derivation t 1°... °t n is computed by the product of the probabilities of its subtrees t i :', 'as we have seen, there may be several distinct derivations that generate the same parse tree.', 'the probability of a parse tree t is thus the sum of the probabilities of its distinct derivations.', 'let t id be the i - th subtree in the derivation d that produces tree t, then the probability of t is given by', ""thus dop1's tree probability model corresponds to a sum - of - products model ( van santen 1993 ) in that it computes the probability of a tree as a sum of products, where each product corresponds to the probability of a distinct derivation generating the particular tree."", 'this distinguishes dop1 from most other statistical parsing models that identify exactly one derivation for each parse tree and thus compute the probability of a tree by only one product of probabilities - - see  #AUTHOR_TAG  #TAUTHOR_TAG,  #AUTHOR_TAG charniak (, 2000 and  #AUTHOR_TAG ( dempster et al']",0
"['include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 )']","['include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 )']","['note, however, that most other stochastic parsers do include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 ). but our parser is the first parser that also includes counts between two or more nonheadwords, to the']","['', 'i. e. containing all lexicalized subtrees with maximally 12 frontier words and all unlexicalized subtrees up', 'to depth the table shows that nonheadwords contribute to higher parse accuracy : the difference between using no', 'and all nonheadwords is 1. 2 % in lp and 1. 0 % in lr. although this difference is relatively small, it does indicate that nonheadword dependencies should not be discarded in the wsj. we should note, however, that most other stochastic parsers do include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 ). but our parser is the first parser that also includes counts between two or more nonheadwords, to the best of our knowledge, and these counts lead to improved performance, as can be seen in the table above']",0
"['include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 )']","['include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 )']","['note, however, that most other stochastic parsers do include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 ). but our parser is the first parser that also includes counts between two or more nonheadwords, to the']","['', 'i. e. containing all lexicalized subtrees with maximally 12 frontier words and all unlexicalized subtrees up', 'to depth the table shows that nonheadwords contribute to higher parse accuracy : the difference between using no', 'and all nonheadwords is 1. 2 % in lp and 1. 0 % in lr. although this difference is relatively small, it does indicate that nonheadword dependencies should not be discarded in the wsj. we should note, however, that most other stochastic parsers do include counts of single nonheadwords : they appear in the backed - off statistics of these parsers ( see collins 1997  #TAUTHOR_TAG charniak 1997 ;', 'goodman 1998 ). but our parser is the first parser that also includes counts between two or more nonheadwords, to the best of our knowledge, and these counts lead to improved performance, as can be seen in the table above']",0
['( see collins 1997  #TAUTHOR_TAG charniak 1997  #AUTHOR_TAG ratnaparkhi 1999 ) with sections 2 through 21 for training ( approx.'],['wsj ( see collins 1997  #TAUTHOR_TAG charniak 1997  #AUTHOR_TAG ratnaparkhi 1999 ) with sections 2 through 21 for training ( approx.'],"['( see collins 1997  #TAUTHOR_TAG charniak 1997  #AUTHOR_TAG ratnaparkhi 1999 ) with sections 2 through 21 for training ( approx. 40, 000 sentences ) and section 23']","['our base line parse accuracy, we used the now standard division of the wsj ( see collins 1997  #TAUTHOR_TAG charniak 1997  #AUTHOR_TAG ratnaparkhi 1999 ) with sections 2 through 21 for training ( approx. 40, 000 sentences ) and section 23 for testing ( 2416 sentences ≤ 100 words ) ; section 22 was used as development set.', 'all trees were stripped off their semantic tags, co - reference information and quotation marks.', 'we used all training set subtrees of depth 1, but due to memory limitations we used a subset of the subtrees larger than depth 1, by taking for each depth a random sample of 400, 000 subtrees.', 'this random subtree sample was not selected from an exhaustive computation of all subtrees of a particular depth ( which is prohibitive for the large wsj ).', 'instead, for each particular depth > 1 we sampled subtrees by first randomly selecting a node in a random tree from the training set, after which we selected random expansions from that node until a subtree of the particular depth was obtained.', 'we repeated this procedure 400, 000 times for each depth > 1 and ≤ 14.', 'thus no subtrees of depth > 14 were used.', 'this resulted in a total set of 5, 217, 529 subtrees which we will call the "" base line subtree set "" and which was smoothed by the technique described in  #AUTHOR_TAG based on goodturing.', 'since our subtrees are allowed to be lexicalized ( at their frontiers ), we did not use a separate part - of - speech tagger : the test sentences were directly parsed by the training set subtrees.', 'for words that were unknown in our subtree set, we guessed their categories by means of the method described in  #AUTHOR_TAG which uses statistics on wordendings, hyphenation and capitalization.', 'the guessed category for each unknown word was converted into a depth - 1 subtree and assigned a probability by means of simple good - turing estimation ( see bod 1996 ).', 'the most probable parse for each test sentence was estimated from the 1, 000 most probable derivations of that sentence, as described in section 3.', 'all experiments were carried out on a sun ultrasparc 60 with a 450 mhz ultrasparc - ii processor and 2 gigabytes of real memory.', 'we used the standard parseval scores ( black et al. 1991 ) to compare a proposed parse p ( i. e. our estimated most probable parse ) with the corresponding correct treebank parse t as follows :', '# correct constituents in p # constituents in p labeled precision = labeled recall']",3
"['g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the']","['g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the importance of', 'including two and more nonheadwords.  #AUTHOR_TAG has even observed that']","['. g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the importance of', 'including two and more nonheadwords.  #AUTHOR_TAG has even observed that "" in an ideal situation we would be able to encode arbitrary features h s, thereby keeping track of counts of arbitrary fragments within parse trees, without having to worry about formulating a derivation that included these features']","['', 'try to improve parse accuracy by gradually letting in more dependencies, we start out by taking into account as many dependencies as possible and then try to constrain them without', 'losing parse accuracy. it is not unlikely that these two opposite directions finally converge to the same, true set of statistical dependencies for natural language parsing. 6 as it happens, some convergence of these two approaches has already taken place. while earlier head - lexicalized models restricted fragments to the locality of headwords of constituents ( e. g.', 'collins 1996 ; eisner 1996 ), later models showed the importance of including additional context from higher nodes in the tree, resulting in improved parse accuracy ( charniak 1997 ; johnson 1998 ). this mirrors our result of the utility of fragments', 'of depth 2 ( and larger ) which was already reported in  #AUTHOR_TAG. the importance of including counts of ( single ) nonheadwords is', 'now also quite uncontroversial ( e. g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the importance of', 'including two and more nonheadwords.  #AUTHOR_TAG has even observed that "" in an ideal situation we would be able to encode arbitrary features h s, thereby keeping track of counts of arbitrary fragments within parse trees, without having to worry about formulating a derivation that included these features. "". this philosophy is in perfect correspondence', 'with the dop approach']",3
"['g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the']","['g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the importance of', 'including two and more nonheadwords.  #AUTHOR_TAG has even observed that']","['. g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the importance of', 'including two and more nonheadwords.  #AUTHOR_TAG has even observed that "" in an ideal situation we would be able to encode arbitrary features h s, thereby keeping track of counts of arbitrary fragments within parse trees, without having to worry about formulating a derivation that included these features']","['', 'try to improve parse accuracy by gradually letting in more dependencies, we start out by taking into account as many dependencies as possible and then try to constrain them without', 'losing parse accuracy. it is not unlikely that these two opposite directions finally converge to the same, true set of statistical dependencies for natural language parsing. 6 as it happens, some convergence of these two approaches has already taken place. while earlier head - lexicalized models restricted fragments to the locality of headwords of constituents ( e. g.', 'collins 1996 ; eisner 1996 ), later models showed the importance of including additional context from higher nodes in the tree, resulting in improved parse accuracy ( charniak 1997 ; johnson 1998 ). this mirrors our result of the utility of fragments', 'of depth 2 ( and larger ) which was already reported in  #AUTHOR_TAG. the importance of including counts of ( single ) nonheadwords is', 'now also quite uncontroversial ( e. g. collins 1997  #TAUTHOR_TAG charniak 2000 ), and the current paper has shown the importance of', 'including two and more nonheadwords.  #AUTHOR_TAG has even observed that "" in an ideal situation we would be able to encode arbitrary features h s, thereby keeping track of counts of arbitrary fragments within parse trees, without having to worry about formulating a derivation that included these features. "". this philosophy is in perfect correspondence', 'with the dop approach']",4
['of unit segmentation  #TAUTHOR_TAG has lead to'],['of unit segmentation  #TAUTHOR_TAG has lead to'],['of unit segmentation  #TAUTHOR_TAG has lead to'],"['mining ( am ) is increasingly applied in different fields of research like fake - news detection and political argumentation and network analysis 1.', 'one crucial part of the am pipeline is to segment written text into argumentative and nonargumentative units.', 'recent research in the area of unit segmentation  #TAUTHOR_TAG has lead to promising results with f1 - scores of up to 0. 90 for in - domain segmentation  #AUTHOR_TAG.', 'nevertheless, there is still a need for more robust approaches.', 'given the recent progress of attention - based models in neural machine translation ( nmt )  #AUTHOR_TAG, this paper evaluates the effectiveness of attention for the task 1 see for example the mardy project ( https : / / www. socium. uni - bremen. de / projekte /?', 'proj = 570 & print = 1, last accessed :', '2019 - 04 - 15, 09 : 50utc + 2 ).', 'of argumentative unit segmentation.', 'the idea of the attention layers added to the recurrent network is to enable the model to prioritize those parts of the input sequence that are important for the current prediction  #AUTHOR_TAG.', 'this can be achieved by learning additional parameters during the training of the model.', '']",0
"['', 'further,  #TAUTHOR_TAG proposed a setup with three bidirectional lstms ( bi - lstms )  #AUTHOR_TAG in total as']","['of unit boundary prediction.', 'for example,  #AUTHOR_TAG reported different long short - term memory ( lstm )  #AUTHOR_TAG architectures.', 'further,  #TAUTHOR_TAG proposed a setup with three bidirectional lstms ( bi - lstms )  #AUTHOR_TAG in total as']","[',  #AUTHOR_TAG reported different long short - term memory ( lstm )  #AUTHOR_TAG architectures.', 'further,  #TAUTHOR_TAG proposed a setup with three bidirectional lstms ( bi - lstms )  #AUTHOR_TAG in total as']","['', 'the main idea is to focus the attention of the underlying network on pointsof - interest in the input that are often surrounded by irrelevant parts  #AUTHOR_TAG.', 'this allows the model to put more weight on the important chunks.', 'while earlier salient detectors were task - specific, newer approaches ( e. g.  #AUTHOR_TAG can be adapted to different tasks, like image description generation  #AUTHOR_TAG, and allow for the parameters of the attention to be tuned during the training.', 'these additional tasks include sequence processing and the application of such networks to different areas of natural language processing ( nlp ).', 'one of the first use - cases for attention mechanisms in the field of nlp was machine translation.', ' #AUTHOR_TAG utilized the attention to improve their nmt model.', 'a few years later,  #AUTHOR_TAG achieved new state - of - the - art ( sota ) results by presenting an encoder - decoder architecture that is based on the attention mechanism, only adding a position - wise feed - forward network and normalizations in between.', ' #AUTHOR_TAG picked up on the encoder part of this architecture to pre - train a bidirectional lm.', 'after fine - tuning, they achieved, again, a new sota performance on different downstream nlp tasks like part - of - speech tagging and questions - answering.', 'a possible way of posing the unit segmentation as nlp task is a token - based sequence labeling  #AUTHOR_TAG.', ' #AUTHOR_TAG used rather simple, non - recurrent classifiers to approach this problem, others mostly applied recurrent networks to the task of unit boundary prediction.', 'for example,  #AUTHOR_TAG reported different long short - term memory ( lstm )  #AUTHOR_TAG architectures.', 'further,  #TAUTHOR_TAG proposed a setup with three bidirectional lstms ( bi - lstms )  #AUTHOR_TAG in total as their best solution.', 'while the first two of them are fully connected and work on word embeddings and task - specific features respectively, the intention for the third is to take the output of the first two as input and learn to correct their errors.', '']",0
"['task  #TAUTHOR_TAG.', 'the architectures proposed in this']","['task  #TAUTHOR_TAG.', 'the architectures proposed in this']","['the same task  #TAUTHOR_TAG.', 'the architectures proposed in this section']","['paper evaluates different machine learning architectures with added attention layers for the task of am, and more specifically unit segmentation.', 'the problem is framed as a multi - class token labeling task, in which each token is assigned one of three labels.', 'a ( b ) label denotes that the token is at the beginning of an argumentative unit, an ( i ) label that it lies inside a unit and an ( o ) label that the token is not part of a unit.', 'this framework has been applied previously for the same task  #TAUTHOR_TAG.', 'the architectures proposed in this section build on  #TAUTHOR_TAG, omitting the second bi - lstm, which was used to process features other than word embeddings ( see section 3. 1 ).', 'they are further being modified by adding attention layers at different positions.', 'the goal is to reuse existing approaches and possibly enhance their ability to model long - range dependencies.', 'additionally, a simpler architecture, consisting of a single bi - lstm paired with an attention layer, is built and evaluated with the aim of decreased complexity.', 'in order to answer the second research question, this paper reports results in combination with improved input embeddings, in order to evaluate their effectiveness and impact on the am downstream task.', 'all models are compared to the modified reimplementation of the architecture, which is defined as the baseline architecture']",3
"['baseline + error. according to  #TAUTHOR_TAG,', '']","['baseline + error. according to  #TAUTHOR_TAG,', '']","['baseline + error. according to  #TAUTHOR_TAG,', 'the latter bi - ls']","['', '). due to the fact that the second bi - lstm in the first layer is', 'only used to encode the non - semantic features like part - of - speech tags and discourse marker labels, it is omitted in the', 're - implementation. hereafter, we will refer to this model as baseline. also, the batch size was increased from 8 to 64, compared to the original implementation, as a trade - off between convergence time and', ""the model's generalization performance  #AUTHOR_TAG. nevertheless, this model achieves comparable scores to"", 'the ones presented in the original paper. the slightly lower performance can probably be attributed to implementation details. baseline + input and baseline + error for both variations, the baseline architecture was used', 'as a basis, as can be seen in figure 1b. multi - headattention layers are added at different positions in the network. the number of attention heads depends on the dimension of', 'the embedding vectors. for the glove ( 300 features ) and the bert ( 3072 features ) embeddings, six heads are used, while the flair ( 4196 features )', 'embeddings require four heads. both numbers were the largest divisor for the respective input vector size that worked inside the computational boundaries available. in the first model, an attention layer was added before the first bi - lstm in an attempt to', 'apply a relevance score directly to the tokens, in order', 'to better capture dependencies of the input sequence. this model will be referred to as baseline + input. the second variation adds the attention layer after the first and before the second bi', '- lstm, which will be called baseline + error. according to  #TAUTHOR_TAG,', '']",3
"['baseline + error. according to  #TAUTHOR_TAG,', '']","['baseline + error. according to  #TAUTHOR_TAG,', '']","['baseline + error. according to  #TAUTHOR_TAG,', 'the latter bi - ls']","['', '). due to the fact that the second bi - lstm in the first layer is', 'only used to encode the non - semantic features like part - of - speech tags and discourse marker labels, it is omitted in the', 're - implementation. hereafter, we will refer to this model as baseline. also, the batch size was increased from 8 to 64, compared to the original implementation, as a trade - off between convergence time and', ""the model's generalization performance  #AUTHOR_TAG. nevertheless, this model achieves comparable scores to"", 'the ones presented in the original paper. the slightly lower performance can probably be attributed to implementation details. baseline + input and baseline + error for both variations, the baseline architecture was used', 'as a basis, as can be seen in figure 1b. multi - headattention layers are added at different positions in the network. the number of attention heads depends on the dimension of', 'the embedding vectors. for the glove ( 300 features ) and the bert ( 3072 features ) embeddings, six heads are used, while the flair ( 4196 features )', 'embeddings require four heads. both numbers were the largest divisor for the respective input vector size that worked inside the computational boundaries available. in the first model, an attention layer was added before the first bi - lstm in an attempt to', 'apply a relevance score directly to the tokens, in order', 'to better capture dependencies of the input sequence. this model will be referred to as baseline + input. the second variation adds the attention layer after the first and before the second bi', '- lstm, which will be called baseline + error. according to  #TAUTHOR_TAG,', '']",3
"['results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the']","['results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the']","['it takes the imbalance of the samples per label into account.', 'for our re - implementation of the baseline, we are able to approximately reproduce the results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the performance']","['evaluate the performance of all architectures on the persuasive essays data set detailed above.', 'the models are re - initialized after every evaluation and do not share any weights.', 'this allows us to answer the first research question of whether additional attention layers have a positive impact on the prediction quality.', 'to answer the second research question, we rerun each training, replacing the glove with bert and flair embeddings.', 'both contextualized embedding methods are tested separately.', 'we contextualize the tokens on the sentence level since the bert model ( google ai  #AUTHOR_TAG only allows for a maximum input length of 512 characters.', 'this makes document - level or paragraphlevel embeddings impractical for the data set.', 'as a performance measure, we report the weighted f1 - score instead of the macro f1 - score, since it takes the imbalance of the samples per label into account.', 'for our re - implementation of the baseline, we are able to approximately reproduce the results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the performance when adding a second bi - lstm to the network ( compare results for bilstm and baseline in table 1 )']",3
"['task  #TAUTHOR_TAG.', 'the architectures proposed in this']","['task  #TAUTHOR_TAG.', 'the architectures proposed in this']","['the same task  #TAUTHOR_TAG.', 'the architectures proposed in this section']","['paper evaluates different machine learning architectures with added attention layers for the task of am, and more specifically unit segmentation.', 'the problem is framed as a multi - class token labeling task, in which each token is assigned one of three labels.', 'a ( b ) label denotes that the token is at the beginning of an argumentative unit, an ( i ) label that it lies inside a unit and an ( o ) label that the token is not part of a unit.', 'this framework has been applied previously for the same task  #TAUTHOR_TAG.', 'the architectures proposed in this section build on  #TAUTHOR_TAG, omitting the second bi - lstm, which was used to process features other than word embeddings ( see section 3. 1 ).', 'they are further being modified by adding attention layers at different positions.', 'the goal is to reuse existing approaches and possibly enhance their ability to model long - range dependencies.', 'additionally, a simpler architecture, consisting of a single bi - lstm paired with an attention layer, is built and evaluated with the aim of decreased complexity.', 'in order to answer the second research question, this paper reports results in combination with improved input embeddings, in order to evaluate their effectiveness and impact on the am downstream task.', 'all models are compared to the modified reimplementation of the architecture, which is defined as the baseline architecture']",4
"['task  #TAUTHOR_TAG.', 'the architectures proposed in this']","['task  #TAUTHOR_TAG.', 'the architectures proposed in this']","['the same task  #TAUTHOR_TAG.', 'the architectures proposed in this section']","['paper evaluates different machine learning architectures with added attention layers for the task of am, and more specifically unit segmentation.', 'the problem is framed as a multi - class token labeling task, in which each token is assigned one of three labels.', 'a ( b ) label denotes that the token is at the beginning of an argumentative unit, an ( i ) label that it lies inside a unit and an ( o ) label that the token is not part of a unit.', 'this framework has been applied previously for the same task  #TAUTHOR_TAG.', 'the architectures proposed in this section build on  #TAUTHOR_TAG, omitting the second bi - lstm, which was used to process features other than word embeddings ( see section 3. 1 ).', 'they are further being modified by adding attention layers at different positions.', 'the goal is to reuse existing approaches and possibly enhance their ability to model long - range dependencies.', 'additionally, a simpler architecture, consisting of a single bi - lstm paired with an attention layer, is built and evaluated with the aim of decreased complexity.', 'in order to answer the second research question, this paper reports results in combination with improved input embeddings, in order to evaluate their effectiveness and impact on the am downstream task.', 'all models are compared to the modified reimplementation of the architecture, which is defined as the baseline architecture']",6
"['baseline + error. according to  #TAUTHOR_TAG,', '']","['baseline + error. according to  #TAUTHOR_TAG,', '']","['baseline + error. according to  #TAUTHOR_TAG,', 'the latter bi - ls']","['', '). due to the fact that the second bi - lstm in the first layer is', 'only used to encode the non - semantic features like part - of - speech tags and discourse marker labels, it is omitted in the', 're - implementation. hereafter, we will refer to this model as baseline. also, the batch size was increased from 8 to 64, compared to the original implementation, as a trade - off between convergence time and', ""the model's generalization performance  #AUTHOR_TAG. nevertheless, this model achieves comparable scores to"", 'the ones presented in the original paper. the slightly lower performance can probably be attributed to implementation details. baseline + input and baseline + error for both variations, the baseline architecture was used', 'as a basis, as can be seen in figure 1b. multi - headattention layers are added at different positions in the network. the number of attention heads depends on the dimension of', 'the embedding vectors. for the glove ( 300 features ) and the bert ( 3072 features ) embeddings, six heads are used, while the flair ( 4196 features )', 'embeddings require four heads. both numbers were the largest divisor for the respective input vector size that worked inside the computational boundaries available. in the first model, an attention layer was added before the first bi - lstm in an attempt to', 'apply a relevance score directly to the tokens, in order', 'to better capture dependencies of the input sequence. this model will be referred to as baseline + input. the second variation adds the attention layer after the first and before the second bi', '- lstm, which will be called baseline + error. according to  #TAUTHOR_TAG,', '']",5
"['results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the']","['results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the']","['it takes the imbalance of the samples per label into account.', 'for our re - implementation of the baseline, we are able to approximately reproduce the results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the performance']","['evaluate the performance of all architectures on the persuasive essays data set detailed above.', 'the models are re - initialized after every evaluation and do not share any weights.', 'this allows us to answer the first research question of whether additional attention layers have a positive impact on the prediction quality.', 'to answer the second research question, we rerun each training, replacing the glove with bert and flair embeddings.', 'both contextualized embedding methods are tested separately.', 'we contextualize the tokens on the sentence level since the bert model ( google ai  #AUTHOR_TAG only allows for a maximum input length of 512 characters.', 'this makes document - level or paragraphlevel embeddings impractical for the data set.', 'as a performance measure, we report the weighted f1 - score instead of the macro f1 - score, since it takes the imbalance of the samples per label into account.', 'for our re - implementation of the baseline, we are able to approximately reproduce the results reported by  #TAUTHOR_TAG.', 'additionally, we can verify that there is no major change in the performance when adding a second bi - lstm to the network ( compare results for bilstm and baseline in table 1 )']",5
"['syntax  #TAUTHOR_TAG, but they enforce linear']","['syntax  #TAUTHOR_TAG, but they enforce linear']","['syntax  #TAUTHOR_TAG, but they enforce linear word order,']","['state - of - the - art parsers score over 90 % on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus.', 'features from n - gram counts over resources like web1t  #AUTHOR_TAG have proven to be useful proxies for syntax  #TAUTHOR_TAG, but they enforce linear word order, and are unable to distinguish between syntactic and non - syntactic co - occurrences.', 'longer n - grams are also noisier and sparser, limiting the range of potential features.', 'in this paper we develop new features for the graph - based mstparser ( mc  #AUTHOR_TAG from the google syntactic ngrams corpus  #AUTHOR_TAG, a collection of stanford dependency subtree counts.', 'these features capture information collated across millions of subtrees produced by a shift - reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.', 'we compare the performance of our syntactic n - gram features against the surface n - gram features of  #TAUTHOR_TAG in - domain on newswire and out - of - domain on the english web treebank ( petrov and mc  #AUTHOR_TAG across conll - style ( lth ) dependencies.', 'we also extend the first - order surface n - gram features to second - order, and compare the utility of web1t and the google books ngram corpus  #AUTHOR_TAG as surface n - gram sources.', 'we find that surface and syntactic n - grams provide statistically significant and complementary accuracy improvements in - and out - of - domain.', '']",1
"[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","['we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after']","['the dependency hold → hearing depicted in figure 1. the final feature encodes the pos tags of the head and argument, directionality, the binned distance between the head and argument,', 'and a bucketed frequency of the syntactic n - gram calculated as per equation 1', ', creating bucket labels from 0 in increments of 5 ( 0, 5, 10, etc. ). additional features for each bucket value', 'up to the maximum are also encoded. we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after each head - argument ambiguity ( see section 3. 2 ). figure 1 depicts the potential context words available the hold → hearing dependency. we experiment with a number of second - order features, mirroring those extracted for surface ngrams in section 3', '. 3. we extract all triple and sibling word and pos structures considered by the parser in the training and test corpora (', 'following the factorization depicted in figure 2 ), and counted their frequency in the syntactic ngrams corpus', '']",1
"['syntax  #TAUTHOR_TAG, but they enforce linear']","['syntax  #TAUTHOR_TAG, but they enforce linear']","['syntax  #TAUTHOR_TAG, but they enforce linear word order,']","['state - of - the - art parsers score over 90 % on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus.', 'features from n - gram counts over resources like web1t  #AUTHOR_TAG have proven to be useful proxies for syntax  #TAUTHOR_TAG, but they enforce linear word order, and are unable to distinguish between syntactic and non - syntactic co - occurrences.', 'longer n - grams are also noisier and sparser, limiting the range of potential features.', 'in this paper we develop new features for the graph - based mstparser ( mc  #AUTHOR_TAG from the google syntactic ngrams corpus  #AUTHOR_TAG, a collection of stanford dependency subtree counts.', 'these features capture information collated across millions of subtrees produced by a shift - reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.', 'we compare the performance of our syntactic n - gram features against the surface n - gram features of  #TAUTHOR_TAG in - domain on newswire and out - of - domain on the english web treebank ( petrov and mc  #AUTHOR_TAG across conll - style ( lth ) dependencies.', 'we also extend the first - order surface n - gram features to second - order, and compare the utility of web1t and the google books ngram corpus  #AUTHOR_TAG as surface n - gram sources.', 'we find that surface and syntactic n - grams provide statistically significant and complementary accuracy improvements in - and out - of - domain.', '']",0
"[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","['we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after']","['the dependency hold → hearing depicted in figure 1. the final feature encodes the pos tags of the head and argument, directionality, the binned distance between the head and argument,', 'and a bucketed frequency of the syntactic n - gram calculated as per equation 1', ', creating bucket labels from 0 in increments of 5 ( 0, 5, 10, etc. ). additional features for each bucket value', 'up to the maximum are also encoded. we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after each head - argument ambiguity ( see section 3. 2 ). figure 1 depicts the potential context words available the hold → hearing dependency. we experiment with a number of second - order features, mirroring those extracted for surface ngrams in section 3', '. 3. we extract all triple and sibling word and pos structures considered by the parser in the training and test corpora (', 'following the factorization depicted in figure 2 ), and counted their frequency in the syntactic ngrams corpus', '']",0
"['syntactic relationship  #TAUTHOR_TAG.', 'n - gram resources']","['syntactic relationship  #TAUTHOR_TAG.', 'n - gram resources']","['to be in a syntactic relationship  #TAUTHOR_TAG.', 'n - gram resources']","['features rely on the intuition that frequently co - occurring words in large unlabeled text collections are likely to be in a syntactic relationship  #TAUTHOR_TAG.', 'n - gram resources such as web1t and google books provide large offline collections from which these co - occurrence statistics can be harvested ; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser.', 'when scanning, the head and argument word may appear immediately adjacent to one another in linear order ( contig ), or with up to three intervening words ( gap1, gap2, and gap3 ) as the maximum n - gram length is five.', 'the total count is then discretized as per equation 1 previously.', 'the final parser features include the pos tags of the potential head and argument, the discretized count, directionality, and the binned length of the dependency.', 'additional cumulative features are generated using each bucket from the pre - calculated up to the maximum bucket size.', 'paraphrase - style surface n - gram features attempt to infer attachments indirectly.', ' #AUTHOR_TAG propose several static patterns to resolve a variety of nominal and prepositional attachment ambiguities.', 'for example, they give the example of sentence ( 1 ) below, paraphrase it into sentence ( 2 ), and examine how frequent the paraphrase is. if it should happen sufficiently often, this serves as evidence for the nominal attachment to demands in sentence ( 1 ) rather than the verbal attachment to meet']",0
"[' #TAUTHOR_TAG, paraphrase features are generated for all']","[' #TAUTHOR_TAG, paraphrase features are generated for all full - parse attachment ambiguities from the surface n - gram corpus.', 'for']","[' #TAUTHOR_TAG, paraphrase features are generated for all full - parse attachment ambiguities from the surface n - gram corpus.', 'for each attachment ambiguity,']","[' #TAUTHOR_TAG, paraphrase features are generated for all full - parse attachment ambiguities from the surface n - gram corpus.', 'for each attachment ambiguity, 3 - grams of the form ( q 1 q 2 ), ( q 1 q 2 ), and ( q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words.', 'then the most frequent words appearing in each of these configurations for each head - argument ambiguity is encoded as a feature with the pos tags of the head and argument 2.', 'given the arc hold → hearing in figure 2, public is the most frequent word appearing in the n - gram ( hold hearing ) in web1t.', 'thus, the final encoded feature is pos ( hold ) ∧ pos ( hearing ) ∧ public ∧ mid.', 'further generalization is achieved by using a unigram pos tagger trained on the wsj data to tag each context word, and encoding features using each unique tag of the most frequent context words']",0
"[' #TAUTHOR_TAG, other feature - based approaches to improving']","[' #TAUTHOR_TAG, other feature - based approaches to improving']","[' #TAUTHOR_TAG, other feature - based approaches to']","['n - gram counts from large web corpora have been used to address np and pp attachment errors  #AUTHOR_TAG aside from  #TAUTHOR_TAG, other feature - based approaches to improving dependency parsing include  #AUTHOR_TAG, who exploits brown clusters and point - wise mutual information of surface n - gram counts to specifically address pp and coordination errors.', ' #AUTHOR_TAG describe a novel way of generating meta - features that work to emphasise important feature types used by the parser.', ' #AUTHOR_TAG generate subtree - based features that are similar to ours.', 'however, they use the in - domain bllip newswire corpus to generate their subtree counts, whereas the syntactic ngrams corpus is out - of - domain and an order of magnitude larger.', '']",0
"['syntax  #TAUTHOR_TAG, but they enforce linear']","['syntax  #TAUTHOR_TAG, but they enforce linear']","['syntax  #TAUTHOR_TAG, but they enforce linear word order,']","['state - of - the - art parsers score over 90 % on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus.', 'features from n - gram counts over resources like web1t  #AUTHOR_TAG have proven to be useful proxies for syntax  #TAUTHOR_TAG, but they enforce linear word order, and are unable to distinguish between syntactic and non - syntactic co - occurrences.', 'longer n - grams are also noisier and sparser, limiting the range of potential features.', 'in this paper we develop new features for the graph - based mstparser ( mc  #AUTHOR_TAG from the google syntactic ngrams corpus  #AUTHOR_TAG, a collection of stanford dependency subtree counts.', 'these features capture information collated across millions of subtrees produced by a shift - reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.', 'we compare the performance of our syntactic n - gram features against the surface n - gram features of  #TAUTHOR_TAG in - domain on newswire and out - of - domain on the english web treebank ( petrov and mc  #AUTHOR_TAG across conll - style ( lth ) dependencies.', 'we also extend the first - order surface n - gram features to second - order, and compare the utility of web1t and the google books ngram corpus  #AUTHOR_TAG as surface n - gram sources.', 'we find that surface and syntactic n - grams provide statistically significant and complementary accuracy improvements in - and out - of - domain.', '']",5
"[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","['we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after']","['the dependency hold → hearing depicted in figure 1. the final feature encodes the pos tags of the head and argument, directionality, the binned distance between the head and argument,', 'and a bucketed frequency of the syntactic n - gram calculated as per equation 1', ', creating bucket labels from 0 in increments of 5 ( 0, 5, 10, etc. ). additional features for each bucket value', 'up to the maximum are also encoded. we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after each head - argument ambiguity ( see section 3. 2 ). figure 1 depicts the potential context words available the hold → hearing dependency. we experiment with a number of second - order features, mirroring those extracted for surface ngrams in section 3', '. 3. we extract all triple and sibling word and pos structures considered by the parser in the training and test corpora (', 'following the factorization depicted in figure 2 ), and counted their frequency in the syntactic ngrams corpus', '']",5
"[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","['we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after']","['the dependency hold → hearing depicted in figure 1. the final feature encodes the pos tags of the head and argument, directionality, the binned distance between the head and argument,', 'and a bucketed frequency of the syntactic n - gram calculated as per equation 1', ', creating bucket labels from 0 in increments of 5 ( 0, 5, 10, etc. ). additional features for each bucket value', 'up to the maximum are also encoded. we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after each head - argument ambiguity ( see section 3. 2 ). figure 1 depicts the potential context words available the hold → hearing dependency. we experiment with a number of second - order features, mirroring those extracted for surface ngrams in section 3', '. 3. we extract all triple and sibling word and pos structures considered by the parser in the training and test corpora (', 'following the factorization depicted in figure 2 ), and counted their frequency in the syntactic ngrams corpus', '']",5
"['with  #TAUTHOR_TAG  #AUTHOR_TAG,']","['with  #TAUTHOR_TAG  #AUTHOR_TAG,']","['with  #TAUTHOR_TAG  #AUTHOR_TAG,']","['with  #TAUTHOR_TAG  #AUTHOR_TAG, we convert the penn treebank to dependencies using pennconverter 3  #AUTHOR_TAG ( henceforth lth ) and generate pos tags with mx - post  #AUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","['we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after']","['the dependency hold → hearing depicted in figure 1. the final feature encodes the pos tags of the head and argument, directionality, the binned distance between the head and argument,', 'and a bucketed frequency of the syntactic n - gram calculated as per equation 1', ', creating bucket labels from 0 in increments of 5 ( 0, 5, 10, etc. ). additional features for each bucket value', 'up to the maximum are also encoded. we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after each head - argument ambiguity ( see section 3. 2 ). figure 1 depicts the potential context words available the hold → hearing dependency. we experiment with a number of second - order features, mirroring those extracted for surface ngrams in section 3', '. 3. we extract all triple and sibling word and pos structures considered by the parser in the training and test corpora (', 'following the factorization depicted in figure 2 ), and counted their frequency in the syntactic ngrams corpus', '']",3
"[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","['we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between,']","[' #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after']","['the dependency hold → hearing depicted in figure 1. the final feature encodes the pos tags of the head and argument, directionality, the binned distance between the head and argument,', 'and a bucketed frequency of the syntactic n - gram calculated as per equation 1', ', creating bucket labels from 0 in increments of 5 ( 0, 5, 10, etc. ). additional features for each bucket value', 'up to the maximum are also encoded. we also develop paraphrase - style features like those of  #TAUTHOR_TAG based on the most frequently occurring words and pos tags before, in', 'between, and after each head - argument ambiguity ( see section 3. 2 ). figure 1 depicts the potential context words available the hold → hearing dependency. we experiment with a number of second - order features, mirroring those extracted for surface ngrams in section 3', '. 3. we extract all triple and sibling word and pos structures considered by the parser in the training and test corpora (', 'following the factorization depicted in figure 2 ), and counted their frequency in the syntactic ngrams corpus', '']",6
"['on broad coverage realization with openccg  #TAUTHOR_TAG.', 'openccg is a parsing']","['on broad coverage realization with openccg  #TAUTHOR_TAG.', 'openccg is a parsing / generation library']","['for shared task inputs that takes advantage of prior work on broad coverage realization with openccg  #TAUTHOR_TAG.', 'openccg is a parsing / generation library']","['generation challenges 2011 shared task system represents an initial attempt to develop a surface realizer for shared task inputs that takes advantage of prior work on broad coverage realization with openccg  #TAUTHOR_TAG.', 'openccg is a parsing / generation library for combinatory categorial grammar  #AUTHOR_TAG.', 'ccg is a unification - based categorial grammar formalism defined almost entirely in terms of lexical entries that encode sub - categorization as well as syntactic features.', 'openccg implements a grammarbased chart realization algorithm in the tradition of  #AUTHOR_TAG approach to bidirectional processing with unification grammars.', 'the chart realizer takes as input logical forms represented internally using hybrid logic dependency semantics ( hlds ), a dependency - based approach to representing linguistic meaning  #AUTHOR_TAG.', 'to illustrate the input to openccg, consider the semantic dependency graph in figure 1.', '']",7
"['graph nodes, much like hypertagging  #TAUTHOR_TAG.', 'training']","['graph nodes, much like hypertagging  #TAUTHOR_TAG.', 'training']","['an auxiliary maxent classifier to pos tag the graph nodes, much like hypertagging  #TAUTHOR_TAG.', 'training data for']","['the shared task graphs used relations between nodes which were often not easily mappable to native openccg relations, we trained a maxent classifier to tag the most likely relation, as well as an auxiliary maxent classifier to pos tag the graph nodes, much like hypertagging  #TAUTHOR_TAG.', 'training data for the classifier was extracted by comparing each relation between two nodes in the input shared task graph with the corresponding relation in the hlds logical form.', 'in case a labeled relation did not exist in the hlds graph, a norel relation label was assigned.', '']",3
['##d facts about public figures / organizations from newswire or wikipedia articles  #TAUTHOR_TAG ;'],['temporally scoped facts about public figures / organizations from newswire or wikipedia articles  #TAUTHOR_TAG ;'],['##d facts about public figures / organizations from newswire or wikipedia articles  #TAUTHOR_TAG ;'],"['', 'it does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event - related sentences.', ""despite considerable prior work in temporal information extraction, to date state - of - the - art resources are designed for extracting temporally scoped facts about public figures / organizations from newswire or wikipedia articles  #TAUTHOR_TAG ; garrido et [ 11 / 15 / 2008 ] i have noticed some pulling recently and i won't start rads until march."", '[ 11 / 20 / 2008 ] it is sloowwwly healing, so slowly, in fact, that she said she hopes it will be healed by march, when i am supposed to start rads.', 'al., 2012 ).', 'when people are instead communicating informally about their lives, they refer to time more informally and frequently from their personal frame of reference rather than from an impersonal third person frame of reference.', 'for example, they may use their own birthday as a time reference.', 'the proportion of relative ( e. g., "" last week "", "" two days from now "" ), or personal time references in our data is more than one and a half times as high as in newswire and wikipedia.', 'therefore, it is not surprising that there would be difficulty in applying a temporal tagger designed for newswire to social media data  #AUTHOR_TAG.', 'recent behavioral studies  #AUTHOR_TAG demonstrate that user - focused event mentions extracted from social media data can provide a useful timeline - like tool for studying how behavior patterns change over time in response to mentioned events.', 'our research contributes towards automating this work']",0
['##s time - rich sentences that include both the query and some tes  #TAUTHOR_TAG'],['only retrieves time - rich sentences that include both the query and some tes  #TAUTHOR_TAG'],"['##s time - rich sentences that include both the query and some tes  #TAUTHOR_TAG.', 'however, sentences that contain only']","['', 'besides a standard event - time classifier for within - sentence event - time anchoring, we leverage a new source of temporal information to train a constraint - based event - time classifier.', 'previous work only retrieves time - rich sentences that include both the query and some tes  #TAUTHOR_TAG.', 'however, sentences that contain only the event mention but no explicit te can also be informative.', 'for example, the post time ( usually referred to as document creation time or dct ) of the sentence "" metastasis was found in my bone "" might be labeled as being after the "" metastasis "" event date.', 'these dcts impose constraints on the possible event dates, which can be integrated with the event - time classifier, as a variant on related work  #AUTHOR_TAG']",0
"['( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored']","['( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored.', 'for example, "" well, i\'m officially a radiation grad! "" indicates the user has done radiation by']","['time - rich sentences ( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored']","['work only retrieves time - rich sentences ( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored.', 'for example, "" well, i\'m officially a radiation grad! "" indicates the user has done radiation by the time of the post ( dct ).', '"" radiation is not a choice for me. "" indicates the user probably never had radiation.', 'the topic of the sentence can also indicate the temporal relation.', 'for example, before chemotherapy, the users tend to talk about choices of drug combinations.', 'after chemotherapy, they talk about side - effects.', ""this section departs from the above date classifier and instead predicts whether each keyword sentence is posted before or overlap - or - after the user's event date."", 'the goal is to automatically learn time constraints for the event.', 'this task is similar to the sentence event - dct ordering task in tempeval - 2 ( uz  #AUTHOR_TAG.', ""we create training examples by computing the temporal relation between the dct and the user's gold event date."", 'if the user has not reported an event date, the label should be unknown.', '']",0
"[' #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 /']","[' #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 / ( ( 1 + | d | ) ) where d is the difference between the values in years.', 'we choose a much stricter evaluation metric']","['of the gold dates.', 'in previous work  #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 /']","['extracted date is only considered correct if it completely matches the gold date.', 'for less than 4 % of users, we have multiple dates for the same event ( e. g., a user had a mastectomy twice ).', 'similar to the evaluation metric in a previous study, in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates.', 'in previous work  #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 / ( ( 1 + | d | ) ) where d is the difference between the values in years.', 'we choose a much stricter evaluation metric because we need a precise event date to study user behavior changes']",0
['##s time - rich sentences that include both the query and some tes  #TAUTHOR_TAG'],['only retrieves time - rich sentences that include both the query and some tes  #TAUTHOR_TAG'],"['##s time - rich sentences that include both the query and some tes  #TAUTHOR_TAG.', 'however, sentences that contain only']","['', 'besides a standard event - time classifier for within - sentence event - time anchoring, we leverage a new source of temporal information to train a constraint - based event - time classifier.', 'previous work only retrieves time - rich sentences that include both the query and some tes  #TAUTHOR_TAG.', 'however, sentences that contain only the event mention but no explicit te can also be informative.', 'for example, the post time ( usually referred to as document creation time or dct ) of the sentence "" metastasis was found in my bone "" might be labeled as being after the "" metastasis "" event date.', 'these dcts impose constraints on the possible event dates, which can be integrated with the event - time classifier, as a variant on related work  #AUTHOR_TAG']",1
"['( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored']","['( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored.', 'for example, "" well, i\'m officially a radiation grad! "" indicates the user has done radiation by']","['time - rich sentences ( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored']","['work only retrieves time - rich sentences ( i. e., date sentences )  #TAUTHOR_TAG.', 'however, keyword sentences can inform temporal constraints for events and therefore should not be ignored.', 'for example, "" well, i\'m officially a radiation grad! "" indicates the user has done radiation by the time of the post ( dct ).', '"" radiation is not a choice for me. "" indicates the user probably never had radiation.', 'the topic of the sentence can also indicate the temporal relation.', 'for example, before chemotherapy, the users tend to talk about choices of drug combinations.', 'after chemotherapy, they talk about side - effects.', ""this section departs from the above date classifier and instead predicts whether each keyword sentence is posted before or overlap - or - after the user's event date."", 'the goal is to automatically learn time constraints for the event.', 'this task is similar to the sentence event - dct ordering task in tempeval - 2 ( uz  #AUTHOR_TAG.', ""we create training examples by computing the temporal relation between the dct and the user's gold event date."", 'if the user has not reported an event date, the label should be unknown.', '']",1
"['##ing task  #TAUTHOR_TAG.', '']","['shared task and timelining task  #TAUTHOR_TAG.', '']","['##ing task  #TAUTHOR_TAG.', '']","['work on te extraction has focused mainly on newswire text  #AUTHOR_TAG.', 'this paper presents a rule - based te extractor that identifies and resolves a higher percentage of nonstandard tes than earlier state - of - art temporal taggers.', 'our task is closest to the temporal slot filling track in the tac - kbp 2011 shared task and timelining task  #TAUTHOR_TAG.', 'their goal was to extract the temporal bounds of event relations.', 'our task has two key differences.', 'first, they used newswire, wikipedia and blogs as data sources from which they extract temporal bounds of facts found in wikipedia infoboxes.', 'second, in the kbp task, the set of gold event relations are provided as input, so that the task is only to identify a date for an event that is guaranteed to have been mentioned.', 'in our task, we provide a set of potential events.', ""however, most of the candidate events won't have ever been reported within a user's posting history."", 'temporal constraints have proven to be useful for producing a globally consistent timeline.', 'in most temporal relation bound extraction systems, the constraints are included as input rather than learned by the system  #AUTHOR_TAG.', 'a notable exception is mccloskyet al. ( 2012 ) who developed an approach to learning constraints such as that people cannot attend school if they have not been born yet.', 'a notable characteristic of our task is that constraints are softer.', 'diseases may occur in very different ways across patients.', 'recurring illnesses falsely appear to have an unpredictable order.', 'thus, there can be no universal logical constraints on the order of cancer events.', 'our approach to using temporal constraints is a variant on previously published approaches.', ' #AUTHOR_TAG made use of dct ( document creation time ) as well, however, they have assumed the dct is within the time - range of the event stated in the document, which is often not true in our data.', ' #AUTHOR_TAG utilized the withinsentence time - dct relation to learn constrains for predicting dct.', 'we learn the event - dct relations to produce constrains for the event date.', 'domly picked one post from each of 1, 000 randomly selected users.', 'we used this sampling technique because each user tends to use a narrow range of date expression forms.', 'from these posts, we manually extracted 601 tes and resolved them to a specific month and year or just year if the month was not mentioned.', 'events not reported to have occurred were annotated as "" unknown "".', 'our corpus for event date extraction consists of the complete posting history of 300 users that were randomly drawn']",3
"['2 ( uz  #AUTHOR_TAG.', 'features for the classifier include many of those in  #TAUTHOR_TAG : namely, event']","['( uz  #AUTHOR_TAG.', 'features for the classifier include many of those in  #TAUTHOR_TAG : namely, event']","['2 ( uz  #AUTHOR_TAG.', 'features for the classifier include many of those in  #TAUTHOR_TAG : namely, event keyword']","['train a maxent classifier to predict the temporal relationship between the retrieved te and the event date as overlap or no - overlap, similar to the within - sentence event - time anchoring task in tempeval - 2 ( uz  #AUTHOR_TAG.', 'features for the classifier include many of those in  #TAUTHOR_TAG : namely, event keyword and its dominant verb, verb and preposition that dominate te, dependency path between te and keyword and its length, unigram and bigram word and pos features.', 'new features include the event - subject, negative and modality features.', ""in online support groups, users not only tell stories about themselves, they also share other patients'stories ( as shown in figure 1 )."", 'so we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its pos tag.', 'modality features include the appearance of modals before the event keyword ( e. g., may, might ).', 'negative features include the presence / absence of negative words ( e. g., no, never ).', 'these two features indicate a hypothetical or counter - factual expression of the event.', 'to calculate the likelihood of a candidate date for an event, we need to aggregate the hard decisions from the classifier.', ""let ds u be the set of the user's date sentences, let d u be the set of dates resolved from each te."", 'we represent a maxent classifier by p relation ( r | t, ds ) for a candidate date t in date sentence ds and possible relation r = { overlap, no - overlap }. we map the distribution over relations to a distribution over dates by defining p datesentence ( t | ds u ) :', 'we refer to this model as the date classifier']",3
"[' #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 /']","[' #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 / ( ( 1 + | d | ) ) where d is the difference between the values in years.', 'we choose a much stricter evaluation metric']","['of the gold dates.', 'in previous work  #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 /']","['extracted date is only considered correct if it completely matches the gold date.', 'for less than 4 % of users, we have multiple dates for the same event ( e. g., a user had a mastectomy twice ).', 'similar to the evaluation metric in a previous study, in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates.', 'in previous work  #TAUTHOR_TAG ;, the evaluation metric score is defined as 1 / ( ( 1 + | d | ) ) where d is the difference between the values in years.', 'we choose a much stricter evaluation metric because we need a precise event date to study user behavior changes']",4
"['.,  #TAUTHOR_TAG employs an']",[' #TAUTHOR_TAG employs an'],"['.,  #TAUTHOR_TAG employs an attention based lstm']",[' #TAUTHOR_TAG'],0
"['.,  #TAUTHOR_TAG employs an']",[' #TAUTHOR_TAG employs an'],"['.,  #TAUTHOR_TAG employs an attention based lstm']",[' #TAUTHOR_TAG'],1
"['s i following  #TAUTHOR_TAG, where s i ∈']","['s i following  #TAUTHOR_TAG, where s i ∈']","['s i following  #TAUTHOR_TAG, where s i ∈']","['architecture of pan is illustrated in figure1.', 'supposing that there are n sentences containing entity e, i. e., s e = { s 1, s 2,..., s n }, and t e is the automatically labeled types based on kbs.', 'firstly pan employs lstm to generate representations of sentences s i following  #TAUTHOR_TAG, where s i ∈ r d is the semantic representation of s i, i ∈ { 1, 2,..., n }. afterwards, we build path - based attention α i, t over sentences s i for each type t ∈ t e, which is expected to focus on relevant sentences to type t. then, the representation of sentence set s e for type t, denoted by s e, t ∈ r d, is calculated through weighted sum of vectors of sentences.', '']",5
"['of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in']","['of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in table1.', '']","['figer ( gold ), and the training dataset of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in']","['are carried on two widely used datasets ontonotes and figer ( gold ), and the training dataset of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in table1.', '']",5
"['of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in']","['of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in table1.', '']","['figer ( gold ), and the training dataset of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in']","['are carried on two widely used datasets ontonotes and figer ( gold ), and the training dataset of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in table1.', '']",5
"['of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in']","['of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in table1.', '']","['figer ( gold ), and the training dataset of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in']","['are carried on two widely used datasets ontonotes and figer ( gold ), and the training dataset of ontonotes is noisy compared to figer ( gold )  #TAUTHOR_TAG.', 'the statistics of the datasets are listed in table1.', '']",4
"['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['dirichlet allocation ( "" lda "" :  #AUTHOR_TAG ) is an approach to document clustering, in which "" topics "" ( multinomial distributions over terms ) and topic allocations ( multinomial distributions over topics per document ) are jointly learned.', 'when the topic model output is to be presented to humans, optimisation of the number of topics is a non - trivial problem.', 'in the seminal paper of  #AUTHOR_TAG, e. g., the authors showed thatcontrary to expectations - extrinsically measured topic coherence correlates negatively with model perplexity.', 'they introduced the word intrusion task, whereby a randomly selected "" intruder "" word is injected into the top - n words of a given topic and users are asked to identify the intruder word.', 'low reliability in identifying the intruder word indicates low coherence ( and vice versa ), based on the intuition that the more coherent the topic, the more clearly the intruder word should be an outlier.', 'since then, several methodologies have been introduced to automate the evaluation of topic coherence.', ' #AUTHOR_TAG found that aggregate pairwise pmi scores over the top - n topic words correlated well with human ratings.', ' #AUTHOR_TAG proposed replacing pmi with conditional probability based on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to automate the word intrusion task directly.', 'their results also reveal the differences between these methodologies in their assessment of topic coherence.', 'a hyper - parameter in all these methodologies is the number of topic words, or its cardinality.', 'these methodologies evaluate coherence over the top - n topic words, where n is selected arbitrarily : for  #AUTHOR_TAG, n = 5, whereas for  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, n = 10.', 'the germ of this paper came when using the automatic word intrusion methodology  #TAUTHOR_TAG, and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.', 'this forms the kernel of this paper : to better understand the impact of the topic cardinality hyper - parameter on the evaluation of topic coherence.', 'to investigate this, we develop a new dataset with human - annotated coherence judgements for a range of cardinality settings ( n = { 5, 10, 15, 20 } ).', 'we experiment with the automatic word intrusion  #TAUTHOR_TAG and discover that correlation with human ratings decreases systematically as cardinality increases.', 'we also test the pmi methodology  #AUTHOR_TAG and make the same observation.', 'to remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.', 'this has broad implications for topic']",0
"['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['dirichlet allocation ( "" lda "" :  #AUTHOR_TAG ) is an approach to document clustering, in which "" topics "" ( multinomial distributions over terms ) and topic allocations ( multinomial distributions over topics per document ) are jointly learned.', 'when the topic model output is to be presented to humans, optimisation of the number of topics is a non - trivial problem.', 'in the seminal paper of  #AUTHOR_TAG, e. g., the authors showed thatcontrary to expectations - extrinsically measured topic coherence correlates negatively with model perplexity.', 'they introduced the word intrusion task, whereby a randomly selected "" intruder "" word is injected into the top - n words of a given topic and users are asked to identify the intruder word.', 'low reliability in identifying the intruder word indicates low coherence ( and vice versa ), based on the intuition that the more coherent the topic, the more clearly the intruder word should be an outlier.', 'since then, several methodologies have been introduced to automate the evaluation of topic coherence.', ' #AUTHOR_TAG found that aggregate pairwise pmi scores over the top - n topic words correlated well with human ratings.', ' #AUTHOR_TAG proposed replacing pmi with conditional probability based on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to automate the word intrusion task directly.', 'their results also reveal the differences between these methodologies in their assessment of topic coherence.', 'a hyper - parameter in all these methodologies is the number of topic words, or its cardinality.', 'these methodologies evaluate coherence over the top - n topic words, where n is selected arbitrarily : for  #AUTHOR_TAG, n = 5, whereas for  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, n = 10.', 'the germ of this paper came when using the automatic word intrusion methodology  #TAUTHOR_TAG, and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.', 'this forms the kernel of this paper : to better understand the impact of the topic cardinality hyper - parameter on the evaluation of topic coherence.', 'to investigate this, we develop a new dataset with human - annotated coherence judgements for a range of cardinality settings ( n = { 5, 10, 15, 20 } ).', 'we experiment with the automatic word intrusion  #TAUTHOR_TAG and discover that correlation with human ratings decreases systematically as cardinality increases.', 'we also test the pmi methodology  #AUTHOR_TAG and make the same observation.', 'to remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.', 'this has broad implications for topic']",0
"['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['dirichlet allocation ( "" lda "" :  #AUTHOR_TAG ) is an approach to document clustering, in which "" topics "" ( multinomial distributions over terms ) and topic allocations ( multinomial distributions over topics per document ) are jointly learned.', 'when the topic model output is to be presented to humans, optimisation of the number of topics is a non - trivial problem.', 'in the seminal paper of  #AUTHOR_TAG, e. g., the authors showed thatcontrary to expectations - extrinsically measured topic coherence correlates negatively with model perplexity.', 'they introduced the word intrusion task, whereby a randomly selected "" intruder "" word is injected into the top - n words of a given topic and users are asked to identify the intruder word.', 'low reliability in identifying the intruder word indicates low coherence ( and vice versa ), based on the intuition that the more coherent the topic, the more clearly the intruder word should be an outlier.', 'since then, several methodologies have been introduced to automate the evaluation of topic coherence.', ' #AUTHOR_TAG found that aggregate pairwise pmi scores over the top - n topic words correlated well with human ratings.', ' #AUTHOR_TAG proposed replacing pmi with conditional probability based on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to automate the word intrusion task directly.', 'their results also reveal the differences between these methodologies in their assessment of topic coherence.', 'a hyper - parameter in all these methodologies is the number of topic words, or its cardinality.', 'these methodologies evaluate coherence over the top - n topic words, where n is selected arbitrarily : for  #AUTHOR_TAG, n = 5, whereas for  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, n = 10.', 'the germ of this paper came when using the automatic word intrusion methodology  #TAUTHOR_TAG, and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.', 'this forms the kernel of this paper : to better understand the impact of the topic cardinality hyper - parameter on the evaluation of topic coherence.', 'to investigate this, we develop a new dataset with human - annotated coherence judgements for a range of cardinality settings ( n = { 5, 10, 15, 20 } ).', 'we experiment with the automatic word intrusion  #TAUTHOR_TAG and discover that correlation with human ratings decreases systematically as cardinality increases.', 'we also test the pmi methodology  #AUTHOR_TAG and make the same observation.', 'to remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.', 'this has broad implications for topic']",0
"['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset for this experiment.', 'following  #TAUTHOR_TAG, we use two domains : ( 1 ) wiki, a collection of 3. 3 million english wikipedia articles ( retrieved november 28th 2009 ) ; and ( 2 ) news, a collection of 1. 2 million new york times articles from 1994 to 2004 ( english gigaword ).', 'we sub - sample approximately 50m tokens ( 100k and 50k articles for wiki and news respectively ) from both domains to create two smaller document collections.', 'we then generate 300 lda topics for each of the sub - sampled collection.', '2 there are two primary approaches to assessing topic coherence : ( 1 ) via word intrusion ( chang et ( 2 ) by directly measuring observed coherence  #TAUTHOR_TAG.', 'with the first method,  #AUTHOR_TAG injects an intruder word into the top - 5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.', 'in preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e. g. when n = 20.', 'as such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.', '3 to collect the coherence judgements, we used amazon mechanical turk and asked turkers to rate topics in terms of coherence using a 3 - point ordinal scale, where 1 indicates incoherent and 3 very coherent  #AUTHOR_TAG.', 'for each topic ( 600 topics in total ) we experiment with 4 cardinality settings : n = { 5, 10, 15, 20 }. for example, for n = 5, we display the top - 5 topic words for coherence judgement.', 'for annotation quality control, we embed a bad topic generated using random words into each hit.', 'workers who fail to consistently rate these bad topics low are filtered out.', '4 on average, we collected approximately 9 ratings per topic in each cardinality setting ( post - filtered ), from which we generate the gold standard via the arithmetic mean.', 'to understand the impact of cardinality ( n ) on topic coherence, we analyse : ( a ) the mean topic rating for each n ( table 1 ), and ( b ) the pairwise pearson correlation coefficient']",0
"['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset for this experiment.', 'following  #TAUTHOR_TAG, we use two domains : ( 1 ) wiki, a collection of 3. 3 million english wikipedia articles ( retrieved november 28th 2009 ) ; and ( 2 ) news, a collection of 1. 2 million new york times articles from 1994 to 2004 ( english gigaword ).', 'we sub - sample approximately 50m tokens ( 100k and 50k articles for wiki and news respectively ) from both domains to create two smaller document collections.', 'we then generate 300 lda topics for each of the sub - sampled collection.', '2 there are two primary approaches to assessing topic coherence : ( 1 ) via word intrusion ( chang et ( 2 ) by directly measuring observed coherence  #TAUTHOR_TAG.', 'with the first method,  #AUTHOR_TAG injects an intruder word into the top - 5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.', 'in preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e. g. when n = 20.', 'as such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.', '3 to collect the coherence judgements, we used amazon mechanical turk and asked turkers to rate topics in terms of coherence using a 3 - point ordinal scale, where 1 indicates incoherent and 3 very coherent  #AUTHOR_TAG.', 'for each topic ( 600 topics in total ) we experiment with 4 cardinality settings : n = { 5, 10, 15, 20 }. for example, for n = 5, we display the top - 5 topic words for coherence judgement.', 'for annotation quality control, we embed a bad topic generated using random words into each hit.', 'workers who fail to consistently rate these bad topics low are filtered out.', '4 on average, we collected approximately 9 ratings per topic in each cardinality setting ( post - filtered ), from which we generate the gold standard via the arithmetic mean.', 'to understand the impact of cardinality ( n ) on topic coherence, we analyse : ( a ) the mean topic rating for each n ( table 1 ), and ( b ) the pairwise pearson correlation coefficient']",0
"['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top']","['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top - n words,']","['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top - n words,']","['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top - n words, and trains a support vector regression model to rank the words.', 'the top - ranked word is then selected as the predicted intruder word.', 'note that even though it is supervised, no manual annotation is required as the identity of the true intruder word is known.', 'following the original paper, we use as features normalised pmi ( npmi ) and two conditional probabilities ( cp1 and cp2 ), computed over the full collection of wiki ( 3. 3 million articles ) and news ( 1. 2 million articles ), respectively.', 'we use 10 - fold cross validation to predict the intruder words for all topics.', 'to generate an intruder for a topic, we select a random word that has a low probability ( p < 0. 0005 ) in the topic but high probability ( p > 0. 01 ) in another topic.', 'we repeat this ten times to generate 10 different intruder words for a topic.', 'the 4 cardinalities of a given topic share the same set of intruder words.', '']",0
"['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit']","['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit']","['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit']","['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the npmi methodology.', 'we compute the topic coherence using the full collection of wiki and news, respectively, for varying n.', 'results are presented in table 4.', 'the in - domain features perform much worse, especially for the wiki topics.', 'npmi assigns very high scores to several incoherent topics, thereby reducing the correlation to almost zero.', 'these topics consist predominantly of wikipedia markup tags, and the high association is due to word statistics idiosyncratic to the collection.', 'once again, aggregating the topic coherence over multiple n values boosts results further.', 'the correlations using aggregation and out - of - domain features again produce the best results for both wiki and news.', 'it is important to note that, while these findings were established based on manual annotation of topic coherence, for practical applications, topic coherence would be calculated in a fully - unsupervised manner ( averaged over different topic cardinalities ), without the use of manual annotations']",0
"['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['dirichlet allocation ( "" lda "" :  #AUTHOR_TAG ) is an approach to document clustering, in which "" topics "" ( multinomial distributions over terms ) and topic allocations ( multinomial distributions over topics per document ) are jointly learned.', 'when the topic model output is to be presented to humans, optimisation of the number of topics is a non - trivial problem.', 'in the seminal paper of  #AUTHOR_TAG, e. g., the authors showed thatcontrary to expectations - extrinsically measured topic coherence correlates negatively with model perplexity.', 'they introduced the word intrusion task, whereby a randomly selected "" intruder "" word is injected into the top - n words of a given topic and users are asked to identify the intruder word.', 'low reliability in identifying the intruder word indicates low coherence ( and vice versa ), based on the intuition that the more coherent the topic, the more clearly the intruder word should be an outlier.', 'since then, several methodologies have been introduced to automate the evaluation of topic coherence.', ' #AUTHOR_TAG found that aggregate pairwise pmi scores over the top - n topic words correlated well with human ratings.', ' #AUTHOR_TAG proposed replacing pmi with conditional probability based on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to automate the word intrusion task directly.', 'their results also reveal the differences between these methodologies in their assessment of topic coherence.', 'a hyper - parameter in all these methodologies is the number of topic words, or its cardinality.', 'these methodologies evaluate coherence over the top - n topic words, where n is selected arbitrarily : for  #AUTHOR_TAG, n = 5, whereas for  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, n = 10.', 'the germ of this paper came when using the automatic word intrusion methodology  #TAUTHOR_TAG, and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.', 'this forms the kernel of this paper : to better understand the impact of the topic cardinality hyper - parameter on the evaluation of topic coherence.', 'to investigate this, we develop a new dataset with human - annotated coherence judgements for a range of cardinality settings ( n = { 5, 10, 15, 20 } ).', 'we experiment with the automatic word intrusion  #TAUTHOR_TAG and discover that correlation with human ratings decreases systematically as cardinality increases.', 'we also test the pmi methodology  #AUTHOR_TAG and make the same observation.', 'to remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.', 'this has broad implications for topic']",1
"['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to']","['dirichlet allocation ( "" lda "" :  #AUTHOR_TAG ) is an approach to document clustering, in which "" topics "" ( multinomial distributions over terms ) and topic allocations ( multinomial distributions over topics per document ) are jointly learned.', 'when the topic model output is to be presented to humans, optimisation of the number of topics is a non - trivial problem.', 'in the seminal paper of  #AUTHOR_TAG, e. g., the authors showed thatcontrary to expectations - extrinsically measured topic coherence correlates negatively with model perplexity.', 'they introduced the word intrusion task, whereby a randomly selected "" intruder "" word is injected into the top - n words of a given topic and users are asked to identify the intruder word.', 'low reliability in identifying the intruder word indicates low coherence ( and vice versa ), based on the intuition that the more coherent the topic, the more clearly the intruder word should be an outlier.', 'since then, several methodologies have been introduced to automate the evaluation of topic coherence.', ' #AUTHOR_TAG found that aggregate pairwise pmi scores over the top - n topic words correlated well with human ratings.', ' #AUTHOR_TAG proposed replacing pmi with conditional probability based on co - document frequency.', ' #AUTHOR_TAG showed that coherence can be measured by a classical distributional similarity approach.', 'more recently,  #TAUTHOR_TAG proposed a methodology to automate the word intrusion task directly.', 'their results also reveal the differences between these methodologies in their assessment of topic coherence.', 'a hyper - parameter in all these methodologies is the number of topic words, or its cardinality.', 'these methodologies evaluate coherence over the top - n topic words, where n is selected arbitrarily : for  #AUTHOR_TAG, n = 5, whereas for  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, n = 10.', 'the germ of this paper came when using the automatic word intrusion methodology  #TAUTHOR_TAG, and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.', 'this forms the kernel of this paper : to better understand the impact of the topic cardinality hyper - parameter on the evaluation of topic coherence.', 'to investigate this, we develop a new dataset with human - annotated coherence judgements for a range of cardinality settings ( n = { 5, 10, 15, 20 } ).', 'we experiment with the automatic word intrusion  #TAUTHOR_TAG and discover that correlation with human ratings decreases systematically as cardinality increases.', 'we also test the pmi methodology  #AUTHOR_TAG and make the same observation.', 'to remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.', 'this has broad implications for topic']",5
"['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset for this experiment.', 'following  #TAUTHOR_TAG, we use two domains : ( 1 ) wiki, a collection of 3. 3 million english wikipedia articles ( retrieved november 28th 2009 ) ; and ( 2 ) news, a collection of 1. 2 million new york times articles from 1994 to 2004 ( english gigaword ).', 'we sub - sample approximately 50m tokens ( 100k and 50k articles for wiki and news respectively ) from both domains to create two smaller document collections.', 'we then generate 300 lda topics for each of the sub - sampled collection.', '2 there are two primary approaches to assessing topic coherence : ( 1 ) via word intrusion ( chang et ( 2 ) by directly measuring observed coherence  #TAUTHOR_TAG.', 'with the first method,  #AUTHOR_TAG injects an intruder word into the top - 5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.', 'in preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e. g. when n = 20.', 'as such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.', '3 to collect the coherence judgements, we used amazon mechanical turk and asked turkers to rate topics in terms of coherence using a 3 - point ordinal scale, where 1 indicates incoherent and 3 very coherent  #AUTHOR_TAG.', 'for each topic ( 600 topics in total ) we experiment with 4 cardinality settings : n = { 5, 10, 15, 20 }. for example, for n = 5, we display the top - 5 topic words for coherence judgement.', 'for annotation quality control, we embed a bad topic generated using random words into each hit.', 'workers who fail to consistently rate these bad topics low are filtered out.', '4 on average, we collected approximately 9 ratings per topic in each cardinality setting ( post - filtered ), from which we generate the gold standard via the arithmetic mean.', 'to understand the impact of cardinality ( n ) on topic coherence, we analyse : ( a ) the mean topic rating for each n ( table 1 ), and ( b ) the pairwise pearson correlation coefficient']",5
"['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset']","['of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting (']","['examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.', 'although there are existing datasets with human - annotated coherence scores  #TAUTHOR_TAG, these topics were annotated using a fixed cardinality setting ( e. g. 5 or 10 ).', 'we thus develop a new dataset for this experiment.', 'following  #TAUTHOR_TAG, we use two domains : ( 1 ) wiki, a collection of 3. 3 million english wikipedia articles ( retrieved november 28th 2009 ) ; and ( 2 ) news, a collection of 1. 2 million new york times articles from 1994 to 2004 ( english gigaword ).', 'we sub - sample approximately 50m tokens ( 100k and 50k articles for wiki and news respectively ) from both domains to create two smaller document collections.', 'we then generate 300 lda topics for each of the sub - sampled collection.', '2 there are two primary approaches to assessing topic coherence : ( 1 ) via word intrusion ( chang et ( 2 ) by directly measuring observed coherence  #TAUTHOR_TAG.', 'with the first method,  #AUTHOR_TAG injects an intruder word into the top - 5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.', 'in preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e. g. when n = 20.', 'as such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.', '3 to collect the coherence judgements, we used amazon mechanical turk and asked turkers to rate topics in terms of coherence using a 3 - point ordinal scale, where 1 indicates incoherent and 3 very coherent  #AUTHOR_TAG.', 'for each topic ( 600 topics in total ) we experiment with 4 cardinality settings : n = { 5, 10, 15, 20 }. for example, for n = 5, we display the top - 5 topic words for coherence judgement.', 'for annotation quality control, we embed a bad topic generated using random words into each hit.', 'workers who fail to consistently rate these bad topics low are filtered out.', '4 on average, we collected approximately 9 ratings per topic in each cardinality setting ( post - filtered ), from which we generate the gold standard via the arithmetic mean.', 'to understand the impact of cardinality ( n ) on topic coherence, we analyse : ( a ) the mean topic rating for each n ( table 1 ), and ( b ) the pairwise pearson correlation coefficient']",5
"['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit']","['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit']","['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit']","['other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top - n words.', ' #AUTHOR_TAG found pmi to be the best association measure, and later studies  #TAUTHOR_TAG found that normalised pmi ( npmi :  #AUTHOR_TAG ) improves pmi further.', 'to see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the npmi methodology.', 'we compute the topic coherence using the full collection of wiki and news, respectively, for varying n.', 'results are presented in table 4.', 'the in - domain features perform much worse, especially for the wiki topics.', 'npmi assigns very high scores to several incoherent topics, thereby reducing the correlation to almost zero.', 'these topics consist predominantly of wikipedia markup tags, and the high association is due to word statistics idiosyncratic to the collection.', 'once again, aggregating the topic coherence over multiple n values boosts results further.', 'the correlations using aggregation and out - of - domain features again produce the best results for both wiki and news.', 'it is important to note that, while these findings were established based on manual annotation of topic coherence, for practical applications, topic coherence would be calculated in a fully - unsupervised manner ( averaged over different topic cardinalities ), without the use of manual annotations']",5
"['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top']","['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top - n words,']","['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top - n words,']","['proposed an automated approach to the word intrusion task.', 'the methodology computes pairwise word association features for the top - n words, and trains a support vector regression model to rank the words.', 'the top - ranked word is then selected as the predicted intruder word.', 'note that even though it is supervised, no manual annotation is required as the identity of the true intruder word is known.', 'following the original paper, we use as features normalised pmi ( npmi ) and two conditional probabilities ( cp1 and cp2 ), computed over the full collection of wiki ( 3. 3 million articles ) and news ( 1. 2 million articles ), respectively.', 'we use 10 - fold cross validation to predict the intruder words for all topics.', 'to generate an intruder for a topic, we select a random word that has a low probability ( p < 0. 0005 ) in the topic but high probability ( p > 0. 01 ) in another topic.', 'we repeat this ten times to generate 10 different intruder words for a topic.', 'the 4 cardinalities of a given topic share the same set of intruder words.', '']",3
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",0
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",0
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",0
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in']","['few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories  #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in their tree - to - string model which integrated with forest 1 - to - string into fully lexicalized rules, non - lexicalized rules and partial lexicalized rules according to the lexicalization levels.', 'as an extension,  #TAUTHOR_TAG.', 'the  #TAUTHOR_TAG stands for the rules which have at least two non - terminal leaf nodes with inverted order in the source and target side.', 'and  #TAUTHOR_TAG refers to the rules having at least one non - terminal leaf node between two terminal leaf nodes.', '( de  #AUTHOR_TAG made an illuminating breakdown of the different kinds of rules.', '']",0
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in']","['few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories  #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in their tree - to - string model which integrated with forest 1 - to - string into fully lexicalized rules, non - lexicalized rules and partial lexicalized rules according to the lexicalization levels.', 'as an extension,  #TAUTHOR_TAG.', 'the  #TAUTHOR_TAG stands for the rules which have at least two non - terminal leaf nodes with inverted order in the source and target side.', 'and  #TAUTHOR_TAG refers to the rules having at least one non - terminal leaf node between two terminal leaf nodes.', '( de  #AUTHOR_TAG made an illuminating breakdown of the different kinds of rules.', '']",0
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in']","['few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories  #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in their tree - to - string model which integrated with forest 1 - to - string into fully lexicalized rules, non - lexicalized rules and partial lexicalized rules according to the lexicalization levels.', 'as an extension,  #TAUTHOR_TAG.', 'the  #TAUTHOR_TAG stands for the rules which have at least two non - terminal leaf nodes with inverted order in the source and target side.', 'and  #TAUTHOR_TAG refers to the rules having at least one non - terminal leaf node between two terminal leaf nodes.', '( de  #AUTHOR_TAG made an illuminating breakdown of the different kinds of rules.', '']",0
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in']","['few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories  #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in their tree - to - string model which integrated with forest 1 - to - string into fully lexicalized rules, non - lexicalized rules and partial lexicalized rules according to the lexicalization levels.', 'as an extension,  #TAUTHOR_TAG.', 'the  #TAUTHOR_TAG stands for the rules which have at least two non - terminal leaf nodes with inverted order in the source and target side.', 'and  #TAUTHOR_TAG refers to the rules having at least one non - terminal leaf node between two terminal leaf nodes.', '( de  #AUTHOR_TAG made an illuminating breakdown of the different kinds of rules.', '']",0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],0
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",1
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",1
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in']","['few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories  #TAUTHOR_TAG.', ' #AUTHOR_TAG differentiated the rules in their tree - to - string model which integrated with forest 1 - to - string into fully lexicalized rules, non - lexicalized rules and partial lexicalized rules according to the lexicalization levels.', 'as an extension,  #TAUTHOR_TAG.', 'the  #TAUTHOR_TAG stands for the rules which have at least two non - terminal leaf nodes with inverted order in the source and target side.', 'and  #TAUTHOR_TAG refers to the rules having at least one non - terminal leaf node between two terminal leaf nodes.', '( de  #AUTHOR_TAG made an illuminating breakdown of the different kinds of rules.', '']",1
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],1
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],1
"['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","['proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'the basic motivation behind  #TAUTHOR_TAG is']","['', 'to overcome these limitations, many syntaxbased smt models have been proposed  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']",5
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],5
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],5
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],4
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],4
[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG nt - t'],[' #TAUTHOR_TAG'],4
['presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG'],['presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG'],"['presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG.', 'taking  #TAUTHOR_TAG as the experimental platform,']","['this paper, we present a refined rule classification system.', 'based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization.', 'especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules.', 'this novel classification system may supports the smt research community with some helpful references.', 'in the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG.', 'taking  #TAUTHOR_TAG as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories.', 'what is more important, the contribution of each rule category can be evaluated seriatim.', 'furthermore, which kinds of rules are preferentially applied in the 1 - best decoding can be studied.', 'all these investigations could reveal very useful information for the optimization of rule extraction and the improvement of the computational models for synchronous grammar based machine translation']",2
['presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG'],['presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG'],"['presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG.', 'taking  #TAUTHOR_TAG as the experimental platform,']","['this paper, we present a refined rule classification system.', 'based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization.', 'especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules.', 'this novel classification system may supports the smt research community with some helpful references.', 'in the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in  #AUTHOR_TAG or in  #TAUTHOR_TAG.', 'taking  #TAUTHOR_TAG as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories.', 'what is more important, the contribution of each rule category can be evaluated seriatim.', 'furthermore, which kinds of rules are preferentially applied in the 1 - best decoding can be studied.', 'all these investigations could reveal very useful information for the optimization of rule extraction and the improvement of the computational models for synchronous grammar based machine translation']",2
['of non - native productions ;  #TAUTHOR_TAG explicitly argue'],['of non - native productions ;  #TAUTHOR_TAG explicitly argue'],['advanced learners.  #AUTHOR_TAG demonstrate that improving fluency ( closely linked to the use of linguistic formulas ) is more important than improving strict grammaticality with respect to native speaker judgments of non - native productions ;  #TAUTHOR_TAG explicitly argue'],"['beyond noun phrases, however, seems relatively limited  #AUTHOR_TAG.  #AUTHOR_TAG ; developed a heuristic method intended for general fs extraction in larger corpora, first using conditional probabilities to do an initial ( single pass ) coarse - grained segmentation of the corpus, followed by a pass through the resulting vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. the work of  #AUTHOR_TAG is an example of an unsupervised approach which does not use association measures : it extends the bayesian word segmentation approach of  #AUTHOR_TAG to multiword tokenization, applying', 'a generative dirichlet process model which jointly constructs a segmentation of the corpus and a corresponding multiword vocabulary. other research in mwes has tended to be rather focused on particular syntactic patterns such as verbnoun combinations  #AUTHOR_TAG. the system of  #AUTHOR_TAG a ) distinguishes a full range of mwe sequences in the english web treebank, including gapped expressions, using a supervised sequence', 'tagging model. though, in theory, automatic lexical resources could be a useful addition to the schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success  #AUTHOR_TAG. the motivations for building lexicons of fs naturally overlap with those for mwe :', 'models of distributional semantics, in particular, can benefit from sensitivity to multiword units  #AUTHOR_TAG, as can parsing  #AUTHOR_TAG and topic models  #AUTHOR_TAG. one major motivation for looking beyond mwes is the ability to carry', 'out broader linguistic analyses. within corpus linguistics, multiword sequences have been studied in the form of lexical bundles  #AUTHOR_TAG, which are simply n - grams that occur above a certain frequency threshold. like fs, lexical bundles generally involve larger phrasal chunks that would be missed by traditional mwe extraction, and so research in this area has tended to focus on how particular', 'formulaic phrases ( e. g., if you look at ) are indicative of particular genres ( e. g., university lectures ). lexical bundles have been applied, in particular, to learner language : for example,  #AUTHOR_TAG show that non - native student writers use a severely restricted range of lexical bundle types, and tend', 'to overuse those types, while  #AUTHOR_TAG investigate the role of proficiency, demonstrating that intermediate learners underuse lower - frequency bigrams and overuse high - frequency bigrams relative to advanced learners.  #AUTHOR_TAG demonstrate that improving fluency ( closely linked to the use of linguistic formulas ) is more important than improving strict grammaticality with respect to native speaker judgments of non - native productions ;  #TAUTHOR_TAG explicitly argue for fs lexicons as a way to identify, track, and improve learner proficiency']",0
['of non - native productions ;  #TAUTHOR_TAG explicitly argue'],['of non - native productions ;  #TAUTHOR_TAG explicitly argue'],['advanced learners.  #AUTHOR_TAG demonstrate that improving fluency ( closely linked to the use of linguistic formulas ) is more important than improving strict grammaticality with respect to native speaker judgments of non - native productions ;  #TAUTHOR_TAG explicitly argue'],"['beyond noun phrases, however, seems relatively limited  #AUTHOR_TAG.  #AUTHOR_TAG ; developed a heuristic method intended for general fs extraction in larger corpora, first using conditional probabilities to do an initial ( single pass ) coarse - grained segmentation of the corpus, followed by a pass through the resulting vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. the work of  #AUTHOR_TAG is an example of an unsupervised approach which does not use association measures : it extends the bayesian word segmentation approach of  #AUTHOR_TAG to multiword tokenization, applying', 'a generative dirichlet process model which jointly constructs a segmentation of the corpus and a corresponding multiword vocabulary. other research in mwes has tended to be rather focused on particular syntactic patterns such as verbnoun combinations  #AUTHOR_TAG. the system of  #AUTHOR_TAG a ) distinguishes a full range of mwe sequences in the english web treebank, including gapped expressions, using a supervised sequence', 'tagging model. though, in theory, automatic lexical resources could be a useful addition to the schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success  #AUTHOR_TAG. the motivations for building lexicons of fs naturally overlap with those for mwe :', 'models of distributional semantics, in particular, can benefit from sensitivity to multiword units  #AUTHOR_TAG, as can parsing  #AUTHOR_TAG and topic models  #AUTHOR_TAG. one major motivation for looking beyond mwes is the ability to carry', 'out broader linguistic analyses. within corpus linguistics, multiword sequences have been studied in the form of lexical bundles  #AUTHOR_TAG, which are simply n - grams that occur above a certain frequency threshold. like fs, lexical bundles generally involve larger phrasal chunks that would be missed by traditional mwe extraction, and so research in this area has tended to focus on how particular', 'formulaic phrases ( e. g., if you look at ) are indicative of particular genres ( e. g., university lectures ). lexical bundles have been applied, in particular, to learner language : for example,  #AUTHOR_TAG show that non - native student writers use a severely restricted range of lexical bundle types, and tend', 'to overuse those types, while  #AUTHOR_TAG investigate the role of proficiency, demonstrating that intermediate learners underuse lower - frequency bigrams and overuse high - frequency bigrams relative to advanced learners.  #AUTHOR_TAG demonstrate that improving fluency ( closely linked to the use of linguistic formulas ) is more important than improving strict grammaticality with respect to native speaker judgments of non - native productions ;  #TAUTHOR_TAG explicitly argue for fs lexicons as a way to identify, track, and improve learner proficiency']",0
"['not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but']","['( is not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but']","['not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but it is calculated differently depending on the on / off status of the node as well as the status of the nodes in']","['approach to fs identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n - gram type.', 'the explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.', 'each node can be considered either "" on "" ( is an fs ) or "" off "" ( is not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but it is calculated differently depending on the on / off status of the node as well as the status of the nodes in its vicinity.', 'nodes are linked based on n - gram subsumption and corpus overlap relationships ( see figure 2 ), with "" on "" nodes typically explaining other nodes.', 'given these relationships, we iterate over the nodes and greedily optimize the on / off choice relative to explainedness in the local neighborhood of each node, until convergence']",5
"['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, min']","['grams which necessarily includes all those over the frequency threshold. once a set of relevant n - grams is identified and counted, other statistics', 'required to calculate the lexical predictability ratio ( "" lpr "" ) for each word in the n - gram are collected. lpr is', 'a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context ( over the same span', 'of words ). formally, the lpr for word w i in the context of a word sequence w 1,..., w i,..., w n with pos tag sequence t 1,..., t n is given by : where w j, k', 'denotes the word sequence w j,..., w i−1, w i + 1,..., w k excluding w i ( similarly for t j, k ). note that the lower bound of', 'lpr is 1, since the ratio for a word with no context is trivially 1. we use the same equation for', 'gapped n - grams, with the caveat that quantities involving sequences which include the location where the gap', 'occurs are derived from special gapped n - gram statistics. note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer fs, where the entire pos context alone might uniquely identify the phrase, resulting in the minimum lpr of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, minlpr for a particular n - gram', 'does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link. for example, in the case of be keep * under', 'wraps ( figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minlpr is focused on the weaker relationship between be and the rest of the', 'phrase. this makes it particularly suited to use in a lattice model of competing n - grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to', 'which be is an essential part of the phrase ; the other affinities are, in effect, irrelevant, because they occur in', 'the smaller n - gram as well']",5
"['corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '. one advantage of a type - based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. as such we entirely excluded from our', 'test set n - grams which just one annotator marked as fs. table 1', 'contains the counts for the four test sets after this filtering step and fleiss\'kappa scores before ( "" pre "" ) and after ( "" post "" ).', 'the second change is that for the main evaluation we collapsed gapped and contiguous n', '- grams into a single test set. the rationale is that the number of positive gapped examples is too low to provide a reliable independent f - score. our primary comparison', 'is with the heuristic lpr model of  #TAUTHOR_TAG, which is scalable to large corpora and includes gapped n - grams. for the bnc, we also benchmark against the dp - seg model of  #AUTHOR_TAG with recommended settings, and the localmaxs algorithm of da  #AUTHOR_TAG using scp ; neither of these methods scale to the larger corpora. 4', 'because these other approaches only generate sequential multiword units, we use only the sequential part of the bnc test set for this evaluation. all comparison approaches have themselves been previously compared against a wide range of', 'association measures. as such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking n - grams according to the', 'measure used in our lattice ( minlpr ) as well as pmi and raw frequency. for', 'each of these association measures we rank all n - grams above the frequency threshold and build a lexicon equal to the size of the', 'lexicon produced by our model. we created small development sets for each corpus and used them to do a thorough testing of parameter settings. although it is generally possible to increase precision by increasing c, we found that across corpora we always obtained near - optimal results with c = 4, so to demonstrate the usefulness of the lattice technique', 'as an entirely off - the - shelf tool, we present the results using identical settings for all four corpora.', 'we treat covering as a fundamental part of the lattice model, but to investigate the efficacy of other node interactions within the', 'model we present results with overlap and clearing node interactions turned off']",5
"['corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '. one advantage of a type - based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. as such we entirely excluded from our', 'test set n - grams which just one annotator marked as fs. table 1', 'contains the counts for the four test sets after this filtering step and fleiss\'kappa scores before ( "" pre "" ) and after ( "" post "" ).', 'the second change is that for the main evaluation we collapsed gapped and contiguous n', '- grams into a single test set. the rationale is that the number of positive gapped examples is too low to provide a reliable independent f - score. our primary comparison', 'is with the heuristic lpr model of  #TAUTHOR_TAG, which is scalable to large corpora and includes gapped n - grams. for the bnc, we also benchmark against the dp - seg model of  #AUTHOR_TAG with recommended settings, and the localmaxs algorithm of da  #AUTHOR_TAG using scp ; neither of these methods scale to the larger corpora. 4', 'because these other approaches only generate sequential multiword units, we use only the sequential part of the bnc test set for this evaluation. all comparison approaches have themselves been previously compared against a wide range of', 'association measures. as such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking n - grams according to the', 'measure used in our lattice ( minlpr ) as well as pmi and raw frequency. for', 'each of these association measures we rank all n - grams above the frequency threshold and build a lexicon equal to the size of the', 'lexicon produced by our model. we created small development sets for each corpus and used them to do a thorough testing of parameter settings. although it is generally possible to increase precision by increasing c, we found that across corpora we always obtained near - optimal results with c = 4, so to demonstrate the usefulness of the lattice technique', 'as an entirely off - the - shelf tool, we present the results using identical settings for all four corpora.', 'we treat covering as a fundamental part of the lattice model, but to investigate the efficacy of other node interactions within the', 'model we present results with overlap and clearing node interactions turned off']",5
"['not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but']","['( is not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but']","['not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but it is calculated differently depending on the on / off status of the node as well as the status of the nodes in']","['approach to fs identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n - gram type.', 'the explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.', 'each node can be considered either "" on "" ( is an fs ) or "" off "" ( is not an fs ).', 'the basis of the calculation of explainedness is the syntax - sensitive lpr association measure of  #TAUTHOR_TAG, but it is calculated differently depending on the on / off status of the node as well as the status of the nodes in its vicinity.', 'nodes are linked based on n - gram subsumption and corpus overlap relationships ( see figure 2 ), with "" on "" nodes typically explaining other nodes.', 'given these relationships, we iterate over the nodes and greedily optimize the on / off choice relative to explainedness in the local neighborhood of each node, until convergence']",4
"['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, min']","['grams which necessarily includes all those over the frequency threshold. once a set of relevant n - grams is identified and counted, other statistics', 'required to calculate the lexical predictability ratio ( "" lpr "" ) for each word in the n - gram are collected. lpr is', 'a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context ( over the same span', 'of words ). formally, the lpr for word w i in the context of a word sequence w 1,..., w i,..., w n with pos tag sequence t 1,..., t n is given by : where w j, k', 'denotes the word sequence w j,..., w i−1, w i + 1,..., w k excluding w i ( similarly for t j, k ). note that the lower bound of', 'lpr is 1, since the ratio for a word with no context is trivially 1. we use the same equation for', 'gapped n - grams, with the caveat that quantities involving sequences which include the location where the gap', 'occurs are derived from special gapped n - gram statistics. note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer fs, where the entire pos context alone might uniquely identify the phrase, resulting in the minimum lpr of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, minlpr for a particular n - gram', 'does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link. for example, in the case of be keep * under', 'wraps ( figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minlpr is focused on the weaker relationship between be and the rest of the', 'phrase. this makes it particularly suited to use in a lattice model of competing n - grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to', 'which be is an essential part of the phrase ; the other affinities are, in effect, irrelevant, because they occur in', 'the smaller n - gram as well']",4
"['corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '. one advantage of a type - based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. as such we entirely excluded from our', 'test set n - grams which just one annotator marked as fs. table 1', 'contains the counts for the four test sets after this filtering step and fleiss\'kappa scores before ( "" pre "" ) and after ( "" post "" ).', 'the second change is that for the main evaluation we collapsed gapped and contiguous n', '- grams into a single test set. the rationale is that the number of positive gapped examples is too low to provide a reliable independent f - score. our primary comparison', 'is with the heuristic lpr model of  #TAUTHOR_TAG, which is scalable to large corpora and includes gapped n - grams. for the bnc, we also benchmark against the dp - seg model of  #AUTHOR_TAG with recommended settings, and the localmaxs algorithm of da  #AUTHOR_TAG using scp ; neither of these methods scale to the larger corpora. 4', 'because these other approaches only generate sequential multiword units, we use only the sequential part of the bnc test set for this evaluation. all comparison approaches have themselves been previously compared against a wide range of', 'association measures. as such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking n - grams according to the', 'measure used in our lattice ( minlpr ) as well as pmi and raw frequency. for', 'each of these association measures we rank all n - grams above the frequency threshold and build a lexicon equal to the size of the', 'lexicon produced by our model. we created small development sets for each corpus and used them to do a thorough testing of parameter settings. although it is generally possible to increase precision by increasing c, we found that across corpora we always obtained near - optimal results with c = 4, so to demonstrate the usefulness of the lattice technique', 'as an entirely off - the - shelf tool, we present the results using identical settings for all four corpora.', 'we treat covering as a fundamental part of the lattice model, but to investigate the efficacy of other node interactions within the', 'model we present results with overlap and clearing node interactions turned off']",4
"['pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking,']","['pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking,']","['pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking,']","['main results for fs acquisition across the four corpora are shown in table 2.', 'as noted in section 2, simple statistical association measures like pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.', 'generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in croatian.', 'when only covering is used, the results are fairly similar to  #TAUTHOR_TAG, which is unsurprising given the extent to which decomposition and covering are related.', 'the japanese and icwsm corpora have relatively high precision and low recall, whereas both the bnc and croatian corpora have low precision and high recall.', 'in the contiguous fs test set for the bnc ( ta - table 2 : results of fs identification in various test sets : countrank = ranking with frequency ; pmirank = pmi - based ranking ; minlprrank = ranking with minlpr ; lprseg = the method of  #AUTHOR_TAG ; "" −cl "" = no clearing ; "" −ovr "" = no penalization of overlaps ; "" p "" = precision ; "" r "" = recall ; and "" f "" = f - score.', 'bold is best in a given column.', 'the performance difference of the lattice model relative to the best baseline for all test sets considered together is significant at p < 0. 01 ( based on the permutation test :  #AUTHOR_TAG ).', 'table 3 : results of fs identification in contiguous bnc test set ; localmaxs = method of da  #AUTHOR_TAG ; dp - seg = method of  #AUTHOR_TAG ble 3 ), we found that both the localmaxs algorithm and the dp - seg method of  #AUTHOR_TAG were able to beat our other baseline methods with roughly similar f - scores, though both are well below our lattice method.', 'some of the difference seems attributable to fairly severe precision / recall imbalance, though we were unable to improve the f - score by changing the parameters from recommended settings for either model']",4
"['decomposition heuristics of  #TAUTHOR_TAG, the time needed to build and optimize the lattice is a']","['decomposition heuristics of  #TAUTHOR_TAG, the time needed to build and optimize the lattice is a']","['decomposition heuristics of  #TAUTHOR_TAG, the time needed to build and optimize the lattice is a']","['functional, the lpr decomp approach is nevertheless algorithmically ungainly, involving multiple layers of heuristic -', 'driven filtering with no possibility of correcting errors. our lattice method is aimed at something between these extremes : a practical, optimizable model, but with various component heuristics that can be improved upon. for instance,', 'though the current version of clearing is effective and has practical advantages relative to simpler options that we tested, it could be', 'enhanced by more careful investigation of the statistical properties of n - grams which contain fs. we can also consider adding new terms to the exponents of the two parts of our objective function, anal', '##agous to the cover, clear, and overlap functions, based on other relationships between nodes in the lattice. one which we have considered', 'is creating new connections between identical or similar syntactic patterns, which could serve to encourage the model to generalize. in english, for instance, it might learn that verb - particle combinations are generally likely to be fs, whereas verb - determiner combinations are not. our initial investigations suggest, however, it', 'may be difficult to apply this idea without merely amplifying existing undesirable biases in the lpr measure. bringing in other information such as simple distributional statistics might help the model identify non - compositional semantics, and could, in combination with the existing lattice competition, focus the model on mwes which could provide a reliable basis for generalization. for all four corpora,', 'the lattice optimization algorithm converged within 10 iterations. although the optimization', 'of the lattice is several orders of magnitude more complex than the decomposition heuristics of  #TAUTHOR_TAG, the time needed to build and optimize the lattice is a fraction of the time required to collect the', 'statistics for lpr calculation, and so the end - to - end runtimes of the two methods are comparable. in the bnc, the full lattice method', 'was much faster than localmaxs and dp - seg, though direct runtime comparisons to these methods are of modest value due to differences in both scope and implementation. finally,', 'though the model was designed specifically for fs extraction, we note that it could be useful for related tasks such as unsupervised learning of morphological lexicons, particularly for agglutinative languages. character or phoneme n - grams could compete in an identically', 'structured lattice to be chosen as the best morphemes for the language, with lpr adapted to use phonological predictability ( i. e., based on vowel / consonant "" tags "" ) instead of syntactic predictability. it is likely, though, that further algorithmic modifications would', 'be necessary to target morphological phenomena well, which we leave for future work']",4
"['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, min']","['grams which necessarily includes all those over the frequency threshold. once a set of relevant n - grams is identified and counted, other statistics', 'required to calculate the lexical predictability ratio ( "" lpr "" ) for each word in the n - gram are collected. lpr is', 'a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context ( over the same span', 'of words ). formally, the lpr for word w i in the context of a word sequence w 1,..., w i,..., w n with pos tag sequence t 1,..., t n is given by : where w j, k', 'denotes the word sequence w j,..., w i−1, w i + 1,..., w k excluding w i ( similarly for t j, k ). note that the lower bound of', 'lpr is 1, since the ratio for a word with no context is trivially 1. we use the same equation for', 'gapped n - grams, with the caveat that quantities involving sequences which include the location where the gap', 'occurs are derived from special gapped n - gram statistics. note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer fs, where the entire pos context alone might uniquely identify the phrase, resulting in the minimum lpr of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, minlpr for a particular n - gram', 'does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link. for example, in the case of be keep * under', 'wraps ( figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minlpr is focused on the weaker relationship between be and the rest of the', 'phrase. this makes it particularly suited to use in a lattice model of competing n - grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to', 'which be is an essential part of the phrase ; the other affinities are, in effect, irrelevant, because they occur in', 'the smaller n - gram as well']",3
"['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, min']","['grams which necessarily includes all those over the frequency threshold. once a set of relevant n - grams is identified and counted, other statistics', 'required to calculate the lexical predictability ratio ( "" lpr "" ) for each word in the n - gram are collected. lpr is', 'a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context ( over the same span', 'of words ). formally, the lpr for word w i in the context of a word sequence w 1,..., w i,..., w n with pos tag sequence t 1,..., t n is given by : where w j, k', 'denotes the word sequence w j,..., w i−1, w i + 1,..., w k excluding w i ( similarly for t j, k ). note that the lower bound of', 'lpr is 1, since the ratio for a word with no context is trivially 1. we use the same equation for', 'gapped n - grams, with the caveat that quantities involving sequences which include the location where the gap', 'occurs are derived from special gapped n - gram statistics. note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer fs, where the entire pos context alone might uniquely identify the phrase, resulting in the minimum lpr of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, minlpr for a particular n - gram', 'does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link. for example, in the case of be keep * under', 'wraps ( figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minlpr is focused on the weaker relationship between be and the rest of the', 'phrase. this makes it particularly suited to use in a lattice model of competing n - grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to', 'which be is an essential part of the phrase ; the other affinities are, in effect, irrelevant, because they occur in', 'the smaller n - gram as well']",3
"['corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '. one advantage of a type - based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. as such we entirely excluded from our', 'test set n - grams which just one annotator marked as fs. table 1', 'contains the counts for the four test sets after this filtering step and fleiss\'kappa scores before ( "" pre "" ) and after ( "" post "" ).', 'the second change is that for the main evaluation we collapsed gapped and contiguous n', '- grams into a single test set. the rationale is that the number of positive gapped examples is too low to provide a reliable independent f - score. our primary comparison', 'is with the heuristic lpr model of  #TAUTHOR_TAG, which is scalable to large corpora and includes gapped n - grams. for the bnc, we also benchmark against the dp - seg model of  #AUTHOR_TAG with recommended settings, and the localmaxs algorithm of da  #AUTHOR_TAG using scp ; neither of these methods scale to the larger corpora. 4', 'because these other approaches only generate sequential multiword units, we use only the sequential part of the bnc test set for this evaluation. all comparison approaches have themselves been previously compared against a wide range of', 'association measures. as such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking n - grams according to the', 'measure used in our lattice ( minlpr ) as well as pmi and raw frequency. for', 'each of these association measures we rank all n - grams above the frequency threshold and build a lexicon equal to the size of the', 'lexicon produced by our model. we created small development sets for each corpus and used them to do a thorough testing of parameter settings. although it is generally possible to increase precision by increasing c, we found that across corpora we always obtained near - optimal results with c = 4, so to demonstrate the usefulness of the lattice technique', 'as an entirely off - the - shelf tool, we present the results using identical settings for all four corpora.', 'we treat covering as a fundamental part of the lattice model, but to investigate the efficacy of other node interactions within the', 'model we present results with overlap and clearing node interactions turned off']",3
"['pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking,']","['pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking,']","['pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking,']","['main results for fs acquisition across the four corpora are shown in table 2.', 'as noted in section 2, simple statistical association measures like pmi do poorly when faced with syntactically - unrestricted n - grams of variable length : minlpr is clearly a much better statistic for this purpose.', 'the lprseg method of  #TAUTHOR_TAG consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.', 'generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in croatian.', 'when only covering is used, the results are fairly similar to  #TAUTHOR_TAG, which is unsurprising given the extent to which decomposition and covering are related.', 'the japanese and icwsm corpora have relatively high precision and low recall, whereas both the bnc and croatian corpora have low precision and high recall.', 'in the contiguous fs test set for the bnc ( ta - table 2 : results of fs identification in various test sets : countrank = ranking with frequency ; pmirank = pmi - based ranking ; minlprrank = ranking with minlpr ; lprseg = the method of  #AUTHOR_TAG ; "" −cl "" = no clearing ; "" −ovr "" = no penalization of overlaps ; "" p "" = precision ; "" r "" = recall ; and "" f "" = f - score.', 'bold is best in a given column.', 'the performance difference of the lattice model relative to the best baseline for all test sets considered together is significant at p < 0. 01 ( based on the permutation test :  #AUTHOR_TAG ).', 'table 3 : results of fs identification in contiguous bnc test set ; localmaxs = method of da  #AUTHOR_TAG ; dp - seg = method of  #AUTHOR_TAG ble 3 ), we found that both the localmaxs algorithm and the dp - seg method of  #AUTHOR_TAG were able to beat our other baseline methods with roughly similar f - scores, though both are well below our lattice method.', 'some of the difference seems attributable to fairly severe precision / recall imbalance, though we were unable to improve the f - score by changing the parameters from recommended settings for either model']",3
"['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lp']","['of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, min']","['grams which necessarily includes all those over the frequency threshold. once a set of relevant n - grams is identified and counted, other statistics', 'required to calculate the lexical predictability ratio ( "" lpr "" ) for each word in the n - gram are collected. lpr is', 'a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context ( over the same span', 'of words ). formally, the lpr for word w i in the context of a word sequence w 1,..., w i,..., w n with pos tag sequence t 1,..., t n is given by : where w j, k', 'denotes the word sequence w j,..., w i−1, w i + 1,..., w k excluding w i ( similarly for t j, k ). note that the lower bound of', 'lpr is 1, since the ratio for a word with no context is trivially 1. we use the same equation for', 'gapped n - grams, with the caveat that quantities involving sequences which include the location where the gap', 'occurs are derived from special gapped n - gram statistics. note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer fs, where the entire pos context alone might uniquely identify the phrase, resulting in the minimum lpr of 1 even for entirely formulaic sequences - an undesirable result. in the segmentation approach of  #TAUTHOR_TAG, lpr for an entire', 'span is calculated as a product of the individual lprs, but here we will use the minimum lpr across the words in the sequence : here, minlpr for a particular n - gram', 'does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link. for example, in the case of be keep * under', 'wraps ( figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minlpr is focused on the weaker relationship between be and the rest of the', 'phrase. this makes it particularly suited to use in a lattice model of competing n - grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to', 'which be is an essential part of the phrase ; the other affinities are, in effect, irrelevant, because they occur in', 'the smaller n - gram as well']",6
"['to pragmatic biases  #TAUTHOR_TAG ; for instance, private state verbs like feel tend']","['to pragmatic biases  #TAUTHOR_TAG ; for instance, private state verbs like feel tend']","['to pragmatic biases  #TAUTHOR_TAG ; for instance, private state verbs like feel tend']","['most important node interaction is covering, which corresponds to discounting or entirely excluding a node due to a node higher in the lattice.', 'our model includes two types of covering : hard and soft.', 'hard covering is based on the idea that, due to very similar counts, we can reasonably conclude that the presence of an n - gram in our statistics is a direct result of a subsuming ( n + i ) - gram.', 'in figure 2, e. g., if we have 143 counts of keep * under wraps and 152 counts of under wraps, the presence of keep * under wraps almost completely explains under wraps, and we should consider these two n - grams as one.', 'we do this by permanently disabling any hard covered node, and setting the minlpr of the covering node to the maximum minlpr among all the nodes it covers ( including itself ) ; this means that longer ngrams with function words ( which often have lower minlpr ) can benefit from the strong statistical relationships between open - class lexical features in ngrams that they cover.', 'this is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice.', 'of course, a threshold for hard covering must be chosen : during development we found that a ratio of 2 / 3 ( corresponding to a significant majority of the counts of a lower node corresponding to the higher node ) worked well.', 'we also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high lpr values due to pragmatic biases  #TAUTHOR_TAG ; for instance, private state verbs like feel tend to have first person singular subjects.', 'in the lattice, n - grams with pronouns are considered covered ( inactive ) unless they cover at least one other node which does not have a pronoun, which allows us to limit fs with pronouns without excluding them entirely : they are included only in cases where they are definitively formulaic.', 'soft covering is used in cases when a single ngram does not entirely account for another, but a turned - on n - gram to some extent may explain some of the statistical irregularity of one lower in the lattice.', 'for instance, in figure 2 keep * under is not hard - covered by keep * under wraps ( since there are fs such as keep * under surveillance and keep it under your hat ), but if keep * under wraps is tagged as an fs, we nevertheless want to discount the portion of the keep * under counts that correspond to']",1
"['corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '']","['and based on written guidelines derived from the definitions of  #AUTHOR_TAG, three native - speaker, educated annotators judged 500 contiguous n', '- grams and another 500 gapped n - grams for each corpus. other than the inclusion of new languages, our test sets differ from  #TAUTHOR_TAG in two ways', '. one advantage of a type - based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. as such we entirely excluded from our', 'test set n - grams which just one annotator marked as fs. table 1', 'contains the counts for the four test sets after this filtering step and fleiss\'kappa scores before ( "" pre "" ) and after ( "" post "" ).', 'the second change is that for the main evaluation we collapsed gapped and contiguous n', '- grams into a single test set. the rationale is that the number of positive gapped examples is too low to provide a reliable independent f - score. our primary comparison', 'is with the heuristic lpr model of  #TAUTHOR_TAG, which is scalable to large corpora and includes gapped n - grams. for the bnc, we also benchmark against the dp - seg model of  #AUTHOR_TAG with recommended settings, and the localmaxs algorithm of da  #AUTHOR_TAG using scp ; neither of these methods scale to the larger corpora. 4', 'because these other approaches only generate sequential multiword units, we use only the sequential part of the bnc test set for this evaluation. all comparison approaches have themselves been previously compared against a wide range of', 'association measures. as such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking n - grams according to the', 'measure used in our lattice ( minlpr ) as well as pmi and raw frequency. for', 'each of these association measures we rank all n - grams above the frequency threshold and build a lexicon equal to the size of the', 'lexicon produced by our model. we created small development sets for each corpus and used them to do a thorough testing of parameter settings. although it is generally possible to increase precision by increasing c, we found that across corpora we always obtained near - optimal results with c = 4, so to demonstrate the usefulness of the lattice technique', 'as an entirely off - the - shelf tool, we present the results using identical settings for all four corpora.', 'we treat covering as a fundamental part of the lattice model, but to investigate the efficacy of other node interactions within the', 'model we present results with overlap and clearing node interactions turned off']",7
['on a large sense - tagged corpus first used in  #TAUTHOR_TAG'],['on a large sense - tagged corpus first used in  #TAUTHOR_TAG'],"['on a large sense - tagged corpus first used in  #TAUTHOR_TAG.', 'the accuracy achieved by our']","['this paper, we report recent improvements to the exemplar - based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy.', 'by using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10 - fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense - tagged corpus first used in  #TAUTHOR_TAG.', '']",4
"['on the other hand,  #TAUTHOR_TAG used an exempl']","['the other hand,  #TAUTHOR_TAG used an exempl']","['other hand,  #TAUTHOR_TAG used an exempl']","['algorithm gives the highest accuracy on the "" line "" corpus tested. past research in machine learning has also reported', 'that the naive - bayes algorithm achieved good performance on other machine learning tasks  #AUTHOR_TAG. this is in spite of the conditional independence assumption made by the naive - bayes algorithm, which', 'may be unjustified in the domains tested. gale, church and yarowsky  #AUTHOR_TAG a ;  #AUTHOR_TAG have also successfully used the naive - bayes algorithm ( and', 'several extensions and variations ) for word sense disambiguation. on the other hand,  #TAUTHOR_TAG used an exemplar - based ( or nearest neighbor ) learning approach. our wsd program, lexas, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations,', 'and verb - object syntactic relation from a sentence containing the word to be disambiguated. these features from a sentence', '']",4
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",4
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",4
"['on the other hand,  #TAUTHOR_TAG used an exempl']","['the other hand,  #TAUTHOR_TAG used an exempl']","['other hand,  #TAUTHOR_TAG used an exempl']","['algorithm gives the highest accuracy on the "" line "" corpus tested. past research in machine learning has also reported', 'that the naive - bayes algorithm achieved good performance on other machine learning tasks  #AUTHOR_TAG. this is in spite of the conditional independence assumption made by the naive - bayes algorithm, which', 'may be unjustified in the domains tested. gale, church and yarowsky  #AUTHOR_TAG a ;  #AUTHOR_TAG have also successfully used the naive - bayes algorithm ( and', 'several extensions and variations ) for word sense disambiguation. on the other hand,  #TAUTHOR_TAG used an exemplar - based ( or nearest neighbor ) learning approach. our wsd program, lexas, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations,', 'and verb - object syntactic relation from a sentence containing the word to be disambiguated. these features from a sentence', '']",0
"['on the other hand,  #TAUTHOR_TAG used an exempl']","['the other hand,  #TAUTHOR_TAG used an exempl']","['other hand,  #TAUTHOR_TAG used an exempl']","['algorithm gives the highest accuracy on the "" line "" corpus tested. past research in machine learning has also reported', 'that the naive - bayes algorithm achieved good performance on other machine learning tasks  #AUTHOR_TAG. this is in spite of the conditional independence assumption made by the naive - bayes algorithm, which', 'may be unjustified in the domains tested. gale, church and yarowsky  #AUTHOR_TAG a ;  #AUTHOR_TAG have also successfully used the naive - bayes algorithm ( and', 'several extensions and variations ) for word sense disambiguation. on the other hand,  #TAUTHOR_TAG used an exemplar - based ( or nearest neighbor ) learning approach. our wsd program, lexas, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations,', 'and verb - object syntactic relation from a sentence containing the word to be disambiguated. these features from a sentence', '']",0
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",0
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",0
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",0
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",0
"['testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple sources of']","['testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple sources of knowledge are indeed', 'useful for ws']","['testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple']","['', 'of the training set. however, more sophisticated indexing methods such as that reported in  #AUTHOR_TAG can reduce this to logarithmic expected time, which will significantly reduce testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple sources of knowledge are indeed', 'useful for wsd. future work will explore the addition of these other features to further improve disambiguation accuracy. besides the parameter k, pebls also contains other learning parameters such as exemplar weights and feature weights. exempl', '##ar weighting has been found to improve classification performance  #AUTHOR_TAG. also, given the relative importance of the various knowledge sources as reported in  #TAUTHOR_TAG,', 'it may be possible to improve disambignation performance by introducing feature weighting. future work can explore the effect of exempl', '##ar weighting and feature weighting on disambiguation accuracy']",0
"['on the other hand,  #TAUTHOR_TAG used an exempl']","['the other hand,  #TAUTHOR_TAG used an exempl']","['other hand,  #TAUTHOR_TAG used an exempl']","['algorithm gives the highest accuracy on the "" line "" corpus tested. past research in machine learning has also reported', 'that the naive - bayes algorithm achieved good performance on other machine learning tasks  #AUTHOR_TAG. this is in spite of the conditional independence assumption made by the naive - bayes algorithm, which', 'may be unjustified in the domains tested. gale, church and yarowsky  #AUTHOR_TAG a ;  #AUTHOR_TAG have also successfully used the naive - bayes algorithm ( and', 'several extensions and variations ) for word sense disambiguation. on the other hand,  #TAUTHOR_TAG used an exemplar - based ( or nearest neighbor ) learning approach. our wsd program, lexas, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations,', 'and verb - object syntactic relation from a sentence containing the word to be disambiguated. these features from a sentence', '']",5
"['.', ' #TAUTHOR_TAG.', 'however, our preliminary investigation indicates that, among the various']","['weighting, no feature weighting, etc.', ' #TAUTHOR_TAG.', 'however, our preliminary investigation indicates that, among the various']","['.', ' #TAUTHOR_TAG.', 'however, our preliminary investigation indicates that, among the various learning parameters']","['', 'each of these parameters has a default value in pebls, eg., k = 1, no exemplar weighting, no feature weighting, etc.', ' #TAUTHOR_TAG.', 'however, our preliminary investigation indicates that, among the various learning parameters of pebls, the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar - based classifier.', 'cross validation is a well - known technique that can be used for estimating the expected error rate of a classifier which has been trained on a particular data set.', 'for instance, the c4. 5 program  #AUTHOR_TAG contains an option for running cross validation to estimate the expected error rate of an induced rule set.', 'cross validation has been proposed as a general technique to automatically determine the parameter settings of a given learning algorithm using a particular data set as training data  #AUTHOR_TAG.', 'in m - fold cross validation, a training data set is partitioned into m ( approximately ) equal - sized blocks, and the learning algorithm is run m times.', 'in each run, one of the m blocks of training data is set aside as test data ( the holdout set ) and the remaining m - 1 blocks are used as training data.', 'the average error rate of the m runs is a good estimate of the error rate of the induced classifier.', 'for a particular parameter setting']",5
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",5
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",5
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",3
"['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', ').']","['. contact the ldc at ldc @ unagi. cis. upenn. edu for details. 2the first five of these seven features were also used in  #TAUTHOR_TAG., 1996', '). the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating', 'wsd programs  #AUTHOR_TAG b ;  #AUTHOR_TAG. there are two instantiations of this strategy in our current evaluation. since wordnet orders its senses such that sense 1', 'is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. this assignment method does not even need to look at the', 'training examples. we call this method "" sense 1 "" in table 1. another assignment method is to determine the most frequently occurring sense in', '']",3
"['testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple sources of']","['testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple sources of knowledge are indeed', 'useful for ws']","['testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple']","['', 'of the training set. however, more sophisticated indexing methods such as that reported in  #AUTHOR_TAG can reduce this to logarithmic expected time, which will significantly reduce testing time. in the present study, we have focused', 'on the comparison of learning algorithms, but not on feature representation of examples.  #TAUTHOR_TAG suggests that multiple sources of knowledge are indeed', 'useful for wsd. future work will explore the addition of these other features to further improve disambiguation accuracy. besides the parameter k, pebls also contains other learning parameters such as exemplar weights and feature weights. exempl', '##ar weighting has been found to improve classification performance  #AUTHOR_TAG. also, given the relative importance of the various knowledge sources as reported in  #TAUTHOR_TAG,', 'it may be possible to improve disambignation performance by introducing feature weighting. future work can explore the effect of exempl', '##ar weighting and feature weighting on disambiguation accuracy']",2
"['.', 'recent studies on kd  #TAUTHOR_TAG 15 ] even']","['teacher.', 'recent studies on kd  #TAUTHOR_TAG 15 ] even']","['', 'recent studies on kd  #TAUTHOR_TAG 15 ] even leverage more sophisticated model - specific distillation loss functions']","['', 'recent studies on kd  #TAUTHOR_TAG 15 ] even leverage more sophisticated model - specific distillation loss functions for better performance.', 'different from previous kd studies which explicitly exploit a distillation loss to minimize the distance between the teacher model and the student model, we propose a new genre of model compression.', 'inspired by the famous thought experiment "" ship of theseus 3 "" in philosophy, where all components of a ship are gradually replaced by new ones until no original component exists, we propose theseus compression for bert ( bert - of - theseus ), which progressively substitutes modules of bert with modules of fewer parameters.', 'we call the original model and compressed model predecessor and successor, in correspondence to the concepts of teacher and student in kd, respectively.', 'as shown in figure 1, we first specify a substitute ( successor module ) for each predecessor module ( i. e., modules in the predecessor model ).', 'then, we randomly replace each predecessor module with its corresponding successor module by a probability and make them work together in the training phase.', 'after convergence, we combine all successor modules to be the successor model for inference.', 'in']",0
"['loss for different tasks and datasets are always laborious  #TAUTHOR_TAG 28 ].', 'second, different']","['loss for different tasks and datasets are always laborious  #TAUTHOR_TAG 28 ].', 'second, different']","['each loss for different tasks and datasets are always laborious  #TAUTHOR_TAG 28 ].', 'second, different']","['##us compression shares a similar idea with kd, which encourages the compressed model to behave like the original, but holds many merits.', 'first, we only use the task - specific loss function in the compression process.', 'however, kd - based methods use task - specific loss, together with one or multiple distillation losses as its optimization objective.', 'the use of only one loss function throughout the whole compression process allows us to unify the different phases and keep the compression in a total end - to - end fashion.', 'also, selecting various loss functions and balancing the weights of each loss for different tasks and datasets are always laborious  #TAUTHOR_TAG 28 ].', 'second, different from recent work [ 15 ], theseus compression does not use transformer - specific features for compression thus is potential to compress a wide spectrum of models.', 'third, instead of using the original model only for inference in kd, our approach allows the predecessor model to work in association with the compressed successor model, enabling a deeper gradient - level interaction and a smoother training process.', 'moreover, the different module permutations mixing both predecessor and successor modules adds extra regularization, similar to dropout [ 32 ].', 'with a curriculum learning [ 1 ] driven replacement scheduler, our approach achieves great performance compressing bert [ 4 ], a large pretrained transformer model.', 'to summarize, our contribution is two - fold : ( 1 ) we propose a novel approach, theseus compression, revealing a new pathway to model compression, with only one loss function and one hyper - parameter.', '( 2 ) our compressed bert model is 1. 94× faster while retaining more than 98 % performance of the original model, outperforming other kd - based compression baselines']",0
"['pretrain bert. patient knowledge dist', '##illation ( pkd )  #TAUTHOR_TAG designs multiple distillation losses between']","['pretrain bert. patient knowledge dist', '##illation ( pkd )  #TAUTHOR_TAG designs multiple distillation losses between']","['a naive knowledge distillation on the', 'same corpus used to pretrain bert. patient knowledge dist', '##illation ( pkd )  #TAUTHOR_TAG designs multiple distillation losses between the module hidden states of the teacher and student models. pre', '##trained distillation']","['', 'the transformer. shen et al. [ 29 ] quantized bert to 2 - bit using hessian information. also, substantial modification has been made', 'to transformer architecture. fan et al. [ 7 ] exploited a structure dropping mechanism to train a bert - like model which is', 'resilient to', 'pruning. albert [ 18 ] leverages matrix decomposition and parameter sharing. however, these models cannot exploit ready - made model weights and require a full', 'retraining. tang et al. [ 36 ] used a bilstm architecture to extract task - specific knowledge from', 'bert. distilbert [ 28 ] applies a naive knowledge distillation on the', 'same corpus used to pretrain bert. patient knowledge dist', '##illation ( pkd )  #TAUTHOR_TAG designs multiple distillation losses between the module hidden states of the teacher and student models. pre', '##trained distillation [ 37 ] pretrains the student', 'model with a self - supervised masked lm objective on a large corpus first, then performs a', 'standard kd on supervised tasks. tinybert [ 15 ] conducts the knowledge distillation twice with data', 'augmentation. mobilebert [ 34 ] devises a more computationally efficient architecture and applies knowledge distillation with a bottom', '- to - top layer training procedure']",0
"['[ 14,  #TAUTHOR_TAG 15 ].', 'this loss']","['[ 14,  #TAUTHOR_TAG 15 ].', 'this loss']","['on the design of the loss function [ 14,  #TAUTHOR_TAG 15 ].', 'this loss function needs to be combined with taskspecific loss  #TAUTHOR_TAG 17 ].', 'different']","['basic idea of theseus compression is very similar to kd.', 'we want the successor model to act like a predecessor model.', 'kd explicitly defines a loss to measure the similarity of the teacher and student.', 'however, the performance greatly relies on the design of the loss function [ 14,  #TAUTHOR_TAG 15 ].', 'this loss function needs to be combined with taskspecific loss  #TAUTHOR_TAG 17 ].', 'different from kd, theseus compression only requires one task - specific loss function ( e. g., cross entropy ), which closely resembles a fine - tuning procedure.', 'inspired by dropout [ 32 ], we propose module replacing, a novel technique for model compression.', 'we call the original model and the target model predecessor and successor, respectively.', 'first, we specify a successor module for each module in the predecessor.', 'for example, in the context of bert compression, we let one transformer layer to be the successor module for two transformer layers.', 'consider a predecessor model p which has n modules and a successor model s which has n predefined modules.', '']",0
"['[ 14,  #TAUTHOR_TAG 15 ].', 'this loss']","['[ 14,  #TAUTHOR_TAG 15 ].', 'this loss']","['on the design of the loss function [ 14,  #TAUTHOR_TAG 15 ].', 'this loss function needs to be combined with taskspecific loss  #TAUTHOR_TAG 17 ].', 'different']","['basic idea of theseus compression is very similar to kd.', 'we want the successor model to act like a predecessor model.', 'kd explicitly defines a loss to measure the similarity of the teacher and student.', 'however, the performance greatly relies on the design of the loss function [ 14,  #TAUTHOR_TAG 15 ].', 'this loss function needs to be combined with taskspecific loss  #TAUTHOR_TAG 17 ].', 'different from kd, theseus compression only requires one task - specific loss function ( e. g., cross entropy ), which closely resembles a fine - tuning procedure.', 'inspired by dropout [ 32 ], we propose module replacing, a novel technique for model compression.', 'we call the original model and the target model predecessor and successor, respectively.', 'first, we specify a successor module for each module in the predecessor.', 'for example, in the context of bert compression, we let one transformer layer to be the successor module for two transformer layers.', 'consider a predecessor model p which has n modules and a successor model s which has n predefined modules.', '']",0
"['.', 'recent studies on kd  #TAUTHOR_TAG 15 ] even']","['teacher.', 'recent studies on kd  #TAUTHOR_TAG 15 ] even']","['', 'recent studies on kd  #TAUTHOR_TAG 15 ] even leverage more sophisticated model - specific distillation loss functions']","['', 'recent studies on kd  #TAUTHOR_TAG 15 ] even leverage more sophisticated model - specific distillation loss functions for better performance.', 'different from previous kd studies which explicitly exploit a distillation loss to minimize the distance between the teacher model and the student model, we propose a new genre of model compression.', 'inspired by the famous thought experiment "" ship of theseus 3 "" in philosophy, where all components of a ship are gradually replaced by new ones until no original component exists, we propose theseus compression for bert ( bert - of - theseus ), which progressively substitutes modules of bert with modules of fewer parameters.', 'we call the original model and compressed model predecessor and successor, in correspondence to the concepts of teacher and student in kd, respectively.', 'as shown in figure 1, we first specify a substitute ( successor module ) for each predecessor module ( i. e., modules in the predecessor model ).', 'then, we randomly replace each predecessor module with its corresponding successor module by a probability and make them work together in the training phase.', 'after convergence, we combine all successor modules to be the successor model for inference.', 'in']",1
['- specific compression setting  #TAUTHOR_TAG'],"['task - specific compression setting  #TAUTHOR_TAG 37 ] instead of a pretraining compression setting [ 28, 34 ].', '']",['under a task - specific compression setting  #TAUTHOR_TAG'],"['test our approach under a task - specific compression setting  #TAUTHOR_TAG 37 ] instead of a pretraining compression setting [ 28, 34 ].', 'that is to say, we use no external unlabeled corpus but only the training set of each task in gleu to compress the model.', 'the reason why we test our model under such a setting is that we intend to straightforwardly verify the effectiveness of our generic compression approach.', 'the fast training process of task - specific compression ( e. g., no longer than 20 gpu hours for any task of glue ) computationally enables us to conduct more analytical experiments.', 'for comparision, distillbert [ 28 ] takes 720 gpu hours to train.', 'plus, in real - world applications, this setting provides with more flexibility when selecting from different pretrained lms ( e. g., bert, roberta [ 21 ] ) for various downstream tasks and it is easy to adopt a newly released model, without a time - consuming pretraining compression.', 'on the other hand, we acknowledge that a general - purpose compressed bert can better facilitate the downstream applications in the community since it requires less computational resource to simply fine - tune a small model than compressing a large one.', 'thus, we release a general - purpose compressed bert as well.', 'formally, we define the task of compression as trying to retain as much performance as possible when compressing the officially released bert - base ( uncased ) 5 to a 6 - layer compact model with the same hidden size, following the settings in [ 28,  #TAUTHOR_TAG 37 ].', 'under this setting, the compressed model has 24m parameters for the token embedding ( identical to the original model ) and 42m parameters for the transformer layers and obtains a 1. 94× speed - up for inference']",5
['- specific compression setting  #TAUTHOR_TAG'],"['task - specific compression setting  #TAUTHOR_TAG 37 ] instead of a pretraining compression setting [ 28, 34 ].', '']",['under a task - specific compression setting  #TAUTHOR_TAG'],"['test our approach under a task - specific compression setting  #TAUTHOR_TAG 37 ] instead of a pretraining compression setting [ 28, 34 ].', 'that is to say, we use no external unlabeled corpus but only the training set of each task in gleu to compress the model.', 'the reason why we test our model under such a setting is that we intend to straightforwardly verify the effectiveness of our generic compression approach.', 'the fast training process of task - specific compression ( e. g., no longer than 20 gpu hours for any task of glue ) computationally enables us to conduct more analytical experiments.', 'for comparision, distillbert [ 28 ] takes 720 gpu hours to train.', 'plus, in real - world applications, this setting provides with more flexibility when selecting from different pretrained lms ( e. g., bert, roberta [ 21 ] ) for various downstream tasks and it is easy to adopt a newly released model, without a time - consuming pretraining compression.', 'on the other hand, we acknowledge that a general - purpose compressed bert can better facilitate the downstream applications in the community since it requires less computational resource to simply fine - tune a small model than compressing a large one.', 'thus, we release a general - purpose compressed bert as well.', 'formally, we define the task of compression as trying to retain as much performance as possible when compressing the officially released bert - base ( uncased ) 5 to a 6 - layer compact model with the same hidden size, following the settings in [ 28,  #TAUTHOR_TAG 37 ].', 'under this setting, the compressed model has 24m parameters for the token embedding ( identical to the original model ) and 42m parameters for the transformer layers and obtains a 1. 94× speed - up for inference']",5
"['studies [ 28,  #TAUTHOR_TAG 15 ].', '']","['studies [ 28,  #TAUTHOR_TAG 15 ].', 'afterward,']","['a predecessor model with comparable performance with that reported in previous studies [ 28,  #TAUTHOR_TAG 15 ].', '']","['fine - tune bert - base as the predecessor model for each task with the batch size of 32, the learning rate of 2 × 10 −5, and the number of epochs as 4.', 'as a result, we are able to obtain a predecessor model with comparable performance with that reported in previous studies [ 28,  #TAUTHOR_TAG 15 ].', 'afterward, for training successor models, following [ 28,  #TAUTHOR_TAG, we use the first 6 layers of bert - base to initialize the successor model since the over - parameterized nature of transformer [ 38 ] could cause the model unable to converge while training on small datasets.', 'during module replacing, we fix the batch size as 32 for all evaluated tasks to reduce the search space.', 'all r variables only sample once for a training batch.', 'the maximum sequence length is set to 256 on qnli and 128 for the other tasks.', 'we perform grid search over the sets of learning rate lr as { 1e - 5, 2e - 5 }, the basic replacing rate b as { 0. 1, 0. 3 }, the scheduler coefficient k making the dynamic replacing rate increase to 1 within the first { 1000, 5000, 10000, 30000 } training steps.', 'we apply an early stopping mechanism and select the model with the best performance on the dev set.', 'we conduct our experiments on a single nvidia v100 16gb gpu.', 'the peak memory usage is identical to fine - tuning a bert - base, since there would be at most 12 layers training at the same time.', 'the training time for each task varies depending on the different sizes of training sets.', 'for example, it takes 20 hours to train on mnli but less than 30 minutes on mrpc']",5
['14 ] as in  #TAUTHOR_TAG'],['up a baseline of vanilla knowledge distillation [ 14 ] as in  #TAUTHOR_TAG'],"['proposed approach to existing methods.', 'we set up a baseline of vanilla knowledge distillation [ 14 ] as in  #TAUTHOR_TAG.', 'additionally, we directly']","['shown in table 1, we compare the layer numbers, parameter numbers, loss function, external data usage and model agnosticism of our proposed approach to existing methods.', 'we set up a baseline of vanilla knowledge distillation [ 14 ] as in  #TAUTHOR_TAG.', 'additionally, we directly fine - tune an initialized 6 - layer bert model on glue tasks to obtain a natural fine - tuning baseline.', 'under the setting of compressing 12 - layer bert - base to a 6 - layer compact model, we choose bert - pkd  #TAUTHOR_TAG, pd - bert [ 37 ], and distillbert [ 28 ] as strong baselines.', 'note that distillbert [ 28 ] is not directly comparable here since it uses a pretraining compression setting.', 'both pd - bert and distillbert use external unlabeled corpus.', 'we do not include tinybert [ 15 ] since it has a different size setting, conducts distillation twice, and leverages extra augmented data for glue tasks.', 'we also exclude mobilebert [ 34 ], due to its redesigned transformer block and different model size.', 'besides, in these two studies, the loss functions are not architecture - agnostic thus limit their applications on other models']",5
['14 ] as in  #TAUTHOR_TAG'],['up a baseline of vanilla knowledge distillation [ 14 ] as in  #TAUTHOR_TAG'],"['proposed approach to existing methods.', 'we set up a baseline of vanilla knowledge distillation [ 14 ] as in  #TAUTHOR_TAG.', 'additionally, we directly']","['shown in table 1, we compare the layer numbers, parameter numbers, loss function, external data usage and model agnosticism of our proposed approach to existing methods.', 'we set up a baseline of vanilla knowledge distillation [ 14 ] as in  #TAUTHOR_TAG.', 'additionally, we directly fine - tune an initialized 6 - layer bert model on glue tasks to obtain a natural fine - tuning baseline.', 'under the setting of compressing 12 - layer bert - base to a 6 - layer compact model, we choose bert - pkd  #TAUTHOR_TAG, pd - bert [ 37 ], and distillbert [ 28 ] as strong baselines.', 'note that distillbert [ 28 ] is not directly comparable here since it uses a pretraining compression setting.', 'both pd - bert and distillbert use external unlabeled corpus.', 'we do not include tinybert [ 15 ] since it has a different size setting, conducts distillation twice, and leverages extra augmented data for glue tasks.', 'we also exclude mobilebert [ 34 ], due to its redesigned transformer block and different model size.', 'besides, in these two studies, the loss functions are not architecture - agnostic thus limit their applications on other models']",5
"['studies [ 28,  #TAUTHOR_TAG 15 ].', '']","['studies [ 28,  #TAUTHOR_TAG 15 ].', 'afterward,']","['a predecessor model with comparable performance with that reported in previous studies [ 28,  #TAUTHOR_TAG 15 ].', '']","['fine - tune bert - base as the predecessor model for each task with the batch size of 32, the learning rate of 2 × 10 −5, and the number of epochs as 4.', 'as a result, we are able to obtain a predecessor model with comparable performance with that reported in previous studies [ 28,  #TAUTHOR_TAG 15 ].', 'afterward, for training successor models, following [ 28,  #TAUTHOR_TAG, we use the first 6 layers of bert - base to initialize the successor model since the over - parameterized nature of transformer [ 38 ] could cause the model unable to converge while training on small datasets.', 'during module replacing, we fix the batch size as 32 for all evaluated tasks to reduce the search space.', 'all r variables only sample once for a training batch.', 'the maximum sequence length is set to 256 on qnli and 128 for the other tasks.', 'we perform grid search over the sets of learning rate lr as { 1e - 5, 2e - 5 }, the basic replacing rate b as { 0. 1, 0. 3 }, the scheduler coefficient k making the dynamic replacing rate increase to 1 within the first { 1000, 5000, 10000, 30000 } training steps.', 'we apply an early stopping mechanism and select the model with the best performance on the dev set.', 'we conduct our experiments on a single nvidia v100 16gb gpu.', 'the peak memory usage is identical to fine - tuning a bert - base, since there would be at most 12 layers training at the same time.', 'the training time for each task varies depending on the different sizes of training sets.', 'for example, it takes 20 hours to train on mnli but less than 30 minutes on mrpc']",3
"[')  #TAUTHOR_TAG, showing']","['patient knowledge distillation ( pkd )  #TAUTHOR_TAG, showing']","[')  #TAUTHOR_TAG, showing']",[' #TAUTHOR_TAG'],4
"['', 'in  #TAUTHOR_TAG 3 ] we describe']","['15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe']","['core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of -']","['being developed for bn typically use lightly supervised transcripts for training [ 6 ]. the ears program led to significant advances in speech recognition technology for both domains, with the development of techniques that could ingest large quantities of', 'unsupervised or semisupervised training data, discriminative training of recognition models, methods to deal with', 'channel and speaker variabilities in the data, real - time decoding of test data, and also approaches to combine outputs from various systems', '[ 8, 9, 10, 11 ]. several of these techniques have further been extended to build asr systems on broadcast news data in various languages [ 12, 13, 14 ]. figure 1 shows progress', 'made in this domain over the past two decades. more recently, as part of the mgb challenge, in addition to the core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of - the - art speech recognition systems on the cts task using multiple lstm and resnet acoustic models trained on various acoustic features along with word and character lstms and convolutional', 'wavenet - style language models. this ad - vanced recipe achieves', '5. 1 % and 9. 9 % on the switchboard and callhome', 'subsets of the hub5 2000 evaluation. in this paper we develop a similar but simpler variant for bn. as described earlier,', 'by developing this system we investigate how these earlier proposed systems can be trained on bn data which are not human annotated but are created using closed', 'captions. to create these systems, instead of adding all the available training data we carefully select a reliable subset. we then train lstm and residual network based', 'acoustic models with a combination of n - gram and neural network language models on this selected data', '. in addition to automatic speech recognition results, similar to  #TAUTHOR_TAG, we also present human performance on the', 'same bn test sets. these evaluations allow us to properly benchmark our automatic system performance.', 'similar to earlier human performance evaluations on cts, we observe a significant gap between human and automatic results. the', 'rest of the paper is organized as follows. in section 2 we describe the human evaluation experiments on two broadcast news test sets - rt04 and dev04f. we also compare the recognition errors we observe with human and automatic recognition systems. section 3 describes the development of our asr systems - training data selection, acoustic and language model building. in section 4 we present wer results using the proposed system.', '']",0
"['', 'in  #TAUTHOR_TAG 3 ] we describe']","['15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe']","['core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of -']","['being developed for bn typically use lightly supervised transcripts for training [ 6 ]. the ears program led to significant advances in speech recognition technology for both domains, with the development of techniques that could ingest large quantities of', 'unsupervised or semisupervised training data, discriminative training of recognition models, methods to deal with', 'channel and speaker variabilities in the data, real - time decoding of test data, and also approaches to combine outputs from various systems', '[ 8, 9, 10, 11 ]. several of these techniques have further been extended to build asr systems on broadcast news data in various languages [ 12, 13, 14 ]. figure 1 shows progress', 'made in this domain over the past two decades. more recently, as part of the mgb challenge, in addition to the core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of - the - art speech recognition systems on the cts task using multiple lstm and resnet acoustic models trained on various acoustic features along with word and character lstms and convolutional', 'wavenet - style language models. this ad - vanced recipe achieves', '5. 1 % and 9. 9 % on the switchboard and callhome', 'subsets of the hub5 2000 evaluation. in this paper we develop a similar but simpler variant for bn. as described earlier,', 'by developing this system we investigate how these earlier proposed systems can be trained on bn data which are not human annotated but are created using closed', 'captions. to create these systems, instead of adding all the available training data we carefully select a reliable subset. we then train lstm and residual network based', 'acoustic models with a combination of n - gram and neural network language models on this selected data', '. in addition to automatic speech recognition results, similar to  #TAUTHOR_TAG, we also present human performance on the', 'same bn test sets. these evaluations allow us to properly benchmark our automatic system performance.', 'similar to earlier human performance evaluations on cts, we observe a significant gap between human and automatic results. the', 'rest of the paper is organized as follows. in section 2 we describe the human evaluation experiments on two broadcast news test sets - rt04 and dev04f. we also compare the recognition errors we observe with human and automatic recognition systems. section 3 describes the development of our asr systems - training data selection, acoustic and language model building. in section 4 we present wer results using the proposed system.', '']",0
"['', 'in  #TAUTHOR_TAG 3 ] we describe']","['15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe']","['core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of -']","['being developed for bn typically use lightly supervised transcripts for training [ 6 ]. the ears program led to significant advances in speech recognition technology for both domains, with the development of techniques that could ingest large quantities of', 'unsupervised or semisupervised training data, discriminative training of recognition models, methods to deal with', 'channel and speaker variabilities in the data, real - time decoding of test data, and also approaches to combine outputs from various systems', '[ 8, 9, 10, 11 ]. several of these techniques have further been extended to build asr systems on broadcast news data in various languages [ 12, 13, 14 ]. figure 1 shows progress', 'made in this domain over the past two decades. more recently, as part of the mgb challenge, in addition to the core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of - the - art speech recognition systems on the cts task using multiple lstm and resnet acoustic models trained on various acoustic features along with word and character lstms and convolutional', 'wavenet - style language models. this ad - vanced recipe achieves', '5. 1 % and 9. 9 % on the switchboard and callhome', 'subsets of the hub5 2000 evaluation. in this paper we develop a similar but simpler variant for bn. as described earlier,', 'by developing this system we investigate how these earlier proposed systems can be trained on bn data which are not human annotated but are created using closed', 'captions. to create these systems, instead of adding all the available training data we carefully select a reliable subset. we then train lstm and residual network based', 'acoustic models with a combination of n - gram and neural network language models on this selected data', '. in addition to automatic speech recognition results, similar to  #TAUTHOR_TAG, we also present human performance on the', 'same bn test sets. these evaluations allow us to properly benchmark our automatic system performance.', 'similar to earlier human performance evaluations on cts, we observe a significant gap between human and automatic results. the', 'rest of the paper is organized as follows. in section 2 we describe the human evaluation experiments on two broadcast news test sets - rt04 and dev04f. we also compare the recognition errors we observe with human and automatic recognition systems. section 3 describes the development of our asr systems - training data selection, acoustic and language model building. in section 4 we present wer results using the proposed system.', '']",1
"['', 'in  #TAUTHOR_TAG 3 ] we describe']","['15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe']","['core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of -']","['being developed for bn typically use lightly supervised transcripts for training [ 6 ]. the ears program led to significant advances in speech recognition technology for both domains, with the development of techniques that could ingest large quantities of', 'unsupervised or semisupervised training data, discriminative training of recognition models, methods to deal with', 'channel and speaker variabilities in the data, real - time decoding of test data, and also approaches to combine outputs from various systems', '[ 8, 9, 10, 11 ]. several of these techniques have further been extended to build asr systems on broadcast news data in various languages [ 12, 13, 14 ]. figure 1 shows progress', 'made in this domain over the past two decades. more recently, as part of the mgb challenge, in addition to the core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of - the - art speech recognition systems on the cts task using multiple lstm and resnet acoustic models trained on various acoustic features along with word and character lstms and convolutional', 'wavenet - style language models. this ad - vanced recipe achieves', '5. 1 % and 9. 9 % on the switchboard and callhome', 'subsets of the hub5 2000 evaluation. in this paper we develop a similar but simpler variant for bn. as described earlier,', 'by developing this system we investigate how these earlier proposed systems can be trained on bn data which are not human annotated but are created using closed', 'captions. to create these systems, instead of adding all the available training data we carefully select a reliable subset. we then train lstm and residual network based', 'acoustic models with a combination of n - gram and neural network language models on this selected data', '. in addition to automatic speech recognition results, similar to  #TAUTHOR_TAG, we also present human performance on the', 'same bn test sets. these evaluations allow us to properly benchmark our automatic system performance.', 'similar to earlier human performance evaluations on cts, we observe a significant gap between human and automatic results. the', 'rest of the paper is organized as follows. in section 2 we describe the human evaluation experiments on two broadcast news test sets - rt04 and dev04f. we also compare the recognition errors we observe with human and automatic recognition systems. section 3 describes the development of our asr systems - training data selection, acoustic and language model building. in section 4 we present wer results using the proposed system.', '']",1
"['', 'in  #TAUTHOR_TAG 3 ] we describe']","['15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe']","['core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of -']","['being developed for bn typically use lightly supervised transcripts for training [ 6 ]. the ears program led to significant advances in speech recognition technology for both domains, with the development of techniques that could ingest large quantities of', 'unsupervised or semisupervised training data, discriminative training of recognition models, methods to deal with', 'channel and speaker variabilities in the data, real - time decoding of test data, and also approaches to combine outputs from various systems', '[ 8, 9, 10, 11 ]. several of these techniques have further been extended to build asr systems on broadcast news data in various languages [ 12, 13, 14 ]. figure 1 shows progress', 'made in this domain over the past two decades. more recently, as part of the mgb challenge, in addition to the core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of - the - art speech recognition systems on the cts task using multiple lstm and resnet acoustic models trained on various acoustic features along with word and character lstms and convolutional', 'wavenet - style language models. this ad - vanced recipe achieves', '5. 1 % and 9. 9 % on the switchboard and callhome', 'subsets of the hub5 2000 evaluation. in this paper we develop a similar but simpler variant for bn. as described earlier,', 'by developing this system we investigate how these earlier proposed systems can be trained on bn data which are not human annotated but are created using closed', 'captions. to create these systems, instead of adding all the available training data we carefully select a reliable subset. we then train lstm and residual network based', 'acoustic models with a combination of n - gram and neural network language models on this selected data', '. in addition to automatic speech recognition results, similar to  #TAUTHOR_TAG, we also present human performance on the', 'same bn test sets. these evaluations allow us to properly benchmark our automatic system performance.', 'similar to earlier human performance evaluations on cts, we observe a significant gap between human and automatic results. the', 'rest of the paper is organized as follows. in section 2 we describe the human evaluation experiments on two broadcast news test sets - rt04 and dev04f. we also compare the recognition errors we observe with human and automatic recognition systems. section 3 describes the development of our asr systems - training data selection, acoustic and language model building. in section 4 we present wer results using the proposed system.', '']",3
"['', 'in  #TAUTHOR_TAG 3 ] we describe']","['15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe']","['core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of -']","['being developed for bn typically use lightly supervised transcripts for training [ 6 ]. the ears program led to significant advances in speech recognition technology for both domains, with the development of techniques that could ingest large quantities of', 'unsupervised or semisupervised training data, discriminative training of recognition models, methods to deal with', 'channel and speaker variabilities in the data, real - time decoding of test data, and also approaches to combine outputs from various systems', '[ 8, 9, 10, 11 ]. several of these techniques have further been extended to build asr systems on broadcast news data in various languages [ 12, 13, 14 ]. figure 1 shows progress', 'made in this domain over the past two decades. more recently, as part of the mgb challenge, in addition to the core asr problem, several', 'other related tasks - speaker diarization and lightly supervised alignment of data have also been studied [ 15, 16 ].', 'in  #TAUTHOR_TAG 3 ] we describe state - of - the - art speech recognition systems on the cts task using multiple lstm and resnet acoustic models trained on various acoustic features along with word and character lstms and convolutional', 'wavenet - style language models. this ad - vanced recipe achieves', '5. 1 % and 9. 9 % on the switchboard and callhome', 'subsets of the hub5 2000 evaluation. in this paper we develop a similar but simpler variant for bn. as described earlier,', 'by developing this system we investigate how these earlier proposed systems can be trained on bn data which are not human annotated but are created using closed', 'captions. to create these systems, instead of adding all the available training data we carefully select a reliable subset. we then train lstm and residual network based', 'acoustic models with a combination of n - gram and neural network language models on this selected data', '. in addition to automatic speech recognition results, similar to  #TAUTHOR_TAG, we also present human performance on the', 'same bn test sets. these evaluations allow us to properly benchmark our automatic system performance.', 'similar to earlier human performance evaluations on cts, we observe a significant gap between human and automatic results. the', 'rest of the paper is organized as follows. in section 2 we describe the human evaluation experiments on two broadcast news test sets - rt04 and dev04f. we also compare the recognition errors we observe with human and automatic recognition systems. section 3 describes the development of our asr systems - training data selection, acoustic and language model building. in section 4 we present wer results using the proposed system.', '']",3
"['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04']","['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04']","['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04']","['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04 and dev04f - are carried out by appen.', 'for these evaluations we limit the audio from the test sets to only regions of speech that are marked for scoring using the original references and scoring scripts provided during the ears evaluation.', 'after processing, the rt04 test set has 4 hours of bn data from 12 shows with about 230 overlapping speakers across the shows.', 'the dev04f test set is smaller, with about 2 hours of data from 6 shows with close to 100 overlapping speakers across the shows.', 'the first round of transcripts was produced by three independent transcribers, followed by quality checking by a fourth senior transcriber.', 'all four transcribers are native us english speakers and were selected based on the quality of their work on past transcription projects.', 'the transcriptions were produced in line with ldc transcription guidelines for hyphenations, spelled abbreviations, contractions, partial words, non - speech sounds, etc.', 'that were used to produce the original transcripts for these test sets.', '']",3
"['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable']","['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance.', 'the convolutional']","['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance.', 'the convolutional network used in that work is']","['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance.', 'the convolutional network used in that work is a residual network ( resnet ) and an lstm is used as the non - convolutional network.', 'the acoustic scores of these systems are subsequently combined for the final decodes.', 'similar to that work, in this paper also we train resnet and lstm based acoustic models.', 'both these acoustic models are based on speaker transformed features.', 'the resnet uses 40 dimensional vtl - warped log - mel fea -, we train an lstm acoustic model with 6 bidirectional layers having 1024 cells per layer ( 512 per direction ), one linear bottleneck layer with 256 units and an output layer with 32k units corresponding to the contextdependent hmm states we derived in the hmm - gmm system build.', 'the model is trained using non - overlapping subsequences of 21 frames.', 'subsequences from different utterances are grouped into mini - batches of size 128 for processing speed and reliable gradient estimates.', 'after the cross - entropy based training on the bn - 1300 corpus has converged we also sequence train the model using the 144 hours of carefully transcribed audio.', 'to complement the lstm acoustic model, we train a deep residual network based on the best performing architecture proposed in  #TAUTHOR_TAG.', 'the resnet has 12 residual blocks followed by 5 fully connected layers.', 'to effectively train this network with 25 convolutional layers, a short - cut connection is placed between each residual block to allow for additional flow of information and gradients.', 'each layer has a batch normalization layer as well.', 'table 2 gives a summary of the network architecture of the resnet model.', 'the resnet consists of several stages with different numbers of feature maps in each stage : 64 in stage 1, 128 in stage 2, 256 in stage 3 and 512 in stage 4.', 'each stage has an initstride which indicates the ( frequency × time ) stride for the first block of that stage as the number of feature maps is increased.', 'the stride applies to both the first 3×3 convolution of each block and also the 1×1 convolution in projection shortcut between each block.', 'the resnet acoustic model is trained using the cross - entropy training criterion on the bn - 1300 corpus and then sequence trained using the 144 hours of carefully transcribed audio']",3
"['cts evaluation with similar acoustic models  #TAUTHOR_TAG,']","['cts evaluation with similar acoustic models  #TAUTHOR_TAG,']","['on the cts evaluation with similar acoustic models  #TAUTHOR_TAG, the lstm']",[' #TAUTHOR_TAG'],3
"['cts evaluation with similar acoustic models  #TAUTHOR_TAG,']","['cts evaluation with similar acoustic models  #TAUTHOR_TAG,']","['on the cts evaluation with similar acoustic models  #TAUTHOR_TAG, the lstm']",[' #TAUTHOR_TAG'],3
['telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is'],['telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is'],"['insertion errors for humans and asr systems on dev04f and rt04', '4. compared to the telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is the back - channel response - this is probably from the very nature of the bn domain.', '']","['', 'most frequent deletion and insertion errors for humans and asr systems on dev04f and rt04', '4. compared to the telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is the back - channel response - this is probably from the very nature of the bn domain.', '5. similar to telephone conversation confusions reported in  #TAUTHOR_TAG, humans performance is much higher because the number of deletions is significantly lower - compare 2. 3 % vs 0. 8 % / 0. 6 % for deletion errors in table 5']",3
"['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04']","['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04']","['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04']","['to  #TAUTHOR_TAG, human performance measurements on two broadcast news tasks - rt04 and dev04f - are carried out by appen.', 'for these evaluations we limit the audio from the test sets to only regions of speech that are marked for scoring using the original references and scoring scripts provided during the ears evaluation.', 'after processing, the rt04 test set has 4 hours of bn data from 12 shows with about 230 overlapping speakers across the shows.', 'the dev04f test set is smaller, with about 2 hours of data from 6 shows with close to 100 overlapping speakers across the shows.', 'the first round of transcripts was produced by three independent transcribers, followed by quality checking by a fourth senior transcriber.', 'all four transcribers are native us english speakers and were selected based on the quality of their work on past transcription projects.', 'the transcriptions were produced in line with ldc transcription guidelines for hyphenations, spelled abbreviations, contractions, partial words, non - speech sounds, etc.', 'that were used to produce the original transcripts for these test sets.', '']",5
"['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable']","['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance.', 'the convolutional']","['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance.', 'the convolutional network used in that work is']","['discussed earlier, one of the key objectives of this work is to verify the usefulness of our earlier proposed system strategy for cts.', 'in  #TAUTHOR_TAG, two kinds of acoustic models, a convolutional and a non - convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance.', 'the convolutional network used in that work is a residual network ( resnet ) and an lstm is used as the non - convolutional network.', 'the acoustic scores of these systems are subsequently combined for the final decodes.', 'similar to that work, in this paper also we train resnet and lstm based acoustic models.', 'both these acoustic models are based on speaker transformed features.', 'the resnet uses 40 dimensional vtl - warped log - mel fea -, we train an lstm acoustic model with 6 bidirectional layers having 1024 cells per layer ( 512 per direction ), one linear bottleneck layer with 256 units and an output layer with 32k units corresponding to the contextdependent hmm states we derived in the hmm - gmm system build.', 'the model is trained using non - overlapping subsequences of 21 frames.', 'subsequences from different utterances are grouped into mini - batches of size 128 for processing speed and reliable gradient estimates.', 'after the cross - entropy based training on the bn - 1300 corpus has converged we also sequence train the model using the 144 hours of carefully transcribed audio.', 'to complement the lstm acoustic model, we train a deep residual network based on the best performing architecture proposed in  #TAUTHOR_TAG.', 'the resnet has 12 residual blocks followed by 5 fully connected layers.', 'to effectively train this network with 25 convolutional layers, a short - cut connection is placed between each residual block to allow for additional flow of information and gradients.', 'each layer has a batch normalization layer as well.', 'table 2 gives a summary of the network architecture of the resnet model.', 'the resnet consists of several stages with different numbers of feature maps in each stage : 64 in stage 1, 128 in stage 2, 256 in stage 3 and 512 in stage 4.', 'each stage has an initstride which indicates the ( frequency × time ) stride for the first block of that stage as the number of feature maps is increased.', 'the stride applies to both the first 3×3 convolution of each block and also the 1×1 convolution in projection shortcut between each block.', 'the resnet acoustic model is trained using the cross - entropy training criterion on the bn - 1300 corpus and then sequence trained using the 144 hours of carefully transcribed audio']",5
['telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is'],['telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is'],"['insertion errors for humans and asr systems on dev04f and rt04', '4. compared to the telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is the back - channel response - this is probably from the very nature of the bn domain.', '']","['', 'most frequent deletion and insertion errors for humans and asr systems on dev04f and rt04', '4. compared to the telephone conversation confusions recorded in  #TAUTHOR_TAG - one symbol that is clearly missing is the back - channel response - this is probably from the very nature of the bn domain.', '5. similar to telephone conversation confusions reported in  #TAUTHOR_TAG, humans performance is much higher because the number of deletions is significantly lower - compare 2. 3 % vs 0. 8 % / 0. 6 % for deletion errors in table 5']",4
"['novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however,']","['to learn novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however']","['novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however,']","['e. g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionans', '##wering, and machine translation  #AUTHOR_TAG. however, to date their exploitation has been limited because', 'for most languages, no levin style classification is available. since manual classification is costly  #AUTHOR_TAG automatic approaches have been proposed recently', 'which could be used to learn novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however, most work on levin type classification has focussed on english. large - scale research on other languages such as german ( schulte im  #AUTHOR_TAG and japanese  #AUTHOR_TAG has focussed on semantic classification. although the two classification systems have', 'shared properties, studies comparing the overlap between verbnet and wordnet  #AUTHOR_TAG have reported that the mapping is only partial and many to many due to fine - grained nature', 'of classes based on synonymy  #AUTHOR_TAG. only few studies have been conducted on levin style classification for languages other than english. in their experiment involving 59 verbs and three classes,  #AUTHOR_TAG applied a supervised approach developed for english to italian, obtaining high accuracy ( 86. 3 % ). in another', 'experiment with 60 verbs and three classes, they showed that features extracted from chinese translations of english verbs can improve', 'english classification. these results are promising, but those from a later experiment by  #AUTHOR_TAG are not. ferrer applied a clustering approach developed for english to spanish, and evaluated it against', '']",0
['and schulte im  #TAUTHOR_TAG and'],['clustering ( spec ) has proved promising in previous verb clustering experiments ( brew and schulte im  #TAUTHOR_TAG and'],"['and schulte im  #TAUTHOR_TAG and other similar nlp tasks involving high dimensional feature space  #AUTHOR_TAG.', 'following  #TAUTHOR_TAG we used the mncut spectral clustering  #AUTHOR_TAG which has a wide applicability and a clear probabilistic interpretation ( von  #AUTHOR_TAG.', 'however, we extended the method to']","['clustering ( spec ) has proved promising in previous verb clustering experiments ( brew and schulte im  #TAUTHOR_TAG and other similar nlp tasks involving high dimensional feature space  #AUTHOR_TAG.', 'following  #TAUTHOR_TAG we used the mncut spectral clustering  #AUTHOR_TAG which has a wide applicability and a clear probabilistic interpretation ( von  #AUTHOR_TAG.', 'however, we extended the method to determine the optimal number of clusters automatically using the technique proposed by ( zelnik -  #AUTHOR_TAG.', 'clustering groups a given set of verbs v = { v n } n n = 1 into a disjoint partition of k classes.', 'spec takes a similarity matrix as input.', 'all our features can be viewed as probabilistic distributions because the combination of different features is performed via parameterization.', 'thus we use the jensen - shannon divergence ( jsd ) to construct the similarity matrix.', '']",0
"['novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however,']","['to learn novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however']","['novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however,']","['e. g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionans', '##wering, and machine translation  #AUTHOR_TAG. however, to date their exploitation has been limited because', 'for most languages, no levin style classification is available. since manual classification is costly  #AUTHOR_TAG automatic approaches have been proposed recently', 'which could be used to learn novel classifications in a cost - effective', 'manner  #AUTHOR_TAG o seaghdha and  #TAUTHOR_TAG. however, most work on levin type classification has focussed on english. large - scale research on other languages such as german ( schulte im  #AUTHOR_TAG and japanese  #AUTHOR_TAG has focussed on semantic classification. although the two classification systems have', 'shared properties, studies comparing the overlap between verbnet and wordnet  #AUTHOR_TAG have reported that the mapping is only partial and many to many due to fine - grained nature', 'of classes based on synonymy  #AUTHOR_TAG. only few studies have been conducted on levin style classification for languages other than english. in their experiment involving 59 verbs and three classes,  #AUTHOR_TAG applied a supervised approach developed for english to italian, obtaining high accuracy ( 86. 3 % ). in another', 'experiment with 60 verbs and three classes, they showed that features extracted from chinese translations of english verbs can improve', 'english classification. these results are promising, but those from a later experiment by  #AUTHOR_TAG are not. ferrer applied a clustering approach developed for english to spanish, and evaluated it against', '']",5
['and schulte im  #TAUTHOR_TAG and'],['clustering ( spec ) has proved promising in previous verb clustering experiments ( brew and schulte im  #TAUTHOR_TAG and'],"['and schulte im  #TAUTHOR_TAG and other similar nlp tasks involving high dimensional feature space  #AUTHOR_TAG.', 'following  #TAUTHOR_TAG we used the mncut spectral clustering  #AUTHOR_TAG which has a wide applicability and a clear probabilistic interpretation ( von  #AUTHOR_TAG.', 'however, we extended the method to']","['clustering ( spec ) has proved promising in previous verb clustering experiments ( brew and schulte im  #TAUTHOR_TAG and other similar nlp tasks involving high dimensional feature space  #AUTHOR_TAG.', 'following  #TAUTHOR_TAG we used the mncut spectral clustering  #AUTHOR_TAG which has a wide applicability and a clear probabilistic interpretation ( von  #AUTHOR_TAG.', 'however, we extended the method to determine the optimal number of clusters automatically using the technique proposed by ( zelnik -  #AUTHOR_TAG.', 'clustering groups a given set of verbs v = { v n } n n = 1 into a disjoint partition of k classes.', 'spec takes a similarity matrix as input.', 'all our features can be viewed as probabilistic distributions because the combination of different features is performed via parameterization.', 'thus we use the jensen - shannon divergence ( jsd ) to construct the similarity matrix.', '']",5
"['measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', '']","['measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', '']","['employ the same measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', 'each cluster is associated with']","['employ the same measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', 'each cluster is associated with its prevalent class.', 'the number of verbs in a cluster k that take this class is denoted by n prevalent ( k ).', 'verbs that do not take it are considered as errors.', 'clusters where n prevalent ( k ) = 1 are disregarded as not to introduce a bias towards singletons']",5
"['. g. ( schulte im  #TAUTHOR_TAG.', 'like these']","['french  #AUTHOR_TAG.', 'systems similar to assci have been used in recent verb classification works e. g. ( schulte im  #TAUTHOR_TAG.', 'like these']","['have been used in recent verb classification works e. g. ( schulte im  #TAUTHOR_TAG.', 'like these other systems, assci takes raw corpus data as input.', 'the data is']","['extracted the features for clustering from lexschem.', 'this large subcategorization lexicon provides scf frequency information for 3, 297 french verbs.', 'it was acquired fully automatically from le monde newspaper corpus ( 200m words from years 1991 - 2000 ) using assci - a recent subcategorization acquisition system for french  #AUTHOR_TAG.', 'systems similar to assci have been used in recent verb classification works e. g. ( schulte im  #TAUTHOR_TAG.', 'like these other systems, assci takes raw corpus data as input.', 'the data is first tagged and lemmatized using the tree - tagger and then parsed using syntex  #AUTHOR_TAG.', 'syntex is a shallow parser which employs a combination of statistics and heuristics to identify grammatical relations ( grs ) in sentences.', 'assci considers grs where the target verbs occur and constructs scfs from nominal, prepositional and adjectival phrases, and infinitival and subordinate clauses.', 'when a verb has no dependency, its scf is considered as intransitive.', 'assci assumes no predefined list of scfs but almost any combination of permitted constructions can appear as a candidate scf.', 'the number of automatically generated scf types in lexschem is 336.', 'many candidate scfs are noisy due to processing errors and the difficulty of argument - adjunct distinction.', 'most scf systems assume that true arguments occur in argument positions more frequently than adjuncts.', 'many systems also integrate filters for removing noise from system output.', 'when lexschem was evaluated after filter - ing its f - measure was 69 - which is similar to that of other current scf systems we used the unfiltered version of the lexicon because english experiments have shown that information about adjuncts can help verb clustering  #AUTHOR_TAG']",3
['and schulte im  #TAUTHOR_TAG and'],['clustering ( spec ) has proved promising in previous verb clustering experiments ( brew and schulte im  #TAUTHOR_TAG and'],"['and schulte im  #TAUTHOR_TAG and other similar nlp tasks involving high dimensional feature space  #AUTHOR_TAG.', 'following  #TAUTHOR_TAG we used the mncut spectral clustering  #AUTHOR_TAG which has a wide applicability and a clear probabilistic interpretation ( von  #AUTHOR_TAG.', 'however, we extended the method to']","['clustering ( spec ) has proved promising in previous verb clustering experiments ( brew and schulte im  #TAUTHOR_TAG and other similar nlp tasks involving high dimensional feature space  #AUTHOR_TAG.', 'following  #TAUTHOR_TAG we used the mncut spectral clustering  #AUTHOR_TAG which has a wide applicability and a clear probabilistic interpretation ( von  #AUTHOR_TAG.', 'however, we extended the method to determine the optimal number of clusters automatically using the technique proposed by ( zelnik -  #AUTHOR_TAG.', 'clustering groups a given set of verbs v = { v n } n n = 1 into a disjoint partition of k classes.', 'spec takes a similarity matrix as input.', 'all our features can be viewed as probabilistic distributions because the combination of different features is performed via parameterization.', 'thus we use the jensen - shannon divergence ( jsd ) to construct the similarity matrix.', '']",3
"['measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', '']","['measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', '']","['employ the same measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', 'each cluster is associated with']","['employ the same measures for evaluation as previously employed e. g. byo seaghdha and  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the first measure is modified purity ( mpur ) - a global measure which evaluates the mean precision of clusters.', 'each cluster is associated with its prevalent class.', 'the number of verbs in a cluster k that take this class is denoted by n prevalent ( k ).', 'verbs that do not take it are considered as errors.', 'clusters where n prevalent ( k ) = 1 are disregarded as not to introduce a bias towards singletons']",3
,,,,3
,,,,3
"['to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we']","['to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we']","['following a lemmatized verb.', 'stop words are removed prior to extraction.', 'we adopt a fully unsupervised approach to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we determine the optimal number of sp clusters automatically following zelnik -  #AUTHOR_TAG.', 'the method is introduced in the following section.', 'the approach involves ( i ) taking the grs']","['following six features include information about the lexical context ( co - occurrences ) of verbs.', 'we adopt the best method of  #AUTHOR_TAG where collocations ( cos ) are extracted from the window of words immediately preceding and following a lemmatized verb.', 'stop words are removed prior to extraction.', 'we adopt a fully unsupervised approach to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we determine the optimal number of sp clusters automatically following zelnik -  #AUTHOR_TAG.', 'the method is introduced in the following section.', 'the approach involves ( i ) taking the grs ( subj, obj, iobj ) associated with verbs, ( ii ) extracting all the argument heads in these grs, and ( iii ) clustering the resulting n most frequent argument heads into m classes.', 'the empirically determined n 200 was used.', 'the method produced 40 sp clusters']",4
"['in the experiment of  #TAUTHOR_TAG.', 'however, it']","['in the experiment of  #TAUTHOR_TAG.', 'however, it']","['in the experiment of  #TAUTHOR_TAG.', 'however, it does compare favourably']","['sufficient corpus data is available, there is a strong correlation between the types of features which perform the best in english and french.', 'when the best features are used, many individual levin classes have similar performance in the two languages.', 'due to differences in data sets direct comparison of performance figures for english and french is not possible.', 'when considering the general level of performance, our best performance for french ( 65. 4 f ) is lower than the best performance for english in the experiment of  #TAUTHOR_TAG.', 'however, it does compare favourably to the performance of other stateof - the - art ( even supervised ) english systems  #AUTHOR_TAG o seaghdha and  #AUTHOR_TAG.', 'this is impressive considering that we experimented with a fully unsupervised approach originally developed for another language.', 'when aiming to improve performance further, employing larger data is critical.', 'most recent experiments on english have employed bigger data sets, and unlike us, some of them have only considered the predominant senses of medium - high frequency verbs.', 'as seen in section 7. 1, such differences in data can have significant impact on performance.', 'however, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further  #TAUTHOR_TAG.', '']",4
"['to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we']","['to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we']","['following a lemmatized verb.', 'stop words are removed prior to extraction.', 'we adopt a fully unsupervised approach to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we determine the optimal number of sp clusters automatically following zelnik -  #AUTHOR_TAG.', 'the method is introduced in the following section.', 'the approach involves ( i ) taking the grs']","['following six features include information about the lexical context ( co - occurrences ) of verbs.', 'we adopt the best method of  #AUTHOR_TAG where collocations ( cos ) are extracted from the window of words immediately preceding and following a lemmatized verb.', 'stop words are removed prior to extraction.', 'we adopt a fully unsupervised approach to sp acquisition using the method of  #TAUTHOR_TAG, with the difference that we determine the optimal number of sp clusters automatically following zelnik -  #AUTHOR_TAG.', 'the method is introduced in the following section.', 'the approach involves ( i ) taking the grs ( subj, obj, iobj ) associated with verbs, ( ii ) extracting all the argument heads in these grs, and ( iii ) clustering the resulting n most frequent argument heads into m classes.', 'the empirically determined n 200 was used.', 'the method produced 40 sp clusters']",6
"['in the experiment of  #TAUTHOR_TAG.', 'however, it']","['in the experiment of  #TAUTHOR_TAG.', 'however, it']","['in the experiment of  #TAUTHOR_TAG.', 'however, it does compare favourably']","['sufficient corpus data is available, there is a strong correlation between the types of features which perform the best in english and french.', 'when the best features are used, many individual levin classes have similar performance in the two languages.', 'due to differences in data sets direct comparison of performance figures for english and french is not possible.', 'when considering the general level of performance, our best performance for french ( 65. 4 f ) is lower than the best performance for english in the experiment of  #TAUTHOR_TAG.', 'however, it does compare favourably to the performance of other stateof - the - art ( even supervised ) english systems  #AUTHOR_TAG o seaghdha and  #AUTHOR_TAG.', 'this is impressive considering that we experimented with a fully unsupervised approach originally developed for another language.', 'when aiming to improve performance further, employing larger data is critical.', 'most recent experiments on english have employed bigger data sets, and unlike us, some of them have only considered the predominant senses of medium - high frequency verbs.', 'as seen in section 7. 1, such differences in data can have significant impact on performance.', 'however, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further  #TAUTHOR_TAG.', '']",2
"['3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tag']","['3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tagger and dependency parser.', 'they used a statistical relational learning approach in which features are constructed declaratively using intentional relation.', 'unlike us and  #AUTHOR_TAG they have used svm - hmm 2 for learning.', 'similar to  #AUTHOR_TAG they did']","['s ) and unstructured ( u ) data are summarised in table 3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tagger and dependency parser.', 'they used a statistical relational learning approach in which features are constructed declaratively using intentional relation.', 'unlike us and  #AUTHOR_TAG they have used svm - hmm 2 for learning.', 'similar to  #AUTHOR_TAG they did 10 fold cross validation and the best microaverage f - score of']","['first attempt to classify abstract sentences based on the piboso schema is made by  #AUTHOR_TAG.', 'they used the conditional random field ( crf ) classifier for learning, and their feature set included lexical features ( unigram and bigram with part - of - speech ), semantic features ( using metathesaurus ), structural features ( sentence positional features ) and sequential features ( features from previous sentences ).', 'they found out that the best features are unigrams, sentence po - sitional attributes, and sequential information.', 'using this best configuration of features and the same data set as in our experiment, they did 10 fold cross validation.', 'the best microaverage fscore for each class or label for both structured ( s ) and unstructured ( u ) data are summarised in table 3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tagger and dependency parser.', 'they used a statistical relational learning approach in which features are constructed declaratively using intentional relation.', 'unlike us and  #AUTHOR_TAG they have used svm - hmm 2 for learning.', 'similar to  #AUTHOR_TAG they did 10 fold cross validation and the best microaverage f - score of their system is also summarised in table 3']",0
"['3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tag']","['3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tagger and dependency parser.', 'they used a statistical relational learning approach in which features are constructed declaratively using intentional relation.', 'unlike us and  #AUTHOR_TAG they have used svm - hmm 2 for learning.', 'similar to  #AUTHOR_TAG they did']","['s ) and unstructured ( u ) data are summarised in table 3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tagger and dependency parser.', 'they used a statistical relational learning approach in which features are constructed declaratively using intentional relation.', 'unlike us and  #AUTHOR_TAG they have used svm - hmm 2 for learning.', 'similar to  #AUTHOR_TAG they did 10 fold cross validation and the best microaverage f - score of']","['first attempt to classify abstract sentences based on the piboso schema is made by  #AUTHOR_TAG.', 'they used the conditional random field ( crf ) classifier for learning, and their feature set included lexical features ( unigram and bigram with part - of - speech ), semantic features ( using metathesaurus ), structural features ( sentence positional features ) and sequential features ( features from previous sentences ).', 'they found out that the best features are unigrams, sentence po - sitional attributes, and sequential information.', 'using this best configuration of features and the same data set as in our experiment, they did 10 fold cross validation.', 'the best microaverage fscore for each class or label for both structured ( s ) and unstructured ( u ) data are summarised in table 3.', 'the other attempt of same 6 way piboso classification on the same dataset is presented by  #TAUTHOR_TAG.', 'in this method, the input sentences are pre - processed with a namedentity tagger and dependency parser.', 'they used a statistical relational learning approach in which features are constructed declaratively using intentional relation.', 'unlike us and  #AUTHOR_TAG they have used svm - hmm 2 for learning.', 'similar to  #AUTHOR_TAG they did 10 fold cross validation and the best microaverage f - score of their system is also summarised in table 3']",4
"['categorised an abstract as structured or unstructured might be a bit different from previous approaches by  #AUTHOR_TAG and  #TAUTHOR_TAG.', '']","['categorised an abstract as structured or unstructured might be a bit different from previous approaches by  #AUTHOR_TAG and  #TAUTHOR_TAG.', '']","['absent in unstructured abstracts.', 'please note that the way we categorised an abstract as structured or unstructured might be a bit different from previous approaches by  #AUTHOR_TAG and  #TAUTHOR_TAG.', '']","['build the ebm classifier we used the 800 expert annotated training abstracts and 200 test abstracts which were given as part of the shared task.', ' #AUTHOR_TAG annotated this data using abstracts retrieved from medline.', 'both the training and test abstracts have two types of abstracts, structured ( s ) and unstructured ( s ).', 'in structured abstracts sentences are organised ( and labelled ) in an orderly fashion such as aim, method, results, conclusions and other whereas these labels are absent in unstructured abstracts.', 'please note that the way we categorised an abstract as structured or unstructured might be a bit different from previous approaches by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'if the first sentence in an abstract is a sentence ordering label then we considered the abstract as structured or else unstructured.', 'there are 1000 abstracts containing 11616 sentences in total.', 'statistics of the dataset used are presented in table 1 and table 2 all s u abstracts 1000 37. 4 % 62. 6 % sentences 11616 54. 4 % 45. 6 % in this section we present the details of our feature set, the training ( classification ) algorithm, the tools used and assumptions made in executing the experiments']",4
"['conclusions in this order.', 'using sentence ordering labels for unstructured abstracts is the main difference compared to earlier methods  #TAUTHOR_TAG.', 'we tried 6 combinations of features which will be discussed in results']","['conclusions in this order.', 'using sentence ordering labels for unstructured abstracts is the main difference compared to earlier methods  #TAUTHOR_TAG.', 'we tried 6 combinations of features which will be discussed in results section.', '']","['conclusions in this order.', 'using sentence ordering labels for unstructured abstracts is the main difference compared to earlier methods  #TAUTHOR_TAG.', 'we tried 6 combinations of features which will be discussed in results section.', '']","['have trained our classifier with different set of features which include lexical features, structural features, sequential features and dependency features 3.', '• lexical features include lemmatized bag - ofwords, their part - of - speech, collocational information, the number of content words, verbs and nouns in the sentence ( we have used the med - post  #AUTHOR_TAG part - of - speech tagger ).', '• structural features include position of the sentence in the abstract, normalised sentence position, reverse sentence position  #AUTHOR_TAG.', '• sequential features include previous sentence label, similar to  #AUTHOR_TAG.', 'additionally, for structured abstracts, we use the sentence ordering labels as features : heading, aim, method, results, conclusions.', 'these are provided in the data.', 'since unstructured abstracts do not have these ordering labels, we automatically annotate the training and testing data with ordering labels using simple heuristics.', 'in the unstructured training data, sentences are classified into an ordering label based on its piboso label : background - > aim, ( population or intervention or study design ) - > method, outcome - > results and other - > other.', 'in the unstructured testing data, we have divided sentences into four equal groups based on their position and mapped them to aim, method, results and conclusions in this order.', 'using sentence ordering labels for unstructured abstracts is the main difference compared to earlier methods  #TAUTHOR_TAG.', 'we tried 6 combinations of features which will be discussed in results section.', '']",4
"['( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged']","['( public board ) and 92. 16 % ( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged f - scores as in']","['( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged f - scores as in table 3.', 'our system']","['sentence classifier uses crf learning algorithm 4.', 'we have also executed few experiments using svm and observed crf performed better over this dataset with our choice of features.', 'due to space constraints in this paper we are not comparing crf versus svm results.', 'for feature selection, we used fselector 5 package from r - system 6.', 'from the pool of features, we select the "" meaningful "" features based on the selecting criteria.', 'we have tested several criteria including ( 1 ) information gain ( 2 ) oner ( 3 ) chisquare test ( 4 ) spearman test.', 'among them, information gain outperformed the others.', 'we select the 700 best features from our pool of features based on information gain score.', 'other technique we used for this shared task is "" bootstrapping "".', 'our system performed very well on training data but did not perform well on test data, perhaps it suffered over - fitting.', 'to overcome this, we ran our current best model on test data ( without using gold - standard labels ) and then merge the result with train data to get the new train.', 'in that way, under roc evaluation, we improved our final scores by 3 %.', 'in addition, we also pre - process the data.', 'since the heading such as "" aim, outcome, introduction etc. "" are always classified as "" other "" in train data, when we find sentence which has less than 20 characters and all in upper case ( our notion of heading ), we directly classify it as "" other "" in test data.', 'in this section we present the analysis of results on structured and unstructured abstracts separately.', 'in all our experiments, we performed 10fold cross validation on the given dataset.', 'shared task organisers have used receiver operating characteristic ( roc ) to evaluate the scores.', 'according to roc our best system scored 93. 78 % ( public board ) and 92. 16 % ( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged f - scores as in table 3.', 'our system outperformed previous works in unstructured abstracts ( 22 % higher than state - of - the - art ).', 'our system performed well in classifying background, outcome and other for both structured and un - structured data.', 'however, our system performed poor in classifying study design as very few instances of it is available in both test and train']",4
['systems  #TAUTHOR_TAG'],['systems  #TAUTHOR_TAG'],"['as part - of - speech information, sentence positional features, collocations and sentence ordering labels.', 'our system outperformed earlier existing state - of - art systems  #TAUTHOR_TAG']","['this paper, we have presented a brief overview of our method to classify sentences to support ebm.', 'we showed that structural and lexical features coupled with a crf classifier is an effective method for dealing with sentence classification tasks.', 'the best features in our setting are found to be words, lexical features such as part - of - speech information, sentence positional features, collocations and sentence ordering labels.', 'our system outperformed earlier existing state - of - art systems  #TAUTHOR_TAG']",4
"['( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged']","['( public board ) and 92. 16 % ( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged f - scores as in']","['( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged f - scores as in table 3.', 'our system']","['sentence classifier uses crf learning algorithm 4.', 'we have also executed few experiments using svm and observed crf performed better over this dataset with our choice of features.', 'due to space constraints in this paper we are not comparing crf versus svm results.', 'for feature selection, we used fselector 5 package from r - system 6.', 'from the pool of features, we select the "" meaningful "" features based on the selecting criteria.', 'we have tested several criteria including ( 1 ) information gain ( 2 ) oner ( 3 ) chisquare test ( 4 ) spearman test.', 'among them, information gain outperformed the others.', 'we select the 700 best features from our pool of features based on information gain score.', 'other technique we used for this shared task is "" bootstrapping "".', 'our system performed very well on training data but did not perform well on test data, perhaps it suffered over - fitting.', 'to overcome this, we ran our current best model on test data ( without using gold - standard labels ) and then merge the result with train data to get the new train.', 'in that way, under roc evaluation, we improved our final scores by 3 %.', 'in addition, we also pre - process the data.', 'since the heading such as "" aim, outcome, introduction etc. "" are always classified as "" other "" in train data, when we find sentence which has less than 20 characters and all in upper case ( our notion of heading ), we directly classify it as "" other "" in test data.', 'in this section we present the analysis of results on structured and unstructured abstracts separately.', 'in all our experiments, we performed 10fold cross validation on the given dataset.', 'shared task organisers have used receiver operating characteristic ( roc ) to evaluate the scores.', 'according to roc our best system scored 93. 78 % ( public board ) and 92. 16 % ( private board ).', 'however, we compare our results with  #AUTHOR_TAG and  #TAUTHOR_TAG using the microaveraged f - scores as in table 3.', 'our system outperformed previous works in unstructured abstracts ( 22 % higher than state - of - the - art ).', 'our system performed well in classifying background, outcome and other for both structured and un - structured data.', 'however, our system performed poor in classifying study design as very few instances of it is available in both test and train']",5
"['of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination']","['of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination becomes. it']","['detailed characteristics of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination']","['', 'indicates that character form, pmi - based and word alignment - based information are useful information to discriminate dialects in the gcr. in order to', 'investigate the detailed characteristics of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination becomes. it also has been verified through our experiments. very often, texts written in a character', '']",0
"['of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination']","['of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination becomes. it']","['detailed characteristics of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination']","['', 'indicates that character form, pmi - based and word alignment - based information are useful information to discriminate dialects in the gcr. in order to', 'investigate the detailed characteristics of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination becomes. it also has been verified through our experiments. very often, texts written in a character', '']",0
['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],"['number of studies on identification of similar languages and language varieties have been carride out.', 'for example, murthy and kumar [ 1 ] focused on indian languages identification.', 'meanwhile, ranaivo - malancon [ 2 ] proposed features based on frequencies of character n - grams to identify malay and indonesian.', 'huang and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach to reflect distances among the three varieties of mandarin in mainland china, taiwan and singapore.', 'zampieri and gebre [ 4 ] found that word uni - grams gave very similar performance to character n - gram features in the framework of the probabilistic language model for the brazilian and european portuguese language discrimination.', 'tiedemann and ljubesic [ 5 ] ; ljubesic and kranjcic [ 6 ] showed that the naive bayes classifier with uni - grams achieved high accuracy for the south slavic languages identification.', 'grefenstette [ 11 ] ; lui and cook [ 12 ] found that bag - of - words features outperformed the syntax or character sequencesbased features for the english varieties.', 'besides these works, other recent studies include : spanish varieties identification [ 13 ], arabic varieties discrimination [ 14, 15, 16, 17 ], and persian and dari identification [ 18 ].', 'among the above related works, study  #TAUTHOR_TAG is the most related work to ours.', 'the differences between study  #TAUTHOR_TAG and our work are two - fold :', '( 1 ) they focus on document - level varieties of mandarin in china, taiwan and singapore, while we deal with sentence - level varieties of mandarin in china, hong kong, taiwan, macao, malaysia and singapore.', 'in order to investigate the detailed characteristic of different dialects of mandarin chinese, we extend dialects in huang and lee  #TAUTHOR_TAG to 6 dialects.', 'also, the more dialects there are, the more difficult the dialects discrimination becomes.', '( 2 ) the top - bag - of - word they proposed in huang and lee [ 3 ] is word uni - gram feature essentially.', 'while in this paper, besides the traditional uni - gram feature, we propose some novel features, such as character form, pmi - based and word alignment - based features']",0
"['of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination']","['of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination becomes. it']","['detailed characteristics of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination']","['', 'indicates that character form, pmi - based and word alignment - based information are useful information to discriminate dialects in the gcr. in order to', 'investigate the detailed characteristics of different dialects of mandarin chinese, we extend  #TAUTHOR_TAG to 6 dialects. in fact, the more dialects there are, the more difficult the dialects discrimination becomes. it also has been verified through our experiments. very often, texts written in a character', '']",6
['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],"['number of studies on identification of similar languages and language varieties have been carride out.', 'for example, murthy and kumar [ 1 ] focused on indian languages identification.', 'meanwhile, ranaivo - malancon [ 2 ] proposed features based on frequencies of character n - grams to identify malay and indonesian.', 'huang and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach to reflect distances among the three varieties of mandarin in mainland china, taiwan and singapore.', 'zampieri and gebre [ 4 ] found that word uni - grams gave very similar performance to character n - gram features in the framework of the probabilistic language model for the brazilian and european portuguese language discrimination.', 'tiedemann and ljubesic [ 5 ] ; ljubesic and kranjcic [ 6 ] showed that the naive bayes classifier with uni - grams achieved high accuracy for the south slavic languages identification.', 'grefenstette [ 11 ] ; lui and cook [ 12 ] found that bag - of - words features outperformed the syntax or character sequencesbased features for the english varieties.', 'besides these works, other recent studies include : spanish varieties identification [ 13 ], arabic varieties discrimination [ 14, 15, 16, 17 ], and persian and dari identification [ 18 ].', 'among the above related works, study  #TAUTHOR_TAG is the most related work to ours.', 'the differences between study  #TAUTHOR_TAG and our work are two - fold :', '( 1 ) they focus on document - level varieties of mandarin in china, taiwan and singapore, while we deal with sentence - level varieties of mandarin in china, hong kong, taiwan, macao, malaysia and singapore.', 'in order to investigate the detailed characteristic of different dialects of mandarin chinese, we extend dialects in huang and lee  #TAUTHOR_TAG to 6 dialects.', 'also, the more dialects there are, the more difficult the dialects discrimination becomes.', '( 2 ) the top - bag - of - word they proposed in huang and lee [ 3 ] is word uni - gram feature essentially.', 'while in this paper, besides the traditional uni - gram feature, we propose some novel features, such as character form, pmi - based and word alignment - based features']",6
['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],"['number of studies on identification of similar languages and language varieties have been carride out.', 'for example, murthy and kumar [ 1 ] focused on indian languages identification.', 'meanwhile, ranaivo - malancon [ 2 ] proposed features based on frequencies of character n - grams to identify malay and indonesian.', 'huang and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach to reflect distances among the three varieties of mandarin in mainland china, taiwan and singapore.', 'zampieri and gebre [ 4 ] found that word uni - grams gave very similar performance to character n - gram features in the framework of the probabilistic language model for the brazilian and european portuguese language discrimination.', 'tiedemann and ljubesic [ 5 ] ; ljubesic and kranjcic [ 6 ] showed that the naive bayes classifier with uni - grams achieved high accuracy for the south slavic languages identification.', 'grefenstette [ 11 ] ; lui and cook [ 12 ] found that bag - of - words features outperformed the syntax or character sequencesbased features for the english varieties.', 'besides these works, other recent studies include : spanish varieties identification [ 13 ], arabic varieties discrimination [ 14, 15, 16, 17 ], and persian and dari identification [ 18 ].', 'among the above related works, study  #TAUTHOR_TAG is the most related work to ours.', 'the differences between study  #TAUTHOR_TAG and our work are two - fold :', '( 1 ) they focus on document - level varieties of mandarin in china, taiwan and singapore, while we deal with sentence - level varieties of mandarin in china, hong kong, taiwan, macao, malaysia and singapore.', 'in order to investigate the detailed characteristic of different dialects of mandarin chinese, we extend dialects in huang and lee  #TAUTHOR_TAG to 6 dialects.', 'also, the more dialects there are, the more difficult the dialects discrimination becomes.', '( 2 ) the top - bag - of - word they proposed in huang and lee [ 3 ] is word uni - gram feature essentially.', 'while in this paper, besides the traditional uni - gram feature, we propose some novel features, such as character form, pmi - based and word alignment - based features']",3
"[""and lee  #TAUTHOR_TAG's""]","[""and lee  #TAUTHOR_TAG's work, in""]","[""and lee  #TAUTHOR_TAG's""]","['shown in table 3, character form based features are very effective ( 94. 36 % for 2 - way dialects classification ).', ""similar to huang and lee  #TAUTHOR_TAG's work, in order to eliminate the trivial issue of character encoding ( simplified and traditional character ), we convert taiwan and hong kong texts to the same simplified character set using zhconvertor 6 utility to focus on actual linguistic and textual features."", 'table 6 shows the experimental results for the dialect identification in the gcr.', 'as shown, again, the bi - gram features work better than both uni - gram and tri - gram features on wikipedia dataset.', '']",3
['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],['and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach'],"['number of studies on identification of similar languages and language varieties have been carride out.', 'for example, murthy and kumar [ 1 ] focused on indian languages identification.', 'meanwhile, ranaivo - malancon [ 2 ] proposed features based on frequencies of character n - grams to identify malay and indonesian.', 'huang and lee  #TAUTHOR_TAG presented the top - bag - of - word similarity based contrastive approach to reflect distances among the three varieties of mandarin in mainland china, taiwan and singapore.', 'zampieri and gebre [ 4 ] found that word uni - grams gave very similar performance to character n - gram features in the framework of the probabilistic language model for the brazilian and european portuguese language discrimination.', 'tiedemann and ljubesic [ 5 ] ; ljubesic and kranjcic [ 6 ] showed that the naive bayes classifier with uni - grams achieved high accuracy for the south slavic languages identification.', 'grefenstette [ 11 ] ; lui and cook [ 12 ] found that bag - of - words features outperformed the syntax or character sequencesbased features for the english varieties.', 'besides these works, other recent studies include : spanish varieties identification [ 13 ], arabic varieties discrimination [ 14, 15, 16, 17 ], and persian and dari identification [ 18 ].', 'among the above related works, study  #TAUTHOR_TAG is the most related work to ours.', 'the differences between study  #TAUTHOR_TAG and our work are two - fold :', '( 1 ) they focus on document - level varieties of mandarin in china, taiwan and singapore, while we deal with sentence - level varieties of mandarin in china, hong kong, taiwan, macao, malaysia and singapore.', 'in order to investigate the detailed characteristic of different dialects of mandarin chinese, we extend dialects in huang and lee  #TAUTHOR_TAG to 6 dialects.', 'also, the more dialects there are, the more difficult the dialects discrimination becomes.', '( 2 ) the top - bag - of - word they proposed in huang and lee [ 3 ] is word uni - gram feature essentially.', 'while in this paper, besides the traditional uni - gram feature, we propose some novel features, such as character form, pmi - based and word alignment - based features']",4
['and lee  #TAUTHOR_TAG did not use character - level n - grams'],['and lee  #TAUTHOR_TAG did not use character - level n - grams'],"['discriminating general languages.', 'compared with english, no space exists between words in chinese sentence.', 'therefore, we use character uni - grams, bi - grams and tri - grams as features.', 'however, huang and lee  #TAUTHOR_TAG did not use character - level n - grams']","['to the related works [ 4, 5, 6 ], word uni - grams are effective features for discriminating general languages.', 'compared with english, no space exists between words in chinese sentence.', 'therefore, we use character uni - grams, bi - grams and tri - grams as features.', 'however, huang and lee  #TAUTHOR_TAG did not use character - level n - grams']",4
"['- test for significance.', 'also the bi - gram and word segmentation based features are better than the huang and lee  #TAUTHOR_TAG - way and 2 - way dialect identification in the']","['systems with p < 0. 01 using paired t - test for significance.', 'also the bi - gram and word segmentation based features are better than the huang and lee  #TAUTHOR_TAG - way and 2 - way dialect identification in the']","['. 01 using paired t - test for significance.', 'also the bi - gram and word segmentation based features are better than the huang and lee  #TAUTHOR_TAG - way and 2 - way dialect identification in the gcr.', 'obviously, the random method does not work for the gcr dialect identification']","['we use a single type of feature, we can see that the uni - gram feature ( baseline system 2 ) is not the best one for chinese dialect detection in the gcr, although it has been found effective for english detection in previous studies in the dsl shared task.', 'instead, bi - gram and word segmentation based features are better than uni - gram one.', 'both of the proposed bi - gram and word segmentation based features significantly outperforms the baseline systems with p < 0. 01 using paired t - test for significance.', 'also the bi - gram and word segmentation based features are better than the huang and lee  #TAUTHOR_TAG - way and 2 - way dialect identification in the gcr.', 'obviously, the random method does not work for the gcr dialect identification']",4
"[""the huang and lee  #TAUTHOR_TAG's top - bag - of - word similarity - based""]","[""the huang and lee  #TAUTHOR_TAG's top - bag - of - word similarity - based""]","['two similar scenarios :', '( 1 ) 3 - way detection : we detect dialects of mainland china, hong kong and taiwan ; ( 2 ) 2 - way detection : we try to distinguish between two groups of dialects, the ones used in mainland china using simplified characters, and the ones used in hong kong and taiwan using traditional characters.', ""baseline system 1 : as mentioned in section 2, we take the huang and lee  #TAUTHOR_TAG's top - bag - of - word similarity - based approach as one of our baseline system."", 'we re - implement their method in this paper using the similar 3 - way news dataset']","['', '( 1 ) 3 - way detection : we detect dialects of mainland china, hong kong and taiwan ; ( 2 ) 2 - way detection : we try to distinguish between two groups of dialects, the ones used in mainland china using simplified characters, and the ones used in hong kong and taiwan using traditional characters.', ""baseline system 1 : as mentioned in section 2, we take the huang and lee  #TAUTHOR_TAG's top - bag - of - word similarity - based approach as one of our baseline system."", 'we re - implement their method in this paper using the similar 3 - way news dataset']",5
['headline generation  #TAUTHOR_TAG'],['headline generation  #TAUTHOR_TAG'],"['headline generation  #TAUTHOR_TAG.', 'this paper also shares a similar goal']","['network - based encoder - decoder models are cutting - edge methodologies for tackling natural language generation ( nlg ) tasks, i. e., machine translation  #AUTHOR_TAG, image captioning  #AUTHOR_TAG, video description  #AUTHOR_TAG, and headline generation  #TAUTHOR_TAG.', '']",0
['has achieved'],['has achieved state - of - the - art'],['has achieved'],"['has achieved state - of - the - art performance on the benchmark data of headline generation including the duc - 2004 dataset  #AUTHOR_TAG.', 'figure 1 illustrates the model structure of  #TAUTHOR_TAG.', 'the model predicts a word sequence ( summary ) based on the combination of the neural network language model and an input sentence encoder.', '']",0
['has achieved'],['has achieved state - of - the - art'],['has achieved'],"['has achieved state - of - the - art performance on the benchmark data of headline generation including the duc - 2004 dataset  #AUTHOR_TAG.', 'figure 1 illustrates the model structure of  #TAUTHOR_TAG.', 'the model predicts a word sequence ( summary ) based on the combination of the neural network language model and an input sentence encoder.', '']",0
['has achieved'],['has achieved state - of - the - art'],['has achieved'],"['has achieved state - of - the - art performance on the benchmark data of headline generation including the duc - 2004 dataset  #AUTHOR_TAG.', 'figure 1 illustrates the model structure of  #TAUTHOR_TAG.', 'the model predicts a word sequence ( summary ) based on the combination of the neural network language model and an input sentence encoder.', '']",0
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",0
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",0
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",0
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",0
['the method of  #TAUTHOR_TAG : the combination of the'],['the method of  #TAUTHOR_TAG : the combination of the'],['the method of  #TAUTHOR_TAG : the combination of the feed - forward neural network language model and attention - based sentence encoder. also adapted the rnn encoder - decoder with attention'],"[', the recurrent neural network ( rnn ) and its variant have been applied successfully to various nlp tasks.', 'for headline generation tasks,  #AUTHOR_TAG exploited the rnn decoder ( and its variant ) with the attention mechanism instead of the method of  #TAUTHOR_TAG : the combination of the feed - forward neural network language model and attention - based sentence encoder. also adapted the rnn encoder - decoder with attention for headline generation tasks.', 'moreover, they made some efforts such as hierarchical attention to improve the performance.', 'in addition to using a variant of rnn, proposed a method to handle infrequent words in natural language generation.', 'note that these recent developments do not conflict with our method using the amr encoder.', 'this is because the amr encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the amr encoder into the baseline.', 'we believe that our amr encoder can possibly further improve the performance of their methods.', 'we will test that hypothesis in future study']",0
['headline generation  #TAUTHOR_TAG'],['headline generation  #TAUTHOR_TAG'],"['headline generation  #TAUTHOR_TAG.', 'this paper also shares a similar goal']","['network - based encoder - decoder models are cutting - edge methodologies for tackling natural language generation ( nlg ) tasks, i. e., machine translation  #AUTHOR_TAG, image captioning  #AUTHOR_TAG, video description  #AUTHOR_TAG, and headline generation  #TAUTHOR_TAG.', '']",6
['headline generation  #TAUTHOR_TAG'],['headline generation  #TAUTHOR_TAG'],"['headline generation  #TAUTHOR_TAG.', 'this paper also shares a similar goal']","['network - based encoder - decoder models are cutting - edge methodologies for tackling natural language generation ( nlg ) tasks, i. e., machine translation  #AUTHOR_TAG, image captioning  #AUTHOR_TAG, video description  #AUTHOR_TAG, and headline generation  #TAUTHOR_TAG.', '']",6
['headline generation  #TAUTHOR_TAG'],['headline generation  #TAUTHOR_TAG'],"['headline generation  #TAUTHOR_TAG.', 'this paper also shares a similar goal']","['network - based encoder - decoder models are cutting - edge methodologies for tackling natural language generation ( nlg ) tasks, i. e., machine translation  #AUTHOR_TAG, image captioning  #AUTHOR_TAG, video description  #AUTHOR_TAG, and headline generation  #TAUTHOR_TAG.', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",7
"['demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in  #TAUTHOR_TAG']","['demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in  #TAUTHOR_TAG']","['demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in  #TAUTHOR_TAG']","['demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in  #TAUTHOR_TAG']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",5
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",3
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",3
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",4
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",4
['.  #TAUTHOR_TAG is already pre - processed.'],['p : abs + amr.  #TAUTHOR_TAG is already pre - processed.'],"['p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of']","['', 'a : crown prince leaves for islamic summit in', 'saudi arabia p : saudi crown prince leaves for islamic summit in riyadh i ( 2 ) : a massive', 'gothic revival building once christened the lunatic asylum west of the < unk > was auctioned off for $ #. # million - lrbeuro #.', '# million - rrb -. g : massive # # th century', 'us mental hospital fetches $ #. # million at auction a : west african art sells for $ #. # million in p :', 'west african art auctioned off for $ #.', '# million i ( 3 ) : brooklyn, the new bastion of cool for many new yorkers, is poised to go mainstream chic. g : high - end retailers are', 'scouting sites in brooklyn a : new yorkers are poised to go mainstream with chic p : new york city is poised to go mainstream chic figure 3', ': examples of generated headlines on gigaword. i : input, g : true headline, a : abs ( re', '- run ), and p : abs + amr.  #TAUTHOR_TAG is already pre - processed. therefore, the quality of the amr parsing results seems relatively worse on this pre - processed data since, for example, many low - occurrence words in the', '']",4
['are aware of  #TAUTHOR_TAG ; followed a'],['proposed architectures we are aware of  #TAUTHOR_TAG ; followed a'],['are aware of  #TAUTHOR_TAG ; followed a pattern similar'],"['generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram all', 'the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG ; followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '']",1
['are aware of  #TAUTHOR_TAG ; followed a'],['proposed architectures we are aware of  #TAUTHOR_TAG ; followed a'],['are aware of  #TAUTHOR_TAG ; followed a pattern similar'],"['generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram all', 'the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG ; followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '']",0
['are aware of  #TAUTHOR_TAG ; followed a'],['proposed architectures we are aware of  #TAUTHOR_TAG ; followed a'],['are aware of  #TAUTHOR_TAG ; followed a pattern similar'],"['generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram all', 'the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG ; followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '']",0
['are aware of  #TAUTHOR_TAG ; followed a'],['proposed architectures we are aware of  #TAUTHOR_TAG ; followed a'],['are aware of  #TAUTHOR_TAG ; followed a pattern similar'],"['generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram all', 'the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG ; followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '']",0
[' #TAUTHOR_TAG ('],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG ('],"['train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.', 'as a consequence, our experiments focus on the english language.', 'the dataset of  #AUTHOR_TAG ( henceforth d nor ) maps definienda to their respective definientia, as well as additional information not used here.', 'in the dataset of  #TAUTHOR_TAG ( henceforth d gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.', 'd nor contains on average shorter definitions than d gad.', 'definitions in d nor have a mean length of 6. 6 and a standard deviation of 5. 78, whereas those in d gad have a mean length of 11. 01 and a standard deviation of 6. 96.', 'stress that the dataset d gad includes many examples where the definiendum is absent from the associated cue.', 'about half of these cues doe not contain an exact match for the corresponding definiendum, but up to 80 % contains either an exact match or an inflected form of the definiendum according to lemmatization by the nltk toolkit  #AUTHOR_TAG.', '']",0
['of  #TAUTHOR_TAG and : the full encoder is dedicated to altering the embedding of the defin'],['of  #TAUTHOR_TAG and : the full encoder is dedicated to altering the embedding of the'],"['of  #TAUTHOR_TAG and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context ; in that, the encoder may be seen as a dedicated']","['', 'when to apply marking, as introduced by eq. 4, is crucial when using the multiplicative marking scheme select.', 'should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder : the resulting system provides out - of - context definitions, like in  #AUTHOR_TAG where the definition is not linked to the context of a word but to its definiendum only.', 'for context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k.', 'figure 1a presents the contextual select mechanism visually.', 'it consists in coercing the decoder to attend only to the contextualized representation for the definiendum.', 'to do so, we encode the full context and then select only the encoded representation of the definiendum, dropping the rest of the context, before running the decoder.', 'in the case of the transformer architecture, this is equivalent to using a multiplicative marking on the encoded representations : vectors that have been zeroed out are ignored during attention and thus cannot influence the behavior of the decoder.', 'this select approach may seem intuitive and naturally interpretable, as it directly controls what information is passed to the decoder - we carefully select only the contextualized definiendum, thus the only remaining zone of uncertainty would be how exactly contextualization is performed.', 'it also seems to provide a strong and reasonable bias for training the definition generation system.', 'such an approach, however, is not guaranteed to excel : forcibly omitted context could contain important information that might not be easily incorporated in the definiendum embedding.', 'being simple and natural, the select approach resembles architectures like that of  #TAUTHOR_TAG and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context ; in that, the encoder may be seen as a dedicated']",3
[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],"['', 'employ a learned character - level convolutional network, which arguably would be able to capture orthography and rudiments of morphology. adding such a', 'sub - module to our proposed architecture might diminish the number of mistagged definienda. another possibility would be to pre - train the model, as', 'was done by  #TAUTHOR_TAG : in our case in particular, the encoder could be trained for pos - tagging or lemmatization. lastly, one important kind of mistakes we observed is hallucinations. consider for instance this production by', 'the add model trained on d ctx, for the word "" beta "" : "" the twentieth', 'letter of the greek alphabet ( κ ), transliterated as\'o \'. "". nearly everything it contains is', 'factually wrong, though the general semantics are close enough to deceive an unaware reader. 8 we conjecture that filtering out halluc', '##inatory productions will be a main challenge for future definition modeling architectures,', 'for two main reasons : firstly, the tools and metrics necessary to assess and handle such hallucinations have yet to be developed ; secondly, the input given to the system', 'being word embeddings, research will be faced with the problem of grounding these distributional representations - how can we ensure that "" beta "" is correctly defined as "" the second letter of the greek alphabet, transliterated as\'b\'"", if we only have access to a', 'representation derived from its contexts of usage? integration of word embeddings with structured knowledge bases might be needed for accurate treatment of such cases']",3
['by  #TAUTHOR_TAG and to'],['by  #TAUTHOR_TAG and to'],['by  #TAUTHOR_TAG and to'],"['introduced an approach to generating word definitions that allows the model to access rich contextual information about the word token to be defined.', ""building on the distributional hypothesis, we naturally treat definition generation as a sequence - to - sequence task of mapping the word's context of usage ( input sequence ) into the contextappropriate definition ( output sequence )."", ""we showed that our approach is competitive against a more naive'contextualize and select'pipeline."", 'this was demonstrated by comparison both to the previous contextualized model by  #TAUTHOR_TAG and to the transformerbased select variation of our model, which differs from the proposed architecture only in the context encoding pipeline.', 'while our results are encouraging, given the existing benchmarks we were limited to perplexity measurements in our quantitative evaluation.', 'a more nuanced semantically driven methodology might be useful in the future to better assess the merits of our system in comparison to alternatives.', 'our model opens several avenues of future explorations.', 'one could straightforwardly extend it to generate definitions of multiword expressions or phrases, or to analyze vector compositionality models by generating paraphrases for vector representations produced by these algorithms.', 'another strength of our approach is that it can provide the basis for a standardized benchmark for contextualized and non - contextual embeddings alike : downstream evaluation tasks for embeddings systems in general either apply to non - contextual embeddings  #AUTHOR_TAG eg. ) or to contextual embeddings  #AUTHOR_TAG eg. ) exclusively, redefining definition modeling as a sequence - tosequence task will allow in future works to compare models using contextual and non - contextual embeddings in a unified fashion.', 'lastly, we also intend to experiment on languages other than english, especially considering that the required resources for our model only amount to a set of pretrained embeddings and a dataset of definitions, either of which are generally simple to obtain.', 'while there is a potential for local improvements, our approach has demonstrated its ability to account for contextualized word meaning in a principled way, while training contextualized token encoding and definition generation end - toend.', 'our implementation is efficient and fast, building on free open source libraries for deep learning, and shows good empirical results.', 'our code, trained models, and data will be made available to the community']",3
[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],"['', 'employ a learned character - level convolutional network, which arguably would be able to capture orthography and rudiments of morphology. adding such a', 'sub - module to our proposed architecture might diminish the number of mistagged definienda. another possibility would be to pre - train the model, as', 'was done by  #TAUTHOR_TAG : in our case in particular, the encoder could be trained for pos - tagging or lemmatization. lastly, one important kind of mistakes we observed is hallucinations. consider for instance this production by', 'the add model trained on d ctx, for the word "" beta "" : "" the twentieth', 'letter of the greek alphabet ( κ ), transliterated as\'o \'. "". nearly everything it contains is', 'factually wrong, though the general semantics are close enough to deceive an unaware reader. 8 we conjecture that filtering out halluc', '##inatory productions will be a main challenge for future definition modeling architectures,', 'for two main reasons : firstly, the tools and metrics necessary to assess and handle such hallucinations have yet to be developed ; secondly, the input given to the system', 'being word embeddings, research will be faced with the problem of grounding these distributional representations - how can we ensure that "" beta "" is correctly defined as "" the second letter of the greek alphabet, transliterated as\'b\'"", if we only have access to a', 'representation derived from its contexts of usage? integration of word embeddings with structured knowledge bases might be needed for accurate treatment of such cases']",5
[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],"['', 'employ a learned character - level convolutional network, which arguably would be able to capture orthography and rudiments of morphology. adding such a', 'sub - module to our proposed architecture might diminish the number of mistagged definienda. another possibility would be to pre - train the model, as', 'was done by  #TAUTHOR_TAG : in our case in particular, the encoder could be trained for pos - tagging or lemmatization. lastly, one important kind of mistakes we observed is hallucinations. consider for instance this production by', 'the add model trained on d ctx, for the word "" beta "" : "" the twentieth', 'letter of the greek alphabet ( κ ), transliterated as\'o \'. "". nearly everything it contains is', 'factually wrong, though the general semantics are close enough to deceive an unaware reader. 8 we conjecture that filtering out halluc', '##inatory productions will be a main challenge for future definition modeling architectures,', 'for two main reasons : firstly, the tools and metrics necessary to assess and handle such hallucinations have yet to be developed ; secondly, the input given to the system', 'being word embeddings, research will be faced with the problem of grounding these distributional representations - how can we ensure that "" beta "" is correctly defined as "" the second letter of the greek alphabet, transliterated as\'b\'"", if we only have access to a', 'representation derived from its contexts of usage? integration of word embeddings with structured knowledge bases might be needed for accurate treatment of such cases']",4
[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],[' #TAUTHOR_TAG :'],"['', 'employ a learned character - level convolutional network, which arguably would be able to capture orthography and rudiments of morphology. adding such a', 'sub - module to our proposed architecture might diminish the number of mistagged definienda. another possibility would be to pre - train the model, as', 'was done by  #TAUTHOR_TAG : in our case in particular, the encoder could be trained for pos - tagging or lemmatization. lastly, one important kind of mistakes we observed is hallucinations. consider for instance this production by', 'the add model trained on d ctx, for the word "" beta "" : "" the twentieth', 'letter of the greek alphabet ( κ ), transliterated as\'o \'. "". nearly everything it contains is', 'factually wrong, though the general semantics are close enough to deceive an unaware reader. 8 we conjecture that filtering out halluc', '##inatory productions will be a main challenge for future definition modeling architectures,', 'for two main reasons : firstly, the tools and metrics necessary to assess and handle such hallucinations have yet to be developed ; secondly, the input given to the system', 'being word embeddings, research will be faced with the problem of grounding these distributional representations - how can we ensure that "" beta "" is correctly defined as "" the second letter of the greek alphabet, transliterated as\'b\'"", if we only have access to a', 'representation derived from its contexts of usage? integration of word embeddings with structured knowledge bases might be needed for accurate treatment of such cases']",2
"['embeddings  #TAUTHOR_TAG.', 'however, the effects of']","['embeddings  #TAUTHOR_TAG.', 'however, the effects of']","['the creation of fixed - sized sentence embeddings  #TAUTHOR_TAG.', 'however, the effects of the size of sentence embeddings and the relation']",[' #TAUTHOR_TAG'],0
"['embeddings  #TAUTHOR_TAG.', 'however, the effects of']","['embeddings  #TAUTHOR_TAG.', 'however, the effects of']","['the creation of fixed - sized sentence embeddings  #TAUTHOR_TAG.', 'however, the effects of the size of sentence embeddings and the relation']",[' #TAUTHOR_TAG'],0
"['embeddings  #TAUTHOR_TAG.', 'however, the effects of']","['embeddings  #TAUTHOR_TAG.', 'however, the effects of']","['the creation of fixed - sized sentence embeddings  #TAUTHOR_TAG.', 'however, the effects of the size of sentence embeddings and the relation']",[' #TAUTHOR_TAG'],1
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['by  #TAUTHOR_TAG.', '']","['', 'due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by  #TAUTHOR_TAG.', 'finally, each decoder follows a common attention mechanism in nmt, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.', 'hence, the decoder receives the information only through the shared attention bridge.', 'the fixed - sized representation coming out of the shared layer can immediately be applied to downstream tasks.', '1 however, selecting a reasonable size of the attention bridge in terms of attention heads ( m i in figure 1 ) is crucial for the performance both in a bilingual and multilingual sce - 1 as in  #AUTHOR_TAG, we note that the attention bridge is independent of the underlying encoder and decoder.', 'while we use lstm, it could be easily replaced with a transformer type network  #AUTHOR_TAG or with a cnn  #AUTHOR_TAG.', 'nario as we will see in the experiments below']",5
"['of', ' #TAUTHOR_TAG']","['findings of', ' #TAUTHOR_TAG and could also be']","['', ' #TAUTHOR_TAG']","['', '= 50 m - to - m table 3 : bleu scores for multilingual models. baseline system in the', 'right - most column. model is provided by a bilingual setting with only one attention head. this is in line with the findings of', ' #TAUTHOR_TAG and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring', '']",5
"['of', ' #TAUTHOR_TAG']","['findings of', ' #TAUTHOR_TAG and could also be']","['', ' #TAUTHOR_TAG']","['', '= 50 m - to - m table 3 : bleu scores for multilingual models. baseline system in the', 'right - most column. model is provided by a bilingual setting with only one attention head. this is in line with the findings of', ' #TAUTHOR_TAG and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring', '']",3
"['of', ' #TAUTHOR_TAG']","['findings of', ' #TAUTHOR_TAG and could also be']","['', ' #TAUTHOR_TAG']","['', '= 50 m - to - m table 3 : bleu scores for multilingual models. baseline system in the', 'right - most column. model is provided by a bilingual setting with only one attention head. this is in line with the findings of', ' #TAUTHOR_TAG and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring', '']",3
"['.', 'more recently,  #TAUTHOR_TAG explored convolution kernels for oh']","['opinion target extraction.', 'more recently,  #TAUTHOR_TAG explored convolution kernels for oh']","['opinion target extraction.', 'more recently,  #TAUTHOR_TAG explored convolution kernels for oh extraction and found that tree kernels']","['has been much research on supervised learning for oh extraction.', ' #AUTHOR_TAG explore oh extraction using crfs with several manually defined linguistic features and automatically learnt surface patterns.', 'the linguistic features focus on named - entity information and syntactic relations to opinion words.', ' #AUTHOR_TAG and  #AUTHOR_TAG examine the usefulness of semantic roles provided by framenet 1 for both oh and opinion target extraction.', 'more recently,  #TAUTHOR_TAG explored convolution kernels for oh extraction and found that tree kernels outperform all other kernel types.', 'in  #AUTHOR_TAG, a re - ranking approach modeling complex relations between multiple opinions in a sentence is presented.', 'rule - based oh extraction heavily relies on lexical cues.', ' #AUTHOR_TAG, for example, use a list of manually compiled communication verbs']",0
['oh extraction  #TAUTHOR_TAG'],['oh extraction  #TAUTHOR_TAG'],"['oh extraction  #TAUTHOR_TAG.', 'table 3 ( lower part ) shows the performance of']","['', '6 as an indication of the intrinsic quality of the extracted words, we mark the words which can also be found in task - specific resources, i. e. communication verbs from the appraisal lexicon ( al )  #AUTHOR_TAG and opinion words from the subjectivity lexicon ( sl )  #AUTHOR_TAG.', 'both resources have been found predictive for oh extraction  #TAUTHOR_TAG.', 'table 3 ( lower part ) shows the performance of the rule - based classifiers based on protoohs using different parts of speech.', 'as hard baselines, the table also shows other rule - based classifiers using the same dependency relations as our rulebased classifier ( see table 2 ) but employing different predicates.', '']",0
"['american news text corpus.', 'as a labeled ( test ) corpus, we use the mpqa corpus.', '2 we use the definition of ohs as described in  #TAUTHOR_TAG.', 'the instance space are all noun phrases ( np ) in that corpus']","['american news text corpus.', 'as a labeled ( test ) corpus, we use the mpqa corpus.', '2 we use the definition of ohs as described in  #TAUTHOR_TAG.', 'the instance space are all noun phrases ( np ) in that corpus']","['a large unlabeled ( training ) corpus, we chose the north american news text corpus.', 'as a labeled ( test ) corpus, we use the mpqa corpus.', '2 we use the definition of ohs as described in  #TAUTHOR_TAG.', 'the instance space are all noun phrases ( np ) in that corpus']","['a large unlabeled ( training ) corpus, we chose the north american news text corpus.', 'as a labeled ( test ) corpus, we use the mpqa corpus.', '2 we use the definition of ohs as described in  #TAUTHOR_TAG.', 'the instance space are all noun phrases ( np ) in that corpus']",5
"['oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures,']","['oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures,']","['oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures,']","['simplest way of using the contexts of agentive protoohs is by using supervised learning.', 'this means that on our unlabeled training corpus we consider each np with the head being an agentive protooh as a positive data instance and all the remaining nps occurring in those sentences as negative instances.', 'with this definition we train a supervised classifier based on convolution kernels  #AUTHOR_TAG as this method has been shown to be quite effective for oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part - of - speech sequences, that are directly provided to the learner.', 'thus a classifier can be built without the taking the burden of implementing an explicit feature extraction.', 'we chose the best performing set of tree kernels  #AUTHOR_TAG from that work.', 'it comprises two tree kernels based on constituency parse trees and a tree kernel based on semantic role trees.', 'apart from a set of sequence kernels  #AUTHOR_TAG, this method also largely outperforms a traditional vector kernel using a set of features that were found predictive in previous work.', 'we exclude sequence and vector kernels in this work not only for reasons of simplicity but also since their addition to tree kernels only results in a marginal improvement.', 'moreover, the features in the vector kernel heavily rely on taskspecific resources, e. g. a sentiment lexicon, which are deliberately avoided in our low - resource classifier as our method should be applicable to any language ( and for many languages sentiment resources are either sparse or do not exist at all ).', 'in addition to  #TAUTHOR_TAG, we have to discard the content of candidate nps ( e. g. the candidate opinion holder np [ n p cand [ n n s advocates ] ] is reduced to [ n p cand ] ), the reason for this being that in our automatically generated training set, ohs will always be protoohs.', 'retaining them in the training data would cause the learner to develop a detrimental bias towards these nouns ( our resulting classifier should detect any oh and not only protoohs )']",5
['following  #TAUTHOR_TAG who add'],['following  #TAUTHOR_TAG who add'],['with additional information by 3 wordnet. princeton. edu following  #TAUTHOR_TAG who add'],[' #TAUTHOR_TAG'],5
['oh extraction  #TAUTHOR_TAG'],['oh extraction  #TAUTHOR_TAG'],"['oh extraction  #TAUTHOR_TAG.', 'table 3 ( lower part ) shows the performance of']","['', '6 as an indication of the intrinsic quality of the extracted words, we mark the words which can also be found in task - specific resources, i. e. communication verbs from the appraisal lexicon ( al )  #AUTHOR_TAG and opinion words from the subjectivity lexicon ( sl )  #AUTHOR_TAG.', 'both resources have been found predictive for oh extraction  #TAUTHOR_TAG.', 'table 3 ( lower part ) shows the performance of the rule - based classifiers based on protoohs using different parts of speech.', 'as hard baselines, the table also shows other rule - based classifiers using the same dependency relations as our rulebased classifier ( see table 2 ) but employing different predicates.', '']",5
['oh extraction  #TAUTHOR_TAG'],['oh extraction  #TAUTHOR_TAG'],"['oh extraction  #TAUTHOR_TAG.', 'table 3 ( lower part ) shows the performance of']","['', '6 as an indication of the intrinsic quality of the extracted words, we mark the words which can also be found in task - specific resources, i. e. communication verbs from the appraisal lexicon ( al )  #AUTHOR_TAG and opinion words from the subjectivity lexicon ( sl )  #AUTHOR_TAG.', 'both resources have been found predictive for oh extraction  #TAUTHOR_TAG.', 'table 3 ( lower part ) shows the performance of the rule - based classifiers based on protoohs using different parts of speech.', 'as hard baselines, the table also shows other rule - based classifiers using the same dependency relations as our rulebased classifier ( see table 2 ) but employing different predicates.', '']",5
"['oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures,']","['oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures,']","['oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures,']","['simplest way of using the contexts of agentive protoohs is by using supervised learning.', 'this means that on our unlabeled training corpus we consider each np with the head being an agentive protooh as a positive data instance and all the remaining nps occurring in those sentences as negative instances.', 'with this definition we train a supervised classifier based on convolution kernels  #AUTHOR_TAG as this method has been shown to be quite effective for oh extraction  #TAUTHOR_TAG.', 'convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part - of - speech sequences, that are directly provided to the learner.', 'thus a classifier can be built without the taking the burden of implementing an explicit feature extraction.', 'we chose the best performing set of tree kernels  #AUTHOR_TAG from that work.', 'it comprises two tree kernels based on constituency parse trees and a tree kernel based on semantic role trees.', 'apart from a set of sequence kernels  #AUTHOR_TAG, this method also largely outperforms a traditional vector kernel using a set of features that were found predictive in previous work.', 'we exclude sequence and vector kernels in this work not only for reasons of simplicity but also since their addition to tree kernels only results in a marginal improvement.', 'moreover, the features in the vector kernel heavily rely on taskspecific resources, e. g. a sentiment lexicon, which are deliberately avoided in our low - resource classifier as our method should be applicable to any language ( and for many languages sentiment resources are either sparse or do not exist at all ).', 'in addition to  #TAUTHOR_TAG, we have to discard the content of candidate nps ( e. g. the candidate opinion holder np [ n p cand [ n n s advocates ] ] is reduced to [ n p cand ] ), the reason for this being that in our automatically generated training set, ohs will always be protoohs.', 'retaining them in the training data would cause the learner to develop a detrimental bias towards these nouns ( our resulting classifier should detect any oh and not only protoohs )']",6
['following  #TAUTHOR_TAG who add'],['following  #TAUTHOR_TAG who add'],['with additional information by 3 wordnet. princeton. edu following  #TAUTHOR_TAG who add'],[' #TAUTHOR_TAG'],4
"['. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1']","['chose 60000 instances ( i. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1 %, 5 %, 10 %, 25 % and 50 % of the training set.', 'from the remaining data instances,']","['. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1']","['a maximum amount of labeled training data we chose 60000 instances ( i. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1 %, 5 %, 10 %, 25 % and 50 % of the training set.', 'from the remaining data instances, we use 25000 instances as test data.', 'in order to deliver generalizing results, we randomly sample the training and test partitions five times and report the averaged results.', 'we compare four different classifiers, a plain classifier using only the convolution kernel configuration from previous experiments ( tkplain ), the augmented convolution kernels ( tkaug ) where additional nodes are added indicating the presence of an oh predicate ( § 4. 3 ), the augmented convolution kernels with the vector kernel encoding the prediction of the best rule - based classifier ( induced by protoohs ) without heuristics ( tkaug + vk ) and the classifier incorporating those heuristics ( tkaug + vk [ heur ] ).', 'instead of just using one feature encoding the overall prediction we use several binary features representing the occurrence of the individual groups of predicates ( i. e. verbs, nouns, or adjectives ) and prediction types ( direct predicate or predicate from cluster extension ).', '']",4
"['. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1']","['chose 60000 instances ( i. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1 %, 5 %, 10 %, 25 % and 50 % of the training set.', 'from the remaining data instances,']","['. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1']","['a maximum amount of labeled training data we chose 60000 instances ( i. e. nps ) which is even a bit more than used in  #TAUTHOR_TAG.', 'in addition, we also test 1 %, 5 %, 10 %, 25 % and 50 % of the training set.', 'from the remaining data instances, we use 25000 instances as test data.', 'in order to deliver generalizing results, we randomly sample the training and test partitions five times and report the averaged results.', 'we compare four different classifiers, a plain classifier using only the convolution kernel configuration from previous experiments ( tkplain ), the augmented convolution kernels ( tkaug ) where additional nodes are added indicating the presence of an oh predicate ( § 4. 3 ), the augmented convolution kernels with the vector kernel encoding the prediction of the best rule - based classifier ( induced by protoohs ) without heuristics ( tkaug + vk ) and the classifier incorporating those heuristics ( tkaug + vk [ heur ] ).', 'instead of just using one feature encoding the overall prediction we use several binary features representing the occurrence of the individual groups of predicates ( i. e. verbs, nouns, or adjectives ) and prediction types ( direct predicate or predicate from cluster extension ).', '']",3
"['tasks without sacrificing annotation quality  #TAUTHOR_TAG.', 'once we']","['many nlp tasks without sacrificing annotation quality  #TAUTHOR_TAG.', 'once we']","['many nlp tasks without sacrificing annotation quality  #TAUTHOR_TAG.', 'once we decide to use al']","['', 'while for the general language english newspaper domain syntactic  #AUTHOR_TAG, semantic  #AUTHOR_TAG, and even discourse  #AUTHOR_TAG annotations are increasingly made available, any language, domain, or genre shift pushes the severe burden on developers of nlp systems to supply comparably sized high - quality annotations.', 'even inner - domain shifts, such as, e. g., moving from hematology  #AUTHOR_TAG to the genetics of cancer  #AUTHOR_TAG within the field of molecular biology may have drastic consequences in the sense that entirely new meta data sets have to produced by annotation teams.', 'thus, reducing the human efforts for the creation of adequate training material is a major challenge.', 'active learning ( al ) copes with this problem as it intelligently selects the data to be labeled.', 'it is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process.', 'al has been successfully applied to speed up the annotation process for many nlp tasks without sacrificing annotation quality  #TAUTHOR_TAG.', '']",0
"['to other tasks as well.', 'in  #TAUTHOR_TAG we introduced the selection agreement ( sa )']","['to other tasks as well.', 'in  #TAUTHOR_TAG we introduced the selection agreement ( sa )']","['to other tasks as well.', 'in  #TAUTHOR_TAG we introduced the selection agreement ( sa']","['the idea that from the learning curve one can read the trade - off between annotation effort and classifier performance gain, we here propose an approach to approximate the progression of the learning curve which comes at no extra annotation costs.', 'this approach is designed for use in committee - based al  #AUTHOR_TAG.', 'a committee consists of k classifiers of the same type trained on different subsets of the already labeled ( training ) data.', 'each committee member then makes its predictions on the pool of unlabeled examples, and those examples on which the committee members express the highest disagreement are considered most informative for learning and are thus selected for manual annotation.', 'to calculate the disagreement among the committee members several metrics have been proposed including the vote entropy  #AUTHOR_TAG as possibly the most well - known one.', 'our approach to approximating the learning curve is based on the disagreement within a committee.', 'however, it is independent of the actual metric used to calculate the disagreement.', 'although in our experiments we considered the nlp task of named entity recognition ( ner ) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well.', 'in  #TAUTHOR_TAG we introduced the selection agreement ( sa ) curve - the average agreement amongst the selected examples plotted over time.', '']",0
"[') as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', 'in']","['( ner ) as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', 'in']","[') as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', 'in']","['our experiments on approximating the learning curves for al - based selection, we chose named entity recognition ( ner ) as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', '']",0
"[') as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', 'in']","['( ner ) as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', 'in']","[') as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', 'in']","['our experiments on approximating the learning curves for al - based selection, we chose named entity recognition ( ner ) as the annotation task in focus.', 'we employed the committee - based al approach described in  #TAUTHOR_TAG.', 'the committee consists of k = 3 maximum entropy ( me ) classifiers  #AUTHOR_TAG.', '']",5
"[', or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","[', or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","[' #AUTHOR_TAG.', 'approximate parsers have therefore been introduced, based on belief propagation  #AUTHOR_TAG, dual decomposition, or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","['', ' #AUTHOR_TAG trained third - order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first - order systems.', 'third - order features have also been included in transition systems  #AUTHOR_TAG and graph - based parsers with cube - pruning ( zhang and mc  #AUTHOR_TAG.', 'unfortunately, non - projective dependency parsers ( appropriate for languages with a more flexible word order, such as czech, dutch, and german ) lag behind these recent advances.', 'the main obstacle is that non - projective parsing is np - hard beyond arc - factored models ( mc  #AUTHOR_TAG.', 'approximate parsers have therefore been introduced, based on belief propagation  #AUTHOR_TAG, dual decomposition, or multi - commodity flows  #TAUTHOR_TAG.', 'these are all instances of turbo parsers, as shown by  #AUTHOR_TAG : the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects.', '']",0
"['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third -']","['arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['decomposition is a class of optimization techniques that tackle the dual of combinatorial figure 1 : parts considered in this paper.', 'firstorder models factor over arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features for grand - and tri - siblings.', 'problems in a modular and extensible manner  #AUTHOR_TAG.', 'in this paper, we employ alternating directions dual decomposition ( ad 3 ;  #TAUTHOR_TAG.', 'like the subgradient algorithm of, ad 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables.', 'the difference is that the ad 3 subproblems have an additional quadratic term to accelerate consensus.', 'recent analysis  #AUTHOR_TAG has shown that : ( i ) ad 3 converges at a faster rate, 2 and ( ii ) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm.', 'this opens the door for larger subproblems ( such as the combination of trees and head automata in instead of a many - components approach  #TAUTHOR_TAG, while still enjoying faster convergence']",0
"['components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']","['components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']","['as trees and head automata  #AUTHOR_TAG or ( ii ) many "" small "" components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']","['a sentence with l words, to which we prepend a root symbol $, let a : = { h, m | h ∈ { 0,..', '., l }, m ∈ { 1,..., l }, h = m } be the set of possible dependency arcs.', 'we parameterize a dependency tree via an indicator vector u : = u a a∈a, where u a is 1 if the arc a is in the tree, and 0 otherwise, and we denote by y ⊆ r | a | the set of such vectors that are indicators of well - 2 concretely, ad 3 needs o ( 1 / ) iterations to converge to a - accurate solution, while subgradient needs o ( 1 / 2 ).', 'formed trees.', 'let { a s } s s = 1 be a cover of a, where each a s ⊆ a. we assume that the score of a parse tree u ∈ y decomposes as f ( u ) : = s s = 1 f s ( z s ), where each z s : = z s, a a∈as is a "" partial view "" of u, and each local score function f s comes from a feature - based linear model.', 'past work in dependency parsing considered either ( i ) a few "" large "" components, such as trees and head automata  #AUTHOR_TAG or ( ii ) many "" small "" components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']",0
"['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['', 'its members represent marginal probabilities over the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among the following iterative updates :', '• z - updates, which decouple over s = 1,..., s, and solve a penalized version of eq. 2 :', 'above, ρ is a constant and the quadratic term penalizes deviations from the current global solution ( stored in u ( t ) ).', '5 we will see ( prop. 2 ) that this problem can be solved iteratively using only the local - max oracle ( eq. 2 ).', '• u - updates, a simple averaging operation :', '• λ - updates, where the lagrange multipliers are adjusted to penalize disagreements :', 'in sum, the only difference between ad 3 and the subgradient method is in the z - updates, which in ad 3 require solving a quadratic problem.', 'while closed - form solutions have been developed for some specialized components  #TAUTHOR_TAG, this problem is in general more difficult than the one arising in the subgradient algorithm.', 'however, the following result, proved in  #AUTHOR_TAG, allows to expand the scope of ad 3 to any problem which satisfies assumption 1. proposition 2.', 'the problem in eq. 3 admits a solution z * s which is spanned by a sparse basis w ⊆ y s with cardinality at most | w | ≤ o ( | a s | ).', '']",0
"['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['', 'its members represent marginal probabilities over the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among the following iterative updates :', '• z - updates, which decouple over s = 1,..., s, and solve a penalized version of eq. 2 :', 'above, ρ is a constant and the quadratic term penalizes deviations from the current global solution ( stored in u ( t ) ).', '5 we will see ( prop. 2 ) that this problem can be solved iteratively using only the local - max oracle ( eq. 2 ).', '• u - updates, a simple averaging operation :', '• λ - updates, where the lagrange multipliers are adjusted to penalize disagreements :', 'in sum, the only difference between ad 3 and the subgradient method is in the z - updates, which in ad 3 require solving a quadratic problem.', 'while closed - form solutions have been developed for some specialized components  #TAUTHOR_TAG, this problem is in general more difficult than the one arising in the subgradient algorithm.', 'however, the following result, proved in  #AUTHOR_TAG, allows to expand the scope of ad 3 to any problem which satisfies assumption 1. proposition 2.', 'the problem in eq. 3 admits a solution z * s which is spanned by a sparse basis w ⊆ y s with cardinality at most | w | ≤ o ( | a s | ).', '']",0
"[', or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","[', or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","[' #AUTHOR_TAG.', 'approximate parsers have therefore been introduced, based on belief propagation  #AUTHOR_TAG, dual decomposition, or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","['', ' #AUTHOR_TAG trained third - order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first - order systems.', 'third - order features have also been included in transition systems  #AUTHOR_TAG and graph - based parsers with cube - pruning ( zhang and mc  #AUTHOR_TAG.', 'unfortunately, non - projective dependency parsers ( appropriate for languages with a more flexible word order, such as czech, dutch, and german ) lag behind these recent advances.', 'the main obstacle is that non - projective parsing is np - hard beyond arc - factored models ( mc  #AUTHOR_TAG.', 'approximate parsers have therefore been introduced, based on belief propagation  #AUTHOR_TAG, dual decomposition, or multi - commodity flows  #TAUTHOR_TAG.', 'these are all instances of turbo parsers, as shown by  #AUTHOR_TAG : the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects.', '']",5
"['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third -']","['arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['decomposition is a class of optimization techniques that tackle the dual of combinatorial figure 1 : parts considered in this paper.', 'firstorder models factor over arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features for grand - and tri - siblings.', 'problems in a modular and extensible manner  #AUTHOR_TAG.', 'in this paper, we employ alternating directions dual decomposition ( ad 3 ;  #TAUTHOR_TAG.', 'like the subgradient algorithm of, ad 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables.', 'the difference is that the ad 3 subproblems have an additional quadratic term to accelerate consensus.', 'recent analysis  #AUTHOR_TAG has shown that : ( i ) ad 3 converges at a faster rate, 2 and ( ii ) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm.', 'this opens the door for larger subproblems ( such as the combination of trees and head automata in instead of a many - components approach  #TAUTHOR_TAG, while still enjoying faster convergence']",5
"['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third -']","['arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['decomposition is a class of optimization techniques that tackle the dual of combinatorial figure 1 : parts considered in this paper.', 'firstorder models factor over arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features for grand - and tri - siblings.', 'problems in a modular and extensible manner  #AUTHOR_TAG.', 'in this paper, we employ alternating directions dual decomposition ( ad 3 ;  #TAUTHOR_TAG.', 'like the subgradient algorithm of, ad 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables.', 'the difference is that the ad 3 subproblems have an additional quadratic term to accelerate consensus.', 'recent analysis  #AUTHOR_TAG has shown that : ( i ) ad 3 converges at a faster rate, 2 and ( ii ) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm.', 'this opens the door for larger subproblems ( such as the combination of trees and head automata in instead of a many - components approach  #TAUTHOR_TAG, while still enjoying faster convergence']",5
"['components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']","['components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']","['as trees and head automata  #AUTHOR_TAG or ( ii ) many "" small "" components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']","['a sentence with l words, to which we prepend a root symbol $, let a : = { h, m | h ∈ { 0,..', '., l }, m ∈ { 1,..., l }, h = m } be the set of possible dependency arcs.', 'we parameterize a dependency tree via an indicator vector u : = u a a∈a, where u a is 1 if the arc a is in the tree, and 0 otherwise, and we denote by y ⊆ r | a | the set of such vectors that are indicators of well - 2 concretely, ad 3 needs o ( 1 / ) iterations to converge to a - accurate solution, while subgradient needs o ( 1 / 2 ).', 'formed trees.', 'let { a s } s s = 1 be a cover of a, where each a s ⊆ a. we assume that the score of a parse tree u ∈ y decomposes as f ( u ) : = s s = 1 f s ( z s ), where each z s : = z s, a a∈as is a "" partial view "" of u, and each local score function f s comes from a feature - based linear model.', 'past work in dependency parsing considered either ( i ) a few "" large "" components, such as trees and head automata  #AUTHOR_TAG or ( ii ) many "" small "" components, coming from a multi - commodity flow formulation  #TAUTHOR_TAG.', '']",5
['consecutive words ( as in  #TAUTHOR_TAG. this'],['consecutive words ( as in  #TAUTHOR_TAG. this'],"['hb ( m, h, h ) is obtained via features that look at the heads of consecutive words ( as in  #TAUTHOR_TAG. this function can be', 'maximized in time o ( l 3 ) with the vit', '##erbi algorithm. arbitrary siblings. we handle arbitrary siblings as in  #TAUTHOR_TAG, defining o ( l 3 ) component functions of the form']","['be the sequence of right modifiers of h, with m 0', '= start and', 'm p + 1 = end. then, we have the following', 'grand - sibling component : where we use the shorthand z | b to denote the subvector of z indexed by the arcs in b ⊆ a. note that this score function absorb', '##s grandparent and consecutive sibling scores, in addition to the grand - sibling scores. 9 for each h, f gsib h, → can be', '8 in fact, there is an', 'asymptotically faster o ( l 2 ) algorithm  #AUTHOR_TAG. moreover, if the set of possible arcs is reduced to a subset b ⊆ a ( via pruning ), then the fastest known algorithm  #AUTHOR_TAG 9 used an identical automaton for their second - order model, but leaving out the grand', '- sibling scores. no pruning tri - sibling head automata. in addition, we define left and right - side tri - sibling head automata that remember the previous two modifiers of a', 'head word. this corresponds to the following component function ( for the right - side case ) :', 'again, each of these functions can be maximized sequential head bigram model. head bigrams can be captured with a simple sequence model : each score σ hb ( m, h, h ) is obtained via features that look at the heads of consecutive words ( as in  #TAUTHOR_TAG. this function can be', 'maximized in time o ( l 3 ) with the vit', '##erbi algorithm. arbitrary siblings. we handle arbitrary siblings as in  #TAUTHOR_TAG, defining o ( l 3 ) component functions of the form f asib h, m, s ( z h, m, z h, s ) = σ as', '##ib ( h, m, s ). in this case, the quadratic problem in eq. 3 can be solved directly in constant time. tab.', '1 details the time complexities of each subproblem. without pruning, each iteration of ad 3 has o ( l 4 ) runtime. with', 'a simple strategy that limits the number of candidate heads per word to a constant k, this drops to cubic', 'time. 10 further speed - ups', 'are possible with more pruning : by limiting the number of possible modifiers to a constant j, the runtime would reduce to o ( l', 'log l ). 10 in our experiments, we employed this strategy with k =', '10, by pruning with a first - order', 'probabilistic model. following, for', 'each word m, we also pruned away incoming arcs h,', 'm with posterior probability less than 0. 0001 times the probability of the most likely head']",5
['consecutive words ( as in  #TAUTHOR_TAG. this'],['consecutive words ( as in  #TAUTHOR_TAG. this'],"['hb ( m, h, h ) is obtained via features that look at the heads of consecutive words ( as in  #TAUTHOR_TAG. this function can be', 'maximized in time o ( l 3 ) with the vit', '##erbi algorithm. arbitrary siblings. we handle arbitrary siblings as in  #TAUTHOR_TAG, defining o ( l 3 ) component functions of the form']","['be the sequence of right modifiers of h, with m 0', '= start and', 'm p + 1 = end. then, we have the following', 'grand - sibling component : where we use the shorthand z | b to denote the subvector of z indexed by the arcs in b ⊆ a. note that this score function absorb', '##s grandparent and consecutive sibling scores, in addition to the grand - sibling scores. 9 for each h, f gsib h, → can be', '8 in fact, there is an', 'asymptotically faster o ( l 2 ) algorithm  #AUTHOR_TAG. moreover, if the set of possible arcs is reduced to a subset b ⊆ a ( via pruning ), then the fastest known algorithm  #AUTHOR_TAG 9 used an identical automaton for their second - order model, but leaving out the grand', '- sibling scores. no pruning tri - sibling head automata. in addition, we define left and right - side tri - sibling head automata that remember the previous two modifiers of a', 'head word. this corresponds to the following component function ( for the right - side case ) :', 'again, each of these functions can be maximized sequential head bigram model. head bigrams can be captured with a simple sequence model : each score σ hb ( m, h, h ) is obtained via features that look at the heads of consecutive words ( as in  #TAUTHOR_TAG. this function can be', 'maximized in time o ( l 3 ) with the vit', '##erbi algorithm. arbitrary siblings. we handle arbitrary siblings as in  #TAUTHOR_TAG, defining o ( l 3 ) component functions of the form f asib h, m, s ( z h, m, z h, s ) = σ as', '##ib ( h, m, s ). in this case, the quadratic problem in eq. 3 can be solved directly in constant time. tab.', '1 details the time complexities of each subproblem. without pruning, each iteration of ad 3 has o ( l 4 ) runtime. with', 'a simple strategy that limits the number of candidate heads per word to a constant k, this drops to cubic', 'time. 10 further speed - ups', 'are possible with more pruning : by limiting the number of possible modifiers to a constant j, the runtime would reduce to o ( l', 'log l ). 10 in our experiments, we employed this strategy with k =', '10, by pruning with a first - order', 'probabilistic model. following, for', 'each word m, we also pruned away incoming arcs h,', 'm with posterior probability less than 0. 0001 times the probability of the most likely head']",5
"[', or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","[', or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","[' #AUTHOR_TAG.', 'approximate parsers have therefore been introduced, based on belief propagation  #AUTHOR_TAG, dual decomposition, or multi - commodity flows  #TAUTHOR_TAG.', 'these are']","['', ' #AUTHOR_TAG trained third - order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first - order systems.', 'third - order features have also been included in transition systems  #AUTHOR_TAG and graph - based parsers with cube - pruning ( zhang and mc  #AUTHOR_TAG.', 'unfortunately, non - projective dependency parsers ( appropriate for languages with a more flexible word order, such as czech, dutch, and german ) lag behind these recent advances.', 'the main obstacle is that non - projective parsing is np - hard beyond arc - factored models ( mc  #AUTHOR_TAG.', 'approximate parsers have therefore been introduced, based on belief propagation  #AUTHOR_TAG, dual decomposition, or multi - commodity flows  #TAUTHOR_TAG.', 'these are all instances of turbo parsers, as shown by  #AUTHOR_TAG : the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects.', '']",4
"['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third -']","['arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['decomposition is a class of optimization techniques that tackle the dual of combinatorial figure 1 : parts considered in this paper.', 'firstorder models factor over arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features for grand - and tri - siblings.', 'problems in a modular and extensible manner  #AUTHOR_TAG.', 'in this paper, we employ alternating directions dual decomposition ( ad 3 ;  #TAUTHOR_TAG.', 'like the subgradient algorithm of, ad 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables.', 'the difference is that the ad 3 subproblems have an additional quadratic term to accelerate consensus.', 'recent analysis  #AUTHOR_TAG has shown that : ( i ) ad 3 converges at a faster rate, 2 and ( ii ) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm.', 'this opens the door for larger subproblems ( such as the combination of trees and head automata in instead of a many - components approach  #TAUTHOR_TAG, while still enjoying faster convergence']",4
"['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third -']","['arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['##parents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features']","['decomposition is a class of optimization techniques that tackle the dual of combinatorial figure 1 : parts considered in this paper.', 'firstorder models factor over arcs  #AUTHOR_TAG mc  #AUTHOR_TAG, and second - order models include also consecutive siblings and grandparents  #AUTHOR_TAG.', 'our parsers add also arbitrary siblings ( not necessarily consecutive ) and head bigrams, as in  #TAUTHOR_TAG, in addition to third - order features for grand - and tri - siblings.', 'problems in a modular and extensible manner  #AUTHOR_TAG.', 'in this paper, we employ alternating directions dual decomposition ( ad 3 ;  #TAUTHOR_TAG.', 'like the subgradient algorithm of, ad 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables.', 'the difference is that the ad 3 subproblems have an additional quadratic term to accelerate consensus.', 'recent analysis  #AUTHOR_TAG has shown that : ( i ) ad 3 converges at a faster rate, 2 and ( ii ) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm.', 'this opens the door for larger subproblems ( such as the combination of trees and head automata in instead of a many - components approach  #TAUTHOR_TAG, while still enjoying faster convergence']",1
"['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['', 'its members represent marginal probabilities over the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among the following iterative updates :', '• z - updates, which decouple over s = 1,..., s, and solve a penalized version of eq. 2 :', 'above, ρ is a constant and the quadratic term penalizes deviations from the current global solution ( stored in u ( t ) ).', '5 we will see ( prop. 2 ) that this problem can be solved iteratively using only the local - max oracle ( eq. 2 ).', '• u - updates, a simple averaging operation :', '• λ - updates, where the lagrange multipliers are adjusted to penalize disagreements :', 'in sum, the only difference between ad 3 and the subgradient method is in the z - updates, which in ad 3 require solving a quadratic problem.', 'while closed - form solutions have been developed for some specialized components  #TAUTHOR_TAG, this problem is in general more difficult than the one arising in the subgradient algorithm.', 'however, the following result, proved in  #AUTHOR_TAG, allows to expand the scope of ad 3 to any problem which satisfies assumption 1. proposition 2.', 'the problem in eq. 3 admits a solution z * s which is spanned by a sparse basis w ⊆ y s with cardinality at most | w | ≤ o ( | a s | ).', '']",1
"['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among']","['', 'its members represent marginal probabilities over the arcs in as.', 'the ad 3 algorithm  #TAUTHOR_TAG alternates among the following iterative updates :', '• z - updates, which decouple over s = 1,..., s, and solve a penalized version of eq. 2 :', 'above, ρ is a constant and the quadratic term penalizes deviations from the current global solution ( stored in u ( t ) ).', '5 we will see ( prop. 2 ) that this problem can be solved iteratively using only the local - max oracle ( eq. 2 ).', '• u - updates, a simple averaging operation :', '• λ - updates, where the lagrange multipliers are adjusted to penalize disagreements :', 'in sum, the only difference between ad 3 and the subgradient method is in the z - updates, which in ad 3 require solving a quadratic problem.', 'while closed - form solutions have been developed for some specialized components  #TAUTHOR_TAG, this problem is in general more difficult than the one arising in the subgradient algorithm.', 'however, the following result, proved in  #AUTHOR_TAG, allows to expand the scope of ad 3 to any problem which satisfies assumption 1. proposition 2.', 'the problem in eq. 3 admits a solution z * s which is spanned by a sparse basis w ⊆ y s with cardinality at most | w | ≤ o ( | a s | ).', '']",6
"['of  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG,,  #AUTHOR_TAG, zhang and mc  #AUTHOR_TAG.', 'the last two are shown separately in']","['of  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG,,  #AUTHOR_TAG, zhang and mc  #AUTHOR_TAG.', 'the last two are shown separately in']","['of  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG,,  #AUTHOR_TAG, zhang and mc  #AUTHOR_TAG.', 'the last two are shown separately in']","['first evaluated our non - projective parser in a projective english dataset, to see how its speed and accuracy compares with recent projective parsers, which can take advantage of dynamic programming.', 'to this end, we converted the penn treebank to dependencies through ( i ) the head rules of  #AUTHOR_TAG ( ptb - ym ) and ( ii ) basic dependencies from the stanford parser 2. 0. 5 ( ptb - s ).', '11 we trained by running 10 epochs of cost - augmented mira  #AUTHOR_TAG.', 'to ensure valid parse trees at test time, we rounded fractional solutions as in  #AUTHOR_TAG - yet, solutions were integral ≈ 95 % of the time.', 'tab. 2 shows the results in the dev - set ( top block ) and in the test - set ( two bottom blocks ).', 'in the dev - set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third - order model ; this comes at the cost of a 6 - fold drop in runtime compared with a first - order model.', 'by looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels ( with the exception of the highly optimized vine cascade approach of  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG,,  #AUTHOR_TAG, zhang and mc  #AUTHOR_TAG.', 'the last two are shown separately in the rightmost columns.', 'in our second experiment ( tab. 3 ), we used 14 datasets, most of which are non - projective, from the conll 2006 and 2008 shared tasks  #AUTHOR_TAG.', 'our third - order model achieved the best reported scores for english, czech, german, and dutchwhich includes the three largest datasets and the ones with the most non - projective dependenciesand is on par with the state of the art for the remaining languages.', 'to our knowledge, the speeds are the highest reported among higherorder non - projective parsers, and only about 3 - 4 times slower than the vine parser of  #AUTHOR_TAG, which has lower accuracies']",3
[' #TAUTHOR_TAG proposed a similar multi - modal embedding method'],"[' #TAUTHOR_TAG proposed a similar multi - modal embedding method for aligning text and image representations in', 'a shared latent']","[' #TAUTHOR_TAG proposed a similar multi - modal embedding method for aligning text and image representations in', 'a shared latent']","['', '. [ 22 ] published recipe1m, the largest publicly available multimodal dataset,', 'that consists of 1 million recipes together with the accompanying images. the emergence of multi - modal databases has led to novel approaches for meal image analysis. the fusion of visual features learned from images by deep convolution', 'neural networks ( cnn ) and textual features lead to outstanding results in food recognition applications. an early approach for recipe retrieval was based on', 'jointly learning to predict food category and its ingredients using deep cnn [ 4 ]. in a following step, the predicted ingredients are matched against a', 'large corpus of recipes. more recent approach is proposed by [ 22 ] and is based on jointly learning recipe - text and image representations in a shared latent space. recurrent neural networks ( rnn ) and cnn are mainly used to map text and image into', 'the shared space. to align the text and image embedding vectors between matching recipe - image pairs, cosine similarity loss with margin was applied. carvalho et al.  #TAUTHOR_TAG proposed a similar multi - modal embedding method for aligning text and image representations in', '']",0
"['17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['[ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for all models we were using selected matching pairs']","[""##ima'19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation."", 'for all models we were using selected matching pairs generated by reducing noisy instruction sentences as described above.', 'recall rates are averaged over the evaluation batches.', 'image to recipe medr r @ 1 r @ 5 r @ 10 1k samples', 'random [ 17 ] 500. 0 0. 001 0. 005 0. 01 jne [ 17 ] 5. 0 ± 0.', 'both [ 17 ] and  #TAUTHOR_TAG use time - consuming instruction text preprocessing over the skip - thought technique [ 15 ].', ""this process doubles the overall training time from three days to six days using two nvidia titan x gpu's."", 'by using online - instruction encoding with the self - attention encoder, we were able train the model for its main task in under 30 hours.', 'furthermore, the proposed approach offers more flexibility for dataset alterations.', 'figure 4 : ingredient - attention based focus on instruction sentences.', 'we use two different mapping matrices for the two ingredient based queries.', 'qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores.', 'depending on meal type, all baseline implementations as well as our ingredient attention based model exhibit a broad range of retrieval accuracy.', 'in figure 5 we present a few typical results on the intended recipe retrieval task.', 'adamine  #TAUTHOR_TAG creates more distinct class clusters than in [ 17 ].', 'in figure 3, we demonstrate the difference in cluster formation using the aforementioned methods for our ingredient attention.', 'we visualize the top ten most common recipe classes in recipe1m using t - sne [ 23 ].', 'since chocolate chip, peanut butter, cream cheese and / or ice cream are used as ingredients in desserts, due to semantic regularization inside the triplet loss, clusters of sweet meals are close together ( figure 3b top right corner ).', ""madima'19, october 21, 2019, nice, france sample 1 figure 5 : the retrieval performance of our model depends heavily on the meal type."", 'we marked matching retrieved ingredients or those of the same family in green.', 'the ingredient attention model performed well on sample 1, and acceptably on sample 2.', '']",0
"['17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['[ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for all models we were using selected matching pairs']","[""##ima'19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation."", 'for all models we were using selected matching pairs generated by reducing noisy instruction sentences as described above.', 'recall rates are averaged over the evaluation batches.', 'image to recipe medr r @ 1 r @ 5 r @ 10 1k samples', 'random [ 17 ] 500. 0 0. 001 0. 005 0. 01 jne [ 17 ] 5. 0 ± 0.', 'both [ 17 ] and  #TAUTHOR_TAG use time - consuming instruction text preprocessing over the skip - thought technique [ 15 ].', ""this process doubles the overall training time from three days to six days using two nvidia titan x gpu's."", 'by using online - instruction encoding with the self - attention encoder, we were able train the model for its main task in under 30 hours.', 'furthermore, the proposed approach offers more flexibility for dataset alterations.', 'figure 4 : ingredient - attention based focus on instruction sentences.', 'we use two different mapping matrices for the two ingredient based queries.', 'qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores.', 'depending on meal type, all baseline implementations as well as our ingredient attention based model exhibit a broad range of retrieval accuracy.', 'in figure 5 we present a few typical results on the intended recipe retrieval task.', 'adamine  #TAUTHOR_TAG creates more distinct class clusters than in [ 17 ].', 'in figure 3, we demonstrate the difference in cluster formation using the aforementioned methods for our ingredient attention.', 'we visualize the top ten most common recipe classes in recipe1m using t - sne [ 23 ].', 'since chocolate chip, peanut butter, cream cheese and / or ice cream are used as ingredients in desserts, due to semantic regularization inside the triplet loss, clusters of sweet meals are close together ( figure 3b top right corner ).', ""madima'19, october 21, 2019, nice, france sample 1 figure 5 : the retrieval performance of our model depends heavily on the meal type."", 'we marked matching retrieved ingredients or those of the same family in green.', 'the ingredient attention model performed well on sample 1, and acceptably on sample 2.', '']",0
"['- attention, whereas [ 17 ] and  #TAUTHOR_TAG chose a vector length of 300.', 'in the following sections, more details about']","['pretrained with the word2vec algorithm [ 19 ] and fine tuned during the joint embedding learning phase.', 'we chose 512 - dimensional word embedding for our model with self - attention, whereas [ 17 ] and  #TAUTHOR_TAG chose a vector length of 300.', 'in the following sections, more details about']","['- attention, whereas [ 17 ] and  #TAUTHOR_TAG chose a vector length of 300.', 'in the following sections, more details about the aforementioned paths are presented']","['proposed model architecture is based on a multi - path approach for each of the involved input data types namely, instructions, ingredients and images, similarly to [ 17 ].', 'in figure 2, the overall structure is presented.', 'for the instruction encoder, we utilized a self - attention mechanism [ 24 ], which learns which words of the instructions are relevant with a certain ingredient.', 'in order to encode the ingredients, a bidirectional rnn is used, since ingredients are an unordered list of words.', 'all rnns in the ingredients path were implemented with long short - term memory ( lstm ) cells [ 11 ].', 'we fixed the ingredient representation to have a length of 600, independent of the amount of ingredients.', 'lastly, the outputs of the self - attention - instruction encoder with ingredient attention and the output of the bidirectional lstm ingredient - encoder are concatenated and mapped to the joint embedding space.', 'the image analysis path is composed of a resnet - 50 model [ 9 ], pretrained on the imagenet dataset [ 7 ], with a custom top layer for mapping the image features to the joint embedding space.', 'all word embeddings are pretrained with the word2vec algorithm [ 19 ] and fine tuned during the joint embedding learning phase.', 'we chose 512 - dimensional word embedding for our model with self - attention, whereas [ 17 ] and  #TAUTHOR_TAG chose a vector length of 300.', 'in the following sections, more details about the aforementioned paths are presented']",4
"['17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['[ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for all models we were using selected matching pairs']","[""##ima'19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation."", 'for all models we were using selected matching pairs generated by reducing noisy instruction sentences as described above.', 'recall rates are averaged over the evaluation batches.', 'image to recipe medr r @ 1 r @ 5 r @ 10 1k samples', 'random [ 17 ] 500. 0 0. 001 0. 005 0. 01 jne [ 17 ] 5. 0 ± 0.', 'both [ 17 ] and  #TAUTHOR_TAG use time - consuming instruction text preprocessing over the skip - thought technique [ 15 ].', ""this process doubles the overall training time from three days to six days using two nvidia titan x gpu's."", 'by using online - instruction encoding with the self - attention encoder, we were able train the model for its main task in under 30 hours.', 'furthermore, the proposed approach offers more flexibility for dataset alterations.', 'figure 4 : ingredient - attention based focus on instruction sentences.', 'we use two different mapping matrices for the two ingredient based queries.', 'qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores.', 'depending on meal type, all baseline implementations as well as our ingredient attention based model exhibit a broad range of retrieval accuracy.', 'in figure 5 we present a few typical results on the intended recipe retrieval task.', 'adamine  #TAUTHOR_TAG creates more distinct class clusters than in [ 17 ].', 'in figure 3, we demonstrate the difference in cluster formation using the aforementioned methods for our ingredient attention.', 'we visualize the top ten most common recipe classes in recipe1m using t - sne [ 23 ].', 'since chocolate chip, peanut butter, cream cheese and / or ice cream are used as ingredients in desserts, due to semantic regularization inside the triplet loss, clusters of sweet meals are close together ( figure 3b top right corner ).', ""madima'19, october 21, 2019, nice, france sample 1 figure 5 : the retrieval performance of our model depends heavily on the meal type."", 'we marked matching retrieved ingredients or those of the same family in green.', 'the ingredient attention model performed well on sample 1, and acceptably on sample 2.', '']",4
"['by  #TAUTHOR_TAG.', 'both objective functions and the semantic regularization by [ 17 ]']","['by  #TAUTHOR_TAG.', 'both objective functions and the semantic regularization by [ 17 ]']","['by  #TAUTHOR_TAG.', 'both objective functions and the semantic regularization by [ 17 ] aim at maximizing intra - class correlation and minimizing inter - class correlation.', 'let us define']","['align text and image embeddings of matching recipe - image pairs alongside each other, we maximize the cosine distance between positive pairs and minimize it between negative pairs.', 'we have trained our model using cosine similarity loss with margin as in [ 17 ] and with the triplet loss proposed by  #TAUTHOR_TAG.', 'both objective functions and the semantic regularization by [ 17 ] aim at maximizing intra - class correlation and minimizing inter - class correlation.', 'let us define the text query embedding as [UNK] q and the embedding of the image query as [UNK] d, then the cosine embedding loss can be defined as follows :', 'where cos ( x, y ) is the normalized cosine similarity and α is a margin ( −1 [UNK] α [UNK] 1 ), that determines how similar negative pairs are allowed to be.', 'positive margins allow negative pairs to share at maximum α similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively.', '']",5
['the triplet loss  #TAUTHOR_TAG'],['the triplet loss  #TAUTHOR_TAG'],['the triplet loss  #TAUTHOR_TAG'],"['used adam [ 14 ] optimizer with an initial learning rate of 10 −4.', 'at the beginning of the training session, we freeze the pretrained resnet - 50 weights and optimize only the text - processing branch until we do no longer make progress.', 'then, we alternate train image and text branch until we switched modality for 10 times.', 'lastly, we fine - tune the overall model by releasing all trainable parameters in the model.', 'our optimization strategy differs from [ 17 ] in that we use an aggressive learning rate decay, namely exponential decay, so that the learning rate is halved all 20 epochs.', 'since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective [ 17 ] and for the triplet loss  #TAUTHOR_TAG']",5
"['17 ] and  #TAUTHOR_TAG, we evaluated our model on']","['testing sets, respectively.', 'similarly to [ 17 ] and  #TAUTHOR_TAG, we evaluated our model on']","['17 ] and  #TAUTHOR_TAG, we evaluated our model on 10 subsets of 1000 samples each.', 'one sample of these subsets is composed of text embedding and image embedding in the shared latent space.', 'since our interest lies in the recipe retrieval task, we optimized']","['', 'similarly to [ 17 ] and  #TAUTHOR_TAG, we evaluated our model on 10 subsets of 1000 samples each.', 'one sample of these subsets is composed of text embedding and image embedding in the shared latent space.', 'since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings.', 'by ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank.', ""the model's performance is best, if the matching text embedding is found at the first rank."", 'further, we estimate the recall percentage at the top k percent over all queries.', 'the recall percentage describes the quantity of queries ranked amid the top k closest results.', 'in table 1 the results are presented, in comparison to baseline methods']",3
"['implementation of  #TAUTHOR_TAG.', 'regarding training time on the other hand, we increased']","['implementation of  #TAUTHOR_TAG.', 'regarding training time on the other hand, we increased']","['baseline implementation of  #TAUTHOR_TAG.', 'regarding training time on the other hand, we increased the efficiency significantly']","['this paper, we have introduced self - attention for instruction encoding in the context of the recipe retrieval task and ingredient attention for disclosing ingredient dependent meal preparation steps.', 'our main contribution is the aforementioned ingredient attention, empowering our model to solve the recipe retrieval without any upstream skip instruction embedding, as well as the light - weight architecture provided by the transformer - like instruction encoder.', 'on the recipe retrieval task, our method performs similarly to our baseline implementation of  #TAUTHOR_TAG.', 'regarding training time on the other hand, we increased the efficiency significantly for crossmodal based retrieval methods.', 'there is no need for a maximum number of instructions for a recipe to be considered as valid for training or testing ; only for total words, making more samples of the large recipe1m corpus usable for training.', 'through ingredient attention, we are able to unveil internal focus in the text processing path by observing attention weights.', 'incorporation of new samples in the train set can be done by retraining just one model.', 'overall, an accurate and flexible method for recipe retrieval from meal images could provide downstream models ( e. g. automatic nutrient content estimation ) with decisive information and significantly improve their results']",3
"['17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['[ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for']","['19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation.', 'for all models we were using selected matching pairs']","[""##ima'19, october 21, 2019, nice, france [ 17 ] and adamine  #TAUTHOR_TAG re - implementation."", 'for all models we were using selected matching pairs generated by reducing noisy instruction sentences as described above.', 'recall rates are averaged over the evaluation batches.', 'image to recipe medr r @ 1 r @ 5 r @ 10 1k samples', 'random [ 17 ] 500. 0 0. 001 0. 005 0. 01 jne [ 17 ] 5. 0 ± 0.', 'both [ 17 ] and  #TAUTHOR_TAG use time - consuming instruction text preprocessing over the skip - thought technique [ 15 ].', ""this process doubles the overall training time from three days to six days using two nvidia titan x gpu's."", 'by using online - instruction encoding with the self - attention encoder, we were able train the model for its main task in under 30 hours.', 'furthermore, the proposed approach offers more flexibility for dataset alterations.', 'figure 4 : ingredient - attention based focus on instruction sentences.', 'we use two different mapping matrices for the two ingredient based queries.', 'qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores.', 'depending on meal type, all baseline implementations as well as our ingredient attention based model exhibit a broad range of retrieval accuracy.', 'in figure 5 we present a few typical results on the intended recipe retrieval task.', 'adamine  #TAUTHOR_TAG creates more distinct class clusters than in [ 17 ].', 'in figure 3, we demonstrate the difference in cluster formation using the aforementioned methods for our ingredient attention.', 'we visualize the top ten most common recipe classes in recipe1m using t - sne [ 23 ].', 'since chocolate chip, peanut butter, cream cheese and / or ice cream are used as ingredients in desserts, due to semantic regularization inside the triplet loss, clusters of sweet meals are close together ( figure 3b top right corner ).', ""madima'19, october 21, 2019, nice, france sample 1 figure 5 : the retrieval performance of our model depends heavily on the meal type."", 'we marked matching retrieved ingredients or those of the same family in green.', 'the ingredient attention model performed well on sample 1, and acceptably on sample 2.', '']",7
"[' #AUTHOR_TAG, and presented by  #TAUTHOR_TAG.', 'next, we introduce our multi - task learning approach of sharing the parameters between abstractive summar']","[' #AUTHOR_TAG, and presented by  #TAUTHOR_TAG.', 'next, we introduce our multi - task learning approach of sharing the parameters between abstractive summarization and']","[' #AUTHOR_TAG, and presented by  #TAUTHOR_TAG.', 'next, we introduce our multi - task learning approach of sharing the parameters between abstractive summarization and']","[', we discuss our baseline model which is similar to the machine translation encoder - alignerdecoder model of  #AUTHOR_TAG, and presented by  #TAUTHOR_TAG.', 'next, we introduce our multi - task learning approach of sharing the parameters between abstractive summarization and entailment generation models']",5
"['model with bilinear attention, similar to  #AUTHOR_TAG and following the details in  #TAUTHOR_TAG']","['model with bilinear attention, similar to  #AUTHOR_TAG and following the details in  #TAUTHOR_TAG']","['with bilinear attention, similar to  #AUTHOR_TAG and following the details in  #TAUTHOR_TAG.', 'here, we encode']","['baseline model is a strong, multi - layered encoder - attention - decoder model with bilinear attention, similar to  #AUTHOR_TAG and following the details in  #TAUTHOR_TAG.', 'here, we encode the source document with a two - layered lstm - rnn and generate the summary using another two - layered lstm - rnn decoder.', 'the word probability distribution at time step t of the decoder is defined as follows :', 'where g is a non - linear function and c t and s t are the context vector and lstm - rnn decoder hidden state at time step t, respectively.', 'the context vector c t = α t, i h i is a weighted combination of encoder hidden states h i, where the attention weights are learned through the bilinear attention mechanism proposed in  #AUTHOR_TAG.', 'for the rest of the paper, we use same notations.', 'we also use the same model architecture for the entailment generation task, i. e., a sequence - tosequence model encoding the premise and decoding the entailed hypothesis, via bilinear attention between them.', 'figure 2 : multi - task learning of the summarization task ( left ) with the entailment generation task ( right )']",5
"[' #TAUTHOR_TAG, we use the']","[' #TAUTHOR_TAG, we use the full - length f1 variant']","[' #TAUTHOR_TAG, we use the full - length f1 variant']","['previous work  #TAUTHOR_TAG, we use the full - length f1 variant of rouge  #AUTHOR_TAG for the gigaword results, and the 75 - bytes length limited recall variant of rouge for duc.', 'additionally, we also report other standard language generation metrics ( as motivated recently by  #AUTHOR_TAG ) : meteor  #AUTHOR_TAG, bleu - 4  #AUTHOR_TAG, and cider - d, based on the ms - coco evaluation script  #AUTHOR_TAG']",5
"['attention - decoder model based on  #AUTHOR_TAG and presented by  #TAUTHOR_TAG.', 'as shown in']","['strong encoder - attention - decoder model based on  #AUTHOR_TAG and presented by  #TAUTHOR_TAG.', 'as shown in']","['our baseline is a strong encoder - attention - decoder model based on  #AUTHOR_TAG and presented by  #TAUTHOR_TAG.', 'as shown in']","['results and previous work our baseline is a strong encoder - attention - decoder model based on  #AUTHOR_TAG and presented by  #TAUTHOR_TAG.', 'as shown in table 1, it is reasonably close to some of the state - of - theart ( comparable ) results in previous work, though making this baseline further strong ( e. g., based on pointer - copy mechanism ) is our next step']",5
['81  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['18 8. 49 23. 81  #TAUTHOR_TAG 28. 97 8. 26 24. 06  #AUTHOR_TAG our  #AUTHOR_TAG baseline model achieves competitive performance with'],"[', we directly use the gigaword - trained model to test on the duc - 2004 dataset ( see tuning discussion in sec. 4. 1 ).', 'in table 2, we again see that et al. ( 2015 ) 28. 18 8. 49 23. 81  #TAUTHOR_TAG 28. 97 8. 26 24. 06  #AUTHOR_TAG our  #AUTHOR_TAG baseline model achieves competitive performance with previous work, esp.', 'on rouge - 2 and rouge - l. next, we show promising multi - task improvements over this baseline of around 0. 4 % across all metrics, despite being a test - only setting and also with the mismatch between the summarization and entailment domains.', 'figure 3 shows some additional interesting output examples of our multi - task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information']",3
"['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance']","['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance']","['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance matrix of the word vectors in']","['documents that are similar to a query using vectors has a long history.', 'earlier methods modeled documents and queries using vector space models via bag - of - words ( bow ) representation  #AUTHOR_TAG.', 'other representations include latent semantic indexing ( lsi )  #AUTHOR_TAG, which can be used to define dense vector representation for documents and / or queries.', 'the past few years have witnessed a big interest in distributed representation for words, sentences, paragraphs and documents.', 'this was achieved by leveraging deep learning methods that learn word vector representation.', 'introduction of neural language models  #AUTHOR_TAG using deep learning allowed to learn word vector representation ( word embedding for simplicity ).', 'the seminal work of mikolov et al. introduced an efficient way to compute dense vectorized representation of words  #AUTHOR_TAG a, b ).', 'a more recent step was taken to move beyond distributed representation of words.', 'this is to find a distributed representation for sentences, paragraphs and documents.', 'most of the presented works study the interrelationship between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document.', 'we call our representation docov descriptor.', 'our descriptor obtains a fixed - length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements.', 'this makes our work distinguished from to the work of  #TAUTHOR_TAG where they study the interrelationship of words in the text snippet']",0
"['paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of']","['we describe our motivation towards the proposal of our novel representation :', '( 1 ) some neural - based paragraph representations such as paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #AUTHOR_TAG are positioned in the space as in figure 1.', '( 2 ) the covariance matrix represents the second order summary statistic of multivariate data.', 'this distinguishes the covariance matrix from the mean vector.', 'in figure 1 we visualize the covariance matrix using confidence ellipse representation.', 'we see that the covariance encodes the shape of the density composed of the words of interest.', 'in the earlier example the mean vectors of two dissimilar documents are put close by the word embedding.', 'on the other hand, the covariance matrices capture the distinctness of the two documents.', '( 3 ) the use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision  #AUTHOR_TAG and brain signal analysis  #AUTHOR_TAG.', 'with this global success of this representation, we believe this method can be useful for text - related tasks.', '( 4 ) the computation of the covariance descriptor is known to be fast and highly parallelizable.', 'moreover, there is no inference steps involved while computing the covariance matrix given its observations.', 'this is an advantage compared to existing methods for generating paragraph vectors, such as  #TAUTHOR_TAG.', 'our contribution in this work is two - fold : ( 1 ) we propose the document - covariance descriptor ( docov ) to represent every document as the covariance of the word embedding of its words.', 'to the best of our knowledge, we are the first to explicitly compute covariance descriptors on word embedding such as word2vec  #AUTHOR_TAG b ) or similar word vectors.', '( 2 ) we empirically show the effectiveness of our novel descriptor in comparison to the state - of - theart methods in various unsupervised and supervised classification tasks.', 'our results show that our descriptor can attain comparable accuracy to state - ofthe - art methods in a diverse set of tasks']",0
"['- thought vectors  #AUTHOR_TAG and fastsent  #TAUTHOR_TAG.', 'some']","['skip - thought vectors  #AUTHOR_TAG and fastsent  #TAUTHOR_TAG.', 'some']","['- thought vectors  #AUTHOR_TAG and fastsent  #TAUTHOR_TAG.', 'some efforts']","['', 'recently other neural - based sentence and paragraph level representations appeared to provide a fixed length representation like skip - thought vectors  #AUTHOR_TAG and fastsent  #TAUTHOR_TAG.', 'some efforts focused on defining a word mover distance ( wmd ) based on word level representation  #AUTHOR_TAG.', 'prior to this work, we proposed earlier trials for using covariance features in community question answering  #AUTHOR_TAG b, a ;  #AUTHOR_TAG.', 'in these trials we used the covariance features in combination with lexical and semantic features.', 'close to our work is  #AUTHOR_TAG, they build an implicit representation of documents using multidimensional gaussian distribution.', 'then they compute a similarity kernel to be used in document classification task.', 'our work is distinguished from  #AUTHOR_TAG as we compute an explicit descriptor for any document.', 'moreover, we use linear models which scale much better than non - linear kernels as introduced in  #AUTHOR_TAG']",0
['vectors  #AUTHOR_TAG and sdae  #TAUTHOR_TAG requires building an encoder'],"['skipthought vectors  #AUTHOR_TAG and sdae  #TAUTHOR_TAG requires building an encoder - decoder model which takes time 3 to learn.', 'for other models']",['skipthought vectors  #AUTHOR_TAG and sdae  #TAUTHOR_TAG requires building an encoder - decoder model which takes time 3 to learn'],"['show the correlation values between the similarities computed via docov and the human judgements.', 'we contrast the performance of other representations in table 2.', 'we observe that docov representation outperforms other representations in this task.', 'other models such as skipthought vectors  #AUTHOR_TAG and sdae  #TAUTHOR_TAG requires building an encoder - decoder model which takes time 3 to learn.', 'for other models like paragraph vectors  #AUTHOR_TAG and fastsent vectors  #TAUTHOR_TAG, they require a gradient descent inference step to compute the paragraph / sentence vectors.', 'using the docov, we just require a pre - trained word embedding model and we do not need any additional training like encoder - decoder models or inference steps via gradient descent']",0
"['paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of']","['we describe our motivation towards the proposal of our novel representation :', '( 1 ) some neural - based paragraph representations such as paragraph vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #AUTHOR_TAG are positioned in the space as in figure 1.', '( 2 ) the covariance matrix represents the second order summary statistic of multivariate data.', 'this distinguishes the covariance matrix from the mean vector.', 'in figure 1 we visualize the covariance matrix using confidence ellipse representation.', 'we see that the covariance encodes the shape of the density composed of the words of interest.', 'in the earlier example the mean vectors of two dissimilar documents are put close by the word embedding.', 'on the other hand, the covariance matrices capture the distinctness of the two documents.', '( 3 ) the use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision  #AUTHOR_TAG and brain signal analysis  #AUTHOR_TAG.', 'with this global success of this representation, we believe this method can be useful for text - related tasks.', '( 4 ) the computation of the covariance descriptor is known to be fast and highly parallelizable.', 'moreover, there is no inference steps involved while computing the covariance matrix given its observations.', 'this is an advantage compared to existing methods for generating paragraph vectors, such as  #TAUTHOR_TAG.', 'our contribution in this work is two - fold : ( 1 ) we propose the document - covariance descriptor ( docov ) to represent every document as the covariance of the word embedding of its words.', 'to the best of our knowledge, we are the first to explicitly compute covariance descriptors on word embedding such as word2vec  #AUTHOR_TAG b ) or similar word vectors.', '( 2 ) we empirically show the effectiveness of our novel descriptor in comparison to the state - of - theart methods in various unsupervised and supervised classification tasks.', 'our results show that our descriptor can attain comparable accuracy to state - ofthe - art methods in a diverse set of tasks']",1
"['', 'we follow the setup used in  #TAUTHOR_TAG']","['against the state - of - the - art approaches in unsupervised paragraph representation.', 'we follow the setup used in  #TAUTHOR_TAG']","['conduct a comparative evaluation against the state - of - the - art approaches in unsupervised paragraph representation.', 'we follow the setup used in  #TAUTHOR_TAG']","['conduct a comparative evaluation against the state - of - the - art approaches in unsupervised paragraph representation.', 'we follow the setup used in  #TAUTHOR_TAG']",5
['contrast our results against the methods reported in  #TAUTHOR_TAG'],['contrast our results against the methods reported in  #TAUTHOR_TAG'],"['contrast our results against the methods reported in  #TAUTHOR_TAG.', 'the competing methods are the paragraph vectors  #AUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG.', 'the mean vector baseline is also implemented']","['contrast our results against the methods reported in  #TAUTHOR_TAG.', 'the competing methods are the paragraph vectors  #AUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fastsent  #TAUTHOR_TAG.', 'the mean vector baseline is also implemented.', 'also, we use the sum of the similarities generated by the docov and the mean vectors.', 'all of our results are reported using the freely available gnews word2vec of dim = 300.', 'we use same evaluation measures  #TAUTHOR_TAG.', 'we use the pearson correlation and spearman correlation with the manual relatedness judgements.', 'the semantic sentence relatedness datasets used in the comparative evaluation the sick dataset  #AUTHOR_TAG consists of 10, 000 pairs of sentences and relatedness judgements and the sts 2014 dataset  #AUTHOR_TAG consists of 3, 750 pairs and ratings from six linguistic domains']",5
"['weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['examined concatenation of bow with tf - idf weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['idf weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', '']","['contrast our results against the same methods of unsupervised paragraph representations.', 'in addition to the results of docov we examined concatenation of bow with tf - idf weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', '']",5
"['weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['examined concatenation of bow with tf - idf weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', ' #AUTHOR_TAG']","['idf weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', '']","['contrast our results against the same methods of unsupervised paragraph representations.', 'in addition to the results of docov we examined concatenation of bow with tf - idf weighting and mean vectors with our docov descriptors.', 'we use linear  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG.', 'our primary task is to evaluate the']","[' #TAUTHOR_TAG.', 'our primary task is to evaluate the']","[' #TAUTHOR_TAG.', 'our primary task is to evaluate the impact of similar words on the sense collocation method, and decrease the sensitivity']","['paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by  #TAUTHOR_TAG.', 'our primary task is to evaluate the impact of similar words on the sense collocation method, and decrease the sensitivity of the classifiers by expanding the range of sense collocations via different semantic relations.', 'our method combines hypernyms, hyponyms and sister words of the component nouns, based on wordnet.', 'the data used in our experiments was taken from the nominal pair interpretation task of semeval - 2007 ( 4th international workshop on semantic evaluation 2007.', 'in the evaluation, we test 7 - way and 2 - way class data, and show that the inclusion of hypernyms improves the performance of the sense collocation method, while the inclusion of hyponym and sister word information leads to a deterioration in performance']",6
"['of  #TAUTHOR_TAG by adding similar words as features focusing on hypernyms, hyponyms']","['of  #TAUTHOR_TAG by adding similar words as features focusing on hypernyms, hyponyms']","['extend the approach of  #TAUTHOR_TAG by adding similar words as features focusing on hypernyms, hyponyms']","['extend the approach of  #TAUTHOR_TAG by adding similar words as features focusing on hypernyms, hyponyms and sister words of the modifier and head noun.', 'we accumulate the features for semantic relations based on different taxonomic relation types, from which we construct a feature vector to build a classifier over.', 'the features of each taxonomic relation types are listed below.', 'the first is features used in the original sense collocation method.', 'the second, third and fourth are our experimental features added hypernyms, hyponyms and sister words respectively.', '']",6
"['as sense collocation in  #TAUTHOR_TAG.', 'for example, ncs such']","['as sense collocation in  #TAUTHOR_TAG.', 'for example, ncs such']","['as sense collocation in  #TAUTHOR_TAG.', 'for example, ncs such']","['', 'interpreting ncs has received much attention of late, and the problem has been addressed in areas of machine translation ( mt ), information extraction ( ie ), and applications such as question - answering ( qa ).', 'processing ncs exhibits many challenges due to the following issues associated with the task  #AUTHOR_TAG : ( 1 ) the compounding process is extremely productive ; ( 2 ) the semantic relationship between the head noun and its modifier ( s ) is implicit ; and ( 3 ) the interpretation of an nc can vary due to contextual and pragmatic factors.', 'due to these challenges, current nc interpretation methods are too error - prone to employ directly in nlp applications without any human intervention or preprocessing.', 'in this paper, we investigate the task of nc interpretation based on sense collocation.', 'it has been shown that ncs with semantically similar compo - nents share the same sr ; this is encapsulated by the phrase coined as sense collocation in  #TAUTHOR_TAG.', 'for example, ncs such as apple pie have the same interpretation as banana cake, where the modifiers of both ncs are semantically similar ( they are both classified as fruit ), and the head nouns of both ncs are a type of baked edible concoction.', 'given that the modifier is a fruit and the head noun is']",0
"[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this area is by  #TAUTHOR_TAG, who used the sense collocation ( i. e. pair - of - word - senses ) as their primary feature in disambiguating ncs.', 'many subsequent studies have been based on this sense collocation method, with the addition of other performance - improving features.', 'for example,  #AUTHOR_TAG added contextual information ( e. g. the grammatical role and pos ) and cross - lingual information from 5 european languages as features to her model.', 'in contrast, utilise sense collocations in a different way : instead of adding additional features in their model, they increase the size of their training data by substituting components of existing training instances to generate additional training instances ( which is assumed to have the same sr as the original ).', 'for an sr to be preserved the newly - generated nc must be semantically similar and hence maintain the same sense collocation as the original nc on which it was based.', ' #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG attempted to interpret ncs by applying implicit sense collocations.', 'in particular, they used various ways to retrieve sense collocations instead of manually assigning word senses to the ncs.', 'hence, their methods do not require direct use of word senses.', ' #AUTHOR_TAG retrieved the sense pairs in the context of a hierarchical class set for the biomedical domain.', ' #AUTHOR_TAG used the similarity measure to express the sense collocation of ncs.', ' #AUTHOR_TAG listed the hypernyms of components as sense features']",0
"[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this area is by  #TAUTHOR_TAG, who used the sense collocation ( i. e. pair - of - word - senses ) as their primary feature in disambiguating ncs.', 'many subsequent studies have been based on this sense collocation method, with the addition of other performance - improving features.', 'for example,  #AUTHOR_TAG added contextual information ( e. g. the grammatical role and pos ) and cross - lingual information from 5 european languages as features to her model.', 'in contrast, utilise sense collocations in a different way : instead of adding additional features in their model, they increase the size of their training data by substituting components of existing training instances to generate additional training instances ( which is assumed to have the same sr as the original ).', 'for an sr to be preserved the newly - generated nc must be semantically similar and hence maintain the same sense collocation as the original nc on which it was based.', ' #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG attempted to interpret ncs by applying implicit sense collocations.', 'in particular, they used various ways to retrieve sense collocations instead of manually assigning word senses to the ncs.', 'hence, their methods do not require direct use of word senses.', ' #AUTHOR_TAG retrieved the sense pairs in the context of a hierarchical class set for the biomedical domain.', ' #AUTHOR_TAG used the similarity measure to express the sense collocation of ncs.', ' #AUTHOR_TAG listed the hypernyms of components as sense features']",0
"['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further']","['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further']","['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further research in this area has shown that not only synonymous ncs share the same sr, but ncs']","['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further research in this area has shown that not only synonymous ncs share the same sr, but ncs whose components are replaced with similar words also have the same sr as the original ncs.', 'for example, car factory, automobile factory and truck factory substituted with a synonym, hypernym and sister word, respectively, share the same sr of product - producer.', 'figure 3 shows an example of semantic neighbours for the two ncs car key and apple pie.', 'car key can be interpreted as product - producer by referring to the training nc automobile key, since they have the same sense collocation.', 'with apple juice, the sense collocation method tries to locate matching sense collocations in the training data, and finds that fruit juice matches closely, with the mod - ifier being a hypernym of apple.', 'from this, we can hope to correctly interpret apple juice as having the sr product - producer.', 'in order to achieve this, we require some means of comparing nouns taxonomically, both vertically to capute hypernyms and hyponyms, and horizontally to capture sister words.', 'as intimated above, our motivation in conducting this research is to be able to include hypernym, hyponym and sister word information without using direct substitution over the training instances, but still preserving the essence of the sense collocation approach.', 'the disadvantage of the method employed by is that noise will inevitably infect the training data, skewing the classifier performance.', 'the original method described in  #TAUTHOR_TAG only relies on observed sense collocations.', 'the components of the ncs are represented as specific synsets in wordnet, and the model does not capture related words.', 'hence, in this paper, we aimed to develop a model that can take advantage of relatedness between wordnet synsets via hypernyms, hyponyms and sister words, without the risk of losing semantic granularity or introducing noisy training data.', 'note that in  #AUTHOR_TAG, we used synonyms, hypernyms and sister words.', 'as synonyms have the identical sense collocation ( i. e. pairing of synsets ) within wordnet, we ignore it in this research.', 'instead, we add hyponyms as a means of broadening the range of sense collocation']",0
['basic idea behind sense collocation method in  #TAUTHOR_TAG was based on the'],"['basic idea behind sense collocation method in  #TAUTHOR_TAG was based on the "" pair - ofword - senses "" from the component nouns in noun compounds as features of the classifier.', 'they also introduced a probability model called semantic scattering, as detailed in equations 1 and 2 below as a supervised classification technique.', 'in essence, the probability']","['basic idea behind sense collocation method in  #TAUTHOR_TAG was based on the "" pair - ofword - senses "" from the component nouns in noun compounds as features of the classifier.', 'they also introduced a probability model called semantic scattering, as detailed in equations 1 and 2 below as a supervised classification technique.', 'in essence, the probability p ( r']","['basic idea behind sense collocation method in  #TAUTHOR_TAG was based on the "" pair - ofword - senses "" from the component nouns in noun compounds as features of the classifier.', 'they also introduced a probability model called semantic scattering, as detailed in equations 1 and 2 below as a supervised classification technique.', 'in essence, the probability p ( r | f i f j ) ( simplified to p ( r | f ij ) ) of a modifier and head noun with word sense f i and f j, respectively, occurring with sr r is calculated based on simple maximum likelihood estimation :', 'the preferred sr r * for the given sense combination is that which maximizes the probability :', '( 2']",0
"['increase in performance, but these works make their own assumptions for interpretation  #TAUTHOR_TAG']","['increase in performance, but these works make their own assumptions for interpretation  #TAUTHOR_TAG']","['the increase in performance, but these works make their own assumptions for interpretation  #TAUTHOR_TAG']","['', 'this has gained much attention in recent decades, as well as controversy,  #AUTHOR_TAG.', 'in the study conducted by  #AUTHOR_TAG, it was claimed that there were 9 distinct srs, which could be discretely defined and interpreted within ncs, while  #AUTHOR_TAG claimed an unlimited number of srs.', 'the problems surrounding this task involve the issue of granularity versus coverage, which to date remains widely debated.', 'syntactic disambiguation ( called bracketing ) is required when ncs are composed of more than 2 components, such as in the case of computer science department, introducing the need for phrasal disambiguation  #AUTHOR_TAG.', ' #AUTHOR_TAG proposed probabilistic models ( based on dependency and adjacency analyses of the data ).', ' #AUTHOR_TAG built upon this by adding linguistic features into these probabilistic models.', 'methods employed in word sense disambiguation ( wsd ) have also been used to enhance nc interpretation ; the noun components that comprise the ncs are disambiguated using these wsd techniques ( sparck  #AUTHOR_TAG. carried out experiments on automatically modeling wsd and attested the usefulness of conducting analysis of the word senses in the nc in determining its sr.', 'in the automatic interpretation of ncs, many claims have been made for the increase in performance, but these works make their own assumptions for interpretation  #TAUTHOR_TAG']",0
"[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this area is by  #TAUTHOR_TAG, who used the sense collocation ( i. e. pair - of - word - senses ) as their primary feature in disambiguating ncs.', 'many subsequent studies have been based on this sense collocation method, with the addition of other performance - improving features.', 'for example,  #AUTHOR_TAG added contextual information ( e. g. the grammatical role and pos ) and cross - lingual information from 5 european languages as features to her model.', 'in contrast, utilise sense collocations in a different way : instead of adding additional features in their model, they increase the size of their training data by substituting components of existing training instances to generate additional training instances ( which is assumed to have the same sr as the original ).', 'for an sr to be preserved the newly - generated nc must be semantically similar and hence maintain the same sense collocation as the original nc on which it was based.', ' #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG attempted to interpret ncs by applying implicit sense collocations.', 'in particular, they used various ways to retrieve sense collocations instead of manually assigning word senses to the ncs.', 'hence, their methods do not require direct use of word senses.', ' #AUTHOR_TAG retrieved the sense pairs in the context of a hierarchical class set for the biomedical domain.', ' #AUTHOR_TAG used the similarity measure to express the sense collocation of ncs.', ' #AUTHOR_TAG listed the hypernyms of components as sense features']",5
"['in  #TAUTHOR_TAG.', 'then we present our method using hypernyms, hyponyms and sister words in']","['in  #TAUTHOR_TAG.', 'then we present our method using hypernyms, hyponyms and sister words in']","['in  #TAUTHOR_TAG.', 'then we present our method using hypernyms, hyponyms and sister words in']","['first, we describe the principal idea of sense collocation method on nc interpretation and the probability model proposed in  #TAUTHOR_TAG.', 'then we present our method using hypernyms, hyponyms and sister words in order to extend sense collocation method']",5
['in  #TAUTHOR_TAG is'],['in  #TAUTHOR_TAG is'],"['in  #TAUTHOR_TAG is considered as a benchmark.', '.']","['ran our first experiment over the 7 - class dataset.', 'the baseline was computed using a zero - r classifier ( i. e. majority vote ).', '2 the performance of the original method proposed in  #TAUTHOR_TAG is considered as a benchmark.', '']",5
['in  #TAUTHOR_TAG is'],['in  #TAUTHOR_TAG is'],"['in  #TAUTHOR_TAG is considered as a benchmark.', '.']","['ran our first experiment over the 7 - class dataset.', 'the baseline was computed using a zero - r classifier ( i. e. majority vote ).', '2 the performance of the original method proposed in  #TAUTHOR_TAG is considered as a benchmark.', '']",5
"[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","[' #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our']","['majority of research undertaken in interpreting ncs have been based on two statistical methods : semantic similarity  #TAUTHOR_TAG semantic inter - pretability  #AUTHOR_TAG.', 'our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.', 'a significant contribution to this area is by  #TAUTHOR_TAG, who used the sense collocation ( i. e. pair - of - word - senses ) as their primary feature in disambiguating ncs.', 'many subsequent studies have been based on this sense collocation method, with the addition of other performance - improving features.', 'for example,  #AUTHOR_TAG added contextual information ( e. g. the grammatical role and pos ) and cross - lingual information from 5 european languages as features to her model.', 'in contrast, utilise sense collocations in a different way : instead of adding additional features in their model, they increase the size of their training data by substituting components of existing training instances to generate additional training instances ( which is assumed to have the same sr as the original ).', 'for an sr to be preserved the newly - generated nc must be semantically similar and hence maintain the same sense collocation as the original nc on which it was based.', ' #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG attempted to interpret ncs by applying implicit sense collocations.', 'in particular, they used various ways to retrieve sense collocations instead of manually assigning word senses to the ncs.', 'hence, their methods do not require direct use of word senses.', ' #AUTHOR_TAG retrieved the sense pairs in the context of a hierarchical class set for the biomedical domain.', ' #AUTHOR_TAG used the similarity measure to express the sense collocation of ncs.', ' #AUTHOR_TAG listed the hypernyms of components as sense features']",1
"['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further']","['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further']","['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further research in this area has shown that not only synonymous ncs share the same sr, but ncs']","['mentioned above,  #TAUTHOR_TAG showed that the sense collocation of ncs is a key feature when interpreting ncs.', 'further research in this area has shown that not only synonymous ncs share the same sr, but ncs whose components are replaced with similar words also have the same sr as the original ncs.', 'for example, car factory, automobile factory and truck factory substituted with a synonym, hypernym and sister word, respectively, share the same sr of product - producer.', 'figure 3 shows an example of semantic neighbours for the two ncs car key and apple pie.', 'car key can be interpreted as product - producer by referring to the training nc automobile key, since they have the same sense collocation.', 'with apple juice, the sense collocation method tries to locate matching sense collocations in the training data, and finds that fruit juice matches closely, with the mod - ifier being a hypernym of apple.', 'from this, we can hope to correctly interpret apple juice as having the sr product - producer.', 'in order to achieve this, we require some means of comparing nouns taxonomically, both vertically to capute hypernyms and hyponyms, and horizontally to capture sister words.', 'as intimated above, our motivation in conducting this research is to be able to include hypernym, hyponym and sister word information without using direct substitution over the training instances, but still preserving the essence of the sense collocation approach.', 'the disadvantage of the method employed by is that noise will inevitably infect the training data, skewing the classifier performance.', 'the original method described in  #TAUTHOR_TAG only relies on observed sense collocations.', 'the components of the ncs are represented as specific synsets in wordnet, and the model does not capture related words.', 'hence, in this paper, we aimed to develop a model that can take advantage of relatedness between wordnet synsets via hypernyms, hyponyms and sister words, without the risk of losing semantic granularity or introducing noisy training data.', 'note that in  #AUTHOR_TAG, we used synonyms, hypernyms and sister words.', 'as synonyms have the identical sense collocation ( i. e. pairing of synsets ) within wordnet, we ignore it in this research.', 'instead, we add hyponyms as a means of broadening the range of sense collocation']",1
['hierarchical attention  #TAUTHOR_TAG'],['hierarchical attention  #TAUTHOR_TAG'],"['hierarchical attention  #TAUTHOR_TAG.', 'despite these promising recent improvements, input document : may is']","['##ive summarization, the task of rewriting a document into a short summary is a significantly more challenging ( and natural ) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary.', 'neural sequence - to - sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder - aligner - decoder approaches, further enhanced via convolutional encoders, pointer - copy mechanisms, and hierarchical attention  #TAUTHOR_TAG.', 'despite these promising recent improvements, input document : may is a pivotal month for moving and storage companies.', 'ground - truth summary : moving companies hit bumps in economic road baseline summary : a month to move storage companies multi - task summary : pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation.', '']",0
"['encoders, switching pointer and copy mechanisms, and hierarchical attention models  #TAUTHOR_TAG.', '']","['using machine translation inspired encoder - aligner - decoder models, convolution - based encoders, switching pointer and copy mechanisms, and hierarchical attention models  #TAUTHOR_TAG.', '']","['decoder models, convolution - based encoders, switching pointer and copy mechanisms, and hierarchical attention models  #TAUTHOR_TAG.', '']","['', 'several advances have been made in this direction using machine translation inspired encoder - aligner - decoder models, convolution - based encoders, switching pointer and copy mechanisms, and hierarchical attention models  #TAUTHOR_TAG.', '']",0
['hierarchical attention  #TAUTHOR_TAG'],['hierarchical attention  #TAUTHOR_TAG'],"['hierarchical attention  #TAUTHOR_TAG.', 'despite these promising recent improvements, input document : may is']","['##ive summarization, the task of rewriting a document into a short summary is a significantly more challenging ( and natural ) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary.', 'neural sequence - to - sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder - aligner - decoder approaches, further enhanced via convolutional encoders, pointer - copy mechanisms, and hierarchical attention  #TAUTHOR_TAG.', 'despite these promising recent improvements, input document : may is a pivotal month for moving and storage companies.', 'ground - truth summary : moving companies hit bumps in economic road baseline summary : a month to move storage companies multi - task summary : pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation.', '']",1
"[' #TAUTHOR_TAG, we use the']","[' #TAUTHOR_TAG, we use the full - length f1 variant']","[' #TAUTHOR_TAG, we use the full - length f1 variant']","['previous work  #TAUTHOR_TAG, we use the full - length f1 variant of rouge  #AUTHOR_TAG for the gigaword results, and the 75 - bytes length limited recall variant of rouge for duc.', 'additionally, we also report other standard language generation metrics ( as motivated recently by  #AUTHOR_TAG ) : meteor  #AUTHOR_TAG, bleu - 4  #AUTHOR_TAG, and cider - d, based on the ms - coco evaluation script  #AUTHOR_TAG']",5
['. 06  #TAUTHOR_TAG our  #AUTHOR_TAG baseline model'],[' #AUTHOR_TAG 28. 97 8. 26 24. 06  #TAUTHOR_TAG our  #AUTHOR_TAG baseline model'],[' #AUTHOR_TAG 28. 97 8. 26 24. 06  #TAUTHOR_TAG our  #AUTHOR_TAG baseline model achieves competitive performance with'],"[', we directly use the gigaword - trained model to test on the duc - 2004 dataset ( see tuning discussion in sec. 4. 1 ).', 'in table 2, we again see that et al. ( 2015 ) 28. 18 8. 49 23. 81  #AUTHOR_TAG 28. 97 8. 26 24. 06  #TAUTHOR_TAG our  #AUTHOR_TAG baseline model achieves competitive performance with previous work, esp.', 'on rouge - 2 and rouge - l. next, we show promising multi - task improvements over this baseline of around 0. 4 % across all metrics, despite being a test - only setting and also with the mismatch between the summarization and entailment domains.', 'figure 3 shows some additional interesting output examples of our multi - task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information']",4
"[', using pointer copy mechanism  #TAUTHOR_TAG ;']",['using pointer copy mechanism  #TAUTHOR_TAG ;'],"['stronger summarization baselines, e. g., using pointer copy mechanism  #TAUTHOR_TAG ;']","['presented a multi - task learning approach to incorporate entailment generation knowledge into summarization models.', 'we demonstrated promising initial improvements based on multiple datasets and metrics, even when the entailment knowledge was extracted from a domain different from the summarization domain.', 'our next steps to this workshop paper include : ( 1 ) stronger summarization baselines, e. g., using pointer copy mechanism  #TAUTHOR_TAG ; ( 3 ) incorporating entailment knowledge from other news - style domains such as the new multi - nli corpus  #AUTHOR_TAG, and ( 4 ) demonstrating mutual improvements on the entailment generation task']",2
"['similar  #TAUTHOR_TAG.', 'the']","['attributionally similar  #TAUTHOR_TAG.', 'the']","['similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphr']","['hypernyms ( e. g. dog - animal ) from cohyponyms ( e. g. dog - cat ) and, in turn, discriminating them from random words ( e. g. dog - fruit ) is a fundamental task in natural language processing ( nlp ).', 'hypernymy in fact represents a key organization principle of semantic memory  #AUTHOR_TAG, the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment  #AUTHOR_TAG.', 'cohyponymy ( or coordination ), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on  #TAUTHOR_TAG.', ""for this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers'ability in such discrimination, generally achieving promising results  #TAUTHOR_TAG."", 'both supervised and unsupervised approaches have been investigated.', 'the former have been shown to outperform the latter in  #TAUTHOR_TAG, even though  #AUTHOR_TAG have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.', 'in this paper, we propose a supervised method, based on a random forest algorithm and 13 corpus - based features.', 'in our evaluation, carried out using the 10 - fold cross validation on 9, 600 pairs, we achieved an accuracy of 88. 3 % when the three classes are present, and of 92. 3 % and 97. 3 % when only two classes are present.', 'such results are competitive with the state - of - the - art  #TAUTHOR_TAG']",0
"['similar  #TAUTHOR_TAG.', 'the']","['attributionally similar  #TAUTHOR_TAG.', 'the']","['similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphr']","['hypernyms ( e. g. dog - animal ) from cohyponyms ( e. g. dog - cat ) and, in turn, discriminating them from random words ( e. g. dog - fruit ) is a fundamental task in natural language processing ( nlp ).', 'hypernymy in fact represents a key organization principle of semantic memory  #AUTHOR_TAG, the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment  #AUTHOR_TAG.', 'cohyponymy ( or coordination ), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on  #TAUTHOR_TAG.', ""for this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers'ability in such discrimination, generally achieving promising results  #TAUTHOR_TAG."", 'both supervised and unsupervised approaches have been investigated.', 'the former have been shown to outperform the latter in  #TAUTHOR_TAG, even though  #AUTHOR_TAG have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.', 'in this paper, we propose a supervised method, based on a random forest algorithm and 13 corpus - based features.', 'in our evaluation, carried out using the 10 - fold cross validation on 9, 600 pairs, we achieved an accuracy of 88. 3 % when the three classes are present, and of 92. 3 % and 97. 3 % when only two classes are present.', 'such results are competitive with the state - of - the - art  #TAUTHOR_TAG']",0
"['similar  #TAUTHOR_TAG.', 'the']","['attributionally similar  #TAUTHOR_TAG.', 'the']","['similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphr']","['hypernyms ( e. g. dog - animal ) from cohyponyms ( e. g. dog - cat ) and, in turn, discriminating them from random words ( e. g. dog - fruit ) is a fundamental task in natural language processing ( nlp ).', 'hypernymy in fact represents a key organization principle of semantic memory  #AUTHOR_TAG, the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment  #AUTHOR_TAG.', 'cohyponymy ( or coordination ), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on  #TAUTHOR_TAG.', ""for this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers'ability in such discrimination, generally achieving promising results  #TAUTHOR_TAG."", 'both supervised and unsupervised approaches have been investigated.', 'the former have been shown to outperform the latter in  #TAUTHOR_TAG, even though  #AUTHOR_TAG have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.', 'in this paper, we propose a supervised method, based on a random forest algorithm and 13 corpus - based features.', 'in our evaluation, carried out using the 10 - fold cross validation on 9, 600 pairs, we achieved an accuracy of 88. 3 % when the three classes are present, and of 92. 3 % and 97. 3 % when only two classes are present.', 'such results are competitive with the state - of - the - art  #TAUTHOR_TAG']",0
"['similar  #TAUTHOR_TAG.', 'the']","['attributionally similar  #TAUTHOR_TAG.', 'the']","['similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphr']","['hypernyms ( e. g. dog - animal ) from cohyponyms ( e. g. dog - cat ) and, in turn, discriminating them from random words ( e. g. dog - fruit ) is a fundamental task in natural language processing ( nlp ).', 'hypernymy in fact represents a key organization principle of semantic memory  #AUTHOR_TAG, the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment  #AUTHOR_TAG.', 'cohyponymy ( or coordination ), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on  #TAUTHOR_TAG.', ""for this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers'ability in such discrimination, generally achieving promising results  #TAUTHOR_TAG."", 'both supervised and unsupervised approaches have been investigated.', 'the former have been shown to outperform the latter in  #TAUTHOR_TAG, even though  #AUTHOR_TAG have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.', 'in this paper, we propose a supervised method, based on a random forest algorithm and 13 corpus - based features.', 'in our evaluation, carried out using the 10 - fold cross validation on 9, 600 pairs, we achieved an accuracy of 88. 3 % when the three classes are present, and of 92. 3 % and 97. 3 % when only two classes are present.', 'such results are competitive with the state - of - the - art  #TAUTHOR_TAG']",0
"['similar  #TAUTHOR_TAG.', 'the']","['attributionally similar  #TAUTHOR_TAG.', 'the']","['similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphr']","['hypernyms ( e. g. dog - animal ) from cohyponyms ( e. g. dog - cat ) and, in turn, discriminating them from random words ( e. g. dog - fruit ) is a fundamental task in natural language processing ( nlp ).', 'hypernymy in fact represents a key organization principle of semantic memory  #AUTHOR_TAG, the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment  #AUTHOR_TAG.', 'cohyponymy ( or coordination ), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on  #TAUTHOR_TAG.', ""for this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers'ability in such discrimination, generally achieving promising results  #TAUTHOR_TAG."", 'both supervised and unsupervised approaches have been investigated.', 'the former have been shown to outperform the latter in  #TAUTHOR_TAG, even though  #AUTHOR_TAG have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.', 'in this paper, we propose a supervised method, based on a random forest algorithm and 13 corpus - based features.', 'in our evaluation, carried out using the 10 - fold cross validation on 9, 600 pairs, we achieved an accuracy of 88. 3 % when the three classes are present, and of 92. 3 % and 97. 3 % when only two classes are present.', 'such results are competitive with the state - of - the - art  #TAUTHOR_TAG']",4
"[' #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is""]","[' #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is""]","['this paper, we have described root13, a classifier for hypernyms, co - hyponyms and random words.', 'the classifier, based on the random forest algorithm, uses only 13 unsupervised corpus - based features, which have been described and their contribution reported.', 'our classifier is competitive with the state - of - the - art  #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is""]","['this paper, we have described root13, a classifier for hypernyms, co - hyponyms and random words.', 'the classifier, based on the random forest algorithm, uses only 13 unsupervised corpus - based features, which have been described and their contribution reported.', 'our classifier is competitive with the state - of - the - art  #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is the classification of switched hypernyms as hypernyms ( e. g. dog - vehicle, car - animal )."", 'how - ever, we were able to remove it - without any sensible loss in accuracy - by training the model also on switched hypernyms labeled as randoms']",4
"['similar  #TAUTHOR_TAG.', 'the']","['attributionally similar  #TAUTHOR_TAG.', 'the']","['similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphr']","['hypernyms ( e. g. dog - animal ) from cohyponyms ( e. g. dog - cat ) and, in turn, discriminating them from random words ( e. g. dog - fruit ) is a fundamental task in natural language processing ( nlp ).', 'hypernymy in fact represents a key organization principle of semantic memory  #AUTHOR_TAG, the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment  #AUTHOR_TAG.', 'cohyponymy ( or coordination ), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar  #TAUTHOR_TAG.', 'the ability of discriminating hypernymy, co - hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on  #TAUTHOR_TAG.', ""for this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers'ability in such discrimination, generally achieving promising results  #TAUTHOR_TAG."", 'both supervised and unsupervised approaches have been investigated.', 'the former have been shown to outperform the latter in  #TAUTHOR_TAG, even though  #AUTHOR_TAG have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.', 'in this paper, we propose a supervised method, based on a random forest algorithm and 13 corpus - based features.', 'in our evaluation, carried out using the 10 - fold cross validation on 9, 600 pairs, we achieved an accuracy of 88. 3 % when the three classes are present, and of 92. 3 % and 97. 3 % when only two classes are present.', 'such results are competitive with the state - of - the - art  #TAUTHOR_TAG']",5
"[' #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is""]","[' #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is""]","['this paper, we have described root13, a classifier for hypernyms, co - hyponyms and random words.', 'the classifier, based on the random forest algorithm, uses only 13 unsupervised corpus - based features, which have been described and their contribution reported.', 'our classifier is competitive with the state - of - the - art  #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is""]","['this paper, we have described root13, a classifier for hypernyms, co - hyponyms and random words.', 'the classifier, based on the random forest algorithm, uses only 13 unsupervised corpus - based features, which have been described and their contribution reported.', 'our classifier is competitive with the state - of - the - art  #TAUTHOR_TAG.', ""in a second run of tests, we have noticed the  #AUTHOR_TAG's effect, that is the classification of switched hypernyms as hypernyms ( e. g. dog - vehicle, car - animal )."", 'how - ever, we were able to remove it - without any sensible loss in accuracy - by training the model also on switched hypernyms labeled as randoms']",5
['total variability subspace model  #TAUTHOR_TAG 11 ]'],['total variability subspace model  #TAUTHOR_TAG 11 ]'],['variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give'],"['a phonotactic vsm. in other cases, a phone n - gram language model is used to model the', 'phone statistics instead of a vsm [ 6, 7, 8 ]. on the other hand, acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, pros', '##ody, shifted delta ceptral coefficients, bottleneck features [ 9, 10 ]. one of the most successful acoustic approaches is, the use of i - vector framework for lid, where i - vectors are extracted for each speech utterance,', 'using an i - vector extractor that consists of a gmm - ubm trained on top of bnf, followed by a total variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give an acoustic vsm ( section 2. 2 ). these methods are also used for did. each of the two vsms is used as an input', 'to a back - end discriminative classifier, which is trained to find a suitable decision boundary in these vector spaces', '. this gives us two did systems built using the acoustic and phonotactic vsms. at prediction time, output scores from', '']",3
['dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],"['. this update information is encoded in a low dimensional latent vector known as an i - vector. the latent variable model used to extract i - vector is called total variability subspace', 'model and is given by the equation : where u is gmm - ubm mean supervector. v is the latent vector, known as the i', '− v ector and t is the lower dimensional vector subspace. the parameters of the model are estimated', 'using maximum likelihood training criterion. for a detailed explanation of i - vector', 'modeling framework, reader is directed to excellent work in [ 15, 11 ]. in this work, gmm - ubm model has 2048', 'gaussian components, mfcc features are extracted using a 25 ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG. finally, we construct the acoustic vsm', ', x a ∈ r n ×400, where the i th row is the 400 dimensional i - vector representation corresponding to the speech utterance, a i. we also', 'perform linear discriminant analysis ( lda ) and within class co - variance normalization ( wccn ) on the acoustic vector space, to increase the discriminative strength of the vsm. this method has been shown to', 'improve did ( lid ) performance  #TAUTHOR_TAG 11 ]. here, we give a brief overview of', 'the mathematical foundations of the cca. fig 2 gives a probabilistic graphical model of cca. nodes of the graph represent random variables ( rvs ) and the structure encodes conditional independence assumptions.', 'x p and x a are the random variables corresponding to the phonotactic and acoustic views of the data. each data view', 'is associated with two latent variables ; 1 ) z c, which is shared,', 'and is the variable of interest that will form the final combined vsm, z c and', '2 ) φ p and φ a, which are the subspaces associated with the phontactic and acoustic views, respectively', '. cca attempts to estimate φ p and φ a such', 'that the correlation between the projections of the phonotactic feature vectors, p, on φ p and acoustic feature vectors,', 'a, on φ a are maximized. hence, cca can be', 'posed as the following optimization problem [ 16 ]. the above optimization formualtion can be massaged into the following eigenvalue', 'problem. for details see [ 17 ]. an equivalent svd formulation of the above eigenvalue problem is given below, which allows us to find φ a and φ p by performing svd', 'of c. for the proof of this formulation, reader is referred to [ 16 ]. we use the above formulation in this paper to learn the latent vector subspaces, φ p and φ a']",3
['dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],"['. this update information is encoded in a low dimensional latent vector known as an i - vector. the latent variable model used to extract i - vector is called total variability subspace', 'model and is given by the equation : where u is gmm - ubm mean supervector. v is the latent vector, known as the i', '− v ector and t is the lower dimensional vector subspace. the parameters of the model are estimated', 'using maximum likelihood training criterion. for a detailed explanation of i - vector', 'modeling framework, reader is directed to excellent work in [ 15, 11 ]. in this work, gmm - ubm model has 2048', 'gaussian components, mfcc features are extracted using a 25 ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG. finally, we construct the acoustic vsm', ', x a ∈ r n ×400, where the i th row is the 400 dimensional i - vector representation corresponding to the speech utterance, a i. we also', 'perform linear discriminant analysis ( lda ) and within class co - variance normalization ( wccn ) on the acoustic vector space, to increase the discriminative strength of the vsm. this method has been shown to', 'improve did ( lid ) performance  #TAUTHOR_TAG 11 ]. here, we give a brief overview of', 'the mathematical foundations of the cca. fig 2 gives a probabilistic graphical model of cca. nodes of the graph represent random variables ( rvs ) and the structure encodes conditional independence assumptions.', 'x p and x a are the random variables corresponding to the phonotactic and acoustic views of the data. each data view', 'is associated with two latent variables ; 1 ) z c, which is shared,', 'and is the variable of interest that will form the final combined vsm, z c and', '2 ) φ p and φ a, which are the subspaces associated with the phontactic and acoustic views, respectively', '. cca attempts to estimate φ p and φ a such', 'that the correlation between the projections of the phonotactic feature vectors, p, on φ p and acoustic feature vectors,', 'a, on φ a are maximized. hence, cca can be', 'posed as the following optimization problem [ 16 ]. the above optimization formualtion can be massaged into the following eigenvalue', 'problem. for details see [ 17 ]. an equivalent svd formulation of the above eigenvalue problem is given below, which allows us to find φ a and φ p by performing svd', 'of c. for the proof of this formulation, reader is referred to [ 16 ]. we use the above formulation in this paper to learn the latent vector subspaces, φ p and φ a']",3
"['is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for']","['is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for']","['and test data used in this work is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for each dialect for training and testing.', 'train 13 10 11 9 10 test 2 2 2 2 2']","['and test data used in this work is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for each dialect for training and testing.', 'train 13 10 11 9 10 test 2 2 2 2 2 table 1. number of hours of training and testing data for each dialect table 2 shows the number of speech utterances that are available for training and testing the did system.', 'train 1720 1907 1059 1934 1820 test 315 348 238 355 265 table 2. number of training and test utterances for did system']",3
['total variability subspace model  #TAUTHOR_TAG 11 ]'],['total variability subspace model  #TAUTHOR_TAG 11 ]'],['variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give'],"['a phonotactic vsm. in other cases, a phone n - gram language model is used to model the', 'phone statistics instead of a vsm [ 6, 7, 8 ]. on the other hand, acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, pros', '##ody, shifted delta ceptral coefficients, bottleneck features [ 9, 10 ]. one of the most successful acoustic approaches is, the use of i - vector framework for lid, where i - vectors are extracted for each speech utterance,', 'using an i - vector extractor that consists of a gmm - ubm trained on top of bnf, followed by a total variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give an acoustic vsm ( section 2. 2 ). these methods are also used for did. each of the two vsms is used as an input', 'to a back - end discriminative classifier, which is trained to find a suitable decision boundary in these vector spaces', '. this gives us two did systems built using the acoustic and phonotactic vsms. at prediction time, output scores from', '']",4
['total variability subspace model  #TAUTHOR_TAG 11 ]'],['total variability subspace model  #TAUTHOR_TAG 11 ]'],['variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give'],"['a phonotactic vsm. in other cases, a phone n - gram language model is used to model the', 'phone statistics instead of a vsm [ 6, 7, 8 ]. on the other hand, acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, pros', '##ody, shifted delta ceptral coefficients, bottleneck features [ 9, 10 ]. one of the most successful acoustic approaches is, the use of i - vector framework for lid, where i - vectors are extracted for each speech utterance,', 'using an i - vector extractor that consists of a gmm - ubm trained on top of bnf, followed by a total variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give an acoustic vsm ( section 2. 2 ). these methods are also used for did. each of the two vsms is used as an input', 'to a back - end discriminative classifier, which is trained to find a suitable decision boundary in these vector spaces', '. this gives us two did systems built using the acoustic and phonotactic vsms. at prediction time, output scores from', '']",4
['total variability subspace model  #TAUTHOR_TAG 11 ]'],['total variability subspace model  #TAUTHOR_TAG 11 ]'],['variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give'],"['a phonotactic vsm. in other cases, a phone n - gram language model is used to model the', 'phone statistics instead of a vsm [ 6, 7, 8 ]. on the other hand, acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, pros', '##ody, shifted delta ceptral coefficients, bottleneck features [ 9, 10 ]. one of the most successful acoustic approaches is, the use of i - vector framework for lid, where i - vectors are extracted for each speech utterance,', 'using an i - vector extractor that consists of a gmm - ubm trained on top of bnf, followed by a total variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give an acoustic vsm ( section 2. 2 ). these methods are also used for did. each of the two vsms is used as an input', 'to a back - end discriminative classifier, which is trained to find a suitable decision boundary in these vector spaces', '. this gives us two did systems built using the acoustic and phonotactic vsms. at prediction time, output scores from', '']",6
['total variability subspace model  #TAUTHOR_TAG 11 ]'],['total variability subspace model  #TAUTHOR_TAG 11 ]'],['variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give'],"['a phonotactic vsm. in other cases, a phone n - gram language model is used to model the', 'phone statistics instead of a vsm [ 6, 7, 8 ]. on the other hand, acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, pros', '##ody, shifted delta ceptral coefficients, bottleneck features [ 9, 10 ]. one of the most successful acoustic approaches is, the use of i - vector framework for lid, where i - vectors are extracted for each speech utterance,', 'using an i - vector extractor that consists of a gmm - ubm trained on top of bnf, followed by a total variability subspace model  #TAUTHOR_TAG 11 ]. the extracted i - vectors give an acoustic vsm ( section 2. 2 ). these methods are also used for did. each of the two vsms is used as an input', 'to a back - end discriminative classifier, which is trained to find a suitable decision boundary in these vector spaces', '. this gives us two did systems built using the acoustic and phonotactic vsms. at prediction time, output scores from', '']",6
"['##r can be found in  #TAUTHOR_TAG.', 'vsm is constructed in two steps ; 1']","['', 'details about the phone recognizer can be found in  #TAUTHOR_TAG.', 'vsm is constructed in two steps ; 1 ) construct']","['##otactic vsm is constructed by modeling the n - gram phone statistics of the phone sequences that are extracted using an arabic phone recognizer.', 'details about the phone recognizer can be found in  #TAUTHOR_TAG.', 'vsm is constructed in two steps ; 1 ) construct']","['##otactic vsm is constructed by modeling the n - gram phone statistics of the phone sequences that are extracted using an arabic phone recognizer.', 'details about the phone recognizer can be found in  #TAUTHOR_TAG.', 'vsm is constructed in two steps ; 1 ) construct a term - document matrix, x ∈ r n ×d ( see fig 1 ), where each speech utterance in represented by a phonotactic feature vector,', ', where n is the number of speech utterances and f ( p, s ) is the number of times a phone n - gram ( term ) s appears in the utterance ( document ) p and 2 ) perform truncated singular value decomposition ( svd ) ( equation 2 ) on x to learn a lower dimensional linear manifold, π ∈ r d×k, where k < < d. svd attempts to discover the latent structure in the high dimensional feature space.', 'note that, k is the number of largest singular values.', 'x is projected down to π to get the phonotactic vsm, x p ( equation 2 ).', 'in our case, the n - gram dictionary consisted of phone 2 - grams and 3 - grams with a total of 8k features i. e. d = 8k.', 'the dimensionality of the vsm, x p, was chosen to be 1200, i. e. k = 1200.', '1200 was the optimal value chosen experimentally']",0
['dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],"['. this update information is encoded in a low dimensional latent vector known as an i - vector. the latent variable model used to extract i - vector is called total variability subspace', 'model and is given by the equation : where u is gmm - ubm mean supervector. v is the latent vector, known as the i', '− v ector and t is the lower dimensional vector subspace. the parameters of the model are estimated', 'using maximum likelihood training criterion. for a detailed explanation of i - vector', 'modeling framework, reader is directed to excellent work in [ 15, 11 ]. in this work, gmm - ubm model has 2048', 'gaussian components, mfcc features are extracted using a 25 ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG. finally, we construct the acoustic vsm', ', x a ∈ r n ×400, where the i th row is the 400 dimensional i - vector representation corresponding to the speech utterance, a i. we also', 'perform linear discriminant analysis ( lda ) and within class co - variance normalization ( wccn ) on the acoustic vector space, to increase the discriminative strength of the vsm. this method has been shown to', 'improve did ( lid ) performance  #TAUTHOR_TAG 11 ]. here, we give a brief overview of', 'the mathematical foundations of the cca. fig 2 gives a probabilistic graphical model of cca. nodes of the graph represent random variables ( rvs ) and the structure encodes conditional independence assumptions.', 'x p and x a are the random variables corresponding to the phonotactic and acoustic views of the data. each data view', 'is associated with two latent variables ; 1 ) z c, which is shared,', 'and is the variable of interest that will form the final combined vsm, z c and', '2 ) φ p and φ a, which are the subspaces associated with the phontactic and acoustic views, respectively', '. cca attempts to estimate φ p and φ a such', 'that the correlation between the projections of the phonotactic feature vectors, p, on φ p and acoustic feature vectors,', 'a, on φ a are maximized. hence, cca can be', 'posed as the following optimization problem [ 16 ]. the above optimization formualtion can be massaged into the following eigenvalue', 'problem. for details see [ 17 ]. an equivalent svd formulation of the above eigenvalue problem is given below, which allows us to find φ a and φ p by performing svd', 'of c. for the proof of this formulation, reader is referred to [ 16 ]. we use the above formulation in this paper to learn the latent vector subspaces, φ p and φ a']",0
"['##er, a crowd source platform, by qcri and is publicly available on their web portal 1.', 'more details about the train and test data can be found in  #TAUTHOR_TAG 18 ].', 'fig 3 gives an']","['set, the recording are of high quality.', 'the test set is labeled using crowdflower, a crowd source platform, by qcri and is publicly available on their web portal 1.', 'more details about the train and test data can be found in  #TAUTHOR_TAG 18 ].', 'fig 3 gives an']","['##er, a crowd source platform, by qcri and is publicly available on their web portal 1.', 'more details about the train and test data can be found in  #TAUTHOR_TAG 18 ].', 'fig 3 gives an overview of our did system, which can be seen as a combination of two broad components ; 1 ) vector space modeling component and 2 ) back - end classifier']","['data consist of recording from the arabic broadcast domain and contains utterances spoken in all the five dialects ; egy, glf, lav, msa and nor.', 'the test set is from the same broadcast domain but is collected from al - jazeera and hence, unlike training data set, the recording are of high quality.', 'the test set is labeled using crowdflower, a crowd source platform, by qcri and is publicly available on their web portal 1.', 'more details about the train and test data can be found in  #TAUTHOR_TAG 18 ].', 'fig 3 gives an overview of our did system, which can be seen as a combination of two broad components ; 1 ) vector space modeling component and 2 ) back - end classifier']",0
['dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],"['. this update information is encoded in a low dimensional latent vector known as an i - vector. the latent variable model used to extract i - vector is called total variability subspace', 'model and is given by the equation : where u is gmm - ubm mean supervector. v is the latent vector, known as the i', '− v ector and t is the lower dimensional vector subspace. the parameters of the model are estimated', 'using maximum likelihood training criterion. for a detailed explanation of i - vector', 'modeling framework, reader is directed to excellent work in [ 15, 11 ]. in this work, gmm - ubm model has 2048', 'gaussian components, mfcc features are extracted using a 25 ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG. finally, we construct the acoustic vsm', ', x a ∈ r n ×400, where the i th row is the 400 dimensional i - vector representation corresponding to the speech utterance, a i. we also', 'perform linear discriminant analysis ( lda ) and within class co - variance normalization ( wccn ) on the acoustic vector space, to increase the discriminative strength of the vsm. this method has been shown to', 'improve did ( lid ) performance  #TAUTHOR_TAG 11 ]. here, we give a brief overview of', 'the mathematical foundations of the cca. fig 2 gives a probabilistic graphical model of cca. nodes of the graph represent random variables ( rvs ) and the structure encodes conditional independence assumptions.', 'x p and x a are the random variables corresponding to the phonotactic and acoustic views of the data. each data view', 'is associated with two latent variables ; 1 ) z c, which is shared,', 'and is the variable of interest that will form the final combined vsm, z c and', '2 ) φ p and φ a, which are the subspaces associated with the phontactic and acoustic views, respectively', '. cca attempts to estimate φ p and φ a such', 'that the correlation between the projections of the phonotactic feature vectors, p, on φ p and acoustic feature vectors,', 'a, on φ a are maximized. hence, cca can be', 'posed as the following optimization problem [ 16 ]. the above optimization formualtion can be massaged into the following eigenvalue', 'problem. for details see [ 17 ]. an equivalent svd formulation of the above eigenvalue problem is given below, which allows us to find φ a and φ p by performing svd', 'of c. for the proof of this formulation, reader is referred to [ 16 ]. we use the above formulation in this paper to learn the latent vector subspaces, φ p and φ a']",5
['dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],['ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG.'],"['. this update information is encoded in a low dimensional latent vector known as an i - vector. the latent variable model used to extract i - vector is called total variability subspace', 'model and is given by the equation : where u is gmm - ubm mean supervector. v is the latent vector, known as the i', '− v ector and t is the lower dimensional vector subspace. the parameters of the model are estimated', 'using maximum likelihood training criterion. for a detailed explanation of i - vector', 'modeling framework, reader is directed to excellent work in [ 15, 11 ]. in this work, gmm - ubm model has 2048', 'gaussian components, mfcc features are extracted using a 25 ms window and the i - vectors are 400 dimensional  #TAUTHOR_TAG. finally, we construct the acoustic vsm', ', x a ∈ r n ×400, where the i th row is the 400 dimensional i - vector representation corresponding to the speech utterance, a i. we also', 'perform linear discriminant analysis ( lda ) and within class co - variance normalization ( wccn ) on the acoustic vector space, to increase the discriminative strength of the vsm. this method has been shown to', 'improve did ( lid ) performance  #TAUTHOR_TAG 11 ]. here, we give a brief overview of', 'the mathematical foundations of the cca. fig 2 gives a probabilistic graphical model of cca. nodes of the graph represent random variables ( rvs ) and the structure encodes conditional independence assumptions.', 'x p and x a are the random variables corresponding to the phonotactic and acoustic views of the data. each data view', 'is associated with two latent variables ; 1 ) z c, which is shared,', 'and is the variable of interest that will form the final combined vsm, z c and', '2 ) φ p and φ a, which are the subspaces associated with the phontactic and acoustic views, respectively', '. cca attempts to estimate φ p and φ a such', 'that the correlation between the projections of the phonotactic feature vectors, p, on φ p and acoustic feature vectors,', 'a, on φ a are maximized. hence, cca can be', 'posed as the following optimization problem [ 16 ]. the above optimization formualtion can be massaged into the following eigenvalue', 'problem. for details see [ 17 ]. an equivalent svd formulation of the above eigenvalue problem is given below, which allows us to find φ a and φ p by performing svd', 'of c. for the proof of this formulation, reader is referred to [ 16 ]. we use the above formulation in this paper to learn the latent vector subspaces, φ p and φ a']",5
"['is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for']","['is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for']","['and test data used in this work is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for each dialect for training and testing.', 'train 13 10 11 9 10 test 2 2 2 2 2']","['and test data used in this work is the same as used in  #TAUTHOR_TAG.', 'table 1 gives the number of hours of data available for each dialect for training and testing.', 'train 13 10 11 9 10 test 2 2 2 2 2 table 1. number of hours of training and testing data for each dialect table 2 shows the number of speech utterances that are available for training and testing the did system.', 'train 1720 1907 1059 1934 1820 test 315 348 238 355 265 table 2. number of training and test utterances for did system']",5
"['11, 12,  #TAUTHOR_TAG, which considers syntactic contexts']","['side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12,  #TAUTHOR_TAG, which considers syntactic contexts']","['11, 12,  #TAUTHOR_TAG, which considers syntactic contexts']","['', 'learning knowledge from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12,  #TAUTHOR_TAG, which considers syntactic contexts']",0
"['11, 12,  #TAUTHOR_TAG,']","['embedding [ 11, 12,  #TAUTHOR_TAG,']","['11, 12,  #TAUTHOR_TAG,']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12,  #TAUTHOR_TAG, which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', '']",0
"['11, 12,  #TAUTHOR_TAG,']","['embedding [ 11, 12,  #TAUTHOR_TAG,']","['11, 12,  #TAUTHOR_TAG,']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12,  #TAUTHOR_TAG, which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', '']",5
"['issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['stem from issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['of question - answering based on real - world images that resemblances the famous turing test [ 28, 29 ] and deviates from other attempts [ 30, 31, 32 ] - and discuss challenges together with tools to', 'benchmark different models on such task. we typically measure the progress in the field by quantifying the performance of different methods against a carefully crafted set of benchmarks. crowdsourcing in combination of machine learning approaches have served us well to generate curated datasets with a unique ground truth at scale [ 33, 34 ]. as the complexity and the openness of the', 'task grows, the quest of crafting good benchmarks also becomes more difficult. first, interpreting and evaluating the answer of a system becomes increasingly difficult and ideally would rely on human judgement. yet we want to have objective metrics that we can evaluate automatically at large scale. second, establishing an evaluation methodology that assigns scores over a large output', 'domain is challenging, as any system based on ontologies will have limited coverage. third, if our aim is to mimic human response', ', we have to deal with inherent ambiguities due to human judgement that stem from issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']",0
"['on how ambiguous and latent notions of reference frames and intentions are understood  #TAUTHOR_TAG, 44 ].', 'depending on the cultural bias and the context, we may use object - centric or observer - centric or even world - centric frames of reference [ 45 ].', 'moreover, it is even unclear']","['on how ambiguous and latent notions of reference frames and intentions are understood  #TAUTHOR_TAG, 44 ].', 'depending on the cultural bias and the context, we may use object - centric or observer - centric or even world - centric frames of reference [ 45 ].', 'moreover, it is even unclear']","['on how ambiguous and latent notions of reference frames and intentions are understood  #TAUTHOR_TAG, 44 ].', 'depending on the cultural bias and the context, we may use object - centric or observer - centric or even world - centric frames of reference [ 45 ].', 'moreover, it is even unclear']","['', 'attributes : the human concepts are not limited to object categories, but also include attributes such as genders, colors, states ( lights can be either on or off ).', 'often these concepts cannot be learned on their own, but rather are contextualized by the associated noun.', 'e. g. white in "" white "" elephant is surly different from "" white "" in white snow.', 'ambiguity in reference resolution : reliably answering on questions is challenging even for humans.', 'the quality of an answer depends on how ambiguous and latent notions of reference frames and intentions are understood  #TAUTHOR_TAG, 44 ].', 'depending on the cultural bias and the context, we may use object - centric or observer - centric or even world - centric frames of reference [ 45 ].', ""moreover, it is even unclear what'with ','beneath ','over'mean."", 'it seems at least difficult to symbolically define them in terms of predicates.', 'while holistic learning and inference encompassing all the aforementioned aspects has yet to be shown, current research directions show promise [ 46, 47, 48 ] by adapting the symbolic - based approaches [ 10, 11, 23, 24 ] with vector - based approaches [ 12, 19, 25 ] to represent the meaning.', 'common sense knowledge it turns out']",0
"['is a challenging, large dataset']","['is a challenging, large dataset']","['is a challenging, large dataset']","['is a challenging, large dataset for a question answering task based on real - world images.', 'the images present real - world indoor scenes [ 50 ], while the questions are unconstrained natural language sentences.', ' #TAUTHOR_TAG language scope is beyond the nouns or tuples that are typical to recognition datasets [ 51, 52, 53 ].', 'other, linguistically rich datasets either do not tackle images at all [ 54, 55 ] or consider only few in very constrained domain [ 15 ], or are more suitable for the learning an embedding / image - sentence retrieval or language generation [ 22, 56, 57, 58 ].', 'in this section we discuss in isolation different challenges reflected in  #TAUTHOR_TAG']",0
"['is a challenging, large dataset']","['is a challenging, large dataset']","['is a challenging, large dataset']","['is a challenging, large dataset for a question answering task based on real - world images.', 'the images present real - world indoor scenes [ 50 ], while the questions are unconstrained natural language sentences.', ' #TAUTHOR_TAG language scope is beyond the nouns or tuples that are typical to recognition datasets [ 51, 52, 53 ].', 'other, linguistically rich datasets either do not tackle images at all [ 54, 55 ] or consider only few in very constrained domain [ 15 ], or are more suitable for the learning an embedding / image - sentence retrieval or language generation [ 22, 56, 57, 58 ].', 'in this section we discuss in isolation different challenges reflected in  #TAUTHOR_TAG']",0
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",0
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",0
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",0
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",0
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",0
"['authors [ 23, 24,  #TAUTHOR_TAG treat the ground']","['authors [ 23, 24,  #TAUTHOR_TAG treat the grounding ( understood here as the logical representation of']","['question answering task is also about understanding hidden intentions of the questioner with grounding as a sub - goal to solve.', 'some authors [ 23, 24,  #TAUTHOR_TAG treat the grounding ( understood here as the logical representation of the meaning of the question ) as a latent variable in the question answering task.', 'others [ 44 ] have modeled the pragmatic effects in the question answering task, but such approaches have never been shown to work in less constrained environments']","['question answering task is also about understanding hidden intentions of the questioner with grounding as a sub - goal to solve.', 'some authors [ 23, 24,  #TAUTHOR_TAG treat the grounding ( understood here as the logical representation of the meaning of the question ) as a latent variable in the question answering task.', 'others [ 44 ] have modeled the pragmatic effects in the question answering task, but such approaches have never been shown to work in less constrained environments']",0
"['issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['stem from issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['of question - answering based on real - world images that resemblances the famous turing test [ 28, 29 ] and deviates from other attempts [ 30, 31, 32 ] - and discuss challenges together with tools to', 'benchmark different models on such task. we typically measure the progress in the field by quantifying the performance of different methods against a carefully crafted set of benchmarks. crowdsourcing in combination of machine learning approaches have served us well to generate curated datasets with a unique ground truth at scale [ 33, 34 ]. as the complexity and the openness of the', 'task grows, the quest of crafting good benchmarks also becomes more difficult. first, interpreting and evaluating the answer of a system becomes increasingly difficult and ideally would rely on human judgement. yet we want to have objective metrics that we can evaluate automatically at large scale. second, establishing an evaluation methodology that assigns scores over a large output', 'domain is challenging, as any system based on ontologies will have limited coverage. third, if our aim is to mimic human response', ', we have to deal with inherent ambiguities due to human judgement that stem from issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']",1
"['is a challenging, large dataset']","['is a challenging, large dataset']","['is a challenging, large dataset']","['is a challenging, large dataset for a question answering task based on real - world images.', 'the images present real - world indoor scenes [ 50 ], while the questions are unconstrained natural language sentences.', ' #TAUTHOR_TAG language scope is beyond the nouns or tuples that are typical to recognition datasets [ 51, 52, 53 ].', 'other, linguistically rich datasets either do not tackle images at all [ 54, 55 ] or consider only few in very constrained domain [ 15 ], or are more suitable for the learning an embedding / image - sentence retrieval or language generation [ 22, 56, 57, 58 ].', 'in this section we discuss in isolation different challenges reflected in  #TAUTHOR_TAG']",1
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",1
['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about'],"['machine world in  #TAUTHOR_TAG is represented as a set of images and questions about their content.', ' #TAUTHOR_TAG contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether ( we use the stanford pos tagger [ 59 ] to extract the nouns from the questions ).', 'if we consider only nouns in singular form in the questions, we still have 573 categories.', 'the current state - of - the - art semantic segmentation methods on the nyu - depth v2 dataset [ 50 ] can discriminate only between up to 37 object categories [ 2, 60, 61 ], much fewer to what is needed.', ' #TAUTHOR_TAG.', '']",1
"['issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['stem from issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']","['of question - answering based on real - world images that resemblances the famous turing test [ 28, 29 ] and deviates from other attempts [ 30, 31, 32 ] - and discuss challenges together with tools to', 'benchmark different models on such task. we typically measure the progress in the field by quantifying the performance of different methods against a carefully crafted set of benchmarks. crowdsourcing in combination of machine learning approaches have served us well to generate curated datasets with a unique ground truth at scale [ 33, 34 ]. as the complexity and the openness of the', 'task grows, the quest of crafting good benchmarks also becomes more difficult. first, interpreting and evaluating the answer of a system becomes increasingly difficult and ideally would rely on human judgement. yet we want to have objective metrics that we can evaluate automatically at large scale. second, establishing an evaluation methodology that assigns scores over a large output', 'domain is challenging, as any system based on ontologies will have limited coverage. third, if our aim is to mimic human response', ', we have to deal with inherent ambiguities due to human judgement that stem from issues like binding, reference frames, social conventions. for instance  #TAUTHOR_TAG reports that', '']",5
"['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft""]","['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft'generalization of accuracy that takes ambiguities of different concepts into account via the set membership measure [UNK] :"", 'where for']","['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft'generalization of accuracy that takes ambig""]","['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft'generalization of accuracy that takes ambiguities of different concepts into account via the set membership measure [UNK] :"", 'where for each i - th question, a i and t i are the answers produced by the architecture and human respectively, and they are represented as bags of words.', 'the authors of  #TAUTHOR_TAG have proposed using wup similarity [ 62 ] as the membership measure [UNK] in the wups score.', 'such choice of [UNK] suffers from the aforementioned coverage problem and the whole metric takes only one human interpretation of the question into account.', '']",5
"['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft""]","['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft'generalization of accuracy that takes ambiguities of different concepts into account via the set membership measure [UNK] :"", 'where for']","['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft'generalization of accuracy that takes ambig""]","['exemplify the aforementioned requirements by illustrating the wups scorean automatic metric that quantifies performance of the holistic architectures proposed by  #TAUTHOR_TAG.', ""this metric is motivated by the development of a'soft'generalization of accuracy that takes ambiguities of different concepts into account via the set membership measure [UNK] :"", 'where for each i - th question, a i and t i are the answers produced by the architecture and human respectively, and they are represented as bags of words.', 'the authors of  #TAUTHOR_TAG have proposed using wup similarity [ 62 ] as the membership measure [UNK] in the wups score.', 'such choice of [UNK] suffers from the aforementioned coverage problem and the whole metric takes only one human interpretation of the question into account.', '']",2
"['manifested in  #TAUTHOR_TAG.', 'to judge competing architectures']","['manifested in  #TAUTHOR_TAG.', 'to judge competing architectures']","['in  #TAUTHOR_TAG.', 'to judge competing architectures']","['goal of this contribution is to sparkle the discussions about benchmarking holistic architectures on complex and more open tasks.', 'we identify particular challenges that holistic tasks should exhibit and exemplify how they are manifested in  #TAUTHOR_TAG.', 'to judge competing architectures and measure the progress on the task, we suggest several directions to further improve existing metrics, and discuss different experimental scenarios']",2
['used in machine translation described in  #TAUTHOR_TAG'],['used in machine translation described in  #TAUTHOR_TAG'],['used in machine translation described in  #TAUTHOR_TAG. we believe that the 2'],"['', ', we investigate the use of two - dimensional lstm ( 2dlstm ) [ 11, 12 ] in sequence - to - sequence modeling as an alternative model for the attention component. in this architecture, we apply a', '2dlstm on top of a deep bidirectional encoder to relate input and output representations in a 2d space. one dimension of the 2dlstm processes the input sequence,', 'and another dimension predicts the output ( sub ) words. in contrast to the attention - based sequence - tosequence model, where the encoder states are not updated and the model is not able to re -', 'interpret the encoder states while decoding, this model enables the computation of the encoding of the observation sequence as a function of the previously generated transcribed words.', 'our model is similar to an architecture used in machine translation described in  #TAUTHOR_TAG. we believe that the 2dlstm is', 'able to capture necessary monotonic alignments as well as retrieve coverage concepts internally by its cell states. experimental results on the 300h - switchboard task show competitive performance compared to an attentionbased sequence - to - sequence system']",3
['marked in blue  #TAUTHOR_TAG'],['marked in blue  #TAUTHOR_TAG'],"['##stm.', 'the additional connections are marked in blue  #TAUTHOR_TAG.', 'matic extraction of features from raw 2']","['way of building multidimensional context into recurrent networks is provided by a strategy that is based on networks with tree - structured update graphs.', 'in handwriting recognition ( hwr ), 2dlstm has shown successful results in auto - fig. 1 : the internal architecture of the standard and the 2dlstm.', 'the additional connections are marked in blue  #TAUTHOR_TAG.', 'matic extraction of features from raw 2d - images over convolutional neural networks ( cnns ) [ 14 ].', 'in order to investigate deeper and larger models using 2dlstm, an algorithm to use the gpu power has been implemented [ 15 ].', 'different neural networks have been proposed in automatic speech recognition ( asr ) to model 2d correlations in the input signal.', 'one of them is a 2dlstm layer which scans the input over both time and frequency jointly for spatiotemporal modeling and aggregates more variations [ 16 ].', 'moreover, various architectures to model time - frequency patterns based on deep dnn, cnn, rnn and 2dlstm layers are compared for large vocabulary asr [ 17 ].', 'as an alternative method to the concept of the 2dlstm, a network of one - dimensional lstm cells arranged in a multidimensional grid has been introduced [ 18 ].', 'in this topology, the lstm cells communicate not only along time sequence but also between the layers.', 'the grid lstm network is also applied for the endpoint detection task in asr to model both spectral and temporal variations [ 19 ].', 'a 2d attention matrix is also applied in a neural pitch accent recognition model [ 20 ], in which graphemes are encoded in one dimension and audio frames are encoded in the other.', 'recently, the 2dlstm layer also has been used for sequence - to - sequence modeling in machine translation  #TAUTHOR_TAG where it implicitly updates the source representation conditioned on the generated target words.', 'in a similar direction, a 2d cnn - based network has been proposed where the positions of the source and the target words define the 2d grid for translation modeling [ 21 ].', 'similar to  #TAUTHOR_TAG, we apply a 2dlstm layer to combine the acoustic model ( the lstm encoder ) and the language model ( the decoder ) without any attention components.', 'the 2dl - stm reconciles the context from both the input and the output sequences and re - interprets the encoder states while a new word has been predicted.', 'compared to  #TAUTHOR_TAG, our model is much deeper.', 'we use max - pooling to select the most relevant encoder state whereas  #TAUTHOR_TAG uses the last horizontal state of the 2dlstm.', 'furthermore, we']",3
"['', 'similar to  #TAUTHOR_TAG,']","['', 'similar to  #TAUTHOR_TAG,']","['a reduction factor.', 'similar to  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],3
['marked in blue  #TAUTHOR_TAG'],['marked in blue  #TAUTHOR_TAG'],"['##stm.', 'the additional connections are marked in blue  #TAUTHOR_TAG.', 'matic extraction of features from raw 2']","['way of building multidimensional context into recurrent networks is provided by a strategy that is based on networks with tree - structured update graphs.', 'in handwriting recognition ( hwr ), 2dlstm has shown successful results in auto - fig. 1 : the internal architecture of the standard and the 2dlstm.', 'the additional connections are marked in blue  #TAUTHOR_TAG.', 'matic extraction of features from raw 2d - images over convolutional neural networks ( cnns ) [ 14 ].', 'in order to investigate deeper and larger models using 2dlstm, an algorithm to use the gpu power has been implemented [ 15 ].', 'different neural networks have been proposed in automatic speech recognition ( asr ) to model 2d correlations in the input signal.', 'one of them is a 2dlstm layer which scans the input over both time and frequency jointly for spatiotemporal modeling and aggregates more variations [ 16 ].', 'moreover, various architectures to model time - frequency patterns based on deep dnn, cnn, rnn and 2dlstm layers are compared for large vocabulary asr [ 17 ].', 'as an alternative method to the concept of the 2dlstm, a network of one - dimensional lstm cells arranged in a multidimensional grid has been introduced [ 18 ].', 'in this topology, the lstm cells communicate not only along time sequence but also between the layers.', 'the grid lstm network is also applied for the endpoint detection task in asr to model both spectral and temporal variations [ 19 ].', 'a 2d attention matrix is also applied in a neural pitch accent recognition model [ 20 ], in which graphemes are encoded in one dimension and audio frames are encoded in the other.', 'recently, the 2dlstm layer also has been used for sequence - to - sequence modeling in machine translation  #TAUTHOR_TAG where it implicitly updates the source representation conditioned on the generated target words.', 'in a similar direction, a 2d cnn - based network has been proposed where the positions of the source and the target words define the 2d grid for translation modeling [ 21 ].', 'similar to  #TAUTHOR_TAG, we apply a 2dlstm layer to combine the acoustic model ( the lstm encoder ) and the language model ( the decoder ) without any attention components.', 'the 2dl - stm reconciles the context from both the input and the output sequences and re - interprets the encoder states while a new word has been predicted.', 'compared to  #TAUTHOR_TAG, our model is much deeper.', 'we use max - pooling to select the most relevant encoder state whereas  #TAUTHOR_TAG uses the last horizontal state of the 2dlstm.', 'furthermore, we']",0
['marked in blue  #TAUTHOR_TAG'],['marked in blue  #TAUTHOR_TAG'],"['##stm.', 'the additional connections are marked in blue  #TAUTHOR_TAG.', 'matic extraction of features from raw 2']","['way of building multidimensional context into recurrent networks is provided by a strategy that is based on networks with tree - structured update graphs.', 'in handwriting recognition ( hwr ), 2dlstm has shown successful results in auto - fig. 1 : the internal architecture of the standard and the 2dlstm.', 'the additional connections are marked in blue  #TAUTHOR_TAG.', 'matic extraction of features from raw 2d - images over convolutional neural networks ( cnns ) [ 14 ].', 'in order to investigate deeper and larger models using 2dlstm, an algorithm to use the gpu power has been implemented [ 15 ].', 'different neural networks have been proposed in automatic speech recognition ( asr ) to model 2d correlations in the input signal.', 'one of them is a 2dlstm layer which scans the input over both time and frequency jointly for spatiotemporal modeling and aggregates more variations [ 16 ].', 'moreover, various architectures to model time - frequency patterns based on deep dnn, cnn, rnn and 2dlstm layers are compared for large vocabulary asr [ 17 ].', 'as an alternative method to the concept of the 2dlstm, a network of one - dimensional lstm cells arranged in a multidimensional grid has been introduced [ 18 ].', 'in this topology, the lstm cells communicate not only along time sequence but also between the layers.', 'the grid lstm network is also applied for the endpoint detection task in asr to model both spectral and temporal variations [ 19 ].', 'a 2d attention matrix is also applied in a neural pitch accent recognition model [ 20 ], in which graphemes are encoded in one dimension and audio frames are encoded in the other.', 'recently, the 2dlstm layer also has been used for sequence - to - sequence modeling in machine translation  #TAUTHOR_TAG where it implicitly updates the source representation conditioned on the generated target words.', 'in a similar direction, a 2d cnn - based network has been proposed where the positions of the source and the target words define the 2d grid for translation modeling [ 21 ].', 'similar to  #TAUTHOR_TAG, we apply a 2dlstm layer to combine the acoustic model ( the lstm encoder ) and the language model ( the decoder ) without any attention components.', 'the 2dl - stm reconciles the context from both the input and the output sequences and re - interprets the encoder states while a new word has been predicted.', 'compared to  #TAUTHOR_TAG, our model is much deeper.', 'we use max - pooling to select the most relevant encoder state whereas  #TAUTHOR_TAG uses the last horizontal state of the 2dlstm.', 'furthermore, we']",0
"['analogously to the other gates  #TAUTHOR_TAG 11 ].', '']","['analogously to the other gates  #TAUTHOR_TAG 11 ].', '']","['activation is computed analogously to the other gates  #TAUTHOR_TAG 11 ].', ""the internal cell state c t, n, is computed based on the sum of the two previous cell's states c t−1, n and c t, n−1, weighted""]","['2dlstm is characterized as a general form of the standard lstm [ 11, 22 ].', 'it has been proposed to process inherent 2d data of arbitrary lengths, t and n.', 'therefore, it uses both horizontal and vertical recurrences.', 'the building block of both the lstm and the 2dlstm are shown in figure 1.', 'at time step ( t, n ), it gets an input x t, n, and its computation relies on both the vertical s t, n−1 and the horizontal hidden states s t−1, n.', 'besides the input i t, n, the forget f t, n and the output o t, n gates that are similar to those in the lstm, the 2dlstm employs an additional lambda gate.', 'as written in equation 5, its activation is computed analogously to the other gates  #TAUTHOR_TAG 11 ].', ""the internal cell state c t, n, is computed based on the sum of the two previous cell's states c t−1, n and c t, n−1, weighted by the lambda gate λ t, n and its complement ( see equation 6 )."", '']",0
"['local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regular']","['local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regularities']","['the local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regularities']","['theories of discourse coherence  #AUTHOR_TAG have been applied successfully in discourse analysis  #AUTHOR_TAG and discourse generation ( scott and de  #AUTHOR_TAG.', 'most of these efforts, however, have limited applicability.', 'those that use manually written rules model only the most visible discourse constraints ( e. g., the discourse connective "" although "" marks a concession relation ), while being oblivious to fine - grained lexical indicators.', 'and the methods that utilize manually annotated corpora  #AUTHOR_TAG and supervised learning algorithms have high costs associated with the annotation procedure, and cannot be easily adapted to different domains and genres.', 'in contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regularities recognized by classic discourse theories  #AUTHOR_TAG.', 'these stochastic coherence models use simple, non - hierarchical representations of discourse, and can be trained with minimal human intervention, using large collections of existing human - authored documents.', 'these models are attractive due to their increased scalability and portability.', 'as each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators.', 'a frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi - document summarization : given discourse units, what is the most coherent ordering of them  #TAUTHOR_TAG? because the problem is np - complete  #AUTHOR_TAG, it is critical how coherence model evaluation is intertwined with search : if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another.', 'if the search is exhaustive, the ordering procedure may take too long to be useful.', 'in this paper, we propose an a ¡ search algorithm for the discourse ordering problem that comes with strong theoretical guarantees.', 'for a wide range of practical problems ( discourse ordering of up to 15 units ), the algorithm finds an optimal solution in reasonable time ( on the order of seconds ).', 'a beam search version of the algorithm enables one to find good, approximate solutions for very large reordering tasks.', 'these algorithms enable us not only to compare head - to - head, for the first time, a set of coherence models, but also to combine these models so as to benefit from their complementary strengths.', 'the model com - bination is accomplished using statistically wellfounded utility training procedures which automatically optimize the contributions of the individual models on a development corpus.', 'we empirically show that utility - based models of discourse coherence outperform each of the individual coherence models']",0
"[""of the most frequently used metrics for the automatic evaluation of document coherence is kendall's  #TAUTHOR_TAG."", 'tau measures the minimum number of adjacent transpositions needed to transform a proposed order']","[""of the most frequently used metrics for the automatic evaluation of document coherence is kendall's  #TAUTHOR_TAG."", 'tau measures the minimum number of adjacent transpositions needed to transform a proposed order']","[""of the most frequently used metrics for the automatic evaluation of document coherence is kendall's  #TAUTHOR_TAG."", 'tau measures the minimum number of adjacent transpositions needed to transform a proposed order']","[""of the most frequently used metrics for the automatic evaluation of document coherence is kendall's  #TAUTHOR_TAG."", 'tau measures the minimum number of adjacent transpositions needed to transform a proposed order into a reference order.', 'the range of the tau metric is between - 1 ( the worst ) to 1 ( the best )']",0
"['of  #TAUTHOR_TAG, while ensuring']","['of  #TAUTHOR_TAG, while ensuring']","['of  #TAUTHOR_TAG, while ensuring optimal solutions, is prohibitively expensive,']","['presented a generic framework that is capable of integrating various stochastic models of discourse coherence into a more powerful model that combines the strengths of the individual models.', 'an important ingredient of this framework are the search algorithms based on idl - expressions, which provide a flexible way of solving discourse generation problems using stochastic models.', 'our generation algorithms are fundamentally different from previously - proposed algorithms for discourse generation.', 'the genetic algorithms of  #AUTHOR_TAG and  #AUTHOR_TAG, as well as the greedy algorithm of  #AUTHOR_TAG, provide no theoretical guarantees on the optimality of the solutions they propose.', 'at the other end of the spectrum, the exhaustive search of  #TAUTHOR_TAG, while ensuring optimal solutions, is prohibitively expensive, and cannot be used to perform utility - based training.', 'the linear programming algorithm of  #AUTHOR_TAG is the only proposal that achieves both good speed and accuracy.', 'their algorithm, however, cannot handle models with hidden states, cannot compute "" - best lists, and does not have the representation flexibility provided by idl - expressions, which is crucial for coherence decoding in realistic applications such as multidocument summarization.', 'for each of the coherence model combinations that we have utility trained, we obtained improved results on the discourse ordering problem compared to the individual models.', 'this is important for two reasons.', 'our improvements can have an immediate impact on multi - document summarization applications  #AUTHOR_TAG.', '']",0
"['local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regular']","['local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regularities']","['the local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regularities']","['theories of discourse coherence  #AUTHOR_TAG have been applied successfully in discourse analysis  #AUTHOR_TAG and discourse generation ( scott and de  #AUTHOR_TAG.', 'most of these efforts, however, have limited applicability.', 'those that use manually written rules model only the most visible discourse constraints ( e. g., the discourse connective "" although "" marks a concession relation ), while being oblivious to fine - grained lexical indicators.', 'and the methods that utilize manually annotated corpora  #AUTHOR_TAG and supervised learning algorithms have high costs associated with the annotation procedure, and cannot be easily adapted to different domains and genres.', 'in contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical  #AUTHOR_TAG and global levels  #TAUTHOR_TAG, while preserving regularities recognized by classic discourse theories  #AUTHOR_TAG.', 'these stochastic coherence models use simple, non - hierarchical representations of discourse, and can be trained with minimal human intervention, using large collections of existing human - authored documents.', 'these models are attractive due to their increased scalability and portability.', 'as each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators.', 'a frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi - document summarization : given discourse units, what is the most coherent ordering of them  #TAUTHOR_TAG? because the problem is np - complete  #AUTHOR_TAG, it is critical how coherence model evaluation is intertwined with search : if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another.', 'if the search is exhaustive, the ordering procedure may take too long to be useful.', 'in this paper, we propose an a ¡ search algorithm for the discourse ordering problem that comes with strong theoretical guarantees.', 'for a wide range of practical problems ( discourse ordering of up to 15 units ), the algorithm finds an optimal solution in reasonable time ( on the order of seconds ).', 'a beam search version of the algorithm enables one to find good, approximate solutions for very large reordering tasks.', 'these algorithms enable us not only to compare head - to - head, for the first time, a set of coherence models, but also to combine these models so as to benefit from their complementary strengths.', 'the model com - bination is accomplished using statistically wellfounded utility training procedures which automatically optimize the contributions of the individual models on a development corpus.', 'we empirically show that utility - based models of discourse coherence outperform each of the individual coherence models']",1
"['our evaluation is information ordering  #TAUTHOR_TAG.', 'in']","['our evaluation is information ordering  #TAUTHOR_TAG.', 'in']","['task on which we conduct our evaluation is information ordering  #TAUTHOR_TAG.', 'in']","['task on which we conduct our evaluation is information ordering  #TAUTHOR_TAG.', 'in this task, a pre - selected set of information - bearing document units ( in our case, sentences ) needs to be arranged in a sequence which maximizes some specific information quality ( in our case, document coherence ).', 'we use the information - ordering task as a means to measure the performance of our algorithms and models in a well - controlled setting.', 'as described in section 3, our framework can be used in applications such as multi - document summarization.', 'in fact,  #AUTHOR_TAG formulate the multi - document summarization problem as an information ordering problem, and show that naive ordering algorithms such as majority ordering ( select most frequent orders across input documents ) and chronological ordering ( order facts according to publication date ) do not always yield coherent summaries']",5
"['content model of  #TAUTHOR_TAG ( cm ),']","['content model of  #TAUTHOR_TAG ( cm ),']","['evaluated the performance of several search algorithms across four stochastic models of document coherence : the ibm £ and ibm £ coherence models, the content model of  #TAUTHOR_TAG ( cm ),']","['evaluated the performance of several search algorithms across four stochastic models of document coherence : the ibm £ and ibm £ coherence models, the content model of  #TAUTHOR_TAG ( cm ), and the entity - based model of  #AUTHOR_TAG ( eb ) ( section 2 ).', '']",5
"['content model of  #TAUTHOR_TAG ( cm ),']","['content model of  #TAUTHOR_TAG ( cm ),']","['evaluated the performance of several search algorithms across four stochastic models of document coherence : the ibm £ and ibm £ coherence models, the content model of  #TAUTHOR_TAG ( cm ),']","['evaluated the performance of several search algorithms across four stochastic models of document coherence : the ibm £ and ibm £ coherence models, the content model of  #TAUTHOR_TAG ( cm ), and the entity - based model of  #AUTHOR_TAG ( eb ) ( section 2 ).', '']",5
"['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account for combined model and search performance.', 'we first note that, unfortunately, we failed to accurately reproduce the model of  #TAUTHOR_TAG.', '']",5
"['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account for combined model and search performance.', 'we first note that, unfortunately, we failed to accurately reproduce the model of  #TAUTHOR_TAG.', '']",4
"['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account for combined model and search performance.', 'we first note that, unfortunately, we failed to accurately reproduce the model of  #TAUTHOR_TAG.', '']",4
"['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account']","['last comparison we provide is between the performance provided by our framework and previously - reported performance results ( table 3 ).', 'we are able to provide this comparison based on the tau figures reported in  #TAUTHOR_TAG.', 'the training and test data for both genres is the same, and therefore the figures can be directly compared.', 'these figures account for combined model and search performance.', 'we first note that, unfortunately, we failed to accurately reproduce the model of  #TAUTHOR_TAG.', '']",4
"['of  #TAUTHOR_TAG, while ensuring']","['of  #TAUTHOR_TAG, while ensuring']","['of  #TAUTHOR_TAG, while ensuring optimal solutions, is prohibitively expensive,']","['presented a generic framework that is capable of integrating various stochastic models of discourse coherence into a more powerful model that combines the strengths of the individual models.', 'an important ingredient of this framework are the search algorithms based on idl - expressions, which provide a flexible way of solving discourse generation problems using stochastic models.', 'our generation algorithms are fundamentally different from previously - proposed algorithms for discourse generation.', 'the genetic algorithms of  #AUTHOR_TAG and  #AUTHOR_TAG, as well as the greedy algorithm of  #AUTHOR_TAG, provide no theoretical guarantees on the optimality of the solutions they propose.', 'at the other end of the spectrum, the exhaustive search of  #TAUTHOR_TAG, while ensuring optimal solutions, is prohibitively expensive, and cannot be used to perform utility - based training.', 'the linear programming algorithm of  #AUTHOR_TAG is the only proposal that achieves both good speed and accuracy.', 'their algorithm, however, cannot handle models with hidden states, cannot compute "" - best lists, and does not have the representation flexibility provided by idl - expressions, which is crucial for coherence decoding in realistic applications such as multidocument summarization.', 'for each of the coherence model combinations that we have utility trained, we obtained improved results on the discourse ordering problem compared to the individual models.', 'this is important for two reasons.', 'our improvements can have an immediate impact on multi - document summarization applications  #AUTHOR_TAG.', '']",4
['by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliter'],"['by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate end - to - end.', '']",['by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate'],"[', we describe briefly the underlying framework, called rnn encoder - decoder, proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate end - to - end.', 'the enoder is a character - based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.', 'this network reads the source name x = ( x 1,..., x t ) and encodes it into a sequence of hidden states h = ( h 1, · · ·, h t ) :', 'each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.', '']",6
['by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliter'],"['by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate end - to - end.', '']",['by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate'],"[', we describe briefly the underlying framework, called rnn encoder - decoder, proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG upon which we build a machine transliteration model that learns to transliterate end - to - end.', 'the enoder is a character - based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.', 'this network reads the source name x = ( x 1,..., x t ) and encodes it into a sequence of hidden states h = ( h 1, · · ·, h t ) :', 'each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.', '']",0
['decoder model  #TAUTHOR_TAG in'],['rnn encoder - decoder model  #TAUTHOR_TAG in'],['rnn encoder - decoder model  #TAUTHOR_TAG in'],"['conducted a set of experiments to show the effectiveness of rnn encoder - decoder model  #TAUTHOR_TAG in the task of machine transliteration using standard benchmark datasets provided by news 2015 - 16 shared task.', 'table 1 shows different datasets in our experiments.', 'each dataset covers different levels of difficulty and training set size.', '']",0
['decoder model  #TAUTHOR_TAG in'],['rnn encoder - decoder model  #TAUTHOR_TAG in'],['rnn encoder - decoder model  #TAUTHOR_TAG in'],"['conducted a set of experiments to show the effectiveness of rnn encoder - decoder model  #TAUTHOR_TAG in the task of machine transliteration using standard benchmark datasets provided by news 2015 - 16 shared task.', 'table 1 shows different datasets in our experiments.', 'each dataset covers different levels of difficulty and training set size.', '']",5
['decoder model  #TAUTHOR_TAG in'],['rnn encoder - decoder model  #TAUTHOR_TAG in'],['rnn encoder - decoder model  #TAUTHOR_TAG in'],"['conducted a set of experiments to show the effectiveness of rnn encoder - decoder model  #TAUTHOR_TAG in the task of machine transliteration using standard benchmark datasets provided by news 2015 - 16 shared task.', 'table 1 shows different datasets in our experiments.', 'each dataset covers different levels of difficulty and training set size.', '']",5
[' #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural'],[' #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural'],[' #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural nets  #AUTHOR_TAG to'],"['extraction is typically reduced to a classification problem.', 'a supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities.', 'traditional methods rely on linguistic or semantic features  #AUTHOR_TAG, or kernels based on syntax or sequences  #AUTHOR_TAG a, b ;  #AUTHOR_TAG to represent sentences of relations.', 'more recently, deep neural nets start to show promising results.', 'most rely on convolutional neural nets  #AUTHOR_TAG ( zeng et al.,, 2015  #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural nets  #AUTHOR_TAG to learn the representation of relations.', 'our supervised base model will be similar to  #AUTHOR_TAG.', 'our initial experiments did not use syntactic features  #TAUTHOR_TAG that require additional parsers.', 'in order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.', 'they used their multi - task model to train on the bilingual ace05 datasets and obtained improvement when there is less training available ( 10 % - 50 % ).', 'our experiments will show our multitask model can make significant improvement on the full training set.', 'in terms of the regularization to the representation,  #AUTHOR_TAG used l2 regularization between the parameters of the same part of two models in multi - task learning.', 'their method is a kind of soft - parameter sharing, which does not involve sharing any part of the model directly.', ' #AUTHOR_TAG applied domain adversarial networks  #AUTHOR_TAG to relation extraction and obtained improvement on out - of - domain evaluation.', 'inspired by the adversarial training, we attempt to use it as a regularization tool in our multi - task model and find some improvement']",0
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",0
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",0
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",0
"[' #TAUTHOR_TAG set, and']","[' #TAUTHOR_TAG set, and']","[').', 'previous work  #TAUTHOR_TAG set, and the other half of bc, cts']","['apply the multi - task learning, we need at least two datasets.', 'we pick ace05 and ere for our case study.', 'the ace05 dataset provides a cross - domain evaluation setting.', 'it contains 6 domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and weblogs ( wl ).', 'previous work  #TAUTHOR_TAG set, and the other half of bc, cts and wl as the test sets.', 'we followed their split of documents and their split of the relation types for asymmetric relations.', 'the ere dataset has a similar relation schema to ace05, but is different in some annotation guidelines  #AUTHOR_TAG.', 'it also has more data than ace05, which we expect to be helpful in the multi - task learning.', 'it contains documents from newswire and discussion forums.', 'we did not find an existing split of this dataset, so we randomly split the documents into train ( 80 % ), dev ( 10 % ) and test ( 10 % )']",0
[' #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural'],[' #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural'],[' #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural nets  #AUTHOR_TAG to'],"['extraction is typically reduced to a classification problem.', 'a supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities.', 'traditional methods rely on linguistic or semantic features  #AUTHOR_TAG, or kernels based on syntax or sequences  #AUTHOR_TAG a, b ;  #AUTHOR_TAG to represent sentences of relations.', 'more recently, deep neural nets start to show promising results.', 'most rely on convolutional neural nets  #AUTHOR_TAG ( zeng et al.,, 2015  #AUTHOR_TAG 2016 ;  #TAUTHOR_TAG or recurrent neural nets  #AUTHOR_TAG to learn the representation of relations.', 'our supervised base model will be similar to  #AUTHOR_TAG.', 'our initial experiments did not use syntactic features  #TAUTHOR_TAG that require additional parsers.', 'in order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.', 'they used their multi - task model to train on the bilingual ace05 datasets and obtained improvement when there is less training available ( 10 % - 50 % ).', 'our experiments will show our multitask model can make significant improvement on the full training set.', 'in terms of the regularization to the representation,  #AUTHOR_TAG used l2 regularization between the parameters of the same part of two models in multi - task learning.', 'their method is a kind of soft - parameter sharing, which does not involve sharing any part of the model directly.', ' #AUTHOR_TAG applied domain adversarial networks  #AUTHOR_TAG to relation extraction and obtained improvement on out - of - domain evaluation.', 'inspired by the adversarial training, we attempt to use it as a regularization tool in our multi - task model and find some improvement']",4
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",3
"[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation']","[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation']","[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation learning from scratch first.', 'our experiments focus on whether']","['separately on the two corpora ( row "" supervised "" in table 1 ), we obtain results on ace05 comparable to previous work  #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation learning from scratch first.', 'our experiments focus on whether we can improve the representation with more sources of data.', 'a common way to do so is pre - training.', 'as a baseline, we pre - train the encoder of the supervised model on ere and then fine - tune on ace05, and vice versa ( row "" pretraining "" in table 1 ).', 'we observe improvement on both fine - tuned datasets.', '']",2
"[' #TAUTHOR_TAG across multiple languages, demonstrating both the unique importance and the broad utility of this approach']","[' #TAUTHOR_TAG across multiple languages, demonstrating both the unique importance and the broad utility of this approach']","[' #TAUTHOR_TAG across multiple languages, demonstrating both the unique importance and the broad utility of this approach']","['', 'guages : english, amharic, arabic, bengali, farsi, hindi, somali, and tagalog.', 'these languages use a variety of scripts and orthographic conventions ( for example, only three use capitalization ), come from different language families, and vary in their morphological complexity.', 'we demonstrate the effectiveness of clms in distinguishing name tokens from non - name tokens, as illustrated by figure 1, which shows perplexity histograms from a clm trained on entity tokens.', 'our models use only individual tokens, but perform extremely well in spite of taking no account of word context.', 'we then assess the utility of directly adding simple features based on this clm implementation to an existing ner system, and show that they have a significant positive impact on performance across many of the languages we tried.', 'by adding very simple clm - based features to the system, our scores approach those of a state - of - the - art ner system  #TAUTHOR_TAG across multiple languages, demonstrating both the unique importance and the broad utility of this approach']",3
['ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG'],"['of training data.', ""4 we compare the clm's entity identification against two state - of - the - art ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG""]","['of the relatively small amount of training data.', ""4 we compare the clm's entity identification against two state - of - the - art ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG."", 'we train the ner systems as usual, but at test time we convert all predictions into binary token - level annotations to']","['', 'for those tokens not excluded using the threshold as described above, we compare the perplexity scores of the two models and assign the label corresponding to the model yielding the lower score.', 'we compare srilm against skip - gram and cbow, as implemented in gensim, and the logbilinear ( lb ) model.', 'we trained both cbow and skip - gram with window size 3, and size 20.', 'we tuned lb, and report results with embedding size 150, and learning rate 0. 1.', 'despite tuning the neural models, the simple n - gram model outperforms them significantly, perhaps because of the relatively small amount of training data.', ""4 we compare the clm's entity identification against two state - of - the - art ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG."", 'we train the ner systems as usual, but at test time we convert all predictions into binary token - level annotations to get the final score.', 'as table 2 shows, the result of ngram clm, which yields the highest performance, is remarkably close to the result of']",3
['ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG'],"['of training data.', ""4 we compare the clm's entity identification against two state - of - the - art ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG""]","['of the relatively small amount of training data.', ""4 we compare the clm's entity identification against two state - of - the - art ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG."", 'we train the ner systems as usual, but at test time we convert all predictions into binary token - level annotations to']","['', 'for those tokens not excluded using the threshold as described above, we compare the perplexity scores of the two models and assign the label corresponding to the model yielding the lower score.', 'we compare srilm against skip - gram and cbow, as implemented in gensim, and the logbilinear ( lb ) model.', 'we trained both cbow and skip - gram with window size 3, and size 20.', 'we tuned lb, and report results with embedding size 150, and learning rate 0. 1.', 'despite tuning the neural models, the simple n - gram model outperforms them significantly, perhaps because of the relatively small amount of training data.', ""4 we compare the clm's entity identification against two state - of - the - art ner systems : cogcompner  #AUTHOR_TAG and lstm - crf  #TAUTHOR_TAG."", 'we train the ner systems as usual, but at test time we convert all predictions into binary token - level annotations to get the final score.', 'as table 2 shows, the result of ngram clm, which yields the highest performance, is remarkably close to the result of']",5
"['standard ner model boosts performance.', 'cogcompner is run with standard features, including brown clusters ;  #TAUTHOR_TAG is run with default parameters and pre - trained embeddings.', 'unseen refers to']","['standard ner model boosts performance.', 'cogcompner is run with standard features, including brown clusters ;  #TAUTHOR_TAG is run with default parameters and pre - trained embeddings.', 'unseen refers to']","['3 : ner results on 8 languages show that even a simplistic addition of clm features to a standard ner model boosts performance.', 'cogcompner is run with standard features, including brown clusters ;  #TAUTHOR_TAG is run with default parameters and pre - trained embeddings.', 'unseen refers to']","['define three simple features that capture information provided by clms and which we expect to be useful for ner.', 'entity feature we define one "" isentity "" feature based on the perplexities of the entity and non - entity clms.', 'we compare the perplexity calculated by entity clm and non - entity clm described in section 3, and return a boolean value indicating whether the entity clm score is lower.', 'language features we define two languagerelated features : "" isarabic "" and "" isrussian "".', 'we observe that there are many names in english text that originate from other languages, resulting in very different orthography than native english names.', 'we therefore build two languagebased clms for arabic and russian.', 'we collect a list of arabic names and a list of russian names by scraping name - related websites, and train an arabic clm and a russian clm.', 'for each token, when the perplexity of either the arabic or the russian clm is lower than the perplexity of the non - entity clm, we return true, indicating that this entity is likely to be a name from arabic / russian.', 'otherwise, we return false.', 'table 3 : ner results on 8 languages show that even a simplistic addition of clm features to a standard ner model boosts performance.', 'cogcompner is run with standard features, including brown clusters ;  #TAUTHOR_TAG is run with default parameters and pre - trained embeddings.', 'unseen refers to performance on named entities in test that were not seen in the training data.', 'full is performance on all entities in test.', 'averages are computed over all languages other than english']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['use cogcompner  #AUTHOR_TAG as our baseline ner system because it allows easy integration of new features, and evaluate on the same datasets as before.', 'for english, we add all features described above.', 'for other languages, due to the limited training data, we only use the "" isentity "" feature.', ""we compare with the state - of - theart character - level neural ner system of  #TAUTHOR_TAG, which inherently encodes comparable information to clms, as a way to investigate how much of that system's performance can be attributed directly to name - internal structure."", 'the results in table 3 show that for six of the eight languages we studied, the baseline ner can be significantly improved by adding simple clm features ; for english and arabic, it performs better even than the neural ner model of  #TAUTHOR_TAG.', 'for tagalog, however, adding clm features actually impairs system performance.', 'in the same table, the rows marked "" unseen "" report systems\'performance on named entities in test that were not seen in the training data.', 'this setting more directly assesses the robustness of a system to identify named entities in new data.', 'by this measure, farsi ner is not improved by nameonly clm features and tagalog is impaired.', 'benefits for english, hindi, and somali are limited, but are quite significant for amharic, arabic, and bengali']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['use cogcompner  #AUTHOR_TAG as our baseline ner system because it allows easy integration of new features, and evaluate on the same datasets as before.', 'for english, we add all features described above.', 'for other languages, due to the limited training data, we only use the "" isentity "" feature.', ""we compare with the state - of - theart character - level neural ner system of  #TAUTHOR_TAG, which inherently encodes comparable information to clms, as a way to investigate how much of that system's performance can be attributed directly to name - internal structure."", 'the results in table 3 show that for six of the eight languages we studied, the baseline ner can be significantly improved by adding simple clm features ; for english and arabic, it performs better even than the neural ner model of  #TAUTHOR_TAG.', 'for tagalog, however, adding clm features actually impairs system performance.', 'in the same table, the rows marked "" unseen "" report systems\'performance on named entities in test that were not seen in the training data.', 'this setting more directly assesses the robustness of a system to identify named entities in new data.', 'by this measure, farsi ner is not improved by nameonly clm features and tagalog is impaired.', 'benefits for english, hindi, and somali are limited, but are quite significant for amharic, arabic, and bengali']",4
"[' #TAUTHOR_TAG clearly includes information comparable to that in the clm,']","[' #TAUTHOR_TAG clearly includes information comparable to that in the clm,']","[' #TAUTHOR_TAG clearly includes information comparable to that in the clm, it requires a']","['results demonstrate the power of clms for recognizing named entity tokens in a diverse range of languages, and that in many cases they can improve off - the - shelf ner system performance even when integrated in a simplistic way.', 'however, the results from section 4. 2 show that this is not true for all languages, especially when only considering unseen entities in test : tagalog and farsi do not follow the trend for the other languages we assessed even though clm performs well for named entity identification.', 'while the end - to - end model developed by  #TAUTHOR_TAG clearly includes information comparable to that in the clm, it requires a fully annotated ner corpus, takes significant time and computational resources to train, and is non - trivial to integrate into a new ner system.', 'the clm approach captures a very large fraction of the entity / non - entity distinction capacity of full ner systems, and can be rapidly trained using only entity and non - entity token lists - i. e., it is corpus - agnostic.', '']",4
"[' #TAUTHOR_TAG clearly includes information comparable to that in the clm,']","[' #TAUTHOR_TAG clearly includes information comparable to that in the clm,']","[' #TAUTHOR_TAG clearly includes information comparable to that in the clm, it requires a']","['results demonstrate the power of clms for recognizing named entity tokens in a diverse range of languages, and that in many cases they can improve off - the - shelf ner system performance even when integrated in a simplistic way.', 'however, the results from section 4. 2 show that this is not true for all languages, especially when only considering unseen entities in test : tagalog and farsi do not follow the trend for the other languages we assessed even though clm performs well for named entity identification.', 'while the end - to - end model developed by  #TAUTHOR_TAG clearly includes information comparable to that in the clm, it requires a fully annotated ner corpus, takes significant time and computational resources to train, and is non - trivial to integrate into a new ner system.', 'the clm approach captures a very large fraction of the entity / non - entity distinction capacity of full ner systems, and can be rapidly trained using only entity and non - entity token lists - i. e., it is corpus - agnostic.', '']",1
"[' #TAUTHOR_TAG clearly includes information comparable to that in the clm,']","[' #TAUTHOR_TAG clearly includes information comparable to that in the clm,']","[' #TAUTHOR_TAG clearly includes information comparable to that in the clm, it requires a']","['results demonstrate the power of clms for recognizing named entity tokens in a diverse range of languages, and that in many cases they can improve off - the - shelf ner system performance even when integrated in a simplistic way.', 'however, the results from section 4. 2 show that this is not true for all languages, especially when only considering unseen entities in test : tagalog and farsi do not follow the trend for the other languages we assessed even though clm performs well for named entity identification.', 'while the end - to - end model developed by  #TAUTHOR_TAG clearly includes information comparable to that in the clm, it requires a fully annotated ner corpus, takes significant time and computational resources to train, and is non - trivial to integrate into a new ner system.', 'the clm approach captures a very large fraction of the entity / non - entity distinction capacity of full ner systems, and can be rapidly trained using only entity and non - entity token lists - i. e., it is corpus - agnostic.', '']",0
['##s2s )  #TAUTHOR_TAG and recurrent'],['( convs2s )  #TAUTHOR_TAG and recurrent'],['##s2s )  #TAUTHOR_TAG and'],"['trained neural machine transaltion systems using the encoder - decoder architecture with attention  #AUTHOR_TAG for english - hindi as well hindi - english translation.', 'we compared convolutional neural network ( convs2s )  #TAUTHOR_TAG and recurrent neural network ( rnns2s )  #AUTHOR_TAG based sequence to sequence learning architectures.', 'while rnn based architectures have proved to be successful and produce state - of - the - art results for machine translation, they take a long time to train.', 'the temporal dependencies between the elements in the sequence due to the rnn state vector requires sequential processing.', 'on the other hand, different parts of the sequence can be processed in parallel using a convs2s.', 'hence, it is appealing to explore convs2s as the basis of an architecture to speed up training and decoding.', 'recent work  #TAUTHOR_TAG has shown that a purely cnn based encoder - decoder network is competitive with a rnn based network']",5
['using nag optimizer  #TAUTHOR_TAG with a'],['using nag optimizer  #TAUTHOR_TAG with a'],['3 decoder layers was trained using nag optimizer  #TAUTHOR_TAG with'],"['rnns2s model was trained using nematus 3 framework.', 'to handle rare words, subword 4 technique was used through byte pair encoding ( bpe )  #AUTHOR_TAG with 16000 bpe operations.', 'since there is no similarity between english and hindi language vocabulary, both the languages were trained separately for bpe.', 'the encoder and decoder hidden layer size was kept at 512 and word embedding size as 256.', 'the model was trained with a batch size of 40 sentences and maximum sentence length of 100 using adadelta  #AUTHOR_TAG optimizer with a learning rate of 0. 0001 and no dropout setting.', 'the output parameters were saved after every 10000 iterations.', 'the decoding was done using a beam size of 12 and ensemble of last 3 models and the best model taken together.', 'the convs2s model was trained using fairseq 5, an open source library developed by facebook for neural machine translation using cnn or rnn networks.', 'for handling the rare words, the source side and target side corpora were segmented using byte pair encoding ( bpe )  #AUTHOR_TAG.', 'the baseline model with 4 encoder layers and 3 decoder layers was trained using nag optimizer  #TAUTHOR_TAG with a learning rate of 0. 25 with 0. 2 as its dropout value and gradient clipping was also applied.', 'table 6 : hindi to english translation systems at wat2017', 'the inferencing was done using beam search with a beam size of 10 for both hindi - english and english - hindi translation task.', 'the model was also trained with more number of layers in the encoder and the decoder.', 'the resulting bleu scores for different number of encoder and decoder layers are shown in table 4.', 'the best results were obtained when the number of encoder layers were set to 13 and decoder layers to 7, with learning rate of 0. 1 and no dropout regularization.', 'the resulting bleu scores with this setting for hindi - english and english - hindi are shown in table 2 and table 3 respectively']",5
['##s2s )  #TAUTHOR_TAG and recurrent'],['( convs2s )  #TAUTHOR_TAG and recurrent'],['##s2s )  #TAUTHOR_TAG and'],"['trained neural machine transaltion systems using the encoder - decoder architecture with attention  #AUTHOR_TAG for english - hindi as well hindi - english translation.', 'we compared convolutional neural network ( convs2s )  #TAUTHOR_TAG and recurrent neural network ( rnns2s )  #AUTHOR_TAG based sequence to sequence learning architectures.', 'while rnn based architectures have proved to be successful and produce state - of - the - art results for machine translation, they take a long time to train.', 'the temporal dependencies between the elements in the sequence due to the rnn state vector requires sequential processing.', 'on the other hand, different parts of the sequence can be processed in parallel using a convs2s.', 'hence, it is appealing to explore convs2s as the basis of an architecture to speed up training and decoding.', 'recent work  #TAUTHOR_TAG has shown that a purely cnn based encoder - decoder network is competitive with a rnn based network']",0
"['convolutional sequence to sequence model  #TAUTHOR_TAG, the input sequence is encoded into distributional vector space using a cnn and']","['convolutional sequence to sequence model  #TAUTHOR_TAG, the input sequence is encoded into distributional vector space using a cnn and']","['convolutional sequence to sequence model  #TAUTHOR_TAG, the input sequence is encoded into distributional vector space using a cnn and']","['convolutional sequence to sequence model  #TAUTHOR_TAG, the input sequence is encoded into distributional vector space using a cnn and decoded back to output sequence again using cnn instead of rnn  #AUTHOR_TAG.', 'each input element embedding is combined with its positional embedding ( signifies the position of the input element ).', 'positional embeddings help the network to realize what part of input it is dealing with, currently.', '']",0
"['- to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['end - to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['- to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['##reading is the process of understanding speech by using solely visual features, i. e. images of the lips of a speaker.', 'in communication between humans, lipreading has a twofold relevance [ 1 ] : first, visual cues play a role in spoken conversation [ 2 ] ; second, hearing - impaired persons may use lipreading as a means to follow verbal speech.', 'with the success of computer - based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by petajan [ 3 ], who used lipreading to augment conventional acoustic speech recognition, and chiou and hwang [ 4 ], who were the first to perform lipreading without resorting to any acoustic signal at all.', 'since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [ 5, 6 ] or for end - to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our previous work  #TAUTHOR_TAG, we proposed a fully neural network based system, using a stack of fully connected and recurrent ( lstm, long shortterm memory ) [ 10, 11 ] neural network layers.', 'the scope of this paper is the introduction of state - of - theart methods for speaker - independent lipreading with neural networks.', 'we evaluate our established system  #TAUTHOR_TAG in a crossspeaker setting, observing a drastic performance drop on unknown speakers.', 'in order to alleviate the discrepancy between training speakers and unknown test speaker, we use domainadversarial training as proposed by ganin and lempitsky [ 12 ] : untranscribed data from the target speaker is used as additional training input to the neural network, with the aim of pushing the network to learn an intermediate data representation which is domain - agnostic, i. e. which does not depend on whether the input data comes from a source speaker or a target speaker.', 'we evaluate our system on a subset of the grid corpus [ 13 ], which contains extensive data from 34 speakers and is therefore ideal for a systematic evaluation of the proposed method']",0
[') best system from  #TAUTHOR_TAG'],['recomputed ) best system from  #TAUTHOR_TAG'],['recomputed ) best system from  #TAUTHOR_TAG'],"['', 'learn to confuse speakers instead of separating them. the speaker classifier and the joint network work for opposite objectives ( hence, "" adversarial "" ) ; an idea first presented in the context of factorial codes [ 39 ].', 'figure 2 shows a graphical overview of the system : the joint part is at the top, at the bottom are word classifier', '( left ) and speaker classifier ( right ). table 1 : baseline word accuracies on single speakers, averaged over the development set, with standard deviation. layer types', 'are fc ( fully connected feedforward ), dp ( dropout ), and lstm, followed by the number of neurons / cells. * marks the ( reimplemented and recomputed ) best system from  #TAUTHOR_TAG']",0
"['- to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['end - to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['- to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['##reading is the process of understanding speech by using solely visual features, i. e. images of the lips of a speaker.', 'in communication between humans, lipreading has a twofold relevance [ 1 ] : first, visual cues play a role in spoken conversation [ 2 ] ; second, hearing - impaired persons may use lipreading as a means to follow verbal speech.', 'with the success of computer - based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by petajan [ 3 ], who used lipreading to augment conventional acoustic speech recognition, and chiou and hwang [ 4 ], who were the first to perform lipreading without resorting to any acoustic signal at all.', 'since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [ 5, 6 ] or for end - to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our previous work  #TAUTHOR_TAG, we proposed a fully neural network based system, using a stack of fully connected and recurrent ( lstm, long shortterm memory ) [ 10, 11 ] neural network layers.', 'the scope of this paper is the introduction of state - of - theart methods for speaker - independent lipreading with neural networks.', 'we evaluate our established system  #TAUTHOR_TAG in a crossspeaker setting, observing a drastic performance drop on unknown speakers.', 'in order to alleviate the discrepancy between training speakers and unknown test speaker, we use domainadversarial training as proposed by ganin and lempitsky [ 12 ] : untranscribed data from the target speaker is used as additional training input to the neural network, with the aim of pushing the network to learn an intermediate data representation which is domain - agnostic, i. e. which does not depend on whether the input data comes from a source speaker or a target speaker.', 'we evaluate our system on a subset of the grid corpus [ 13 ], which contains extensive data from 34 speakers and is therefore ideal for a systematic evaluation of the proposed method']",3
"['- to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['end - to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['- to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our']","['##reading is the process of understanding speech by using solely visual features, i. e. images of the lips of a speaker.', 'in communication between humans, lipreading has a twofold relevance [ 1 ] : first, visual cues play a role in spoken conversation [ 2 ] ; second, hearing - impaired persons may use lipreading as a means to follow verbal speech.', 'with the success of computer - based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by petajan [ 3 ], who used lipreading to augment conventional acoustic speech recognition, and chiou and hwang [ 4 ], who were the first to perform lipreading without resorting to any acoustic signal at all.', 'since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [ 5, 6 ] or for end - to - end - training  #TAUTHOR_TAG 8, 9 ].', 'in our previous work  #TAUTHOR_TAG, we proposed a fully neural network based system, using a stack of fully connected and recurrent ( lstm, long shortterm memory ) [ 10, 11 ] neural network layers.', 'the scope of this paper is the introduction of state - of - theart methods for speaker - independent lipreading with neural networks.', 'we evaluate our established system  #TAUTHOR_TAG in a crossspeaker setting, observing a drastic performance drop on unknown speakers.', 'in order to alleviate the discrepancy between training speakers and unknown test speaker, we use domainadversarial training as proposed by ganin and lempitsky [ 12 ] : untranscribed data from the target speaker is used as additional training input to the neural network, with the aim of pushing the network to learn an intermediate data representation which is domain - agnostic, i. e. which does not depend on whether the input data comes from a source speaker or a target speaker.', 'we evaluate our system on a subset of the grid corpus [ 13 ], which contains extensive data from 34 speakers and is therefore ideal for a systematic evaluation of the proposed method']",3
"['5 ].', 'since then, several end - to - end trainable systems were presented  #TAUTHOR_TAG 8, 9 ].', 'the current']","['[ 5 ].', 'since then, several end - to - end trainable systems were presented  #TAUTHOR_TAG 8, 9 ].', 'the current state - of - the - art']","['- art learning techniques ( and the necessary hardware ).', 'the first deep neural network for lipreading was a seven - layer convolutional net as a preprocessing stage for an hmm - based word recognizer [ 5 ].', 'since then, several end - to - end trainable systems were presented  #TAUTHOR_TAG 8, 9 ].', 'the current state - of - the - art accuracy on the grid corpus is 3. 3 % error [ 9 ] using a very large set of additional training data ; so their result is not directly comparable to ours.', 'in domain adaptation, it is assumed that a learning task exhibits a domain shift between the training ( or source ) and test ( or target ) data.', 'this can be mitigated in several ways [ 35 ] ; we apply domain - adversarial training [ 12 ], where an intermediate layer in a multi - layer network is driven to']","['##reading can be used to complement or augment speech recognition, particularly in the presence of noise [ 3, 14 ], and for purely visual speech recognition [ 4, 15, 5 ].', 'in the latter case, ambiguities due to incomplete information ( e. g. about voicing ) can be mitigated by augmenting the video stream with ultrasound images of the vocal tract [ 16 ].', 'visual speech processing is an instance of a silent speech interface [ 17 ] ; further promising approaches include capturing the movement of the articulators by electric or permanent magnetic articulography [ 18, 19 ], and capturing of muscle activity using electromyography [ 20, 21, 22, 23 ].', 'versatile lipreading features have been proposed, such as active appearance models [ 24 ], local binary patterns [ 25 ], and pca - based eigenlips [ 26 ] and eigentongues [ 27 ].', 'for tackling speaker dependency, diverse scaling and normalization techniques have been employed [ 28, 29 ].', 'classification is often done with hidden markov models ( hmms ), e. g. [ 30, 15, 31, 32 ].', 'mouth tracking is done as a preprocessing step [ 32, 15, 5 ].', 'for a comprehensive review see [ 33 ].', 'neural networks have early been applied to the lipreading task [ 34 ], however, they have become widespread only in recent years, with the advent of state - of - the - art learning techniques ( and the necessary hardware ).', 'the first deep neural network for lipreading was a seven - layer convolutional net as a preprocessing stage for an hmm - based word recognizer [ 5 ].', 'since then, several end - to - end trainable systems were presented  #TAUTHOR_TAG 8, 9 ].', 'the current state - of - the - art accuracy on the grid corpus is 3. 3 % error [ 9 ] using a very large set of additional training data ; so their result is not directly comparable to ours.', 'in domain adaptation, it is assumed that a learning task exhibits a domain shift between the training ( or source ) and test ( or target ) data.', 'this can be mitigated in several ways [ 35 ] ; we apply domain - adversarial training [ 12 ], where an intermediate layer in a multi - layer network is driven to learn a representation of the input data which is optimized to be domain - agnostic, i. e. to make it difficult to detect whether an input sample is from the source or the target domain.', 'a great advantage of this approach is the end - to']",3
"['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus']","['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus [ 13 ], which consists of video and audio recordings of 34 speakers ( which we name s1 to s34 ) saying 1000 sentences each.', 'all sentences have a fixed structure : command ( 4 ) + color ( 4 ) + preposition ( 4 ) + letter ( 25 ) + digit ( 10 ) + adverb ( 4 ), for example "" place red']","['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus [ 13 ], which consists of video and audio recordings of 34 speakers ( which we name s1 to s34 ) saying 1000 sentences each.', 'all sentences have a fixed']","['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus [ 13 ], which consists of video and audio recordings of 34 speakers ( which we name s1 to s34 ) saying 1000 sentences each.', 'all sentences have a fixed structure : command ( 4 ) + color ( 4 ) + preposition ( 4 ) + letter ( 25 ) + digit ( 10 ) + adverb ( 4 ), for example "" place red at j 2, please "", where the number of alternative words is given in parentheses.', 'there are 51 distinct words ; alternatives are randomly distributed so that context cannot be used for classification.', 'each sentence has a length of 3 seconds at 25 frames per second, so the total data per speaker is 3000 seconds ( 50 minutes ).', 'using the annotations contained in the corpus, we segmented all videos at word level, yielding 6000 word samples per speaker.', 'we experiment on speakers s1 - s19 : speakers 1 - 9 form the development speakers, used to determine optimal parameters ; speakers 10 - 19 are the evaluation speakers, held back until the final evaluation of the systems.', '']",3
"['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus']","['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus [ 13 ], which consists of video and audio recordings of 34 speakers ( which we name s1 to s34 ) saying 1000 sentences each.', 'all sentences have a fixed structure : command ( 4 ) + color ( 4 ) + preposition ( 4 ) + letter ( 25 ) + digit ( 10 ) + adverb ( 4 ), for example "" place red']","['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus [ 13 ], which consists of video and audio recordings of 34 speakers ( which we name s1 to s34 ) saying 1000 sentences each.', 'all sentences have a fixed']","['follow the data preprocessing protocol from  #TAUTHOR_TAG.', 'we use the grid corpus [ 13 ], which consists of video and audio recordings of 34 speakers ( which we name s1 to s34 ) saying 1000 sentences each.', 'all sentences have a fixed structure : command ( 4 ) + color ( 4 ) + preposition ( 4 ) + letter ( 25 ) + digit ( 10 ) + adverb ( 4 ), for example "" place red at j 2, please "", where the number of alternative words is given in parentheses.', 'there are 51 distinct words ; alternatives are randomly distributed so that context cannot be used for classification.', 'each sentence has a length of 3 seconds at 25 frames per second, so the total data per speaker is 3000 seconds ( 50 minutes ).', 'using the annotations contained in the corpus, we segmented all videos at word level, yielding 6000 word samples per speaker.', 'we experiment on speakers s1 - s19 : speakers 1 - 9 form the development speakers, used to determine optimal parameters ; speakers 10 - 19 are the evaluation speakers, held back until the final evaluation of the systems.', '']",3
"[' #TAUTHOR_TAG.', 'we run the lipreader as a single - speaker system with different topologies, optionally using dropout ( always with 50']","[' #TAUTHOR_TAG.', 'we run the lipreader as a single - speaker system with different topologies, optionally using dropout ( always with 50 % dropout ratio ) to avoid overfitting the training set.', 'adversarial training is not used ( i. e.']","['prior work  #TAUTHOR_TAG.', 'we run the lipreader as a single - speaker system with different topologies, optionally using dropout ( always with 50 % dropout ratio ) to avoid overfitting the training set.', 'adversarial training is not used (']","['first experiment deals with establishing a baseline for our experiments, building on prior work  #TAUTHOR_TAG.', 'we run the lipreader as a single - speaker system with different topologies, optionally using dropout ( always with 50 % dropout ratio ) to avoid overfitting the training set.', 'adversarial training is not used ( i. e. the weight in figure 2 is set to zero ).', 'table 1 shows the resulting test set accuracies averaged over the development speakers.', 'without using dropout, the accuracy on the test set is ∼79 %.', 'note in particular that the baseline cannot substantially be improved by increasing the layer size or adding more layers.', '']",3
