{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of token training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "file_path = \"../output/llm2vec_llama3/\"\n",
    "\n",
    "os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "result\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_data=[]\n",
    "for fp in os.listdir(file_path):\n",
    "    if fp.split('_')[0:3]==[\"token\",'llama','scopes']:\n",
    "        print()\n",
    "        print(fp)\n",
    "        fp_path=os.path.join(file_path,fp)\n",
    "        #print(fp_path)\n",
    "        json_list=os.listdir(fp_path)\n",
    "        json_list=sorted(json_list)\n",
    "        arr=[[] for _ in range(2)]\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            if js.split('.')[0]=='model_setup':\n",
    "                arr[0].append(js)\n",
    "            elif js.split('_')[0]=='eval':\n",
    "                arr[1].append(js)\n",
    "        print(arr)# split json files 3types\n",
    "        tmp_dict={}\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            #print(os.path.join(fp_path,js))\n",
    "            json_path=os.path.join(fp_path,js)\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if js==arr[0][0]:#Metadata of model\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "                if js==arr[1][-3]:#eval_metrics\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "        print(tmp_dict)\n",
    "        json_data.append(tmp_dict)\n",
    "\n",
    "print()\n",
    "print(\"result\")\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_json=pd.DataFrame(json_data).replace(\"None\", np.nan)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument must be a color, a sequence of colors, or a sequence of numbers, not 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/deallab/djk/ssh-finecite/FINECite/venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4618\u001b[0m, in \u001b[0;36mAxes._parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4617\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# Is 'c' acceptable as PathCollection facecolors?\u001b[39;00m\n\u001b[0;32m-> 4618\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[43mmcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_rgba_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4619\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/deallab/djk/ssh-finecite/FINECite/venv/lib/python3.10/site-packages/matplotlib/colors.py:496\u001b[0m, in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid color value.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: 'loss' is not a valid color value.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/deallab/djk/ssh-finecite/FINECite/venv/lib/python3.10/site-packages/matplotlib/pyplot.py:3903\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[1;32m   3885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m   3886\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3901\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3902\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PathCollection:\n\u001b[0;32m-> 3903\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3905\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3913\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3915\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3917\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3920\u001b[0m     sci(__ret)\n\u001b[1;32m   3921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/deallab/djk/ssh-finecite/FINECite/venv/lib/python3.10/site-packages/matplotlib/__init__.py:1502\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_namer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m args_and_kwargs:\n\u001b[1;32m   1499\u001b[0m     new_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _label_from_arg(\n\u001b[1;32m   1500\u001b[0m         args_and_kwargs\u001b[38;5;241m.\u001b[39mget(label_namer), auto_label)\n\u001b[0;32m-> 1502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deallab/djk/ssh-finecite/FINECite/venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4805\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edgecolors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4803\u001b[0m     orig_edgecolor \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   4804\u001b[0m c, colors, edgecolors \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 4805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_scatter_color_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_next_color_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_patches_for_fill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_color\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plotnonfinite \u001b[38;5;129;01mand\u001b[39;00m colors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4810\u001b[0m     c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mmasked_invalid(c)\n",
      "File \u001b[0;32m~/deallab/djk/ssh-finecite/FINECite/venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4627\u001b[0m, in \u001b[0;36mAxes._parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4624\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m invalid_shape_exception(c\u001b[38;5;241m.\u001b[39msize, xsize) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m         \u001b[38;5;66;03m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[39;00m\n\u001b[1;32m   4626\u001b[0m         \u001b[38;5;66;03m# severe failure => one may appreciate a verbose feedback.\u001b[39;00m\n\u001b[0;32m-> 4627\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4628\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument must be a color, a sequence of colors, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4629\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a sequence of numbers, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   4630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(colors) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, xsize):\n\u001b[1;32m   4632\u001b[0m         \u001b[38;5;66;03m# NB: remember that a single color is also acceptable.\u001b[39;00m\n\u001b[1;32m   4633\u001b[0m         \u001b[38;5;66;03m# Besides *colors* will be an empty array if c == 'none'.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not 'loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(data = df_json, x = 'learning_rate', y = 'dropout', c = 'loss', cmap = 'Blues')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('dropout')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of token training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.273438</td>\n",
       "      <td>0.700625</td>\n",
       "      <td>0.614546</td>\n",
       "      <td>0.780167</td>\n",
       "      <td>0.671416</td>\n",
       "      <td>0.491694</td>\n",
       "      <td>0.460228</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.710938</td>\n",
       "      <td>0.713130</td>\n",
       "      <td>0.605669</td>\n",
       "      <td>0.774243</td>\n",
       "      <td>0.684308</td>\n",
       "      <td>0.522946</td>\n",
       "      <td>0.367003</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.429688</td>\n",
       "      <td>0.689653</td>\n",
       "      <td>0.602960</td>\n",
       "      <td>0.774505</td>\n",
       "      <td>0.692127</td>\n",
       "      <td>0.490948</td>\n",
       "      <td>0.400133</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.515625</td>\n",
       "      <td>0.691369</td>\n",
       "      <td>0.601645</td>\n",
       "      <td>0.765771</td>\n",
       "      <td>0.679727</td>\n",
       "      <td>0.479624</td>\n",
       "      <td>0.416680</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.382812</td>\n",
       "      <td>0.694373</td>\n",
       "      <td>0.598974</td>\n",
       "      <td>0.760178</td>\n",
       "      <td>0.688687</td>\n",
       "      <td>0.482089</td>\n",
       "      <td>0.394595</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.234375</td>\n",
       "      <td>0.670651</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>0.759811</td>\n",
       "      <td>0.677958</td>\n",
       "      <td>0.490066</td>\n",
       "      <td>0.397729</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.664276</td>\n",
       "      <td>0.592735</td>\n",
       "      <td>0.762177</td>\n",
       "      <td>0.692179</td>\n",
       "      <td>0.476972</td>\n",
       "      <td>0.396792</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.515625</td>\n",
       "      <td>0.688427</td>\n",
       "      <td>0.591285</td>\n",
       "      <td>0.770767</td>\n",
       "      <td>0.679397</td>\n",
       "      <td>0.465055</td>\n",
       "      <td>0.389128</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.657963</td>\n",
       "      <td>0.589755</td>\n",
       "      <td>0.774765</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.488596</td>\n",
       "      <td>0.421199</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.773438</td>\n",
       "      <td>0.704793</td>\n",
       "      <td>0.589340</td>\n",
       "      <td>0.761868</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.456197</td>\n",
       "      <td>0.390887</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.273438  0.700625  0.614546  0.780167  0.671416  0.491694  0.460228   \n",
       "1  1.710938  0.713130  0.605669  0.774243  0.684308  0.522946  0.367003   \n",
       "2  1.429688  0.689653  0.602960  0.774505  0.692127  0.490948  0.400133   \n",
       "3  1.515625  0.691369  0.601645  0.765771  0.679727  0.479624  0.416680   \n",
       "4  1.382812  0.694373  0.598974  0.760178  0.688687  0.482089  0.394595   \n",
       "5  1.234375  0.670651  0.592803  0.759811  0.677958  0.490066  0.397729   \n",
       "6  1.304688  0.664276  0.592735  0.762177  0.692179  0.476972  0.396792   \n",
       "7  1.515625  0.688427  0.591285  0.770767  0.679397  0.465055  0.389128   \n",
       "8  0.968750  0.657963  0.589755  0.774765  0.646465  0.488596  0.421199   \n",
       "9  1.773438  0.704793  0.589340  0.761868  0.666667  0.456197  0.390887   \n",
       "\n",
       "   full_training      segment    mode      model_name  batch_size  max_epoch  \\\n",
       "0          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "1          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "2          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "3          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "4          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "5          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "6          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "7          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "8          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "9          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "\n",
       "   learning_rate  dropout  \n",
       "0        0.00005     0.15  \n",
       "1        0.00005     0.20  \n",
       "2        0.00012     0.15  \n",
       "3        0.00003     0.00  \n",
       "4        0.00005     0.05  \n",
       "5        0.00012     0.20  \n",
       "6        0.00005     0.10  \n",
       "7        0.00010     0.15  \n",
       "8        0.00007     0.05  \n",
       "9        0.00005     0.15  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=df_json.sort_values(['macro_f1','total_f1','acc'],ignore_index=True, ascending=False)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best batch_size and max_epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loss                   1.273438\n",
       "acc                    0.700625\n",
       "macro_f1               0.614546\n",
       "total_f1               0.780167\n",
       "inf_f1                 0.671416\n",
       "perc_f1                0.491694\n",
       "backg_f1               0.460228\n",
       "full_training             False\n",
       "segment             token_llama\n",
       "mode                     scopes\n",
       "model_name       llm2vec_llama3\n",
       "batch_size                    2\n",
       "max_epoch                    20\n",
       "learning_rate           0.00005\n",
       "dropout                    0.15\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best batch_size and max_epochs\")\n",
    "df_json.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of full training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_full_sentence_majo_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.2265625, 'acc': 0.6437612771987915, 'macro_f1': 0.4992097020149231, 'total_f1': 0.711670458316803, 'inf_f1': 0.47863247990608215, 'perc_f1': 0.35424354672431946, 'backg_f1': 0.35230353474617004}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.2265625, 'acc': 0.6437612771987915, 'macro_f1': 0.4992097020149231, 'total_f1': 0.711670458316803, 'inf_f1': 0.47863247990608215, 'perc_f1': 0.35424354672431946, 'backg_f1': 0.35230353474617004, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_majo_total_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 0.51953125, 'acc': 0.7622061371803284, 'total_f1': 0.7074527144432068}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.51953125, 'acc': 0.7622061371803284, 'total_f1': 0.7074527144432068, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_majo_total_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 0.6640625, 'acc': 0.7784810066223145, 'total_f1': 0.7161065936088562}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.6640625, 'acc': 0.7784810066223145, 'total_f1': 0.7161065936088562, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_sentence_majo_scopes_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 2.265625, 'acc': 0.5072332620620728, 'macro_f1': 0.40494704246520996, 'total_f1': 0.6166008114814758, 'inf_f1': 0.3378378450870514, 'perc_f1': 0.3232323229312897, 'backg_f1': 0.28205129504203796}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 2.265625, 'acc': 0.5072332620620728, 'macro_f1': 0.40494704246520996, 'total_f1': 0.6166008114814758, 'inf_f1': 0.3378378450870514, 'perc_f1': 0.3232323229312897, 'backg_f1': 0.28205129504203796, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_token_llama_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.7578125, 'acc': 0.6620519757270813, 'macro_f1': 0.5815226435661316, 'total_f1': 0.7361472845077515, 'inf_f1': 0.6974146366119385, 'perc_f1': 0.4614795744419098, 'backg_f1': 0.3635459840297699}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.7578125, 'acc': 0.6620519757270813, 'macro_f1': 0.5815226435661316, 'total_f1': 0.7361472845077515, 'inf_f1': 0.6974146366119385, 'perc_f1': 0.4614795744419098, 'backg_f1': 0.3635459840297699, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_token_llama_scopes_2_1e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.25, 'acc': 0.6440873742103577, 'macro_f1': 0.5397127866744995, 'total_f1': 0.7333402633666992, 'inf_f1': 0.6392151713371277, 'perc_f1': 0.37776654958724976, 'backg_f1': 0.3408135771751404}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': 1e-05, 'dropout': 0.0}\n",
      "{'loss': 1.25, 'acc': 0.6440873742103577, 'macro_f1': 0.5397127866744995, 'total_f1': 0.7333402633666992, 'inf_f1': 0.6392151713371277, 'perc_f1': 0.37776654958724976, 'backg_f1': 0.3408135771751404, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': 1e-05, 'dropout': 0.0}\n",
      "\n",
      "_full_sentence_prio_scopes_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.3046875, 'acc': 0.5913200974464417, 'macro_f1': 0.3104233741760254, 'total_f1': 0.5307692289352417, 'inf_f1': 0.4354243576526642, 'perc_f1': 0.06185567006468773, 'backg_f1': 0.0}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 1.3046875, 'acc': 0.5913200974464417, 'macro_f1': 0.3104233741760254, 'total_f1': 0.5307692289352417, 'inf_f1': 0.4354243576526642, 'perc_f1': 0.06185567006468773, 'backg_f1': 0.0, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_sentence_prio_total_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 0.53515625, 'acc': 0.7676311135292053, 'total_f1': 0.6815365552902222}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.53515625, 'acc': 0.7676311135292053, 'total_f1': 0.6815365552902222, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_prio_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 1.1953125, 'acc': 0.6754068732261658, 'macro_f1': 0.45716944336891174, 'total_f1': 0.641697883605957, 'inf_f1': 0.760765552520752, 'perc_f1': 0.1269841343164444, 'backg_f1': 0.14432989060878754}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.1953125, 'acc': 0.6754068732261658, 'macro_f1': 0.45716944336891174, 'total_f1': 0.641697883605957, 'inf_f1': 0.760765552520752, 'perc_f1': 0.1269841343164444, 'backg_f1': 0.14432989060878754, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_prio_total_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 0.6171875, 'acc': 0.764014482498169, 'total_f1': 0.701714277267456}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.6171875, 'acc': 0.764014482498169, 'total_f1': 0.701714277267456, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_token_llama_total_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.478515625, 'acc': 0.777102530002594, 'total_f1': 0.7239619493484497}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.478515625, 'acc': 0.777102530002594, 'total_f1': 0.7239619493484497, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_token_llama_scopes_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 0.9921875, 'acc': 0.6630598902702332, 'macro_f1': 0.5665122270584106, 'total_f1': 0.7210903763771057, 'inf_f1': 0.6916230916976929, 'perc_f1': 0.4040044844150543, 'backg_f1': 0.36685454845428467}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.9921875, 'acc': 0.6630598902702332, 'macro_f1': 0.5665122270584106, 'total_f1': 0.7210903763771057, 'inf_f1': 0.6916230916976929, 'perc_f1': 0.4040044844150543, 'backg_f1': 0.36685454845428467, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_token_llama_total_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.470703125, 'acc': 0.7729819416999817, 'total_f1': 0.7468429803848267}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.470703125, 'acc': 0.7729819416999817, 'total_f1': 0.7468429803848267, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "result\n",
      "[{'loss': 1.2265625, 'acc': 0.6437612771987915, 'macro_f1': 0.4992097020149231, 'total_f1': 0.711670458316803, 'inf_f1': 0.47863247990608215, 'perc_f1': 0.35424354672431946, 'backg_f1': 0.35230353474617004, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.51953125, 'acc': 0.7622061371803284, 'total_f1': 0.7074527144432068, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.6640625, 'acc': 0.7784810066223145, 'total_f1': 0.7161065936088562, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 2.265625, 'acc': 0.5072332620620728, 'macro_f1': 0.40494704246520996, 'total_f1': 0.6166008114814758, 'inf_f1': 0.3378378450870514, 'perc_f1': 0.3232323229312897, 'backg_f1': 0.28205129504203796, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 1.7578125, 'acc': 0.6620519757270813, 'macro_f1': 0.5815226435661316, 'total_f1': 0.7361472845077515, 'inf_f1': 0.6974146366119385, 'perc_f1': 0.4614795744419098, 'backg_f1': 0.3635459840297699, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 1.25, 'acc': 0.6440873742103577, 'macro_f1': 0.5397127866744995, 'total_f1': 0.7333402633666992, 'inf_f1': 0.6392151713371277, 'perc_f1': 0.37776654958724976, 'backg_f1': 0.3408135771751404, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': 1e-05, 'dropout': 0.0}, {'loss': 1.3046875, 'acc': 0.5913200974464417, 'macro_f1': 0.3104233741760254, 'total_f1': 0.5307692289352417, 'inf_f1': 0.4354243576526642, 'perc_f1': 0.06185567006468773, 'backg_f1': 0.0, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 0.53515625, 'acc': 0.7676311135292053, 'total_f1': 0.6815365552902222, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 1.1953125, 'acc': 0.6754068732261658, 'macro_f1': 0.45716944336891174, 'total_f1': 0.641697883605957, 'inf_f1': 0.760765552520752, 'perc_f1': 0.1269841343164444, 'backg_f1': 0.14432989060878754, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.6171875, 'acc': 0.764014482498169, 'total_f1': 0.701714277267456, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 0.478515625, 'acc': 0.777102530002594, 'total_f1': 0.7239619493484497, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.9921875, 'acc': 0.6630598902702332, 'macro_f1': 0.5665122270584106, 'total_f1': 0.7210903763771057, 'inf_f1': 0.6916230916976929, 'perc_f1': 0.4040044844150543, 'backg_f1': 0.36685454845428467, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 0.470703125, 'acc': 0.7729819416999817, 'total_f1': 0.7468429803848267, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "json_data=[]\n",
    "for fp in os.listdir(file_path):\n",
    "    if fp.split('_')[1:2]==['full']:\n",
    "        print()\n",
    "        print(fp)\n",
    "        fp_path=os.path.join(file_path,fp)\n",
    "        #print(fp_path)\n",
    "        json_list=os.listdir(fp_path)\n",
    "        json_list=sorted(json_list)\n",
    "        arr=[[] for _ in range(2)]\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            if js.split('.')[0]=='model_setup':\n",
    "                arr[0].append(js)\n",
    "            elif js.split('_')[0]=='eval':\n",
    "                arr[1].append(js)\n",
    "        print(arr)# split json files 3types\n",
    "        tmp_dict={}\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            #print(os.path.join(fp_path,js))\n",
    "            json_path=os.path.join(fp_path,js)\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if js==arr[0][0]:#Metadata of model\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "                if js==arr[1][-3]:#eval_metrics\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "        print(tmp_dict)\n",
    "        json_data.append(tmp_dict)\n",
    "\n",
    "print()\n",
    "print(\"result\")\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.643761</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.762206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.507233</td>\n",
       "      <td>0.404947</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.757812</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>0.581523</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.697415</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.539713</td>\n",
       "      <td>0.733340</td>\n",
       "      <td>0.639215</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.340814</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.591320</td>\n",
       "      <td>0.310423</td>\n",
       "      <td>0.530769</td>\n",
       "      <td>0.435424</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.767631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.195312</td>\n",
       "      <td>0.675407</td>\n",
       "      <td>0.457169</td>\n",
       "      <td>0.641698</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.764014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.226562  0.643761  0.499210  0.711670  0.478632  0.354244  0.352304   \n",
       "1  0.519531  0.762206       NaN  0.707453       NaN       NaN       NaN   \n",
       "2  0.664062  0.778481       NaN  0.716107       NaN       NaN       NaN   \n",
       "3  2.265625  0.507233  0.404947  0.616601  0.337838  0.323232  0.282051   \n",
       "4  1.757812  0.662052  0.581523  0.736147  0.697415  0.461480  0.363546   \n",
       "5  1.250000  0.644087  0.539713  0.733340  0.639215  0.377767  0.340814   \n",
       "6  1.304688  0.591320  0.310423  0.530769  0.435424  0.061856  0.000000   \n",
       "7  0.535156  0.767631       NaN  0.681537       NaN       NaN       NaN   \n",
       "8  1.195312  0.675407  0.457169  0.641698  0.760766  0.126984  0.144330   \n",
       "9  0.617188  0.764014       NaN  0.701714       NaN       NaN       NaN   \n",
       "\n",
       "   full_training        segment    mode      model_name  batch_size  \\\n",
       "0           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "1           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "2           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "3           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "4           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "5           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "6           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "7           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "8           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "9           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "\n",
       "   max_epoch learning_rate  dropout  \n",
       "0         20       0.00005      0.1  \n",
       "1         20       0.00005      0.1  \n",
       "2         10         1e-04      0.0  \n",
       "3         10         1e-04      0.0  \n",
       "4         20       0.00005      0.1  \n",
       "5         10       0.00001      0.0  \n",
       "6         10         1e-04      0.0  \n",
       "7         20       0.00005      0.1  \n",
       "8         20       0.00005      0.1  \n",
       "9         10         1e-04      0.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=pd.DataFrame(json_data).replace(\"None\", np.nan)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of full training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.757812</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>0.581523</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.697415</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.663060</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>0.691623</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>0.366855</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.539713</td>\n",
       "      <td>0.733340</td>\n",
       "      <td>0.639215</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.340814</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.643761</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.195312</td>\n",
       "      <td>0.675407</td>\n",
       "      <td>0.457169</td>\n",
       "      <td>0.641698</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.507233</td>\n",
       "      <td>0.404947</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.591320</td>\n",
       "      <td>0.310423</td>\n",
       "      <td>0.530769</td>\n",
       "      <td>0.435424</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.470703</td>\n",
       "      <td>0.772982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.478516</td>\n",
       "      <td>0.777103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.723962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.757812  0.662052  0.581523  0.736147  0.697415  0.461480  0.363546   \n",
       "1  0.992188  0.663060  0.566512  0.721090  0.691623  0.404004  0.366855   \n",
       "2  1.250000  0.644087  0.539713  0.733340  0.639215  0.377767  0.340814   \n",
       "3  1.226562  0.643761  0.499210  0.711670  0.478632  0.354244  0.352304   \n",
       "4  1.195312  0.675407  0.457169  0.641698  0.760766  0.126984  0.144330   \n",
       "5  2.265625  0.507233  0.404947  0.616601  0.337838  0.323232  0.282051   \n",
       "6  1.304688  0.591320  0.310423  0.530769  0.435424  0.061856  0.000000   \n",
       "7  0.470703  0.772982       NaN  0.746843       NaN       NaN       NaN   \n",
       "8  0.478516  0.777103       NaN  0.723962       NaN       NaN       NaN   \n",
       "9  0.664062  0.778481       NaN  0.716107       NaN       NaN       NaN   \n",
       "\n",
       "   full_training        segment    mode      model_name  batch_size  \\\n",
       "0           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "1           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "2           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "3           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "4           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "5           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "6           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "7           True    token_llama   total  llm2vec_llama3           2   \n",
       "8           True    token_llama   total  llm2vec_llama3           2   \n",
       "9           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "\n",
       "   max_epoch learning_rate  dropout  \n",
       "0         20       0.00005      0.1  \n",
       "1         10         1e-04      0.0  \n",
       "2         10       0.00001      0.0  \n",
       "3         20       0.00005      0.1  \n",
       "4         20       0.00005      0.1  \n",
       "5         10         1e-04      0.0  \n",
       "6         10         1e-04      0.0  \n",
       "7         10         1e-04      0.0  \n",
       "8         20       0.00005      0.1  \n",
       "9         10         1e-04      0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=df_json.sort_values(['macro_f1','total_f1','acc'],ignore_index=True, ascending=False)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.470703</td>\n",
       "      <td>0.772982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.757812</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>0.581523</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.697415</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.539713</td>\n",
       "      <td>0.733340</td>\n",
       "      <td>0.639215</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.340814</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.478516</td>\n",
       "      <td>0.777103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.723962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.663060</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>0.691623</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>0.366855</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.643761</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.762206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.764014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.767631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  0.470703  0.772982       NaN  0.746843       NaN       NaN       NaN   \n",
       "1  1.757812  0.662052  0.581523  0.736147  0.697415  0.461480  0.363546   \n",
       "2  1.250000  0.644087  0.539713  0.733340  0.639215  0.377767  0.340814   \n",
       "3  0.478516  0.777103       NaN  0.723962       NaN       NaN       NaN   \n",
       "4  0.992188  0.663060  0.566512  0.721090  0.691623  0.404004  0.366855   \n",
       "5  0.664062  0.778481       NaN  0.716107       NaN       NaN       NaN   \n",
       "6  1.226562  0.643761  0.499210  0.711670  0.478632  0.354244  0.352304   \n",
       "7  0.519531  0.762206       NaN  0.707453       NaN       NaN       NaN   \n",
       "8  0.617188  0.764014       NaN  0.701714       NaN       NaN       NaN   \n",
       "9  0.535156  0.767631       NaN  0.681537       NaN       NaN       NaN   \n",
       "\n",
       "   full_training        segment    mode      model_name  batch_size  \\\n",
       "0           True    token_llama   total  llm2vec_llama3           2   \n",
       "1           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "2           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "3           True    token_llama   total  llm2vec_llama3           2   \n",
       "4           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "5           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "6           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "7           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "8           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "9           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "\n",
       "   max_epoch learning_rate  dropout  \n",
       "0         10         1e-04      0.0  \n",
       "1         20       0.00005      0.1  \n",
       "2         10       0.00001      0.0  \n",
       "3         20       0.00005      0.1  \n",
       "4         10         1e-04      0.0  \n",
       "5         10         1e-04      0.0  \n",
       "6         20       0.00005      0.1  \n",
       "7         20       0.00005      0.1  \n",
       "8         10         1e-04      0.0  \n",
       "9         20       0.00005      0.1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=df_json.sort_values(['total_f1','acc'],ignore_index=True, ascending=False)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for full training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loss                   0.470703\n",
       "acc                    0.772982\n",
       "macro_f1                    NaN\n",
       "total_f1               0.746843\n",
       "inf_f1                      NaN\n",
       "perc_f1                     NaN\n",
       "backg_f1                    NaN\n",
       "full_training              True\n",
       "segment             token_llama\n",
       "mode                      total\n",
       "model_name       llm2vec_llama3\n",
       "batch_size                    2\n",
       "max_epoch                    10\n",
       "learning_rate             1e-04\n",
       "dropout                     0.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best parameters for full training\")\n",
    "df_json.iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "djk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
