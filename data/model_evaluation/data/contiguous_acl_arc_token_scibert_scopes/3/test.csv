token_context,word_context,seg_context,sent_cotext,label
"['., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","[', dutch, and few other languages ( marslen -  #AUTHOR_TAG grainger, et al., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","['., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than']","['is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of english, hebrew, italian, french, dutch, and few other languages ( marslen -  #AUTHOR_TAG grainger, et al., 1991 ;  #TAUTHOR_TAG.', 'however, we do not know of any such investigations for indian languages, which are morphologically richer than many of their indo - european cousins.', 'moreover, indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'on the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent  #AUTHOR_TAG.', 'therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages']",0
"['based  #TAUTHOR_TAG c ).', 'the majority of indirect associations can be']","['based  #TAUTHOR_TAG c ).', 'the majority of indirect associations can be']","[' #TAUTHOR_TAG c ).', 'the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with']","['##we could just as easily use other symmetric "" association "" measures, such as ¢2 or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates ""  #AUTHOR_TAG.', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #TAUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",0
"[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.', ' #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"[""'  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify,']","[""confused by collocates''  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify,']","[""'  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify,']","['##we could just as easily use other symmetric "" association "" measures, such as ¢2 or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', ""models of translational equivalence that are ignorant of indirect associations have ` ` a tendency... to be confused by collocates''  #TAUTHOR_TAG."", 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",0
"['. g.  #TAUTHOR_TAG b ) ), concordan']","['machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing']","['. g.  #TAUTHOR_TAG b ) ), concordan']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['bilingual lexicography  #TAUTHOR_TAG, computer - assisted language']","['bilingual lexicography  #TAUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information']","['bilingual lexicography  #TAUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval (']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #TAUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['. g.  #TAUTHOR_TAG b ) ), concordan']","['machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing']","['. g.  #TAUTHOR_TAG b ) ), concordan']","['the past decade, researchers at ibm have devel - oped a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computation - ally expensive to apply.', 'table look - up using an ex - plicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #TAUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG, computer - assisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","['02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within']","['02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","['##we could just as easily use other symmetric "" association "" measures, such as 02  #AUTHOR_TAG or the dice coefficient  #TAUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates ""  #AUTHOR_TAG.', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.', ' #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG a ).', 'then, two word tokens (']","[' #TAUTHOR_TAG a ).', 'then, two word tokens ( u, v ) are said to co - occur in']","[' #TAUTHOR_TAG a ).', 'then, two word tokens (']","['- occurrence with the exception of  #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other  #TAUTHOR_TAG a ).', '']",0
"['- to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']","['induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards, aligned using the method in  #TAUTHOR_TAG']",5
"['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",0
"['weather reports.', ' #TAUTHOR_TAG reported that the translation lexicon']","['weather reports.', ' #TAUTHOR_TAG reported that the translation lexicon']","['english weather reports.', ' #TAUTHOR_TAG reported that the translation lexicon']","['', ' #TAUTHOR_TAG']",0
"['##s  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bite']","[' #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where']","['##s  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bite']","['- occurrence with the exception of  #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #TAUTHOR_TAG a ;  #AUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other  #AUTHOR_TAG a ).', '']",0
"['bilingual lexicography  #TAUTHOR_TAG, computerassisted language']","['bilingual lexicography  #TAUTHOR_TAG, computerassisted language']","['bilingual lexicography  #TAUTHOR_TAG, computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingu']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #AUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.  #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #TAUTHOR_TAG, computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"[""` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","[""` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy.', ""another interesting extension is to broaden the definition of a ` ` word'' to include multi - word lexical units  #TAUTHOR_TAG."", 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"[') 1, following  #TAUTHOR_TAG 2.', 'when the']","[') 1, following  #TAUTHOR_TAG 2.', ""when the l ( u, v ) are re - estimated, the model's hidden parameters come into play""]","[') 1, following  #TAUTHOR_TAG 2.', 'when the']","['translation model consists of the hidden parameters a + and a -, and likelihood ratios l ( u, v ).', 'the two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'l ( u, v ) represents the likelihood that u and v can be mutual translations.', 'for each co - occurring pair of word types u and v, these likelihoods are initially set proportional to their co - occurrence frequency ( a, v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1, following  #TAUTHOR_TAG 2.', ""when the l ( u, v ) are re - estimated, the model's hidden parameters come into play""]",5
"['##s  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bite']","[' #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where']","['##s  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bite']","['the exception of  #AUTHOR_TAG b ), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'a bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'word co - occurrence can be defined in various ways.', 'the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments si and ti are translations of each other  #AUTHOR_TAG a ).', '']",0
"['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"['has no local maxima.', 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence']","['has no local maxima.', 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence']","['- to - word model is that their objective function has no local maxima.', 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence']","[""advantage that brown et al.'s model i has over our word - to - word model is that their objective function has no local maxima."", 'by using the em algorithm  #TAUTHOR_TAG, they can guarantee convergence towards the globally optimum parameter set.', 'in contrast, the dynamic nature of the competitive linking algorithm changes the pr ( datalmodel ) in a non - monotonic fashion.', 'we have adopted the simple heuristic that the model "" has converged "" when this probability stops increasing']",0
"['link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","['##ed link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account']","['better accuracy can be achieved with a more fine - grained link class structure.', 'promising features for classification include part of speech, frequency of co - occurrence, relative word position, and translational entropy  #TAUTHOR_TAG.', 'another interesting extension is to broaden the definition of a "" word "" to include multi - word lexical units  #AUTHOR_TAG.', 'if such units can be identified a priori, their translations can be estimated without modifying the word - to - word model.', 'in this manner, the model can account for a wider range of translation phenomena']",3
"['- to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']","['the basic word - to - word model, the hidden parameters a + and a - depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words  #TAUTHOR_TAG']",0
"['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain']","['negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are']","['. discard all likelihood scores for word types deemed unlikely to be mutual translations, i. e. all l ( u, v ) < 1.', 'this step significantly reduces the computational burden of the algorithm.', 'it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values  #TAUTHOR_TAG.', 'to retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly']",1
"['02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","['02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within']","['02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG.', 'co - occur is called a direct association.', 'now, suppose that uk and uk +']","[""##we could just as easily use other symmetric ` ` association'' measures, such as 02  #TAUTHOR_TAG or the dice coefficient  #AUTHOR_TAG."", 'co - occur is called a direct association.', 'now, suppose that uk and uk + z often co - occur within their language.', 'then vk and uk + l will also co - occur more often than expected by chance.', 'the arrow connecting vk and u ~ + l in figure 1 represents an indirect association, since the association between vk and uk + z arises only by virtue of the association between each of them and uk.', 'models of translational equivalence that are ignorant of indirect associations have "" a tendency... to be confused by collocates ""  #AUTHOR_TAG.', 'fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based  #AUTHOR_TAG c ).', ""the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens ui in one half of the bitext co - occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood l ( u, v ) of translational equivalence is highest."", 'the competitive linking algorithm implements this heuristic']",1
"[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","[' #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to']","['the past decade, researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation  #TAUTHOR_TAG a ).', 'however, the ibm models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications, including "" crummy "" mt on the world wide web ( church & i -  #AUTHOR_TAG, certain machine - assisted translation tools ( e. g.', ' #AUTHOR_TAG b ) ), concordancing for bilingual lexicography  #AUTHOR_TAG computerassisted language learning, corpus linguistics ( melby. 1981 ), and cross - lingual information retrieval ( oard &  #AUTHOR_TAG']",0
"['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['that has only one correct translation.', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']","['', 'this imbalance foils thresholding strategies, clever as they might be  #TAUTHOR_TAG.', 'the likelihoods in the word - to - word model remain unnormalized, so they do not compete']",0
"['models  #TAUTHOR_TAG b ).', 'for']","['models  #TAUTHOR_TAG b ).', 'for']","['models  #TAUTHOR_TAG b ).', 'for']","['account for this difference, we can estimate separate values of x + and a - for different ranges of n ( u, v ).', 'similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'just as easily, we can model links that coincide with entries in a pre - existing translation lexicon separately from those that do not.', 'this method of incorporating dictionary information seems simpler than the method proposed by brown et al. for their models  #TAUTHOR_TAG b ).', 'for their models  #AUTHOR_TAG b ).', 'when the hidden parameters are conditioned on different link classes, the estimation method does not change ; it is just repeated for each link class']",1
"['by  #TAUTHOR_TAG, who trained brown et']","['by  #TAUTHOR_TAG, who trained brown et']","['detailed evaluation of link tokens to date was performed by  #TAUTHOR_TAG, who trained brown et']","['', 'for many applications, this is the desired behavior.', ""the most detailed evaluation of link tokens to date was performed by  #TAUTHOR_TAG, who trained brown et al.'s model 2 on 74 million words of the canadian hansards."", 'these authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'we generated links in the same 51 sentences using our two - class word - to - word model, and manually evaluated the content - word links from both models.', 'the ibm models are directional ; i. e. they posit the english words that gave rise to each french word, but ignore the distribution of the english words.', 'therefore, we ignored english words that were linked to nothing']",1
"['in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the']","['in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the']","['in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the performance of samt system is similar with the method of labeling scfg rules with pos tags.', 'thus, to be convenient, we only']","['the u - trees, we run the gibbs sampler for 1000 iterations on the whole corpus.', 'the sampler uses 1, 087s per iteration, on average, using a single core, 2. 3 ghz intel xeon machine.', 'for the hyperparameters, we set i to 0. 1 and p expand = 1 / 3 to give a preference to the rules with small fragments.', 'we built an s2t translation system with the achieved u - trees after the 1000th iteration.', 'we only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 from  #TAUTHOR_TAG, we find that the performance of samt system is similar with the method of labeling scfg rules with pos tags.', 'thus, to be convenient, we only conduct experiments with the samt system']",4
"['', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring']","['of tree structures in our sampler.', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring']","['', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and']","['', 'in addition, it should be noted that the word alignment is fixed 8, and we only explore the entire space of tree structures in our sampler.', 'differently,  #TAUTHOR_TAG designed a sampler to infer an stsg by fixing the tree structure and exploring the space of alignment.', 'we believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'this subject will be one of our future work topics']",4
"['system  #TAUTHOR_TAG.', ' #AUTHOR_TAG employed a bayesian method']","['system  #TAUTHOR_TAG.', ' #AUTHOR_TAG employed a bayesian method']","['hierarchical phrase - based system  #TAUTHOR_TAG.', ' #AUTHOR_TAG employed a bayesian method']",[' #TAUTHOR_TAG'],1
"['encoded into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods']","['encoded into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods']","['into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods']","['unsupervised tree structure induction, de  #AUTHOR_TAG adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work  #AUTHOR_TAG designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', ' #TAUTHOR_TAG, 2009, 2010 ) utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', '']",1
"[').', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","[').', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']",5
[' #TAUTHOR_TAG designed an embased method'],[' #TAUTHOR_TAG designed an embased method'],"['syntactic pre - reordering.', 'our previous work  #TAUTHOR_TAG designed an embased method']","['unsupervised tree structure induction, de  #AUTHOR_TAG adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre - reordering.', 'our previous work  #TAUTHOR_TAG designed an embased method to construct unsupervised trees for tree - based translation models.', 'this work differs from the above work in that we design a novel bayesian model to induce unsupervised u - trees, and prior knowledge can be encoded into the model more freely and effectively.', ' #AUTHOR_TAG utilized bayesian methods to learn synchronous context free grammars ( scfg ) from a parallel corpus.', '']",1
[' #TAUTHOR_TAG and use the resulting binary parse trees to'],[' #TAUTHOR_TAG and use the resulting binary parse trees to'],"['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser  #AUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #TAUTHOR_TAG and use the resulting binary parse trees to']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser  #AUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #TAUTHOR_TAG and use the resulting binary parse trees to build another s2t system']",5
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']",0
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #AUTHOR_TAG 2006 ;  #TAUTHOR_TAG b )']",0
"['', ' #TAUTHOR_TAG adopted a bayesian method']","['alignment.', ' #TAUTHOR_TAG adopted a bayesian method']","['', ' #TAUTHOR_TAG adopted a bayesian method']","['', ' #TAUTHOR_TAG adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '']",1
"['source string str in the model, we follow  #TAUTHOR_TAG']","['source string str in the model, we follow  #TAUTHOR_TAG']","['tree fragment frag and a source string str in the model, we follow  #TAUTHOR_TAG']","['base distribution 0 ( | ) p r n is designed to assign prior probabilities to the stsg production rules.', 'because each rule r consists of a target tree fragment frag and a source string str in the model, we follow  #TAUTHOR_TAG and decompose the prior probability p0 ( r | n ) into two factors as follows']",5
"['', ' #TAUTHOR_TAG further labeled']","['by extended syntactic categories.', ' #TAUTHOR_TAG further labeled']","['by extended syntactic categories.', ' #TAUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs']","['', ' #AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', ' #AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', ' #TAUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['nodes  #TAUTHOR_TAG.', '']","['nodes  #TAUTHOR_TAG.', '']","['of frontier nodes  #TAUTHOR_TAG.', 'frontier nodes are the tree nodes that can map onto contiguous substrings on']","['the initial target u - trees, source sentences and word alignment, we extract minimal ghkm translation rules7 in terms of frontier nodes  #TAUTHOR_TAG.', 'frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', '']",5
"['', ' #TAUTHOR_TAG']","['on parse trees.', ' #TAUTHOR_TAG']","['on parse trees.', ' #TAUTHOR_TAG']","['', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', ' #AUTHOR_TAG and  #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', ' #TAUTHOR_TAG']",1
"['- 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']","['by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']","['by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']","['experiments are conducted on chinese - to - english translation.', 'the training data are the fbis corpus with approximately 7. 1 million chinese words and 9. 2 million english words.', 'we obtain the bidirectional word alignment with giza + +, and then adopt the grow - diag - final - and strategy to obtain the final symmetric alignment.', 'we train a 5gram language model on the xinhua portion of the english gigaword corpus and the english part of the training data.', 'for tuning and testing, we use the nist mt 2003 evaluation data as the development set, and use the nist mt04 and mt05 data as the test set.', 'we use mert  #AUTHOR_TAG to tune parameters.', 'since mert is prone to search errors, we run mert 5 times and select the best tuning parameters in the tuning set.', 'the translation quality is evaluated by case - insensitive bleu - 4 with the shortest length penalty.', 'the statistical significance test is performed by the re - sampling approach  #TAUTHOR_TAG']",5
"[').', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['( abbreviated as s2t ).', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","[').', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on  #TAUTHOR_TAG and ( marcu et al. 2006 ).', 'in the system, we extract both the minimal ghkm rules  #AUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']",5
"['system  #AUTHOR_TAG.', ' #TAUTHOR_TAG employed a bayesian method']","['system  #AUTHOR_TAG.', ' #TAUTHOR_TAG employed a bayesian method']","['hierarchical phrase - based system  #AUTHOR_TAG.', ' #TAUTHOR_TAG employed a bayesian method']","['', ' #TAUTHOR_TAG employed a bayesian method to learn discontinuous scfg rules.', '']",1
"['.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint']","['models than scfg.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint']","['.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank']","['', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', ' #TAUTHOR_TAG and  #AUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '']",1
"['', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce']","['in future.', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce']","['', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce the complexity of the sampler']","['', 'our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 we only use the minimal ghkm rules  #TAUTHOR_TAG here to reduce the complexity of the sampler']",5
"[').', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","[').', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']","['translation system used for testing the effectiveness of our u - trees is our in - house stringto - tree system ( abbreviated as s2t ).', 'the system is implemented based on  #AUTHOR_TAG and ).', 'in the system, we extract both the minimal ghkm rules  #TAUTHOR_TAG, and the rules of spmt model 1  #AUTHOR_TAG with phrases up to length l = 5 on the source side']",5
"['algorithm  #TAUTHOR_TAG, we']","['would define two different u - trees.', 'using the ghkm algorithm  #TAUTHOR_TAG, we']","['would define two different u - trees.', 'using the ghkm algorithm  #TAUTHOR_TAG,']","['', 'otherwise, we change its state to the right state o, and transform the u - tree to figure 3 obviously, towards an s - node for sampling, the two values of o would define two different u - trees.', 'using the ghkm algorithm  #TAUTHOR_TAG, we can get two different stsg derivations from the two u - trees based on the fixed word alignment.', 'each derivation carries a set of stsg rules ( i. e., minimal ghkm translation rules ) of its own.', '']",5
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG, 2006 ;  #AUTHOR_TAG b )']",0
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['recent years, tree - based translation models are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #AUTHOR_TAG 2009 ;  #TAUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']",0
"['hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']","['hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']","['hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']","['create the baseline system, we use the opensource joshua 4. 0 system  #AUTHOR_TAG to build a hierarchical phrase - based ( hpb ) system, and a syntax - augmented mt ( samt ) 11 system  #TAUTHOR_TAG respectively']",5
"['with the berkeley parser  #TAUTHOR_TAG.', 'then, we binari']","['with the berkeley parser  #TAUTHOR_TAG.', 'then, we binarize']","[', which is generated by parsing the english side of the bilingual data with the berkeley parser  #TAUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #AUTHOR_TAG and use the resulting binary parse trees to']","['build the above s2t system, we first use the parse tree, which is generated by parsing the english side of the bilingual data with the berkeley parser  #TAUTHOR_TAG.', 'then, we binarize the english parse trees using the head binarization approach  #AUTHOR_TAG and use the resulting binary parse trees to build another s2t system']",5
['the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )'],['the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )'],['the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )'],"['the probability of producing the target tree fragment frag.', 'to generate frag, used a geometric prior to decide how many child nodes to assign each node.', 'differently, we require that each multi - word non - terminal node must have two child nodes.', 'this is because the binary structure has been verified to be very effective for tree - based translation  #TAUTHOR_TAG a )']",4
"['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']","['recent years, tree - based translation models1 are drawing more and more attention in the community of statistical machine translation ( smt ).', 'due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree - based translation models have shown promising progress in improving translation quality  #TAUTHOR_TAG, 2009 ;  #AUTHOR_TAG 2006 ;  #AUTHOR_TAG b )']",0
['training tree - based translation models  #TAUTHOR_TAG'],['training tree - based translation models  #TAUTHOR_TAG'],['training tree - based translation models  #TAUTHOR_TAG'],"[', for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of tree - bank resources for training.', '2 ) parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'this indicates that parse trees are usually not the optimal choice for training tree - based translation models  #TAUTHOR_TAG']",0
"['.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint']","['models than scfg.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint']","['.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank']","['', 'our u - trees are learned based on stsg, which is more appropriate for tree - based translation models than scfg.', ' #AUTHOR_TAG and  #TAUTHOR_TAG focused on joint parsing and alignment.', 'they utilized the bilingual tree - bank to train a joint model for both parsing and word alignment.', 'adopted a bayesian method to infer an stsg by exploring the space of alignments based on parse trees.', '']",1
"['', ' #TAUTHOR_TAG utilized a transformation - based method']","['alignment.', ' #TAUTHOR_TAG utilized a transformation - based method']","['', ' #TAUTHOR_TAG utilized a transformation - based method']","['', ' #TAUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', ' #AUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', ' #AUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
"['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']","['( str | frag ) in equation ( 4 ) is the probability of generating the source string, which contains several source words and variables.', 'inspired by  #TAUTHOR_TAG and  #AUTHOR_TAG, we define p ( str | frag ) as follows : where csw is the number of words in the source string']",4
"['grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scf']","['the heuristic scfg grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']","['the heuristic scfg grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree']","['addition, we find that the bayesian scfg grammar can not even significantly outperform the heuristic scfg grammar  #TAUTHOR_TAG 5.', 'this would indicate that the scfg - based derivation tree as by - product is also not such good for tree - based translation models.', 'considering the above reasons, we believe that the stsg - based learning procedure would result in a better translation grammar for tree - based models']",1
"['', ' #TAUTHOR_TAG substituted']","['models.', ' #TAUTHOR_TAG substituted']","['', ' #TAUTHOR_TAG substituted the non - terminal x in']","['', ' #AUTHOR_TAG utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation.', 'compared to their work, we do not rely on any tree - bank resources and focus on generating effective unsupervised tree structures for tree - based translation models.', ' #TAUTHOR_TAG substituted the non - terminal x in hierarchical phrase - based model by extended syntactic categories.', ' #AUTHOR_TAG further labeled the scfg rules with pos tags and unsupervised word classes.', 'our work differs from theirs in that we present a bayesian model to learn effective stsg translation rules and u - tree structures for tree - based translation models, rather than designing a labeling strategy for translation rules']",1
[' #TAUTHOR_TAG using'],[' #TAUTHOR_TAG using'],['in the forward search  #TAUTHOR_TAG using'],"['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti ""  #AUTHOR_TAG, or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan  #AUTHOR_TAG.', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search  #TAUTHOR_TAG using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', '']",0
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo cre']","['generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no']","['generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances']","['have been several efforts aimed at developing a domain - independent method for generating responses from a frame representation of user requests  #TAUTHOR_TAG.', 'incorporating such techniques would deo crease the system developer workload.', 'however, there has been no work on domain - independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which wit handles well.', 'therefore incorporating those techniques remains as a future work']",3
"['current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']","['current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']","['based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']","['defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'they can also shift the dialogue 2the notion of the initiative in this paper is different from that of the dialogue initiative of  #TAUTHOR_TAG']",1
"['acoustical society of japan  #TAUTHOR_TAG.', '']","['acoustical society of japan  #TAUTHOR_TAG.', '']","['the acoustical society of japan  #TAUTHOR_TAG.', '']","['hypotheses.', 'as the recogn / fion engine, either voicerex, developed by nti ""  #AUTHOR_TAG, or htk from entropic research can be used.', 'acoustic models for htk is trained with the continuous speech database of the acoustical society of japan  #TAUTHOR_TAG.', 'this recognizer incrementally outputs word hypotheses as soon as they are found in the best - scored path in the forward search  #AUTHOR_TAG using the istar ( incremental structure transmitter and receiver ) protocol, which conveys word graph information as well as word hypotheses.', 'this incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real - time responses are possible.', 'this module continuously runs and outputs recognition results when it detects a speech interval.', 'this enables the language generation module to react immediately to user interruptions while the system is speaking.', 'the language model for speech recognition is a network ( regular ) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'a phrase is a sequence of words, which is to be defined in a domain - dependent way.', 'sentences can be decomposed into a couple of phrases.', 'the reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self - repairs and repetition.', 'in japanese, bunsetsu is appropriate for defining phrases.', 'a bunsetsu consists of one content word and a number ( possibly zero ) of function words.', 'in the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation']",5
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['system  #TAUTHOR_TAG b ),']","['system  #TAUTHOR_TAG b ),']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #TAUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #AUTHOR_TAG.', 'the meeting room reservation system has vocabulary']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #TAUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #AUTHOR_TAG.', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is']","['recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems  #TAUTHOR_TAG.', 'one of the next research goals is to make these systems task - portable, that is, to simplify the process of porting to another task domain']",0
"['grammar  #TAUTHOR_TAG.', 'the language generation']","['of unification grammar  #TAUTHOR_TAG.', 'the language generation']","['with in the framework of unification grammar  #TAUTHOR_TAG.', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning  #AUTHOR_TAG.', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', 'states can be represented by dialogue phases in wit']","['previous finite - state - model - based toolkits place many severe restrictions on domain descriptions, wit has enough descriptive power to build a variety of dialogue systems.', 'although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'for example, it is possible to represent a discourse stack whose depth is limited.', 'recording some dialogue history is also possible.', 'since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'for example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar  #TAUTHOR_TAG.', 'the language generation module features common lisp functions, so there is no limitation on the description.', 'some of the systems we have developed feature a generation method based on hierarchical planning  #AUTHOR_TAG.', 'it is also possible to build a simple finite - state - model - based dialogue system using wit.', 'states can be represented by dialogue phases in wit']",3
"[', utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', 'thus']","[', utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', 'thus']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', '']","['the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking  #TAUTHOR_TAG a ).', 'thus the system can respond immediately after user pauses when the user has the initiative.', 'when the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way  #AUTHOR_TAG']",0
"['system  #TAUTHOR_TAG.', 'the meeting room reservation']","['system  #TAUTHOR_TAG.', 'the meeting room reservation']","[', and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #AUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #TAUTHOR_TAG.', 'the meeting room reservation system has vocabulary']","['has been implemented in common lisp and c on unix, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system  #AUTHOR_TAG b ), a video - recording programming system, a schedule management system  #AUTHOR_TAG a ), and a weather infomiation system  #TAUTHOR_TAG.', 'the meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'a sample dialogue between this system and a naive user is shown in figure 2.', 'this system employs htk as the speech recognition engine.', ""the weather information system can answer the user's questions about weather forecasts in japan."", 'the vocabulary size is around 500, and the number of phrase structure rules is 31.', 'the number of attributes in the semantic flame is 11, and the number of the files of the pre - recorded speech is about 13, 000']",2
"['generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that']","['generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that']","['generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that']","['paper presents wit 1, which is a toolkit iwit is an acronym of workable spoken dialogue lnter - for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'wit features an incremental understanding method  #TAUTHOR_TAG b ) that makes it possible to build a robust and real - time system.', 'in addition, wit compiles domain - dependent system specifications into internal knowledge sources so that building systems is easier.', 'although wit requires more domaindependent specifications than finite - state - modelbased toolkits, wit - based systems are capable of taking full advantage of language processing technology.', 'wit has been implemented and used to build several spoken dialogue systems']",5
"[' #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase']","['consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions  #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions  #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase']","['domain - dependent knowledge used in this module consists of a unification - based lexicon and phrase structure rules.', 'disjunctive feature descriptions are also possible ; wit incorporates an efficient method for handling disjunctions  #TAUTHOR_TAG.', 'when a phrase boundary is detected, the feature structure for a phrase is computed using some built - in rules from the feature structure rules for the words in the phrase.', 'the phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""two kinds of sentenees can be considered ; domain - related ones that express the user's intention about the reser - vafion and dialogue - related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'considering the meeting room reservation system, examples of domain - related sentences are "" i need to book room 2 on wednesday "", "" i need to book room 2 "", and "" room 2 "" and dialogue - related ones are "" yes "", "" no "", and "" okay ""']",5
"['incremental understanding method  #TAUTHOR_TAG b ).', 'when the command']","['incremental understanding method  #TAUTHOR_TAG b ).', 'when the command']","['disambiguating interpretation in the incremental understanding method  #TAUTHOR_TAG b ).', 'when the command']","['roles are similar to dcg  #AUTHOR_TAG rules ; they can include logical variables and these variables can be bound when these rules are applied.', 'it is possible to add to the rules constraints that stipulate relationships that must hold among variables ( nakano, 199 i ), but we do not explain these constraints in detail in this paper.', 'the priorities are used for disambiguating interpretation in the incremental understanding method  #TAUTHOR_TAG b ).', ""when the command on the right - hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'the command is one of the following']",5
"['building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which']","['building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which']","['building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which']","['this end, several toolkits for building spoken dialogue systems have been developed  #TAUTHOR_TAG.', 'one is the cslu toolkit  #AUTHOR_TAG, which enables rapid prototyping of a spoken dialogue system that incorporates a finite - state dialogue model.', 'it decreases the amount of the effort required in building a spoken dialogue system in a user - defined task domain.', 'however, it limits system functions ; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'another is galaxy - ii ( seneffet al., 1998 ), * mikio nakano is currently a visiting scientist at mit laboratory for computer science. which enables modules in a dialogue system to communicate with each other.', 'it consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'although it requires more specifications than finitestate - model - based toolkits, it places less limitations on system functions']",0
"['sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss']","['utilizes isss ( incremental significant - utterance sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss']","[', attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss']","['language understanding : module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame ( i. e., attribute - value pairs ).', 'the understanding module utilizes isss ( incremental significant - utterance sequence search )  #TAUTHOR_TAG b ), which is an integrated parsing and discourse processing method.', 'isss enables the incremental understanding of user utterances that are not segmented into sentences prior to pars - ing by incrementally finding the most plausible sequence of sentences ( or significant utterances in the isss terms ) out of the possible sentence sequences for the input word sequence.', 'isss also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time']",5
"['.  #TAUTHOR_TAG.', 'while word sense']","['is term translation, e. g., via a bilingual lexicon.  #TAUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations']","['common approach is term translation, e. g., via a bilingual lexicon.  #TAUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations']","['common approach is term translation, e. g., via a bilingual lexicon.  #TAUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiernstra and de  #AUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']",1
"['.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat']","['whole query.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat']","['and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat different translations']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG.', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery  #AUTHOR_TAG synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
['- test  #TAUTHOR_TAG at significance'],"['a small number of queries.', 'the one - sided t - test  #TAUTHOR_TAG at significance']","['', 'the one - sided t - test  #TAUTHOR_TAG at significance level 0. 05 indicated that the improvement on tre']","['results in table 4 show that manual disambiguation improves performance by 17 % on trec5c, 4 % on trec4s, but not at all on trec6c.', 'furthermore, the improvement on trec5c appears to be caused by big improvements for a small number of queries.', 'the one - sided t - test  #TAUTHOR_TAG at significance level 0. 05 indicated that the improvement on trec5c is not statistically significant']",5
"['##sm ),  #TAUTHOR_TAG.', 'we believe our']","['general vector space model ( gvsm ),  #TAUTHOR_TAG.', 'we believe our']","['##sm ),  #TAUTHOR_TAG.', 'we believe our']","['third approach to cross - lingual retrieval is to map queries and documents to some intermediate representation, e. g latent semantic indexing ( lsi )  #AUTHOR_TAG, or the general vector space model ( gvsm ),  #TAUTHOR_TAG.', 'we believe our approach is computationally less costly than ( lsi and gvsm ) and assumes less resources ( wordnet in  #AUTHOR_TAG']",1
['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],['##¢ the transition probability a is 0. 7 using the em algorithm  #TAUTHOR_TAG on the trec4 ad - hoc query set'],5
"['##nstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['value of disambiguation for cross - lingual ir include hiernstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['##nstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']","['common approach is term translation, e. g., via a bilingual lexicon.', ' #AUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. other studies on the value of disambiguation for cross - lingual ir include hiernstra and de  #AUTHOR_TAG.', ' #TAUTHOR_TAG studied the issue of disambiguation for mono - lingual m']",0
"['incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']","['incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']","['the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']","['common approach is term translation, e. g., via a bilingual lexicon.', ' #AUTHOR_TAG.', 'while word sense disambiguation has been a central topic in previous studies for cross - lingual ir, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'other studies on the value of disambiguation for cross - lingual ir include hiemstra and de  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied the issue of disarnbiguation for mono - lingual ir']",0
['to the language of the queries  #TAUTHOR_TAG'],['to the language of the queries  #TAUTHOR_TAG'],"['to the language of the queries  #TAUTHOR_TAG.', 'for most languages, there are no mt systems at all']","['approaches to cross - lingual ir have been published.', 'one common approach is using machine translation ( mt ) to translate the queries to the language of the documents or translate documents to the language of the queries  #TAUTHOR_TAG.', 'for most languages, there are no mt systems at all.', 'our focus is on languages where no mt exists, but a bilingual dictionary may exist or may be derived']",1
"['.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations']","['whole query.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations']","['and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations']","['second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'this approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'there are several variations of such a method  #TAUTHOR_TAG ; hull 1997 ).', 'one such method is to treat different translations of the same term as synonyms.', 'ballesteros, for example, used the inquery  #AUTHOR_TAG synonym operator to group translations of different query terms.', 'however, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'by contrast, our hmm approach supports translation probabilities.', ""the synonym approach is equivalent to changing all non - zero translation probabilities p ( w ~ [ wy )'s to 1 in our retrieyal function."", 'even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations']",1
['- occur  #TAUTHOR_TAG'],['terms are more likely to co - occur  #TAUTHOR_TAG'],['- occur  #TAUTHOR_TAG'],"['', 'since all terms are treated as equal in the translated query, this gives terms with more translations ( potentially the more common terms ) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'that is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', 'however, the second document is more likely to be relevant since correct translations of the query terms are more likely to co - occur  #TAUTHOR_TAG']",0
"['.', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the']","['translations on average.', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the']","['.', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the treatment of chinese is to include the english word as one of its own translations in addition to its spanish translations in the lexicon.', 'this is useful']","['', 'a cooccurrence based stemmer  #TAUTHOR_TAG was used to stem spanish words.', 'one difference from the treatment of chinese is to include the english word as one of its own translations in addition to its spanish translations in the lexicon.', 'this is useful for translating proper nouns, which often have identical spellings in english and spanish but are routinely excluded from a lexicon']",5
"['generation process include  #TAUTHOR_TAG.', 'our']","['generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our work has focused on cross - lingual retrieval']",1
"['generation process include  #TAUTHOR_TAG.', 'our']","['generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our']","['studies which view lr as a query generation process include  #TAUTHOR_TAG.', 'our work has focused on cross - lingual retrieval']",1
"['of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement']","['technique is automatic discovery of translations from parallel or non - parallel corpora  #TAUTHOR_TAG.', 'since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus - specific vocabularies']",0
"[' #TAUTHOR_TAG, the ir']","[' #TAUTHOR_TAG, the ir']","[' #TAUTHOR_TAG, the ir system ranks']","[' #TAUTHOR_TAG, the ir system ranks documents according to the probability that a document d is relevant given the query q, p ( d is r iq ).', 'using bayes rule, and the fact that p ( q ) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to p ( q [ d is r ) is the same as ranking them according to p ( d is riq ).', 'the approach therefore estimates the probability that a query q is generated, given the document d is relevant.', '( a glossary of symbols used appears below.']",5
"['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']",0
"['- 2000 ( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse']","['( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse']","['- 2000 ( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of']","['first is the one used in the chunking competition in conll - 2000 ( tjong kim  #TAUTHOR_TAG.', 'in this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'the goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'the chunk types are based on the syntactic category part of the bracket label in the treebank.', 'roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'the phrases are : adjective phrase ( adjp ), adverb phrase ( advp ), conjunction phrase ( conjp ), interjection phrase ( intj ), list marker ( lst ), noun phrase ( np ), preposition phrase ( pp ), particle ( prt ), subordinated clause ( sbar ), unlike coordinated phrase ( ucp ), verb phrase ( vp ).', '( see details in  #AUTHOR_TAG.']",5
"['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abney\'s work  #AUTHOR_TAG, who has suggested to "" chunk "" sentences to base level phrases.', '']",0
"['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been']","['of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']",0
['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG']",0
"['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5  #AUTHOR_TAG']",5
['of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG'],['of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG'],['of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG'],"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG']",0
"['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
"['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abney\'s work  #AUTHOR_TAG, who has suggested to "" chunk "" sentences to base level phrases.', '']",0
"['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['of the syntactic structures in a text  #TAUTHOR_TAG.', 'a']","['- sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot']","['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #TAUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abney\'s work  #AUTHOR_TAG, who has suggested to "" chunk "" sentences to base level phrases.', '']",0
['and text summarization  #TAUTHOR_TAG'],['and text summarization  #TAUTHOR_TAG'],"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
['##ong kim  #TAUTHOR_TAG to compare'],['under the exact conditions of conll - 2000 ( tjong kim  #TAUTHOR_TAG to compare'],['##ong kim  #TAUTHOR_TAG to compare'],"['earlier versions of the snow based cscl were used only to identify single phrases  #AUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000 ( tjong kim  #TAUTHOR_TAG to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],['of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG'],"[', the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'however, since work in this direction has started, a significant progress has also been made in the research on statistical learning of full parsers, both in terms of accuracy and processing time  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG']",0
"['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that']","['2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that']","['2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that']","['start by reporting the results in which we compare the full parser and the shallow parser on the "" clean "" wsj data.', 'table 2 shows the results on identifying all phrases - - chunking in conll2000 ( tjong kim  #TAUTHOR_TAG terminology.', 'the results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'next, we compared the performance of the parsers on the task of identifying atomic phrases 2.', 'here, again, the shallow parser exhibits significantly better performance.', 'table 3 shows the results of extracting atomic phrases']",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],"['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained']","['earlier versions of the snow based cscl were used only to identify single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000  #AUTHOR_TAG to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest'],['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest'],['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest'],"['on shallow parsing was inspired by psycholinguistics arguments  #TAUTHOR_TAG that suggest that in many scenarios ( e. g., conversational ) full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint']",0
"['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', '']","['michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents']","['perform our comparison using two state - ofthe - art parsers.', 'for the full parser, we use the one developed by michael collins  #TAUTHOR_TAG - - one of the most accurate full parsers around.', 'it represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'after that, it will choose the candidate parse tree with the highest probability as output.', 'the experiments use the version that was trained ( by collins ) on sections 02 - 21 of the penn treebank.', 'the reported results for the full parse tree ( on section 23 ) are recall / precision of 88. 1 / 87. 5  #AUTHOR_TAG']",5
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
"['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #TAUTHOR_TAG.', 'snow  #AUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
"['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier']","['shallow parser used is the snow - based cscl parser  #AUTHOR_TAG.', 'snow  #TAUTHOR_TAG is a multi - class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large, of which nlp is a principal example.', 'it works by learning a sparse network of linear functions over a pre - defined or incrementally learned feature space.', 'typically, snow is used as a classifier, and predicts using a winner - take - all mechanism over the activation value of the target classes.', 'however, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'indeed, in cscl ( constraint satisfaction with classifiers ), snow is used to learn several different classifiers - each detects the beginning or end of a phrase of some type ( noun phrase, verb phrase, etc. ).', ""the outcomes of these classifiers are then combined in a way that satisfies some constraints - non - overlapping constraints in this case - using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes""]",5
['as follows ( tjong kim  #TAUTHOR_TAG :'],['as follows ( tjong kim  #TAUTHOR_TAG :'],['as follows ( tjong kim  #TAUTHOR_TAG :'],"['parsing is studied as an alternative to full - sentence parsing.', 'rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text  #AUTHOR_TAG.', 'a lot of recent work on shallow parsing has been influenced by abneys work  #AUTHOR_TAG, who has suggested to chunk sentences to base level phrases.', 'for example, the sentence he reckons the current account deficit will narrow to only $ 1. 8 billion in september.', 'would be chunked as follows ( tjong kim  #TAUTHOR_TAG : [ np he ] [ vp reckons ] [ np the current account deficit ] [ vp will narrow ] [ pp to ] [ np only $ 1. 8 billion ] [ pp in ] [ np september ]']",0
['and text summarization  #TAUTHOR_TAG'],['and text summarization  #TAUTHOR_TAG'],"[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to']","[', it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( nps ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization  #TAUTHOR_TAG.', 'second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'if all that is available is a collection of sentences annotated for nps, it can be used to produce this level of analysis.', 'this can be augmented later if more information is available.', 'finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low - sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes']",0
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],['on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG'],"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #TAUTHOR_TAG']",0
"['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been']","['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been']","['of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']","['', 'would be chunked as follows  #AUTHOR_TAG : while earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information - by examining the pattern itself, its nearby context and the local part - of - speech information.', 'thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship  #AUTHOR_TAG']",0
['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at'],"['single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained']","['earlier versions of the snow based cscl were used only to identify single phrases  #TAUTHOR_TAG and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of conll - 2000  #AUTHOR_TAG to compare it to other shallow parsers.', 'table 1 shows that it ranks among the top shallow parsers evaluated there 1']",2
"[' #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink']","['was done on the penn treebank  #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for']","['was done on the penn treebank  #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for']","['was done on the penn treebank  #TAUTHOR_TAG wall street journal data, sections 02 - 21.', 'to train the cscl shallow parser we had first to convert the wsj data to a flat format that directly provides the phrase annotations.', 'this is done using the "" chunklink "" program provided for conll - 2000 ( tjong kim sang and  #AUTHOR_TAG']",5
"['##ag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of']","['ltag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of']","['##ag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts']","['##eisi et al. also translated ltag into hpsg  #TAUTHOR_TAG.', 'however, their method depended on translators intuitive analy - sis of the original grammar.', 'thus the transla - tion was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equiva - lence between the original and obtained gram - mars.', 'other works  #AUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater ex - pressive power of hpsg, it is impossible to con - vert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capac - ity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad - vantages']",1
"['models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['have been many studies on parsing techniques  #AUTHOR_TAG, ones on disambiguation models  #TAUTHOR_TAG, and ones on programming / grammar - development environ -  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['ltag parser  #TAUTHOR_TAG.', 'this']","['ltag parser  #TAUTHOR_TAG.', 'this']","['the xtag english grammar ( the xtag  #AUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser  #TAUTHOR_TAG.', 'this result implies that parsing techniques']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag  #AUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser  #TAUTHOR_TAG.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",1
"['parser  #TAUTHOR_TAG, c + + implementation of the two - phase']","['hpsg parser  #TAUTHOR_TAG, c + + implementation of the two - phase']","['feature unification ( phase 2 ).', 'tnt refers to the hpsg parser  #TAUTHOR_TAG, c + + implementation of the two - phase parsing algorithm']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van  #AUTHOR_TAG without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser  #TAUTHOR_TAG, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', '']",1
"['( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically']","['( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically']","['head - driven phrase structure grammar ( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 ) ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #TAUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
['##ag  #TAUTHOR_TAG is an extension of the ltag formalism'],"['symbol u ( figure 4 ).', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism']","['##ag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag,']","['', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"[' #TAUTHOR_TAG.', 'these works are']","[' #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']","[' #TAUTHOR_TAG.', 'these works are']","[' #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the rela - tion between them is not well discussed.', 'investi - gating the relation will be apparently valuable for both communities']",0
['##1 )  #TAUTHOR_TAG'],[')  #TAUTHOR_TAG'],['##1 )  #TAUTHOR_TAG'],"['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 )  #TAUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],['##ag  #TAUTHOR_TAG is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - figure 6 : parsing with an hpsg'],0
"['verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high -']","['verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['in the verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #AUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #TAUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']",0
"['verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #AUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #TAUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']",0
"['', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', '']","['obscures the equivalence between the original and obtained grammars.', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', 'thus, the']","['', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', '']","['', 'the derivation translator module takes hpsg parse  #AUTHOR_TAG.', ""however, their method depended on translator's intuitive analysis of the original grammar."", 'thus the translation was manual and grammar dependent.', 'the manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'other works  #TAUTHOR_TAG convert hpsg grammars into ltag grammars.', 'however, given the greater expressive power of hpsg, it is impossible to convert an arbitrary hpsg grammar into an ltag grammar.', 'therefore, a conversion from hpsg into ltag often requires some restrictions on the hpsg grammar to suppress its generative capacity.', 'thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages']",1
['japanese dependency analyzer  #TAUTHOR_TAG'],"['verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #TAUTHOR_TAG']","['in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #TAUTHOR_TAG']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #AUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #TAUTHOR_TAG']",0
"[', each of which is described with typed feature structures  #TAUTHOR_TAG.', 'a']","[', each of which is described with typed feature structures  #TAUTHOR_TAG.', 'a']","['and id grammar rules, each of which is described with typed feature structures  #TAUTHOR_TAG.', '']","['hpsg grammar consists of lexical entries and id grammar rules, each of which is described with typed feature structures  #TAUTHOR_TAG.', 'a lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'an id grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'figure 6 illustrates an example of bottom - up parsing with an hpsg grammar.', 'first, lexical entries for "" can "" and "" run "" are unified respectively with the daughter feature structures']",0
"['xtag research  #TAUTHOR_TAG, which is a large -']","['xtag research  #TAUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg']","['the xtag english grammar ( the xtag research  #TAUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we apply our system to the latest version of the xtag english grammar ( the xtag research  #TAUTHOR_TAG, which is a large - scale fb - ltag grammar.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['', 'the xtag group  #TAUTHOR_TAG']","['', 'the xtag group  #TAUTHOR_TAG']","['', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #TAUTHOR_TAG']","['', 'fb - ltag ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #TAUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['hpsg - style grammar  #TAUTHOR_TAG.', 'strong']","['hpsg - style grammar  #TAUTHOR_TAG.', 'strong']","['a strongly equivalent hpsg - style grammar  #TAUTHOR_TAG.', 'strong equivalence means that both grammars']","['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoin - ing grammar ( fb - ltag 1 ) ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar  #TAUTHOR_TAG.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase']","['ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase parsing algorithm']","['2 shows the average parsing time with the ltag and hpsg parsers.', 'in table 2, lem refers to the ltag parser  #TAUTHOR_TAG, ansi c implementation of the two - phase parsing algorithm that performs the head corner parsing ( van  #AUTHOR_TAG without features ( phase 1 ), and then executes feature unification ( phase 2 ).', 'tnt refers to the hpsg parser, c + + implementation of the two - phase parsing algorithm that performs filtering with a compiled cfg ( phase 1 ) and then executes feature unification ( phase 2 ).', '']",1
['##ag  #TAUTHOR_TAG is an extension of the ltag formalism'],"['symbol u ( figure 4 ).', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism']","['##ag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag,']","['', 'fbltag  #TAUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag  #AUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"[') project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed']","['lingo ) project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed']","[') project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed']","['are a variety of works on efficient parsing with hpsg, which allow the use of hpsgbased processing in practical application contexts.', 'stanford university is developing the english resource grammar, an hpsg grammar for english, as a part of the linguistic grammars online ( lingo ) project  #TAUTHOR_TAG.', 'in practical context, german, english, and japanese hpsg - based grammars are developed and used in the verbmobil project  #AUTHOR_TAG.', 'our group has developed a wide - coverage hpsg grammar for japanese  #AUTHOR_TAG, which is used in a high - accuracy japanese dependency analyzer  #AUTHOR_TAG']",0
"['models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #AUTHOR_TAG, ones on disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to']","['processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to']","['processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly']","['rental system is implemented in lil - fes  #AUTHOR_TAG 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system  #TAUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #AUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",0
"['share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']","['share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']","['share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']","['', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing.', 'another paper  #TAUTHOR_TAG describes the detailed analysis on the factor of the difference of parsing performance']",0
['##7 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6'],"['number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",['##7 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6'],"['rental system is implemented in lil - fes  #AUTHOR_TAG 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system  #AUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #TAUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
"[' #TAUTHOR_TAG 2.', '']","[' #TAUTHOR_TAG 2.', '']","['##lfes  #TAUTHOR_TAG 2.', '']","['rental system is implemented in lilfes  #TAUTHOR_TAG 2.', 'lilfes is one of the fastest inference engines for processing feature structure logic, and efficient hpsg parsers have already been built on this system  #AUTHOR_TAG.', 'we applied our system to the xtag english grammar ( the xtag  #AUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'the original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the atis corpus  #AUTHOR_TAG 6 ( the average length is 6. 32 words ).', 'this result empirically attested the strong equivalence of our algorithm']",5
['##1 )  #TAUTHOR_TAG'],[')  #TAUTHOR_TAG'],['##1 )  #TAUTHOR_TAG'],"['paper describes an approach for sharing resources in various grammar formalisms such as feature - based lexicalized tree adjoining grammar ( fb - ltag1 )  #TAUTHOR_TAG and head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG by a method of grammar conversion.', 'the rental system automatically converts an fb - ltag grammar into a strongly equivalent hpsg - style grammar.', 'strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the ltag grammars and lexicons in hpsg applications.', 'our system can reduce considerable workload to develop a huge resource ( grammars and lexicons ) from scratch']",0
"['models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #AUTHOR_TAG, ones on disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development  #TAUTHOR_TAG.', 'these works are re - stricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
"['formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG,']","['formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG,']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG,']","['concern is, however, not limited to the sharing of grammars and lexicons.', 'strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'there have been many studies on parsing techniques  #TAUTHOR_TAG, ones on disambiguation models  #AUTHOR_TAG, and ones on programming / grammar - development environment  #AUTHOR_TAG.', 'these works are restricted to each closed community, and the relation between them is not well discussed.', 'investigating the relation will be apparently valuable for both communities']",0
['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],['grammar conversion from ltag to hpsg  #TAUTHOR_TAG is the core portion of the rental system'],0
"['for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG']","['for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG']","['', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG']","['', 'fb - ltag ( vijay -  #AUTHOR_TAG vijay -  #AUTHOR_TAG is an extension of the ltag formalism.', 'in fb - ltag, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'figure 5 shows a result of ltag analysis, which is described not there are several grammars developed in the fb - ltag formalism, including the xtag english grammar, a large - scale grammar for english ( the xtag research  #TAUTHOR_TAG.', 'the xtag group  #AUTHOR_TAG at the university of pennsylvania is also developing korean, chinese, and hindi grammars.', 'development of a large - scale french grammar ( abeille and  #AUTHOR_TAG has also started at the university of pennsylvania and is expanded at university of paris 7']",0
"['xtag research  #TAUTHOR_TAG 3, which is a large -']","['xtag research  #TAUTHOR_TAG 3, which is a large - scale fb - ltag']","['( the xtag research  #TAUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques']","['this paper, we show that the strongly equivalent grammars enable the sharing of "" parsing techniques "", which are dependent on each computational framework and have never been shared among hpsg and ltag communities.', 'we applied our system to the xtag english grammar ( the xtag research  #TAUTHOR_TAG 3, which is a large - scale fb - ltag grammar for english.', 'a parsing experiment shows that an efficient hpsg parser with the obtained grammar achieved a significant speed - up against an existing ltag parser.', 'this result implies that parsing techniques for hpsg are also beneficial for ltag parsing.', 'we can say that the grammar conversion enables us to share hpsg parsing techniques in ltag parsing']",5
"['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
"['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstr']","['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstrably']","[""limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
"['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
"['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstr']","['string concatenation operator.', 'mers  #TAUTHOR_TAG demonstrably']","[""limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #TAUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
"['[UNK] o, 1988 ;  #TAUTHOR_TAG,']","['[UNK] o, 1988 ;  #TAUTHOR_TAG,']","['##t [UNK] o, 1988 ;  #TAUTHOR_TAG,']","['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system  #AUTHOR_TAG j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
"['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['has been some controversy, at least for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
"[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']","[', but at high ones its precision decreases almost dramatically.', 'unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in  #TAUTHOR_TAG ) one can not really recommend this method']",1
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],"['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #TAUTHOR_TAG']",0
"['relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity']","['relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity']","['assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity']","['unbiased evaluation of our approach, we used a home - grown search engine ( implemented in the python script language ).', 'it crawls text / html files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf - idf metric.', 'the retrieval process relies on the vector space model  #TAUTHOR_TAG, with the cosine measure expressing the similarity between a query and a document.', 'the search engine produces a ranked output of documents']",5
"['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis']","['has been some controversy, at least for simple stemmers  #TAUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
['attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG'],['attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG'],"['to as neo - classical compounding ( mc -  #AUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding ( mc -  #AUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #TAUTHOR_TAG']",0
"['( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we']","['mappings of our synonym identifiers to a large medical thesaurus ( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we']","['mappings of our synonym identifiers to a large medical thesaurus ( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies  #AUTHOR_TAG']","['', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh,  #TAUTHOR_TAG ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies  #AUTHOR_TAG']",3
"['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some']","['document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries']","['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #TAUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #AUTHOR_TAG']",0
"['[UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu']","['( j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu']","['retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu et al.,']","['efforts required for performing morphologi - cal analysis vary from language to language.', 'for english, known for its limited number of inflection patterns, lexicon - free general - purpose stemmers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ;  #TAUTHOR_TAG ; ekmekc [UNK] ioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
['of dederivation and decomposition  #TAUTHOR_TAG'],['of dederivation and decomposition  #TAUTHOR_TAG'],"['specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( jappinen and niemisto, 1988 ;  #AUTHOR_TAG ekmekcioglu et al., 1995 ;  #AUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #TAUTHOR_TAG']",0
['think of user - centered comparative studies  #TAUTHOR_TAG'],['think of user - centered comparative studies  #TAUTHOR_TAG'],"['.', 'alternatively, we may think of user - centered comparative studies  #TAUTHOR_TAG']","['', 'it would be interesting to evaluate the retrieval effectiveness ( in terms of precision and recall ) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'this will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( mesh, ( nlm, 2001 ) ) are incorporated into our system.', 'alternatively, we may think of user - centered comparative studies  #TAUTHOR_TAG']",3
['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],['without access to such lexical repositories  #TAUTHOR_TAG'],"['has been some controversy, at least for simple stemmers  #AUTHOR_TAG, about the effectiveness of morphological analysis for document retrieval  #AUTHOR_TAG.', 'the key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'empirical evidence has been brought forward that inflectional and / or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories  #TAUTHOR_TAG']",0
"['as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at']","['as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at']","['as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at']","['one may argue that single - word compounds are quite rare in english ( which is not the case in the medical domain either ), this is certainly not true for german and other basically agglutinative languages known for excessive single - word nominal compounding.', 'this problem becomes even more pressing for technical sublanguages, such as medical german ( e. g., blut druck mess gera _ _ t translates to device for measuring blood pressure ).', 'the problem one faces from an ir point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'hence, enumerating morphological variants in a semi - automatically generated lexicon, such as proposed for french  #TAUTHOR_TAG, turns out to be infeasible, at least for german and related languages']",0
"[';  #TAUTHOR_TAG.', 'when']","[';  #TAUTHOR_TAG.', 'when']","[';  #TAUTHOR_TAG.', 'when it comes']","['efforts required for performing morphological analysis vary from language to language.', ""for english, known for its limited number of inflection patterns, lexicon - free general - purpose stem - 1'¡'denotes the string concatenation operator."", 'mers  #AUTHOR_TAG demonstrably improve retrieval performance.', 'this has been reported for other languages, too, dependent on the generality of the chosen approach ( j [UNK] appinen and niemist [UNK] o, 1988 ;  #AUTHOR_TAG ekmekc [UNK] ioglu et al., 1995 ;  #TAUTHOR_TAG.', 'when it comes to a broader scope of morphological analysis, including derivation and composition, even for the english language only restricted, domain - specific algorithms exist.', 'this is particularly true for the medical domain.', 'from an ir view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico - semantic aspects of dederivation and decomposition  #AUTHOR_TAG']",0
['ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and nie'],['ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and'],['ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and nie'],"['alterations of a search term have a negative impact on the recall performance of an information retrieval ( ir ) system  #TAUTHOR_TAG ; j [UNK] appinen and niemist [UNK] o, 1988 ;  #AUTHOR_TAG, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.', ""in order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection ( e. g.,'search ed ','search ing') 1, derivation ( e. g.,'search er'or'search able') and composition ( e. g., german'blut hoch druck'['high blood pressure'] )."", ""the goal is to map all occurring morphological variants to some canonical base forme. g.,'search'in the examples from above""]",0
"['##ing  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']","['to as neo - classical compounding  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']","['to as neo - classical compounding  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']","[', medical terminology is characterized by a typical mix of latin and greek roots with the corresponding host language ( e. g., german ), often referred to as neo - classical compounding  #TAUTHOR_TAG.', 'while this is simply irrelevant for general - purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free - texts in an ir setting  #AUTHOR_TAG']",0
"[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG']",0
"['( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text,']","['( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text,']","['( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text,']","['this paper, a flexible annotation schema called structured string - tree correspondence ( sstc )  #TAUTHOR_TAG will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'the correspondence between the string and its associated representation tree structure is defined in terms of the sub - correspondence between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation.', 'such correspondence is defined in a way that is able to handle some non - standard cases ( e. g.', 'non - projective correspondence )']",0
"['. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']","['the treatment of linguistic phenomena, which are non - standard, e. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']","['. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']","['sstc is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective  #AUTHOR_TAG.', 'these features are very much desired in the design of an annotation scheme, in particular for the treatment of linguistic phenomena, which are non - standard, e. g. crossed dependencies  #TAUTHOR_TAG.', 'crossed dependencies  #AUTHOR_TAG']",0
['( sstc ) was introduced in  #TAUTHOR_TAG to record'],['( sstc ) was introduced in  #TAUTHOR_TAG to record'],"['( sstc ) was introduced in  #TAUTHOR_TAG to record the string of terms,']","['this section, we stress on the fact that in order to describe natural language ( nl ) in a natural manner, three distinct components need to be expressed by the linguistic formalisms ; namely, the text, its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two.', 'actually, nl is not only a correspondence between different representation levels, as stressed by mtt postulates, but also a sub - correspondence between them.', 'for instance, between the string in a language and its representation tree structure, it is important to specify the sub - correspondences between parts of the string ( substrings ) and parts of the tree structure ( subtrees ), which can be interpreted for both analysis and generation in nlp.', 'it is well known that many linguistic constructions are not projective ( e. g.', 'scrambling, cross serial dependencies, etc. ).', 'hence, it is very much desired to define the correspondence in a way to be able to handle the non - standard cases ( e. g.', 'non - projective correspondence ), see figure 1.', 'towards this aim, a flexible annotation structure called structured string - tree correspondence ( sstc ) was introduced in  #TAUTHOR_TAG to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the sub - correspondences recorded as part of a sstc']",0
"['the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","['the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","['the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']","[', what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""the proposed s - sstc annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages'structures."", 's - sstc very well suited for the construction of a bkb, which is needed for the ebmt applications.', ' #TAUTHOR_TAG presented an approach for constructing a bkb based on the s - sstc']",0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['( tag ) introduced by  #AUTHOR_TAG to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by  #AUTHOR_TAG to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal  #AUTHOR_TAG,  #TAUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples  #AUTHOR_TAG']",0
"['as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt']","['as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt']","[') 1 point of view, natural language ( nl ) is considered as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt point of']","['the meaning - text theory ( mtt ) 1 point of view, natural language ( nl ) is considered as a correspondence between meanings and texts  #TAUTHOR_TAG.', 'the mtt point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community']",0
"['., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', '']","['problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', '']","['for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', '']","['mentioned earlier, there are some non - standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'in this section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages ( e. g.  #TAUTHOR_TAG cases ).', ' #AUTHOR_TAG cases ).', 'due to lack of space we will only brief on some of these non - standard cases without going into the details']",0
"[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG, such as the relation between syntax and semantic']",0
"[',  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or']","[',  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or']","[',  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or']","['are governed by the following constraints :.', 'this means allowing one - to - one, one - to - many and many - to - many, but the mappings do not overlap.', 'note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two sstcs of the s - sstc ( i. e. between the two languages ).', ""for instance, when building translation units in ebmt approaches  #AUTHOR_TAG,  #AUTHOR_TAG, ( al  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, etc., where s - sstc can be used to represent the entries of the bkb or when s - sstc used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules'extraction from parallel parsed""]",0
"['of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et']","['of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e.', 'synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']","['is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures  #TAUTHOR_TAG']",0
"[', see  #TAUTHOR_TAG']","['"" he picks the box up "".', 'for more details on the proprieties of sstc, see  #TAUTHOR_TAG']","[', see  #TAUTHOR_TAG']","['case depicted in figure 2, describes how the sstc structure treats some non - standard linguistic phenomena.', 'the particle "" up "" is featurised into the verb "" pick "" and in discontinuous manner ( e. g.', '"" up "" ( 4 - 5 ) in "" pick - up "" ( 1 - 2 + 4 - 5 ) ) in the sentence "" he picks the box up "".', 'for more details on the proprieties of sstc, see  #TAUTHOR_TAG']",0
"['to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency']","['substring to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency']","['to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
"['., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['- tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['of a non - tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['the construction of a non - tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages,']","['', 's - tag is a variant of tree adjoining grammar ( tag ) introduced by  #AUTHOR_TAG to characterize correspondences between tree adjoining languages.', 'considering the original definition of s - tags, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'it allows the construction of a non - tal  #TAUTHOR_TAG,  #AUTHOR_TAG.', 'as a result,  #AUTHOR_TAG propose a restricted definition for s - tag, namely, the is - tag for isomorphic s - tag.', 'in this case only tal can be formed in each component.', 'this isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'also contrastive well - known translation phenomena exist in different languages, which cannot be expressed by is - tag, figure 3 illustrates some examples  #AUTHOR_TAG']",0
"['., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","[',  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']","['to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema ( i. e. synchronous structured string - tree correspondence ( s - sstc ) ) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'for example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, ( kaji et al., 1992 ), and example - base machine translation ebmt3  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG, ( al -  #AUTHOR_TAG']",0
"['to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency']","['substring to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency']","['to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and']","['2 illustrates the sentence "" john picks the box up "" with its corresponding sstc.', 'it contains a nonprojective correspondence.', 'an interval is assigned to each word in the sentence, i. e. ( 0 - 1 ) for "" john "", ( 1 - 2 ) for "" picks "", ( 2 - 3 ) for "" the "", ( 3 - 4 ) for "" box "" and ( 4 - 5 ) for "" up "".', 'a substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to snode of 2 these definitions are based on the discussion in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['g. the english gigaword corpus ( linguistic data  #TAUTHOR_TAG.', 'recent work  #AUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus ( linguistic data  #TAUTHOR_TAG.', 'recent work  #AUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"['systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['final interface we intend to implement is a collection of web services for nlp.', 'a web service provides a remote procedure that can be called using xml based encodings ( xmlrpc or soap ) of function names, arguments and results transmitted via internet protocols such as http.', 'systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with wsdl and uddi.', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #TAUTHOR_TAG and speech recognition  #AUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
"['example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient']","['example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient']","['example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient']","['discussed earlier, there are two main requirements of the system that are covered by "" high performance "" : speed and state of the art accuracy.', 'efficiency is required both in training and processing.', 'efficient training is required because the amount of data available for training will increase significantly.', 'also, advanced methods often require many training iterations, for example active learning  #AUTHOR_TAG and co - training  #TAUTHOR_TAG.', 'processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly']",0
[')  #TAUTHOR_TAG and'],[')  #TAUTHOR_TAG and'],[')  #TAUTHOR_TAG and the alembic workbench  #AUTHOR_TAG )'],"['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate )  #TAUTHOR_TAG and the alembic workbench  #AUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #AUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #AUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']","['', 'this standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'there have already been several attempts to develop distributed nlp systems for dialogue systems  #AUTHOR_TAG and speech recognition  #TAUTHOR_TAG.', 'web services will allow components developed by different researchers in different locations to be composed to build larger systems']",0
['mxpost pos tagger  #TAUTHOR_TAG will simply involve composing'],['mxpost pos tagger  #TAUTHOR_TAG will simply involve composing'],"['to be rapidly composed from many elemental components.', 'for instance, implementing an efficient version of the mxpost pos tagger  #TAUTHOR_TAG will simply involve composing']","['generative programming approach to nlp infrastructure development will allow tools such as sentence boundary detectors, pos taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'for instance, implementing an efficient version of the mxpost pos tagger  #TAUTHOR_TAG will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component']",3
"['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tag']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #AUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p']","['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p']","['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p o s tagger, chunker, c c g supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #TAUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #AUTHOR_TAG.', 'we have already implemented a p o s tagger, chunker, c c g supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training ma - terials and tag faster than t n t, the fastest existing p o s tagger.', 'these tools use a highly optimised g i s imple - mentation and provide sophisticated gaussian smoothing  #AUTHOR_TAG.', 'we expect even faster train - ing times when we move to conjugate gradient methods']",4
['iterative estimation algorithms used by  #TAUTHOR_TAG'],['iterative estimation algorithms used by  #TAUTHOR_TAG'],"['corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #TAUTHOR_TAG']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #TAUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so']","['configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so']","['configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #AUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #TAUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
"[' #TAUTHOR_TAG, currently']","['penn treebank  #TAUTHOR_TAG, currently']","['electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus  #AUTHOR_TAG will have manually corrected pos tags, a tenfold increase over the penn treebank  #TAUTHOR_TAG, currently']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus  #AUTHOR_TAG will have manually corrected pos tags, a tenfold increase over the penn treebank  #TAUTHOR_TAG, currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
"['ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']","['ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']","['ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']","['c + + is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'to overcome this problem we have implemented an interface to the infrastructure in the python scripting language.', 'python has a number of advantages over other options, such as java and perl.', 'python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'it has already been used to implement a framework for teaching nlp  #TAUTHOR_TAG']",2
['alembic workbench  #TAUTHOR_TAG )'],['alembic workbench  #TAUTHOR_TAG )'],[')  #AUTHOR_TAG and the alembic workbench  #TAUTHOR_TAG )'],"['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g. general architecture for text engineering ( gate )  #AUTHOR_TAG and the alembic workbench  #TAUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #AUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #AUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['ontology editors  #TAUTHOR_TAG.', '']","['ontology editors  #TAUTHOR_TAG.', '']","['ontology editors  #TAUTHOR_TAG.', '']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate )  #AUTHOR_TAG and the alembic workbench  #AUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #TAUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #AUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on'],['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on'],"['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']","['engineering research on generative programming  #TAUTHOR_TAG attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems.', 'our infrastructure for nlp will provide high performance 1 components inspired by generative programming principles']",0
"['american national corpus  #TAUTHOR_TAG will have manually corrected pos tags, a']","['american national corpus  #TAUTHOR_TAG will have manually corrected pos tags, a tenfold']","['', 'for example, 10 million words of the american national corpus  #TAUTHOR_TAG will have manually corrected pos tags,']","['##p is experiencing an explosion in the quantity of electronic text available.', 'some of this new data will be manually annotated.', 'for example, 10 million words of the american national corpus  #TAUTHOR_TAG will have manually corrected pos tags, a tenfold increase over the penn treebank  #AUTHOR_TAG, currently used for training pos taggers.', 'this will require more efficient learning algorithms and implementations']",0
['of lt tools  #TAUTHOR_TAG perform token'],"['of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #AUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
"['static version of the code we will use policy templates  #TAUTHOR_TAG, and for']","['static version of the code we will use policy templates  #TAUTHOR_TAG, and for']","['will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates  #TAUTHOR_TAG, and for the dynamic version we will use configuration classes']","['infrastructure will be implemented in c / c + +.', 'templates will be used heavily to provide generality without significantly impacting on efficiency.', 'however, because templates are a static facility we will also provide dynamic versions ( using inheritance ), which will be slower but accessible from scripting languages and user interfaces.', 'to provide the required configurability in the static version of the code we will use policy templates  #TAUTHOR_TAG, and for the dynamic version we will use configuration classes']",5
"['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data']","['g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting']","[', the greatest increase is in the amount of raw text available to be processed, e. g. the english gigaword corpus  #AUTHOR_TAG.', 'recent work  #TAUTHOR_TAG has suggested that some tasks will benefit from using significantly more data.', 'also, many potential applications of nlp will involve processing very large text databases.', 'for instance, biomedical text - mining involves extracting information from the vast body of biological and medical literature ; and search engines may eventually apply nlp techniques to the whole web.', 'other potential applications must process text online or in realtime.', 'for example, google currently answers 250 million queries per day, thus processing time must be minimised.', 'clearly, efficient nlp components will need to be developed.', 'at the same time, state - of - the - art performance will be needed for these systems to be of practical use']",0
['of lt tools  #TAUTHOR_TAG perform token'],"['of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and']","['number of stand - alone tools have also been developed.', 'for example, the suite of lt tools  #TAUTHOR_TAG perform tokenization, tagging and chunking on xml marked - up text directly.', 'these tools also store their configuration state, e. g. the transduction rules used in lt chunk, in xml configuration files.', 'this gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'other tools have been designed around particular techniques, such as finite state machines  #AUTHOR_TAG.', 'however, the source code for these tools is not freely available, so they cannot be extended']",0
['( tbl )  #TAUTHOR_TAG and memory - based'],['( tbl )  #TAUTHOR_TAG and memory - based'],"[': transformation - based learning ( tbl )  #TAUTHOR_TAG and memory - based learning ( mbl )  #AUTHOR_TAG have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka  #AUTHOR_TAG']","['learning methods should be interchangeable : transformation - based learning ( tbl )  #TAUTHOR_TAG and memory - based learning ( mbl )  #AUTHOR_TAG have been applied to many different problems, so a single interchangeable component should be used to represent each method.', 'we will base these components on the design of weka  #AUTHOR_TAG']",4
[') toolkit  #TAUTHOR_TAG which dramatically'],[') toolkit  #TAUTHOR_TAG which dramatically'],"[') toolkit  #TAUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #TAUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']",0
"['.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']","['fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']","['##t, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging ( curran and.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #TAUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']",5
"['very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train']","['very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train']","['very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #TAUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']","[', the natural language toolkit ( nltk ) is a package of nlp components implemented in python  #TAUTHOR_TAG.', 'python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple']",0
"['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tag']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger']","['implementation has been inspired by experience in extracting information from very large corpora  #AUTHOR_TAG and performing experiments on maximum entropy sequence tagging  #TAUTHOR_TAG.', 'we have already implemented a pos tagger, chunker, ccg supertagger and named entity recogniser using the infrastructure.', 'these tools currently train in less than 10 minutes on the standard training materials and tag faster than tnt, the fastest existing pos tagger.', 'these tools use a highly optimised gis implementation and provide sophisticated gaussian smoothing  #AUTHOR_TAG.', 'we expect even faster training times when we move to conjugate gradient methods']",4
"['converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based']","['converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based']","['converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #TAUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #AUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']","['basic python reflection has already been implemented and used for large scale experiments with pos tagging, using pympi ( a message passing interface library for python ) to coordinate experiments across a cluster of over 100 machines  #TAUTHOR_TAG.', 'an example of using the python tagger interface is shown in figure 1']",0
[' #TAUTHOR_TAG has also been designed to train'],"['very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #TAUTHOR_TAG has also been designed to train']","['very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #TAUTHOR_TAG has also been designed to train']","['has not been a focus for nlp research in general.', 'however, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'an example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by  #AUTHOR_TAG that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly  #AUTHOR_TAG.', 'other attempts to address efficiency include the fast transformation based learning ( tbl ) toolkit  #AUTHOR_TAG which dramatically speeds up training tbl systems, and the translation of tbl rules into finite state machines for very fast tagging  #AUTHOR_TAG.', 'the tnt pos tagger  #TAUTHOR_TAG has also been designed to train and run very quickly, tagging between 30, 000 and 60, 000 words per second']",0
"['by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']","['by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']","['earlier systems by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']","['are a number of generalised nlp systems in the literature.', 'many provide graphical user interfaces ( gui ) for manual annotation ( e. g.', 'general architecture for text engineering ( gate )  #AUTHOR_TAG and the alembic workbench  #AUTHOR_TAG ) as well as nlp tools and resources that can be manipulated from the gui.', 'for instance, gate currently provides a pos tagger, named entity recogniser and gazetteer and ontology editors  #AUTHOR_TAG.', 'gate goes beyond earlier systems by using a component - based infrastructure  #TAUTHOR_TAG which the gui is built on top of.', 'this allows components to be highly configurable and simplifies the addition of new components to the system']",0
"['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and  #AUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #TAUTHOR_TAG.', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity  #AUTHOR_TAG.', 'techniques']",0
"['to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn']","['to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn']","['have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #AUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #TAUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #AUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #TAUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way']","['to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a']","['to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a class novn1 which groups together all the initial trees representing the possible syntactic configurations in which']","['intercategorial synonymic links.', ""a first investigation of anne abeille's tag for french suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'for instance, as figures 3, 4 and 5 show, the ftag trees assigned on syntactic grounds by anne abeille ftag to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'to address this problem, we are currently working on developing a metagrammar in the sense of  #TAUTHOR_TAG.', 'this metagrammar allows us to factorise both syntactic and semantic information.', 'syntactic information is factorised in the usual way.', 'for instance, there will be a class novn1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'but additionnally there will be semantic classes such as, "" binary predicate of semantic type x "" which will be associated with the relevant syntactic classes for instance, novn1 ( the class of transitive verbs with nominal arguments ), binary npred ( the class of binary predicative nouns ), novsupnn1, the class of support verb constructions taking two nominal arguments.', 'by further associating semantic units ( e. g., "" cost "" ) with the appropriate semantic classes ( e. g., "" binary predicate of semantic type x "" ), we can in this way capture both intra and intercategorial paraphrasing links in a general way']",3
"['to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a']","['to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a']","['have similar arguments are taken to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #AUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #TAUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['##net inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', '']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach,']","['##net inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', '']","['represent the semantics of predicative units, we use framenet inventory of frames and frame elements ( c.  #TAUTHOR_TAG.', ' #AUTHOR_TAG.', 'framenet is an online lexical resource for english based on the principles of frame semantics.', 'in this approach, a word evokes a frame i. e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'finally each frame is associated with a set of target words, the words that evoke that frame']",5
"['is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument']","['is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument']","['is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in']","['recently, work in information extraction ( ie ) and question answering ( qa ) has triggered a renewed research interest in paraphrases as ie and qa systems typically need to be able to recognise various verbalisations of the content.', 'because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'for instance,  #TAUTHOR_TAG acquire two - argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'similarly,  #AUTHOR_TAG and  #AUTHOR_TAG learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'and  #AUTHOR_TAG use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts']",0
"['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #AUTHOR_TAG.', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity  #TAUTHOR_TAG.', 'techniques']",3
['according to linguistics phenomena  #TAUTHOR_TAG ;'],['according to linguistics phenomena  #TAUTHOR_TAG ;'],['grouping them according to linguistics phenomena  #TAUTHOR_TAG ;'],"['on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar ( does it generate all the sentences of the described language? )', 'and its degree of overgeneration ( does it generate only the sentences of the described language? )', 'while corpus driven efforts along the par - seval lines  #AUTHOR_TAG are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena  #TAUTHOR_TAG ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information  #AUTHOR_TAG.', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",1
"['of  #TAUTHOR_TAG.', 'however']","['of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"[')', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG']","['', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG']","['it generate only the sentences of the described language? )', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG']","['', 'while corpus driven efforts along the parseval lines  #TAUTHOR_TAG are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlett - packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena  #AUTHOR_TAG ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information  #AUTHOR_TAG']",0
"['of  #TAUTHOR_TAG.', 'however']","['of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses']","['converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses']","['converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses']","['shuffling paraphrases, french alternations are partially described in ( saint -  #AUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #TAUTHOR_TAG lists the converses of some 3 500 predicative nouns']",3
['of a meta - grammar  #TAUTHOR_TAG'],['of a meta - grammar  #TAUTHOR_TAG'],"['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar  #TAUTHOR_TAG thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically']","['we shall briefly discuss in section 4, the grammar is developed with the help of a meta - grammar  #TAUTHOR_TAG thus ensuring an additional level of abstraction.', 'the metagrammar is an abstract specification of the linguistic properties ( phrase structure, valency, realisation of grammatical functions etc. ) encoded in the grammar basic units.', 'this specification is then compiled to automatically produce a specific grammar']",5
['##l tables  #TAUTHOR_TAG can furthermore be resort'],"['complementing this database and for converse constructions, the ladl tables  #TAUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses']","['complementing this database and for converse constructions, the ladl tables  #TAUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses']","['shuffling paraphrases, french alternations are partially described in ( saint -  #AUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables  #TAUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses of some 3 500 predicative nouns']",3
"['being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet']","['being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet']","['of them being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet']","['', 'while corpus driven efforts along the par - seval lines  #AUTHOR_TAG are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'for english, there is for instance the 15 year old hewlettpackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena  #AUTHOR_TAG ; and more recently, the much more sophisticated tsnlp ( test suite for natural language processing ) which includes some 9500 test items for english, french and german, each of them being annotated with syntactic and application related information  #TAUTHOR_TAG.', 'yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'to remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'in such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'the construction and annotation of the paraphrases reflects the paraphrase typology.', 'in a first phase, we concentrate on simple, non - recursive predicate / argument structure.', 'given such a structure, the construction and annotation of a test item proceeds as follows']",0
"['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['on distributional similarity  #TAUTHOR_TAG.', 'techniques']","['of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available  #AUTHOR_TAG.', 'multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'for these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity  #TAUTHOR_TAG.', 'techniques']",3
['derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lf'],['derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics'],['derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lf'],"['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #AUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and  #TAUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
[' #TAUTHOR_TAG rather than - - as is more common in tag - - from'],[' #TAUTHOR_TAG rather than - - as is more common in tag - - from'],['derived tree  #TAUTHOR_TAG rather than - - as is more common in tag - - from'],"['construction proceeds from the derived tree  #TAUTHOR_TAG rather than - - as is more common in tag - - from the derivation tree.', 'this is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'the association between tree nodes and unification variables encodes the syntax / semantics interface - it specifies which node in the tree provides the value for which variable in the final semantic representation']",0
"['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction']","['semantic grammars "" already exist which describe not only the syntax but also the semantics of natural language.', 'thus for instance,  #TAUTHOR_TAG describes a head driven phrase structure grammar ( hpsg ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and  #AUTHOR_TAG show how to equip lexical functional grammar ( lfg ) with a glue semantics']",0
"['of  #TAUTHOR_TAG.', 'however']","['of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however']","['language chosen for semantic representation is a flat semantics along the line of  #TAUTHOR_TAG.', 'however because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate / arguments and modifiers / modified relationships.', 'thus the semantic representations we assume are simply set of literals of the form p n ( x 1,...', ', x n ) where p n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing']",1
"['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a']","['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a']","['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', '']","['shuffling paraphrases, french alternations are partially described in  #TAUTHOR_TAG and a resource is available which describes alternation and the mapping verbs / alternations for roughly 1 700 verbs.', 'for complementing this database and for converse constructions, the ladl tables  #AUTHOR_TAG can furthermore be resorted to, which list detailed syntactico - semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'in particular,  #AUTHOR_TAG lists the converses of some 3 500 predicative nouns']",0
"['algorithms  #TAUTHOR_TAG, with a']","['algorithms  #TAUTHOR_TAG, with a']","['canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
"['algorithms  #TAUTHOR_TAG, with a']","['algorithms  #TAUTHOR_TAG, with a']","['canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance']","['root extraction process is concerned with the transformation of all arabic word derivatives to their single common root or canonical form.', 'this process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic / conceptual relationships between the different forms of the same root.', 'in this work, we use the arabic root extraction technique in  #AUTHOR_TAG.', 'it compares favorably to other stemming or root extraction algorithms  #TAUTHOR_TAG, with a performance of over 97 % for extracting the correct root in web documents, and it addresses the challenge of the arabic broken plural and hollow verbs.', 'in the remainder of this paper, we will use the term "" root "" and "" term "" interchangeably to refer to canonical forms obtained through this root extraction process']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #TAUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['initiated in  #TAUTHOR_TAG, which reports an overall nb classification']","['initiated in  #TAUTHOR_TAG, which reports an overall nb classification']","['initiated in  #TAUTHOR_TAG, which reports an overall nb classification correctness']","['arabic, one automatic categorizer has been reported to have been put under operational use to classify arabic documents ; it is referred to as "" sakhr\'s categorizer ""  #AUTHOR_TAG.', 'unfortunately, there is no technical documentation or specification concerning this arabic categorizer.', ""sakhr's marketing literature claims that this categorizer is based on arabic morphology and some research that has been carried out on natural language processing."", 'the present work evaluates the performance on arabic documents of the naive bayes algorithm ( nb ) - one of the simplest algorithms applied to english document categorization  #AUTHOR_TAG.', 'the aim of this work is to gain some insight as to whether arabic document categorization ( using nb ) is sensitive to the root extraction algorithm used or to different data sets.', 'this work is a continuation of that initiated in  #TAUTHOR_TAG, which reports an overall nb classification correctness of 75. 6 %, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different arabic portals ).', 'a 50 % overall classification accuracy is also reported when testing with a separately collected evaluation set ( 3 documents for each of the 12 categories ).', 'the present work expands the work in  #AUTHOR_TAG by experimenting with the use of a better root extraction algorithm  #AUTHOR_TAG for document preprocessing, and using a different data set, collected from the largest arabic site on the web : aljazeera. net']",2
"['algorithm from those used in  #TAUTHOR_TAG.', 'in this']","['algorithm from those used in  #TAUTHOR_TAG.', 'in this work, the']","['arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in  #TAUTHOR_TAG.', 'in this']","['sum up, this work has been carried out to automatically classify arabic documents using the nb algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in  #TAUTHOR_TAG.', '']",1
"['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #TAUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"['language  #TAUTHOR_TAG, and that']","['to the fact that arabic is a non - concatenative language  #TAUTHOR_TAG, and that']","['to the fact that arabic is a non - concatenative language  #TAUTHOR_TAG, and that the stem / infix']","['arabic, however, the use of stems will not yield satisfactory categorization.', 'this is mainly due to the fact that arabic is a non - concatenative language  #TAUTHOR_TAG, and that the stem / infix obtained by suppression of infix and prefix add - ons is not the same for words derived from the same origin called the root.', 'the infix form ( or stem ) needs further to be processed in order to obtain the root.', 'this processing is not straightforward : it necessitates expert knowledge in arabic language word morphology ( al -  #AUTHOR_TAG.', 'as an example, two close roots ( i. e., roots made of the same letters ), but semantically different, can yield the same infix form thus creating ambiguity']",0
"['minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong']","['minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong']","['minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong correlations']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #TAUTHOR_TAG has found strong correlations between df, ig and the x2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #TAUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', 'such']","[' #TAUTHOR_TAG.', 'such']",[' #TAUTHOR_TAG'],"['', 'automatic text categorization has been used in search engines, digital library systems, and document management systems  #TAUTHOR_TAG.', 'such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'barq for instance uses automatic categorization to provide similar documents feature  #AUTHOR_TAG.', 'in this paper, nb which is a statistical machine learning algorithm is used to learn to classify non - vocalized 1 arabic web text documents']",0
"['in  #TAUTHOR_TAG.', 'tf - idf ( term']","['in  #TAUTHOR_TAG.', 'tf - idf ( term']","['in  #TAUTHOR_TAG.', 'tf - idf (']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #TAUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",1
"['df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term']","['df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a']","['df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']","['the tf measurement concerns the importance of a term in a given document, idf seeks to measure the relative importance of a term in a collection of documents.', 'the importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'tf is given by tf d, t, and it denotes frequency of term t in document d. idf is given by idf t = log ( n / df t ), where n is the number of documents in the collection, and df t is the number of documents containing the term t.  #TAUTHOR_TAG proposed the combination of tf and idf as weighting schemes, and it has been shown that their product gave better performance.', 'thus, the weight of each term / root in a document is given by w d, t = tf d, t * idf t']",4
"[' #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong']","[' #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong']","[' #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong correlations']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #TAUTHOR_TAG, and the x2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #AUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"[', early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']","[', early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']","[', early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']","['machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, respectively']",0
"[') is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically,']","[') is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically,']","[') is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved']","['selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document ; a selection is made to keep only the more relevant words.', 'various feature selection techniques have been used in automatic text categorization ; they include document frequency ( df ), information gain ( ig )  #AUTHOR_TAG, minimum description length principal  #AUTHOR_TAG, and the χ 2 statistic.', ' #AUTHOR_TAG has found strong correlations between df, ig and the χ 2 statistic for a term.', 'on the other hand,  #AUTHOR_TAG reports the χ 2 to produce best performance.', 'in this paper, we use tf - idf ( a kind of augmented df ) as a feature selection criterion, in order to ensure results are comparable with those in  #AUTHOR_TAG.', 'tf - idf ( term frequency - inverse document frequency ) is one of the widely used feature selection techniques in information retrieval  #TAUTHOR_TAG.', 'specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents']",0
"['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']","['good study comparing document categorization algorithms can be found in  #AUTHOR_TAG.', 'more recently,  #AUTHOR_TAG has performed a good survey of document categorization ; recent works can also be found in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG']",0
"['has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']","['bulk of the text categorization work has been devoted to cope with automatic categorization of english and latin character documents.', 'for example,  #TAUTHOR_TAG discusses the evaluation of two different text categorization strategies with several variations of their feature spaces']",0
"[', on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques']","[', on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques']","[', on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques  #AUTHOR_TAG, on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist )  #AUTHOR_TAG or, more frequently, on a combination of the two  #TAUTHOR_TAG, for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results,']","['of newspaper articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results,']","['of newspaper articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']","['construct this test set, we have focused our attention on ten domain - specific terms : commande ( command ), configuration, fichier ( file ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #AUTHOR_TAG and called termostat.', 'the ten most specific nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles  #TAUTHOR_TAG.', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierent']","['semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierentiated']","['semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierent']","['number of applications have relied on distributional analysis  #AUTHOR_TAG in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness  #TAUTHOR_TAG, for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not dierentiated']",0
"['between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as']","['between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #TAUTHOR_TAG uses derivational morphology ;  #AUTHOR_TAG use, as a starting point, a number of identical characters']",0
"['( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus']","['( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus']","['( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde,']","['construct this test set, we have focused our attention on ten domain - speci c terms : commande ( command ), con guration, chier ( le ), internet, logiciel ( software ), option, ordinateur ( computer ), serveur ( server ), systme ( system ), utilisateur ( user ).', 'the terms have been identified as the most specific to our corpus by a program developed by  #TAUTHOR_TAG and called ter1vlostat.', 'the ten most speci c nouns have been produced by comparing our corpus of computing to the french corpus le monde, composed of newspaper articles  #AUTHOR_TAG.', 'note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '( they were removed from the example set.']",5
"['between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']","['between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']","['recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms :  #AUTHOR_TAG uses derivational morphology ;  #TAUTHOR_TAG use, as a starting point, a number of identical characters']",0
"[')  #TAUTHOR_TAG or, more']","['of these endeavours have focused on purely statistical acquisition techniques  #AUTHOR_TAG, on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist )  #TAUTHOR_TAG or, more frequently, on']","['by a linguist )  #TAUTHOR_TAG or, more frequently, on a combination']","['the other hand, other work has been carried out in order to acquire collocations.', 'most of these endeavours have focused on purely statistical acquisition techniques  #AUTHOR_TAG, on linguisitic acquisition ( by the use of part - of - speech filters hand - crafted by a linguist )  #TAUTHOR_TAG or, more frequently, on a combination of the two  #AUTHOR_TAG kilgarri and  #AUTHOR_TAG for example ).', 'it is worth noting that although these techniques are able to identify n - v pairs, they do not specify the relationship between n and v, nor are they capable of focusing on a subset of n - v pairs.', 'the original acquisition methodology we present in the next section will allow us to overcome this limitation']",1
"['technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which']","['technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which']","['##res is based on a machine learning technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which']","['##res is based on a machine learning technique, inductive logic programming ( ilp )  #TAUTHOR_TAG, which infers general morpho - syntactic patterns from a set of examples ( this set is noted e + hereafter ) and counter - examples ( e a ) of the elements one wants to acquire and their context.', 'the contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'the acquisition process can be summarized in 3 steps']",0
"['based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on']","['based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on']","[', most strategies are based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on the form of terms or on the information gathered from contexts.', '(']","[', most strategies are based on ` ` internal or ` ` external methods  #TAUTHOR_TAG, i. e. methods that rely on the form of terms or on the information gathered from contexts.', '( in some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process. )', 'the work reported here infers specific semantic relationships based on sets of examples and counterexamples']",1
"['combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']","['combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']","['verb combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']","['this paper, the method is applied to a french corpus on computing to and noun - verb combinations in which verbs convey a meaning of realization.', 'the work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information  #TAUTHOR_TAG']",4
"['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']","['addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques  #TAUTHOR_TAG']",4
"[' #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","[' #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['of word pairs sharing semantic relations defined in the generative lexicon framework  #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework  #TAUTHOR_TAG and called qualia relations  #AUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
['by  #TAUTHOR_TAG with wordnet relations )'],['by  #TAUTHOR_TAG with wordnet relations )'],['by  #TAUTHOR_TAG with wordnet relations )'],"['though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'indeed, such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by  #TAUTHOR_TAG with wordnet relations )']",1
"['qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to']","['##res has been previously applied to the acquisition of word pairs sharing semantic relations defined in the generative lexicon framework  #AUTHOR_TAG and called qualia relations  #TAUTHOR_TAG.', 'here, we propose to use asares in a quite similar way to retrieve our valid n - v pairs.', 'however, the n - v combinations sought are more specific than those that were identified in these previous experiments']",0
"['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']","['main benefits of this acquisition technique lie in the inferred patterns.', 'indeed, contrary to the more classical statistical methods ( mutual information, loglike..., see below ) used for collocation acquisition ( see  #TAUTHOR_TAG for a review ), these patterns allow']",0
"['.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']","['method used for the acquisition of n - v pairs relies mainly on asares, a pattern inference tool.', 'asares is presented in detail in  #TAUTHOR_TAG.', 'we simply give a short account of its basic principles herein']",5
['number of applications have relied on distributional analysis  #TAUTHOR_TAG in'],['number of applications have relied on distributional analysis  #TAUTHOR_TAG in'],['number of applications have relied on distributional analysis  #TAUTHOR_TAG in'],"['number of applications have relied on distributional analysis  #TAUTHOR_TAG in order to build classes of semantically related terms.', 'this approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness  #AUTHOR_TAG for example ), does not specify the relationship itself.', 'hence, synonyms, co - hyponyms, hyperonyms, etc. are not differentiated']",0
['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],"['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging  #TAUTHOR_TAG']",0
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
['program  #TAUTHOR_TAG : we will call'],['program  #TAUTHOR_TAG : we will call'],[' #TAUTHOR_TAG : we will call'],"['', 'usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the ner task consists of identifying each of these occurrences.', 'instead, we will adopt the nomenclature of the automatic content extraction program  #TAUTHOR_TAG : we will call the instances of textual references to objects / abstractions mentions, which can be either named ( e. g. john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'john mayor ), nominal ( the president ) or pronominal ( she, it ).', 'an entity is the aggregate of all the mentions ( of any level ) which refer to one conceptual entity.', 'for instance, in the sentence president john smith said he has no comments there are two mentions ( named and pronomial ) but only one entity, formed by the set { john smith, he }']",5
"['chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of']","['chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of']","['chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of']","['addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n - grams, similar to those used for chinese segmentation  #TAUTHOR_TAG.', 'for these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n - gram character contexts that appear in the training data.', 'the character based model alone achieves a 94. 5 % exact match segmentation accuracy, considerably less accurate then the dictionary based model']",1
"[', as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the']","[', as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['coreference resolution performance, as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']","['types of features result in an improvement in both the mention detection and coreference resolution performance, as shown through experiments on the  #TAUTHOR_TAG arabic data.', 'the experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'in addition, we also report results on the official test data']",5
"['punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']","['punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']","['more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']","['2 ).', 'we define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'each punctuation symbol is considered a separate token.', 'character classes, such as punctuation, are defined according to the unicode standard  #TAUTHOR_TAG']",5
['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],['arabic language processing challenging  #TAUTHOR_TAG'],"['separate the edr task into two parts : a mention detection step, which identifies and classifies all the mentions in a text - and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'in its entirety, the edr task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non - named mentions ( nominal and pronominal ) and the requirement of grouping mentions into entities.', 'this is particularly true for arabic where nominals and pronouns are also attached to the word they modify.', 'in fact, most arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form arabic surface forms ( blank - delimited words ).', 'in addition to the different forms of the arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the arabic surface word.', 'it is these orthographic variations and complex morphological structure that make arabic language processing challenging  #TAUTHOR_TAG']",0
['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],['coreference system system is similar to the bell tree algorithm as described by  #TAUTHOR_TAG'],1
"['##s  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['##s and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example,']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this']","['( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this']","['( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in']","['formulate the mention detection problem as a classification problem, which takes as input segmented arabic text.', 'we assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'we use a maximum entropy markov model ( memm ) classifier.', 'the principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ), the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model )  #TAUTHOR_TAG.', 'one big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision']",0
['- category of the mention type  #TAUTHOR_TAG ('],"['mention sub - type, which is a sub - category of the mention type  #TAUTHOR_TAG ( e. g. orggovernmental, facilitypath, etc. )']","['mention sub - type, which is a sub - category of the mention type  #TAUTHOR_TAG (']","['mention sub - type, which is a sub - category of the mention type  #TAUTHOR_TAG ( e. g. orggovernmental, facilitypath, etc. )']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
['demonstrates a technique'],['demonstrates a technique'],['demonstrates a technique'],"['demonstrates a technique for segmenting arabic text and uses it as a morphological processing step in machine translation.', 'a trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules']",5
"['described in.', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we']","['described in.', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we']","['in.', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero']","['', 'both systems are built around from the maximum - entropy technique  #TAUTHOR_TAG.', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness']",5
['presented in  #TAUTHOR_TAG and the coreference resolution system is similar to'],['presented in  #TAUTHOR_TAG and the coreference resolution system is similar to'],['presented in  #TAUTHOR_TAG and the coreference resolution system is similar to'],"['tasks are performed with a statistical framework : the mention detection system is similar to the one presented in  #TAUTHOR_TAG and the coreference resolution system is similar to the one described in  #AUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we formulate the mention detection task as a sequence classi cation problem.', 'while this approach is language independent, it must be modified to accomodate the particulars of the arabic language.', 'the arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'we begin with a segmentation of the written text before starting the classification.', 'this segmentation process consists of separating the normal whitespace delimited words into ( hypothesized ) prefixes, stems, and suffixes, which become the subject of analysis ( tokens ).', 'the resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label ( for instance, in the case of nominal and pronominal mentions ).', 'additionally, because the pre xes and su _ xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness']",1
"['not  #TAUTHOR_TAG.', 'we']","['not  #TAUTHOR_TAG.', 'we']","['not  #TAUTHOR_TAG.', 'we denote these features as backward token']","['context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not  #TAUTHOR_TAG.', 'we denote these features as backward token tri - grams and forward token tri - grams for the previous and next context of t i respectively.', 'for a token t i, the backward token n - gram feature will contains the previous n − 1 tokens in the history ( t i−n + 1,...', 't i−1 ) and the forward token n - gram feature will contains the next n − 1 tokens ( t i + 1,...', 't i + n−1 )']",0
"['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']","['using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution  #TAUTHOR_TAG.', 'for arabic, since words are morphologically derived from a list of roots ( stems ), we expected that a feature based on the right and left stems would lead to improvement in system accuracy']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG,']","['to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG,']","['do not appear in the training data.', 'we seeked to exploit this ability to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG, we used unsupervised training data which is automatically segmented']","[', an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'we seeked to exploit this ability to generalize to improve the dictionary based model.', 'as in  #TAUTHOR_TAG, we used unsupervised training data which is automatically segmented to discover previously unseen stems.', 'in our case, the character n - gram model is used to segment a portion of the arabic gigaword corpus.', 'from this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']","['system is trained on the arabic ace 2003 and part of the 2004 data.', 'we introduce here a clearly defined and replicable split of the  #TAUTHOR_TAG data, so that future investigations can accurately and correctly compare against the results presented here']",5
"[' #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1']","['trigram language model, specifically a kneser - ney  #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']","['identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney  #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']","['our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'the first machine, illustrated in figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'the second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'the final machine is a trigram language model, specifically a kneser - ney  #TAUTHOR_TAG based backoff language model.', 'differing from  #AUTHOR_TAG, we have also introduced an explicit model for un - known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'using 0. 5m words from the combined arabic treebanks 1v2, 2v2 and 3v1, the dictionary based segmenter achieves a exact word match 97. 8 % correct segmentation']",5
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
"['recent investigations  #TAUTHOR_TAG, and have been']","['recent investigations  #TAUTHOR_TAG, and have been']","['several recent investigations  #TAUTHOR_TAG, and have been']",[' #TAUTHOR_TAG'],0
['- f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how'],"['and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how']",['- f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how'],"['this section, we present the coreference results on the devtest defined earlier.', 'first, to see the effect of stem matching features, we compare two coreference systems : one with the stem features, the other with - out.', 'we test the two systems on both "" true "" and system mentions of the devtest set.', 'true mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'we report results with two metrics : ecm - f and ace - value.', 'ecm - f is an entity - constrained mention fmeasure ( cfxxx  #TAUTHOR_TAG for how ecm - f is computed ), and ace - value is the official ace evaluation metric.', 'the result is shown in table 4 : the baseline numbers without stem features are listed under \\ base, "" and the results of the coreference system with stem features are listed under \\ base + stem.']",5
"['##s  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the']","['##s and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example,']","['has two kinds of plurals : broken plurals and sound plurals  #TAUTHOR_TAG.', 'the formation of broken plurals is common, more complex and often irregular.', 'as an example, the plural form of the noun ( man ) is ( men ), which is formed by inserting the infix.', 'the plural form of the noun ( book ) is ( books ), which is formed by deleting the infix.', 'the plural form and the singular form may also be completely different ( e. g. for woman, but for women ).', 'the sound plurals are formed by adding plural suffixes to singular nouns ( e. g., meaning researcher ) : the plural suffix is for feminine nouns in grammatical cases ( e. g.']",0
"['described in  #TAUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we']","['described in  #TAUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'both systems are built around from the maximum - entropy technique  #AUTHOR_TAG.', 'we formulate the mention detection task as a sequence classification problem.', 'while this approach is language independent, it must be modified to accomodate']",[' #TAUTHOR_TAG'],1
[' #TAUTHOR_TAG where the system will'],[' #TAUTHOR_TAG where the system will'],"['', ""as stated before, the experiments are run in the ace'04 framework  #TAUTHOR_TAG where the system will identify mentions""]","['want to investigate the usefulness of stem n - gram features in the mention detection system.', ""as stated before, the experiments are run in the ace'04 framework  #TAUTHOR_TAG where the system will identify mentions and will label them ( cfxxx section 4 ) with a type ( person, organization, etc ), a sub - type ( orgcommercial, orggovernmental, etc ), a mention level ( named, nominal, etc ), and a class ( specific, generic, etc )."", 'detecting the mention boundaries ( set of consecutive tokens ) and their main type is one of the important steps of our mention detection system.', 'the score that the ace community uses ( ace value ) attributes a higher importance ( outlined by its weight ) to the main type compared to other sub - tasks, such as the mention level and the class.', 'hence, to build our mention detection system we spent a lot of effort in improving the rst step : detecting the mention boundary and their main type.', 'in this paper, we report the results in terms of precision, recall, and f - measure3']",5
"['e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']","['e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']","['mk is one mention in entity e, and the basic model building block pl ( l = 1 | e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']","['mk is one mention in entity e, and the basic model building block pl ( l = 1 | e, mk, m ) is an exponential or maximum entropy model  #TAUTHOR_TAG']",5
"['were not supervised during the experiment.', ' #TAUTHOR_TAG observed that']","['were not supervised during the experiment.', ' #TAUTHOR_TAG observed that']","['automatically generated concept pair.', 'test subjects were invited via email to participate in the experiment.', 'thus, they were not supervised during the experiment.', ' #TAUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', '']","['developed a web - based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'test subjects were invited via email to participate in the experiment.', 'thus, they were not supervised during the experiment.', ' #TAUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness.', 'their results differed particularly in cases of antonymy or distributionally related pairs.', 'we created a manual with a detailed introduction to sr stressing the crucial points.', 'the manual was presented to the subjects before the experiment and could be re - accessed at any time.', 'during the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'subjects had to assign a discrete relatedness value { 0, 1, 2, 3, 4 } to each pair.', ""figure 2 shows the system's gui""]",4
"['document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['##ing correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']","['distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'it is defined on different kinds of textual units, e. g.', 'documents, parts of a document ( e. g.', 'words and their surrounding context ), words or concepts  #TAUTHOR_TAG. 2 linguistic distance between words is inverse to their semantic similarity or relatedness']",0
"['experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r']","['experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein,']","['experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #TAUTHOR_TAG. table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['to  #TAUTHOR_TAG, there are three prevalent approaches']","['to  #TAUTHOR_TAG, there are three prevalent approaches']","['to  #TAUTHOR_TAG, there are three prevalent approaches']","['to  #TAUTHOR_TAG, there are three prevalent approaches for evaluating sr measures : mathematical analysis, applicationspecific evaluation and comparison with human judgments']",0
"['remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of']","['remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of']","['concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of']","['distribution of averaged human judgments on the whole test set ( see figure 3 ) is almost balanced with a slight underrepresentation of highly related concepts.', 'to create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain - ing could be used.', 'however, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', ' #TAUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by rubenstein and goodenough display an empty horizontal band that could be used to separate related and unrelated pairs.', 'this empty band is not observed here.', 'however, figure 4 shows the distribution of averaged judgments with the highest agreement between annotators ( standard deviation < 0. 8 ).', 'the plot clearly shows an empty horizontal band with no judgments.', 'the connection between averaged judgments and standard deviation is plotted in figure 5']",1
"['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs (']","['rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about']","['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs (']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #TAUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #AUTHOR_TAG table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
['by  #TAUTHOR_TAG with 10'],['by  #TAUTHOR_TAG with 10'],"['by  #TAUTHOR_TAG with 10 subjects.', 'table']","['the seminal work by  #AUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', ' #AUTHOR_TAG replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by  #TAUTHOR_TAG with 10 subjects.', 'table']",0
"[""''project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts""]","[""''project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts""]","['extracted word pairs from three different domain - specific corpora ( see table 2 ).', 'this is motivated by the aim to enable research in information retrieval incorporating sr measures.', ""in particular, the ` ` semantic information retrieval'' project ( sir  #TAUTHOR_TAG systematically investigates the use of lexical - semantic relations between words or concepts for improving the performance of information retrieval systems""]",4
"[' #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG']","[' #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG']","[' #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #TAUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['expanded to concept pairs using germanet  #TAUTHOR_TAG, the']","['expanded to concept pairs using germanet  #TAUTHOR_TAG, the']","['noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet  #TAUTHOR_TAG, the german equivalent to']","['implemented a set of filters for word pairs.', 'one group of filters removed unwanted word pairs.', 'word pairs are filtered if they contain at least one word that a ) has less than three letters b ) contains only uppercase letters ( mostly acronyms ) or c ) can be found in a stoplist.', 'another filter enforced a specified fraction of combinations of nouns ( n ), verbs ( v ) and adjectives ( a ) to be present in the result set.', 'we used the following parameters : were noun - noun pairs, 15 % noun - verb pairs and so on.', 'word pairs containing polysemous words are expanded to concept pairs using germanet  #TAUTHOR_TAG, the german equivalent to wordnet, as a sense inventory for each word.', 'it is the most complete resource of this type for german']",5
"['. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us']","['to some formal properties, e. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us']","['. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us']","['analysis can assess a measure with respect to some formal properties, e. g. whether a measure is a metric  #TAUTHOR_TAG. 4', 'however, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application']",0
"['.', ' #TAUTHOR_TAG reported an intra -']","['r =. 647.', ' #TAUTHOR_TAG reported an intra - subject correlation']","['.', ' #TAUTHOR_TAG reported an intra - subject correlation']","['our experiment, intra - subject correlation was r =. 670 for the first and r =. 623 for the second individual who repeated the experiment, yielding a summarized intra - subject correlation of r =. 647.', ' #TAUTHOR_TAG reported an intra - subject correlation of r =. 85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs.', 'the values may again not be compared directly.', 'furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low']",1
"['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'we used the revised experimental setup  #TAUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",5
"['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity']","['our experiment, we annotated a high number of pairs similar in size to the test sets by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'we used the revised experimental setup  #AUTHOR_TAG, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'we annotated semantic relatedness instead of similarity and included also non noun - noun pairs.', 'additionally, our corpusbased approach includes domain - specific technical terms and enables evaluation of the robustness of a measure']",1
"['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']","['are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']","['they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']","['similarity is typically defined via the lexical relations of synonymy ( automobile - car ) and hypernymy ( vehicle - car ), while semantic relatedness ( sr ) is defined to cover any kind of lexical or functional association that may exist be - tween two words  #AUTHOR_TAG. 3', 'dissimilar words can be semantically related, e. g.', 'via functional relationships ( night - dark ) or when they are antonyms ( high - low ).', 'many nlp applications require knowledge about semantic relatedness rather than just similarity  #TAUTHOR_TAG']",0
"['by  #TAUTHOR_TAG, similarity']","['by  #TAUTHOR_TAG, similarity']","['the seminal work by  #TAUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to']","['the seminal work by  #TAUTHOR_TAG, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'test subjects were instructed to order the cards according to the "" similarity of meaning "" and then assign a continuous similarity value ( 0. 0 - 4. 0 ) to each card.', ' #AUTHOR_TAG replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'this experiment was again replicated by  #AUTHOR_TAG with 10 subjects.', 'table']",0
"['. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","[', e. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g. word sense disambiguation  #TAUTHOR_TAG or malapropism detection  #AUTHOR_TAG.  #AUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
"['. g.', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['application, e. g.', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses']","['latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e. g.', 'word sense disambiguation  #AUTHOR_TAG or malapropism detection  #AUTHOR_TAG.', ' #TAUTHOR_TAG argue for application - specific evaluation of similarity measures, because measures are always used for some task.', 'but they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'a certain measure may work well in one application, but not in another.', 'application - based evaluation can only state the fact, but give little explanation about the reasons']",0
['- weighting scheme  #TAUTHOR_TAG'],['8 tf. idf - weighting scheme  #TAUTHOR_TAG'],['8 tf. idf - weighting scheme  #TAUTHOR_TAG'],"['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger  #AUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart ` ltc'8 tf. idf - weighting scheme  #TAUTHOR_TAG""]",5
"['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', ' #AUTHOR_TAG reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #TAUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']","['brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']","['brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']","['##et contains only a few conceptual glosses.', 'as they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e. g. for brother : ` ` brother, male sibling vs. ` ` brother, comrade, friend  #TAUTHOR_TAG.', 'we removed words which had more than three senses']",5
"['are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonym']","['are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and']","['to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonym']","[', manually selected word pairs are often biased towards highly related pairs  #AUTHOR_TAG, because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #TAUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs (']","['rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about']","['with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs (']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #AUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #TAUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #AUTHOR_TAG table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
"['using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. id""]","['using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme  #AUTHOR_TAG""]","['lemmatization ) are performed using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme  #AUTHOR_TAG""]","['three preprocessing steps ( tokenization, pos - tagging, lemmatization ) are performed using treetagger  #TAUTHOR_TAG.', ""the resulting list of pos - tagged lemmas is weighted using the smart'ltc'8 tf. idf - weighting scheme  #AUTHOR_TAG""]",5
"['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #TAUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['g.', 'dictionary - based  #TAUTHOR_TAG,']","['concepts have been proposed, e. g.', 'dictionary - based  #TAUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG']","['concepts have been proposed, e. g.', 'dictionary - based  #TAUTHOR_TAG,']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g.', 'dictionary - based  #TAUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #AUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
"['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']","['differences in meaning between senses are very fine - grained, distinguishing between them is hard even for humans  #TAUTHOR_TAG. 6', 'pairs containing such words are not suitable for evaluation.', 'to limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'words with a number of senses above the threshold are removed from the list']",0
"['pairs  #TAUTHOR_TAG, because human annotators']","['pairs  #TAUTHOR_TAG, because human annotators']","['pairs  #TAUTHOR_TAG, because human annotators']","['assume that due to lexical - semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'resulting from our corpus - based approach, test sets will also contain domain - specific terms.', 'previous studies only included general terms as opposed to domain - specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain - specific or technical terms.', 'this is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms ( porsche ) rather than general ones ( car ).', 'furthermore, manually selected word pairs are often biased towards highly related pairs  #TAUTHOR_TAG, because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'automatic corpus - based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical - semantic relations.', ' #AUTHOR_TAG pointed out that many relations between words in a text are non - classical ( i. e.', 'other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity']",0
"['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a']","['of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a correlation of']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', ' #AUTHOR_TAG reported a correlation of r =. 9026. 10', 'the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #TAUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['. 10', ' #TAUTHOR_TAG reported a']","['r =. 9026. 10', ' #TAUTHOR_TAG reported a']","['. 10', ' #TAUTHOR_TAG reported a correlation']","['summarized inter - subject correlation between 21 subjects was r =. 478 ( cf. is statistically significant at p <. 05.', 'this correlation coefficient is an upper bound of performance for automatic sr measures applied on the same dataset.', ' #AUTHOR_TAG reported a correlation of r =. 9026. 10', ' #TAUTHOR_TAG reported a correlation of r =. 9026. 10 the results are not directly comparable, because he only used noun - noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', ' #AUTHOR_TAG did not report inter - subject correlation for their larger dataset.', ' #AUTHOR_TAG reported a correlation of r =. 69.', 'test subjects were trained students of computational linguistics, and word pairs were selected analytically']",1
"['.', ' #TAUTHOR_TAG replicated the']","['of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #TAUTHOR_TAG replicated the']","['##ed continuous values is felt to overstrain the test subjects.', ' #TAUTHOR_TAG replicated the experiment']","['comprehensive evaluation of sr measures requires a higher number of word pairs.', 'however, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'furthermore, semantic relatedness is an intuitive concept and being forced to assign fine - grained continuous values is felt to overstrain the test subjects.', ' #TAUTHOR_TAG replicated the experiment of rubenstein and goodenough with the original 65 word pairs translated into german.', 'she used an adapted experimental setup where test subjects had to assign discrete values { 0, 1, 2, 3, 4 } and word pairs were presented in isolation.', 'this setup is also scalable to a higher number of word pairs ( 350 ) as was shown in  #AUTHOR_TAG.', ' #AUTHOR_TAG annotated a larger set of word pairs ( 353 ), too.', 'they used a 0 - 10 range of relatedness scores, but did not give further details about their experimental setup.', 'in psycholinguistics, relatedness of words can also be determined through association tests ( schulte im  #AUTHOR_TAG table 1 : comparison of previous experiments.', 'r / g = rubenstein and goodenough, m / c = miller and charles, res = resnik, fin = finkelstein, gur = gurevych, z / g = zesch and gurevych similarity from "" not similar "" to "" synonymous "".', 'this elaborate process is not feasible for a larger dataset or if domain - specific test sets should be compiled quickly.', 'therefore, we automatically create word pairs using a corpus - based approach']",0
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations ( like cooccurrence of words ) and is thus a more complicated task.', 'judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'while most people may agree 10 note that resnik used the averaged correlation coefficient.', 'we computed the summarized correlation coefficient using a fisher z - value transformation.', 'that ( car - vehicle ) are highly related, a strong connection between ( parts - speech ) may only be established by a certain group.', 'due to the corpusbased approach, many domain - specific concept pairs are introduced into the test set.', 'therefore, inter - subject correlation is lower than the results obtained by  #TAUTHOR_TAG']",1
"['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG']","['- based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used']","['approaches for computing semantic relatedness of words or concepts have been proposed, e. g. dictionary - based  #AUTHOR_TAG, ontology - based  #AUTHOR_TAG, information - based  #TAUTHOR_TAG or distributional  #AUTHOR_TAG.', 'the knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora']",0
[' #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG'],[' #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG'],[' #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG'],"['site based corpus annotation - in which the user can specify a web site to annotate • domain based corpus annotation - in which the user specifies a content domain ( with the use of keywords ) to annotate • crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages from a computational linguistic view, the framework will also need to take into account the granularity of the unit ( for example, pos tagging requires sentence - units, but anaphoric annotation needs paragraphs or larger ).', 'secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by  #TAUTHOR_TAG b ) and shingling techniques described by  #AUTHOR_TAG']",3
"["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","['', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]",0
"['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', 'this topic generated intense interest']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #TAUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', '']",0
"['escience  #TAUTHOR_TAG.', 'while such an approach promises much in']","['escience  #TAUTHOR_TAG.', 'while such an approach promises much in']","['escience  #TAUTHOR_TAG.', 'while such an approach promises much in']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for escience  #TAUTHOR_TAG.', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet  #AUTHOR_TAG.', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","["" #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]","['', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #TAUTHOR_TAG""]",0
"["" #TAUTHOR_TAG a ) and the linguist's search""]","["" #TAUTHOR_TAG a ) and the linguist's search""]","[""##er  #TAUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]","['', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #TAUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]",0
"['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special']","['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special']","['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special issue']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG, 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', '']",0
"['are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem,']","['are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem, e. g.', 'bn']","['key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'word frequency counts in internet search engines are inconsistent and unreliable  #TAUTHOR_TAG.', 'tools based on static corpora do not suffer from this problem, e. g.', 'bncweb 7, developed at the university of zurich, and view 8 ( variation in english words and phrases, developed at brigham young university ) are both based on the british national corpus.', 'both bncweb and view enable access to annotated corpora and facilitate searching on part - ofspeech tags.', 'in addition, pie 9 ( phrases in english ), developed at usna, which performs searches on n - grams ( based on words, parts - ofspeech and characters ), is currently restricted to the british national corpus as well, although other static corpora are being added to its database.', 'in contrast, little progress has been made toward annotating sizable sample corpora from the web']",0
['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue'],"['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two ap - proaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin - gle - server systems.', 'this corpus annotation bot - tleneck becomes even more problematic for vo - luminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguists search en - gine  #AUTHOR_TAG']",0
"['are well documented  #TAUTHOR_TAG.', '']","['are well documented  #TAUTHOR_TAG.', '']","['##to in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', '']","['', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
"['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', 'this topic generated intense interest']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #TAUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #AUTHOR_TAG.', '']",0
"['tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plug - in.', 'this will involve implementing new functionality and integrating this with our existing annotation tools (']","['second stage of our work will involve im - plementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding  #TAUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #AUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plug - in.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
"['internet  #TAUTHOR_TAG.', 'better known for']","['internet  #TAUTHOR_TAG.', 'better known for file - sharing']","['the internet  #TAUTHOR_TAG.', 'better known for']","['the areas of natural language processing ( nlp ) and computational linguistics, proposals have been made for using the computational grid for data - intensive nlp and text - mining for e - science  #AUTHOR_TAG.', 'while such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a p2p approach.', 'in simple terms, p2p is a technology that takes advantage of the resources and services available at the edge of the internet  #TAUTHOR_TAG.', 'better known for file - sharing and instant messenger applications, p2p has increasingly been applied in distributed computational systems.', 'examples include seti @ home ( looking for radio evidence of extraterrestrial life ), climateprediction. net ( studying climate change ), predictor @ home ( investigating protein - related diseases ) and einstein @ home ( searching for gravitational signals )']",0
"['digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation']","['digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plugin.', 'this will involve implementing new functionality and integrating this with our existing annotation tools (']","['second stage of our work will involve implementing the framework within a p2p environment.', ""we have already developed a prototype of an object - oriented application environment to support p2p system development using jxta ( sun's p2p api )."", 'we have designed this environment so that specific application functionality can be captured within plug - ins that can then integrate with the environment and utilise its functionality.', 'this system has been successfully tested with the development of plug - ins supporting instant messaging, distributed video encoding  #AUTHOR_TAG, distributed virtual worlds  #AUTHOR_TAG and digital library management  #TAUTHOR_TAG.', 'it is our intention to implement our distributed corpus annotation framework as a plugin.', 'this will involve implementing new functionality and integrating this with our existing annotation tools ( such as claws 11 ).', 'the development environment is also flexible enough to utilise the boinc platform, and such support will be built into it']",0
['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue'],"['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', '']",0
"['the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled']","['the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled']","['the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #AUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #TAUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]",0
"['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : web']","['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG,']","['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwic']","['', ' #TAUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', ""prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguist's search engine  #AUTHOR_TAG""]",0
"['##up of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']","['cleanup of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']","['##up of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']","['key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'most web - derived corpora have exploited raw text or html pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like hyppia - bte, tidy and parcels3  #TAUTHOR_TAG']",0
"['problem  #TAUTHOR_TAG.', 'this topic generated intense']","['problem  #TAUTHOR_TAG.', 'this topic generated intense']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #TAUTHOR_TAG.', 'this topic generated intense interest']","['corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible  #AUTHOR_TAG : 56 ) unless the web is used as a corpus  #AUTHOR_TAG.', 'increasingly, corpus researchers are tapping the web to overcome the sparse data problem  #TAUTHOR_TAG.', '']",0
[' #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line'],[' #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line'],"['the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system  #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line text collections.', 'gs']","['', 'a new collection does not become available for analysis until the lse completes the annotation process, which may entail significant delay with multiple users of the lse server.', 'the gsearch system  #TAUTHOR_TAG also selects sentences by syntactic criteria from large on - line text collections.', 'gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre - existing syntactic mark - up.', 'in contrast, the sketch engine system to assist lexicographers to construct dictionary entries requires large pre - annotated corpora.', ""a word sketch is an automatic one - page corpus - derived summary of a word's grammatical and collocational behaviour."", 'word sketches were first used to prepare the macmillan english dictionary for  #AUTHOR_TAG, edited by michael rundell ).', 'they have also served as the starting point for high - accuracy word sense disambiguation.', 'more recently, the sketch engine was used to develop the new edition of the oxford thesaurus of  #AUTHOR_TAG, edited by maurice waite )']",0
['word sense tagging  #TAUTHOR_TAG and in the'],['word sense tagging  #TAUTHOR_TAG and in the long - standing'],['word sense tagging  #TAUTHOR_TAG and in the long - standing use of part -'],"['annotation of corpora contributes crucially to the study of language at several levels : morphology, syntax, semantics, and discourse.', 'its significance is reflected both in the growing interest in annotation software for word sense tagging  #TAUTHOR_TAG and in the long - standing use of part - of - speech taggers, parsers and morphological analysers for data from english and many other languages']",0
['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special'],['teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue'],"['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g. automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws4, amal - gam5, connexor6 ) are available, for example, in the application of part - of - speech tags to corpora.', 'existing tagging systems are small scale and typically impose some limitation to prevent over - load ( e. g. restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #TAUTHOR_TAG 2004b ) and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #AUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text collected from a web crawler.', ' #AUTHOR_TAG built a corpus by iteratively searching google for a small set of seed terms.', 'prototypes of internet search engines for linguists, corpus linguists and lexicographers have been proposed : webcorp  #AUTHOR_TAG, kwicfinder  #AUTHOR_TAG a ) and the linguists search en - gine  #AUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG extracts word']","['the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #TAUTHOR_TAG extracts word co - occurrence probabilities from unlabelled']","['the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #TAUTHOR_TAG extracts word co - occurrence probabilities from unlabelled text']","['vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi - automatic combination of the two approaches e. g.', 'automated tagging followed by manual correction.', 'in most cases a stand - alone system or client - server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'only a handful of web - based or email services ( claws 4, amalgam 5, connexor 6 ) are available, for example, in the application of part - of - speech tags to corpora.', ""existing tagging systems are'small scale'and typically impose some limitation to prevent overload ( e. g."", 'restricted access or document size ).', 'larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single - server systems.', 'this corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'the use of the web as a corpus for teaching and research on language has been proposed a number of times  #AUTHOR_TAG fletcher,, 2004b and received a special issue of the journal computational linguistics  #AUTHOR_TAG.', 'studies have used several different methods to mine web data.', ' #TAUTHOR_TAG']",0
"['to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']","['to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']","['to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']","['will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'the initial solution will be to store the resulting reference 11 http : / / www. comp. lancs. ac. uk / ucrel / claws / corpus in the sketch engine.', 'we will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web - based corpus studies based in general.', 'current practise elsewhere includes the distribution of url lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by  #TAUTHOR_TAG a ).', 'other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here']",0
"['are well documented  #TAUTHOR_TAG.', '']","['are well documented  #TAUTHOR_TAG.', '']","['##to in april 2006.', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', '']","['', 'in addition, the advantages of using linguistically annotated data over raw data are well documented  #TAUTHOR_TAG.', 'as the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega - corpus']",0
[''],[''],[''],[''],0
"['do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']","['do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']","['believe that ownership has an important role and we do not want to force our users to take a non - attributive copyright licence to their work.', 'we consider the creative commons model as the most suitable one to let each author choose the rights to reserve  #TAUTHOR_TAG.', 'narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture  #AUTHOR_TAG']",5
