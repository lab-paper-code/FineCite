,citing_int_id,cited_int_id,intent_labels,section_name,citance,context,citing_ss_id,citing_title,citing_year,citing_authors,cited_ss_id,cited_title,cited_year,cited_authors
0,W06-2933,External_18690,[0],experiments,"Typical examples are Bulgarian <TARGET_CITATION/> , Chinese <CITATION/> , Danish <CITATION/> , and Swedish <CITATION/> .","before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. Typical examples are Bulgarian <TARGET_CITATION/> , Chinese <CITATION/> , Danish <CITATION/> , and Swedish <CITATION/> . Typical examples are Bulgarian <CITATION/>, Chinese <CITATION/>, Danish <CITATION/>, and Swedish <CITATION/>. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,7686796dae48a1d59f8c806ea4cab45eba99d2fe,Practical Annotation Scheme for an HPSG Treebank of Bulgarian,2003,K. Simov; P. Osenova
2,N04-2004,External_29663,[0],,"These observations and this line of reasoning has not escaped the attention of theoretical linguists : <TARGET_CITATION/> propose that argument structure is , in fact , encoded syntactically .","Sometimes, these functional elements are overtly realized, e.g., en. Often, however, these functional elements responsible for licensing event interpretations are not phonologically realized. These observations and this line of reasoning has not escaped the attention of theoretical linguists : <TARGET_CITATION/> propose that argument structure is , in fact , encoded syntactically . These observations and this line of reasoning has not escaped the attention of theoretical linguists: <CITATION/> propose that argument structure is, in fact, encoded syntactically. Often, however, these functional elements responsible for licensing event interpretations are not phonologically realized. Sometimes, these functional elements are overtly realized, e.g., en.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,244b8a2de2ff31039585ae8dbcf4b1ce29ed9460,On Argument Structure and the Lexical Expression of Syntactic Relations,1993,K. Hale; S. J. Keyser
3,W06-1639,External_17783,[0],related work,interdocument references in the form of hyperlinks <TARGET_CITATION/> .,"Relationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicit interdocument references in the form of hyperlinks <TARGET_CITATION/> . interdocument references in the form of hyperlinks <CITATION/>. Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitRelationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,22291dcd8b5255b6d0d51a86138ee5bf57b34550,Mining newsgroups using networks arising from social behavior,2003,R. Agrawal; S. Rajagopalan; R. Srikant; Yirong Xu
4,J06-2002,External_3883,[0],experiments,"While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."," While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . While IA is generally thought to be consistent with findings on human language production <CITATION/>, the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,881a30a7c9e39da5b0f2079e4a7b0eedcc5f63c1,Psychologie der Objektbenennung,1976,T. Herrmann; W. Deutsch
6,D08-1066,P08-1024,[5],conclusion,"Secondly , as <TARGET_CITATION/> show , marginalizing out the different segmentations during decoding leads to improved performance .","There are various strands of future research. Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior. Secondly , as <TARGET_CITATION/> show , marginalizing out the different segmentations during decoding leads to improved performance . Secondly, as <CITATION/> show, marginalizing out the different segmentations during decoding leads to improved performance. Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior. There are various strands of future research.",3701075b318ea45284c9805f2486f3a3177ec87f,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,2008,M. Mylonakis; K. Sima'an,02bcc68113cff36226eb9d977f7367f14e2157e5,A Discriminative Latent Variable Model for Statistical Machine Translation,2008,Phil Blunsom; Trevor Cohn; M. Osborne
8,W04-1610,External_9370,[0],related work,"More recently , <CITATION/> has performed a good survey of document categorization ; recent works can also be found in <TARGET_CITATION/> .","For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces. A good study comparing document categorization algorithms can be found in <CITATION/>. More recently , <CITATION/> has performed a good survey of document categorization ; recent works can also be found in <TARGET_CITATION/> . More recently, <CITATION/> has performed a good survey of document categorization; recent works can also be found in <CITATION/>. A good study comparing document categorization algorithms can be found in <CITATION/>. For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,learning to classify text using svm,2002,T Joachims
9,W01-1510,External_15846,[0],,The grammar conversion from LTAG to HPSG <TARGET_CITATION/> is the core portion of the RenTAL system ., The grammar conversion from LTAG to HPSG <TARGET_CITATION/> is the core portion of the RenTAL system . The grammar conversion from LTAG to HPSG <CITATION/> is the core portion of the RenTAL system.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,grammar conversion from fbltag to hpsg,2001,Naoki Yoshinaga; Yusuke Miyao
10,D11-1138,D07-1003,[0],introduction,"This includes work on question answering <TARGET_CITATION/> , sentiment analysis <CITATION/> , MT reordering <CITATION/> , and many other tasks .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <TARGET_CITATION/> , sentiment analysis <CITATION/> , MT reordering <CITATION/> , and many other tasks . This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,5ab6ddd1d45302bf635cce5cb93fbaf4ea79458a,What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA,2007,Mengqiu Wang; Noah A. Smith; T. Mitamura
12,D08-1007,J03-3005,[4],experiments,"We study the cases where a 9Recall that even the <TARGET_CITATION/> system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .","Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text <CITATION/>. We study the cases where a 9Recall that even the <TARGET_CITATION/> system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) . We study the cases where a 9Recall that even the <CITATION/> system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text <CITATION/>.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,5dfed29550d75cca99019aa52d40038dcb23b3cb,Using the Web to Obtain Frequencies for Unseen Bigrams,2003,Frank Keller; Mirella Lapata
13,Q13-1020,D09-1037,[1],method,"Inspired by <TARGET_CITATION/> , we define P ( str  frag ) as follows : where csw is the number of words in the source string .","c. Determine that the right child of PRP...RB is a twoword nonterminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the twoword nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB.generating the source string, which contains several source words and variables. Inspired by <TARGET_CITATION/> , we define P ( str  frag ) as follows : where csw is the number of words in the source string . Inspired by <CITATION/>, we define P(str  frag) as follows: where csw is the number of words in the source string. generating the source string, which contains several source words and variables. c. Determine that the right child of PRP...RB is a twoword nonterminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the twoword nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,dc605e05765f4948328525d1c60aeca58970afe1,A Bayesian Model of Syntax-Directed Tree to String Grammar Induction,2009,Trevor Cohn; Phil Blunsom
14,Q13-1020,P09-1088,[1],method,"Inspired by <TARGET_CITATION/> , we define P ( str  frag ) as follows : where csw is the number of words in the source string .","c. Determine that the right child of PRP...RB is a twoword nonterminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the twoword nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB.generating the source string, which contains several source words and variables. Inspired by <TARGET_CITATION/> , we define P ( str  frag ) as follows : where csw is the number of words in the source string . Inspired by <CITATION/>, we define P(str  frag) as follows: where csw is the number of words in the source string. generating the source string, which contains several source words and variables. c. Determine that the right child of PRP...RB is a twoword nonterminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the twoword nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,46e06cf27255dedcd40a99751dbc14d7fd44f80e,A Gibbs Sampler for Phrasal Synchronous Grammar Induction,2009,Phil Blunsom; Trevor Cohn; Chris Dyer; M. Osborne
15,W03-0806,External_74898,[0],experiments,"The TNT POS tagger <TARGET_CITATION/> has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .","An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit <CITATION/> which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging <CITATION/>. The TNT POS tagger <TARGET_CITATION/> has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second . The TNT POS tagger <CITATION/> has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second. Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit <CITATION/> which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging <CITATION/>. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,tnt  a statistical partofspeech tagger,2000,Thorsten Brants
16,P11-1134,External_13191,[0],introduction,"Some previous works <TARGET_CITATION/> indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in <CITATION/>, even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works <TARGET_CITATION/> indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words . Some previous works <CITATION/> indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. As emerges from the ablation tests reported in <CITATION/>, even the most common resources proved to have a positive impact on some systems and a negative impact on others. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,5dbf9d4c177a1cd207ccf205c7e223b90d0d867b,Extracting paraphrase patterns from bilingual parallel corpora,2009,Shiqi Zhao; Haifeng Wang; Ting Liu; Sheng Li
18,D13-1115,External_14809,[2],method,"To solve these scaling issues , we implement Online Variational Bayesian Inference <TARGET_CITATION/> for our models .","Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues , we implement Online Variational Bayesian Inference <TARGET_CITATION/> for our models . To solve these scaling issues, we implement Online Variational Bayesian Inference <CITATION/> for our models. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,2d8cbd7370b4ce666edd864e66f83ebf20963516,Online Learning for Latent Dirichlet Allocation,2010,M. Hoffman; D. Blei; F. Bach
19,W00-1312,External_24243,[4],related work,Other studies which view lR as a query generation process include <TARGET_CITATION/> .," Other studies which view lR as a query generation process include <TARGET_CITATION/> . Other studies which view lR as a query generation process include <CITATION/>; Miller et al, 1999.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,on relevance probabilistic indexing and information retrievalquot,1960,M E Maron; K L Kuhns
20,P07-1068,N06-1025,[0],introduction,"As a result , researchers have readopted the oncepopular knowledgerich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , <CITATION/> ) , their semantic similarity as computed using WordNet ( e.g. , <CITATION/> ) or Wikipedia <TARGET_CITATION/> , and the contextual role played by an NP ( see <CITATION/> ) .","While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have readopted the oncepopular knowledgerich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , <CITATION/> ) , their semantic similarity as computed using WordNet ( e.g. , <CITATION/> ) or Wikipedia <TARGET_CITATION/> , and the contextual role played by an NP ( see <CITATION/> ) . As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>). In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,6d375f8b45d8e5e961875bf74a89f32394759b89,"Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution",2006,Simone Paolo Ponzetto; M. Strube
21,W01-1510,External_325,[0],introduction,"There have been many studies on parsing techniques <TARGET_CITATION/> , ones on disambiguation models <CITATION/> , and ones on programming/grammardevelopment environ ","Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques <TARGET_CITATION/> , ones on disambiguation models <CITATION/> , and ones on programming/grammardevelopment environ There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environStrongly equivalent grammars enable the sharing of ideas developed in each formalism. Our concern is, however, not limited to the sharing of grammars and lexicons.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,twostep tag parsing revisited,1998,Peter Poller; Tilman Becker
22,W03-0806,W02-2018,[0],experiments,"An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by <CITATION/> that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly <TARGET_CITATION/> .","Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by <CITATION/> that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly <TARGET_CITATION/> . An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. However, it will be increasingly important as techniques become more complex and corpus sizes grow. Efficiency has not been a focus for NLP research in general.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,878783964ab23c97052ea82685368099d85c500d,A Comparison of Algorithms for Maximum Entropy Parameter Estimation,2002,Robert Malouf
23,W02-1601,External_76424,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,,examplebased machine translation,1991,S Sato
24,N10-1084,External_36166,[0],related work,"Later works , such as <TARGET_CITATION/> ) , <CITATION/> , further made use of partofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by <CITATION/>. Later works , such as <TARGET_CITATION/> ) , <CITATION/> , further made use of partofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method . Later works, such as <CITATION/>, further made use of partofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. The first lexical substitution method was proposed by <CITATION/>. The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,d64dc949b8ceae642e8fcad7ccd29ee523116371,Natural language processing for information assurance and security: an overview and implementations,2001,M. Atallah; C. McDonough; V. Raskin; S. Nirenburg
26,W06-3813,External_15779,[2],experiments,"We work with a semitechnical text on meteorological phenomena <TARGET_CITATION/> , meant for primary school students ."," We work with a semitechnical text on meteorological phenomena <TARGET_CITATION/> , meant for primary school students . We work with a semitechnical text on meteorological phenomena <CITATION/>, meant for primary school students.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,32141024037e4cfb5bd47ef1016b80e3ac59677b,Junior science book of icebergs and glaciers,1961,P. Lauber
28,D08-1034,N04-1032,[2],,The candidate feature templates include : Voice from <TARGET_CITATION/> .,"First, we built a pool of feature templates which has proven to be useful on the SRC. Most of the feature templates are standard, so only the new ones will be explained. The candidate feature templates include : Voice from <TARGET_CITATION/> . The candidate feature templates include: Voice from <CITATION/>. Most of the feature templates are standard, so only the new ones will be explained. First, we built a pool of feature templates which has proven to be useful on the SRC.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,df87a18357f3c462fc21df0ce9f60160714fd1e3,Shallow Semantic Parsing of Chinese,2004,H. Sun; Dan Jurafsky
29,P97-1063,External_6772,[0],introduction,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <TARGET_CITATION/> ."," Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <TARGET_CITATION/> . Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,the mathematics of statistical machine translation parameter estimationquot,1993,P F Brown; V J Della Pietra; S A Della Pietra; R L Mercer
31,D08-1034,External_1123,[2],,The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank <TARGET_CITATION/> ., The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank <TARGET_CITATION/> . The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank <CITATION/>.,74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,2c72257ae7a4a32dc60569f4e1fe4504b2678112,The Penn Chinese TreeBank: Phrase structure annotation of a large corpus,2005,Nianwen Xue; Fei Xia; Fu-Dong Chiou; Martha Palmer
32,W06-1104,External_61155,[0],related work,"Furthermore , manually selected word pairs are often biased towards highly related pairs <TARGET_CITATION/> , because human annotators tend to select only highly related pairs connected by relations they are aware of .","Previous studies only included general terms as opposed to domainspecific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domainspecific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore , manually selected word pairs are often biased towards highly related pairs <TARGET_CITATION/> , because human annotators tend to select only highly related pairs connected by relations they are aware of . Furthermore, manually selected word pairs are often biased towards highly related pairs <CITATION/>, because human annotators tend to select only highly related pairs connected by relations they are aware of. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Previous studies only included general terms as opposed to domainspecific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domainspecific or technical terms.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3b5b95038c6b065f29649c1b11ea3e7855c00a53,Thinking beyond the nouns - computing semantic relatedness across parts of speech,2006,Iryna Gurevych
33,W00-1017,P91-1040,[2],,Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions <TARGET_CITATION/> .,ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time. The domaindependent knowledge used in this module consists of a unificationbased lexicon and phrase structure rules. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions <TARGET_CITATION/> . Disjunctive feature descriptions are also possible; WIT incorporates an efficient method for handling disjunctions <CITATION/>. The domaindependent knowledge used in this module consists of a unificationbased lexicon and phrase structure rules. ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.,143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,8412d17cca783d67c6eac3ef84429a76b92cfe84,Constraint Projection: An Efficient Treatment of Disjunctive Feature Descriptions,1991,Mikio Nakano
34,K15-1003,N07-1018,[2],,Our strategy is based on the approach presented by <TARGET_CITATION/> .,"We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences. Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. Our strategy is based on the approach presented by <TARGET_CITATION/> . Our strategy is based on the approach presented by <CITATION/>. Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution. We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,ece4a51f9d1fab08230a527efbb801c57e0249c5,Bayesian Inference for PCFGs via Markov Chain Monte Carlo,2007,Mark Johnson; T. Griffiths; S. Goldwater
35,W04-1805,External_8695,[0],method,"Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see <TARGET_CITATION/> for a review ) , these patterns allow :","The background knowledge encodes information about each word occurring in the example or counterexample, namely the meaning of its tag (e.g., adjective in plural form, infinitive verb). The main benefits of this acquisition technique lie in the inferred patterns. Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see <TARGET_CITATION/> for a review ) , these patterns allow : Indeed, contrary to the more classical statistical methods (Mutual Information, Loglike..., see below) used for collocation acquisition (see <CITATION/> for a review), these patterns allow:The main benefits of this acquisition technique lie in the inferred patterns. The background knowledge encodes information about each word occurring in the example or counterexample, namely the meaning of its tag (e.g., adjective in plural form, infinitive verb).",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,2f0b726dcf4766331a656f9a029fd9bd7df97173,A Comparative Evaluation of Collocation Extraction Techniques,2002,Darren Pearce
36,J06-2002,External_29893,[0],experiments,"A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( <TARGET_CITATION/> , pages 10  12 ) .","straighforward (Section 9.3). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.) A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( <TARGET_CITATION/> , pages 10  12 ) . A further complication is that different speakers can regard very different values as prototypical, making it difficult to assess which of two objects is greener even on one dimension (Berlin and Kay 1969, pages 1012). (The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.) straighforward (Section 9.3).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,d86ec68c5f32d2e517163e020bb8af92f8e27f05,Basic color terms in Thai,2022,Nischanade Panitanang; C. Phuangsuwan; Mitsuo Ikeda
37,P02-1001,External_58,[2],,"The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della <TARGET_CITATION/> ; Chen and","To learn the weights A, v, , , count expected heads/tails for each coin.  If arc probabilities (or even A, v, , ) have loglinear parameterization, then the E step must compute c = Ei ecf(xi, yi), where ec(x, y) denotes the expected vector of total feature counts along a random path in f whose (input, output) matches (x, y). The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della <TARGET_CITATION/> ; Chen and The M step then treats c as fixed, observed data and adjusts 0 until the predicted vector of total feature counts equals c, using Improved Iterative Scaling (Della <CITATION/>; Chen and If arc probabilities (or even A, v, , ) have loglinear parameterization, then the E step must compute c = Ei ecf(xi, yi), where ec(x, y) denotes the expected vector of total feature counts along a random path in f whose (input, output) matches (x, y). To learn the weights A, v, , , count expected heads/tails for each coin.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,b951b9f78b98a186ba259027996a48e4189d37e5,Inducing Features of Random Fields,1995,S. D. Pietra; V. D. Pietra; J. Lafferty
38,P10-2059,External_1704,[2],introduction,Both kinds of annotation were carried out using ANVIL <TARGET_CITATION/> .,"In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels  including dialogue acts  and gesture annotation. Both kinds of annotation were carried out using ANVIL <TARGET_CITATION/> . Both kinds of annotation were carried out using ANVIL <CITATION/>. For this study, we added semantic labels  including dialogue acts  and gesture annotation. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,32d19b46dfef6ec36040986df955866f13699039,Gesture generation by imitation: from human behavior to computer character animation,2005,Michael Kipp
39,W14-1704,W12-2032,[2],experiments,The article classifier is a discriminative model that draws on the stateoftheart approach described in <TARGET_CITATION/> .,"Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only ngram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in <CITATION/>). The article classifier is a discriminative model that draws on the stateoftheart approach described in <TARGET_CITATION/> . The article classifier is a discriminative model that draws on the stateoftheart approach described in <CITATION/>. Because the Google corpus does not contain complete sentences but only ngram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in <CITATION/>). Thus, the classifiers trained on the learner data make use of a discriminative model.",1a40105b9eda0cecdd596b5758fb4ad85b7f636b,The Illinois-Columbia System in the CoNLL-2014 Shared Task,2014,Alla Rozovskaya; Kai-Wei Chang; Mark Sammons; D. Roth; Nizar Habash,0e429a43ecc59535aa0c447361c6b0ef0b9f2bdb,The UI System in the HOO 2012 Shared Task on Error Correction,2012,Alla Rozovskaya; Mark Sammons; D. Roth
40,D13-1115,External_9241,[0],related work,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) ."," The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) . The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,869171b2f56cfeaa9b81b2626cb4956fea590a57,Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope,2001,A. Oliva; A. Torralba
41,J02-3002,External_86962,[0],introduction,"<TARGET_CITATION/> ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that  sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''","Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said ... , or they can be just capitalized common words, as in White elephants are .... The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably. <TARGET_CITATION/> ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that  sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . '' <CITATION/> studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that sometimes case variants refer to the same thing (hurricane and Hurricane), sometimes they refer to different things (continental and Continental) and sometimes they don't refer to much of anything (e.g., anytime and Anytime).'' The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably. Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said ... , or they can be just capitalized common words, as in White elephants are ....",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,one term or two” in,1995,Kenneth Church
42,J04-3001,External_9007,[2],,We follow the notation convention of <TARGET_CITATION/> .,"For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form.14 The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of <TARGET_CITATION/> . We follow the notation convention of <CITATION/>. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form.14 The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language,1990,Karim A Lari; Steve J Young
43,N10-1084,External_56141,[0],related work,"Later works , such as <TARGET_CITATION/> ) , further made use of partofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by <CITATION/>. Later works , such as <TARGET_CITATION/> ) , further made use of partofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method . Later works, such as <CITATION/>, further made use of partofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. The first lexical substitution method was proposed by <CITATION/>. The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,0d3ba30dd4c6a29349539e6c94ab0ffd95c22b3e,Words are not enough: sentence level natural language watermarking,2006,Mercan Topkara; Umut Topkara; M. Atallah
44,W06-1104,External_20456,[4],experiments,<TARGET_CITATION/> did not report intersubject correlation for their larger dataset .,"This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. <CITATION/> reported a correlation of r=.9026.10 The results are not directly comparable, because he only used nounnoun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. <TARGET_CITATION/> did not report intersubject correlation for their larger dataset . <CITATION/> did not report intersubject correlation for their larger dataset. <CITATION/> reported a correlation of r=.9026.10 The results are not directly comparable, because he only used nounnoun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,e0c01df98a6b633b25c96c1a99b713ac96f1c5be,Placing search in context: the concept revisited,2002,L. Finkelstein; E. Gabrilovich; Yossi Matias; E. Rivlin; Zach Solan; G. Wolfman; E. Ruppin
45,N13-1036,W11-2602,[4],,This is a similar conclusion to our previous work in <TARGET_CITATION/> .,The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries. Comparing the best performer to the OOV selection mode system shows that translating low frequency invocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation. This is a similar conclusion to our previous work in <TARGET_CITATION/> . This is a similar conclusion to our previous work in <CITATION/>.Comparing the best performer to the OOV selection mode system shows that translating low frequency invocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation. The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.,75c71a379a3e5a9641f879a76860071f0f923b16,Dialectal Arabic to English Machine Translation: Pivoting through Modern Standard Arabic,2013,Wael Salloum; Nizar Habash,aaf1c2c9f5057ae675b1bb4ca56922b8750a580d,Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation,2011,Wael Salloum; Nizar Habash
46,J09-4010,External_20399,[2],,We then use the program Snob <TARGET_CITATION/> to cluster these experiences .,"We train the system by clustering the experiences'' of the responsegeneration methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program Snob <TARGET_CITATION/> to cluster these experiences . We then use the program Snob <CITATION/> to cluster these experiences. We train the system by clustering the experiences'' of the responsegeneration methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,a1d76ee9b6fb4d441e31abcd4dadc4e44c576017,An Information Measure for Classification,1968,C. S. Wallace; D. Boulton
47,W00-1017,P99-1026,[2],experiments,The priorities are used for disambiguating interpretation in the incremental understanding method <TARGET_CITATION/> .,"It is possible to add to the rules constraints that stipulate relationships that must hold among variables <CITATION/>, but we do not explain these constraints in detail in thispaper. The priorities are used for disambiguating interpretation in the incremental understanding method <TARGET_CITATION/> . The priorities are used for disambiguating interpretation in the incremental understanding method <CITATION/>. paper. It is possible to add to the rules constraints that stipulate relationships that must hold among variables <CITATION/>, but we do not explain these constraints in detail in this",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,b9317d1aa658d94f18fe7cbad6c8ab3ac64b73f9,Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems,1999,Mikio Nakano; Noboru Miyazaki; Jun-ichi Hirasawa; Kohji Dohsaka; T. Kawabata
48,A00-1004,External_15612,[0],method,"A number of alignment techniques have been proposed , varying from statistical methods <TARGET_CITATION/> to lexical methods <CITATION/> .","Some are highly parallel and easy to align while others can be very noisy. Aligning EnglishChinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages. A number of alignment techniques have been proposed , varying from statistical methods <TARGET_CITATION/> to lexical methods <CITATION/> . A number of alignment techniques have been proposed, varying from statistical methods <CITATION/> to lexical methods <CITATION/>. Aligning EnglishChinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages. Some are highly parallel and easy to align while others can be very noisy.",14ffbd58082d1197ea454ec9162b5cfd36cac9f9,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,2000,Jiang Chen; Jian-Yun Nie,4fe2a45babab10c1bfae05d2464363f4e52bbaf9,A Program for Aligning Sentences in Bilingual Corpora,1993,W. Gale; Kenneth Ward Church
49,J00-4002,External_32773,[0],,"<TARGET_CITATION/> present an illustrative firstorder fragment along these lines and are able to supply a coherent formal semantics for the CLFQLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .","QLF formulas containing quantifiers are prefixed by a scoping metavariable which scoping resolution rules instantiate to a list of the indices associated with quantifiers, in an order that indicates the preferred scoping:The denotational semantics of RQLF structures involving instantiated scope and referent metavariables is given in terms of simple interpretation rules that have the effect of interpreting the quantifiers as having the scopes indicated by the lists of indices, and the pronouns as having the interpretation of the instantiation of the metavariable (in these cases at least). <TARGET_CITATION/> present an illustrative firstorder fragment along these lines and are able to supply a coherent formal semantics for the CLFQLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise . <CITATION/> present an illustrative firstorder fragment along these lines and are able to supply a coherent formal semantics for the CLFQLFs themselves, using a technique essentially equivalent to supervaluations: a QLF is true iff all its possible RQLFs are, false iff they are all false, and undefined otherwise. The denotational semantics of RQLF structures involving instantiated scope and referent metavariables is given in terms of simple interpretation rules that have the effect of interpreting the quantifiers as having the scopes indicated by the lists of indices, and the pronouns as having the interpretation of the instantiation of the metavariable (in these cases at least). QLF formulas containing quantifiers are prefixed by a scoping metavariable which scoping resolution rules instantiate to a list of the indices associated with quantifiers, in an order that indicates the preferred scoping:",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,e87314e4e465ff21001e04a38b8fcdfeadae714e,Monotonic Semantic Interpretation,1992,H. Alshawi; Dick Crouch
50,N13-1036,P07-2045,[2],,We use the opensource Moses toolkit <TARGET_CITATION/> to build a phrasebased SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data ., We use the opensource Moses toolkit <TARGET_CITATION/> to build a phrasebased SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data . We use the opensource Moses toolkit <CITATION/> to build a phrasebased SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.,75c71a379a3e5a9641f879a76860071f0f923b16,Dialectal Arabic to English Machine Translation: Pivoting through Modern Standard Arabic,2013,Wael Salloum; Nizar Habash,4ee2eab4c298c1824a9fb8799ad8eed21be38d21,Moses: Open Source Toolkit for Statistical Machine Translation,2007,Philipp Koehn; Hieu T. Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; N. Bertoldi; Brooke Cowan; Wade Shen; C. Moran; Richard Zens; Chris Dyer; Ondrej Bojar; Alexandra Constantin; Evan Herbst
51,J97-4003,External_1396,[4],related work,"This approach is taken , for example , in LKB <TARGET_CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .","Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB <TARGET_CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) . This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). Another common approach to lexical rules is to encode them as unary phrase structure rules.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,the representation of lexical semantic information cognitive science research paper csrp 280,1992,Ann Copestake
53,J02-3002,W01-0516,[0],,<TARGET_CITATION/> recently described a hybrid method for finding abbreviations and their definitions .,The main reason for restricting abbreviation discovery to a single document isthat this does not presuppose the existence of a corpus in which the current document is similar to other documents. <TARGET_CITATION/> recently described a hybrid method for finding abbreviations and their definitions . <CITATION/> recently described a hybrid method for finding abbreviations and their definitions. that this does not presuppose the existence of a corpus in which the current document is similar to other documents. The main reason for restricting abbreviation discovery to a single document is,3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,f673e2b1d125ab30c7b27612b941fa9b3787c3e8,Hybrid Text Mining for Finding Abbreviations and their Definitions,2001,Youngja Park; Roy J. Byrd
54,J03-3004,T75-2013,[0],introduction,"More specifically , the notion of the phrasal lexicon ( used first by <TARGET_CITATION/> ) has been used successfully in a number of areas :","<CITATION/> observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics. More specifically , the notion of the phrasal lexicon ( used first by <TARGET_CITATION/> ) has been used successfully in a number of areas : More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics. <CITATION/> observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,745eb34b5b341af868bbf1a87ea9dd03110c1cde,The Phrasal Lexicon,1975,Joseph D. Becker
55,J05-3003,W93-0109,[0],related work,<TARGET_CITATION/> run a finitestate NP parser on a POStagged corpus to calculate the relative frequency of the same six subcategorization verb classes .,The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. <TARGET_CITATION/> run a finitestate NP parser on a POStagged corpus to calculate the relative frequency of the same six subcategorization verb classes . <CITATION/> run a finitestate NP parser on a POStagged corpus to calculate the relative frequency of the same six subcategorization verb classes. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. The frames do not include details of specific prepositions.,ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,1a4e9c79bf09d7071895261df0ea762a7e4f389a,The Automatic Acquisition of Frequencies of Verb Subcategorization Frames from Tagged Corpora,2002,Akira Ushioda; David A. Evans; Ted Gibson; A. Waibel
58,J06-2002,External_18164,[0],experiments,<TARGET_CITATION/> asked subjects to identify the target of a vague description in a visual scene ., <TARGET_CITATION/> asked subjects to identify the target of a vague description in a visual scene . <CITATION/> asked subjects to identify the target of a vague description in a visual scene.,0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,74f6152e2c68a5eac9c4d1d6bf3dd08dc73368ca,Achieving incremental semantic interpretation through contextual representation,1999,Julie C. Sedivy; M. Tanenhaus; C. Chambers; G. Carlson
59,D10-1002,N10-1003,[4],conclusion,"Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches <CITATION/> and products of latent variable grammars <TARGET_CITATION/> , despite being a single generative PCFG .","First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single selftrained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches <CITATION/> and products of latent variable grammars <TARGET_CITATION/> , despite being a single generative PCFG . Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches <CITATION/> and products of latent variable grammars <CITATION/>, despite being a single generative PCFG. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single selftrained grammars.",e441126a8dd0cb8363272b7b54207ae92e155bc0,Self-Training with Products of Latent Variable Grammars,2010,Zhongqiang Huang; M. Harper; Slav Petrov,9dccaf6ea0fa19772cf8067295b16df3eb7b4dda,Products of Random Latent Variable Grammars,2010,Slav Petrov
60,P10-4003,E03-1075,[3],experiments,Our recovery policy is modeled on the TargetedHelp <TARGET_CITATION/> policy used in taskoriented dialogue .,"At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy <CITATION/>. Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp <TARGET_CITATION/> policy used in taskoriented dialogue . Our recovery policy is modeled on the TargetedHelp <CITATION/> policy used in taskoriented dialogue. Since the system accepts unrestricted input, interpretation errors are unavoidable. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,6278dd4e0b69afd39ef25d9fba3500dff87cb7d9,Targeted Help for Spoken Dialogue Systems,2003,Beth Ann Hockey; Oliver Lemon; E. Campana; Laura M. Hiatt; Gregory Aist; J. Hieronymus; A. Gruenstein; J. Dowding
61,J06-2002,External_2473,[4],experiments,"It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process <TARGET_CITATION/> ."," It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process <TARGET_CITATION/> . It has been argued that, in an incremental approach, gradable properties should be given a low preference ranking because they are difficult to process <CITATION/>.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,efficient contextsensitive generation of referring expressions,2002,Emiel Krahmer; Mari¨et Theune
63,J09-4010,External_42507,[2],method,"Specifically , we used Decision Graphs <TARGET_CITATION/> for DocPred , and SVMs <CITATION/> for SentPred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .","The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically , we used Decision Graphs <TARGET_CITATION/> for DocPred , and SVMs <CITATION/> for SentPred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) . Specifically, we used Decision Graphs <CITATION/> for DocPred, and SVMs <CITATION/> for SentPred.11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,73f1d17df0e1232da9e2331878a802a941f351c6,Decision Graphs - An Extension of Decision Trees,1993,Jonathan J. Oliver
64,J00-2004,P98-1033,[0],method,"There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars <TARGET_CITATION/> , lexical conceptual structures <CITATION/> and WordNet synsets <CITATION/> .","ways to generate an assignment of size 1.6 It follows that the probability of generating a pair of bags (B1, B2) with a particular assignment A of size 1 isThe above equation holds regardless of how we represent concepts. There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars <TARGET_CITATION/> , lexical conceptual structures <CITATION/> and WordNet synsets <CITATION/> . There are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars <CITATION/>, lexical conceptual structures <CITATION/> and WordNet synsets <CITATION/>. The above equation holds regardless of how we represent concepts. ways to generate an assignment of size 1.6 It follows that the probability of generating a pair of bags (B1, B2) with a particular assignment A of size 1 is",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,6edb0995aae6a7891d50fcc2013f9d8d5b299433,Building Parallel LTAG for French and Italian,1998,Marie Candito
65,P11-1134,External_1008,[0],introduction,"ones , DIRT <CITATION/> , VerbOcean <CITATION/> , FrameNet <TARGET_CITATION/> , and Wikipedia <CITATION/> .","Besides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>. These include, just to mention the most popular ones , DIRT <CITATION/> , VerbOcean <CITATION/> , FrameNet <TARGET_CITATION/> , and Wikipedia <CITATION/> . ones, DIRT <CITATION/>, VerbOcean <CITATION/>, FrameNet <CITATION/>, and Wikipedia <CITATION/>. These include, just to mention the most popularBesides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,547f23597f9ec8a93f66cedaa6fbfb73960426b1,The Berkeley FrameNet Project,1998,Collin F. Baker; C. Fillmore; John B. Lowe
66,J97-4003,External_98507,[1],introduction,"In the latter case , we can also take care of transferring the value of z. However , as discussed by <TARGET_CITATION/> , creating several instances of lexical rules can be avoided .","To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case. In the above example, this would result in two lexical rules: one for words with t1 as their c value and one for those with t2 as their c value. In the latter case , we can also take care of transferring the value of z. However , as discussed by <TARGET_CITATION/> , creating several instances of lexical rules can be avoided . In the latter case, we can also take care of transferring the value of z. However, as discussed by <CITATION/>, creating several instances of lexical rules can be avoided. In the above example, this would result in two lexical rules: one for words with t1 as their c value and one for those with t2 as their c value. To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,6fe64d5e31abdf3d3673decbafc7d6ddfaab1e24,"On Implementing an HPSG theory - Aspects of the logical architecture, the formalization, and the implementation of head-driven phrase structure grammars",1994,Walt Detmar Meurers
67,J00-3003,J95-2001,[0],,"It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct <TARGET_CITATION/> .","Dialogue Act Modeling sequence with the highest posterior probability:The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <CITATION/> and tagging <CITATION/>. It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct <TARGET_CITATION/> . It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct <CITATION/>. The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <CITATION/> and tagging <CITATION/>. Dialogue Act Modeling sequence with the highest posterior probability:",22d45dadde6b5837eff11dc031045754bc5901c3,Dialogue act modeling for automatic tagging and recognition of conversational speech,2000,A. Stolcke; K. Ries; N. Coccaro; Elizabeth Shriberg; R. Bates; Dan Jurafsky; P. Taylor; Rachel Martin; C. V. Ess-Dykema; M. Meteer,becd3afeacb87c3b673254c2b418988bb40fe8dd,Automatic Stochastic Tagging of Natural Language Texts,1995,E. Dermatas; G. Kokkinakis
68,W02-1601,C88-1013,[2],,A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in <TARGET_CITATION/> .,"It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (01) for John'', (12) for picks'', (23) for the'', (34) for box'' and (45) for up''. A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in <TARGET_CITATION/> . A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in <CITATION/>.An interval is assigned to each word in the sentence, i.e. (01) for John'', (12) for picks'', (23) for the'', (34) for box'' and (45) for up''. It contains a nonprojective correspondence.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,545e6410b23db2b0c6d3178430f61cb472a41e5e,Representation Trees and String-Tree Correspondences,1988,C. Boitet; Yusoff Zaharin
69,D10-1056,J92-4003,[0],conclusion,"We found that the oldest system <TARGET_CITATION/> yielded the best prototypes , and that using these prototypes gave stateoftheart performance on WSJ , as well as improvements on nearly all of the nonEnglish corpora .","We found that on nonEnglish languages, Clark's (2003) system performed best. Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes. We found that the oldest system <TARGET_CITATION/> yielded the best prototypes , and that using these prototypes gave stateoftheart performance on WSJ , as well as improvements on nearly all of the nonEnglish corpora . We found that the oldest system <CITATION/> yielded the best prototypes, and that using these prototypes gave stateoftheart performance on WSJ, as well as improvements on nearly all of the nonEnglish corpora. Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes. We found that on nonEnglish languages, Clark's (2003) system performed best.",48d704f56d074a72f23d4dea85c8202337d20a13,Two Decades of Unsupervised POS Induction: How Far Have We Come?,2010,Christos Christodoulopoulos; S. Goldwater; Mark Steedman,3de5d40b60742e3dfa86b19e7f660962298492af,Class-Based n-gram Models of Natural Language,1992,P. Brown; V. D. Pietra; P. D. Souza; J. Lai; R. Mercer
70,N01-1012,W98-0708,[0],,Other definitions of predicates may be found in <TARGET_CITATION/> .,"4 Verbs of CommunicationIn this section, we explain the predicate communicate so that the algorithm may be followed by the reader. Other definitions of predicates may be found in <TARGET_CITATION/> . Other definitions of predicates may be found in <CITATION/>. In this section, we explain the predicate communicate so that the algorithm may be followed by the reader. 4 Verbs of Communication",8c4b26ee4b838e204c71fe36f467fbbcbdd72652,An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet,2001,F. Gomez,39b402c924dda1be8ab8611ccd5a3e5ed0f1ee8b,Linking WordNet Verb Classes to Semantic Interpretation,1998,F. Gomez
71,P11-1134,N10-1146,[4],experiments,"For the sake of completeness , we report in this section also the results obtained adopting the  basic solution '' proposed by <TARGET_CITATION/> .","Building on the positive results achieved on the crosslingual scenario, we investigate the possibility to exploit bilingual parallel corpora in the traditional monolingual scenario. Using the same approach discussed in Section 4, we compare the results achieved with English paraphrase tables with those obtained with other widely used monolingual knowledge resources over two RTE datasets. For the sake of completeness , we report in this section also the results obtained adopting the  basic solution '' proposed by <TARGET_CITATION/> . For the sake of completeness, we report in this section also the results obtained adopting the basic solution'' proposed by <CITATION/>. Using the same approach discussed in Section 4, we compare the results achieved with English paraphrase tables with those obtained with other widely used monolingual knowledge resources over two RTE datasets. Building on the positive results achieved on the crosslingual scenario, we investigate the possibility to exploit bilingual parallel corpora in the traditional monolingual scenario.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,4876b9d79886960e034a5d52adcdad640b363c76,Syntactic/Semantic Structures for Textual Entailment Recognition,2010,Yashar Mehdad; Alessandro Moschitti; Fabio Massimo Zanzotto
74,W06-1104,External_8822,[0],experiments,"If differences in meaning between senses are very finegrained , distinguishing between them is hard even for humans <TARGET_CITATION/> .6 Pairs containing such words are not suitable for evaluation .","An external dictionary of word senses is necessary for this step. It is also used to add a gloss for each word sense that enables test subjects to distinguish between senses. If differences in meaning between senses are very finegrained , distinguishing between them is hard even for humans <TARGET_CITATION/> .6 Pairs containing such words are not suitable for evaluation . If differences in meaning between senses are very finegrained, distinguishing between them is hard even for humans <CITATION/>.6 Pairs containing such words are not suitable for evaluation. It is also used to add a gloss for each word sense that enables test subjects to distinguish between senses. An external dictionary of word senses is necessary for this step.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,9462ed90e4480058d5b15e6bceb7b3cc3d13410b,Automatic generation of a coarse grained WordNet,2001,Rada Mihalcea; D. Moldovan
76,D13-1115,External_780,[0],related work,"<CITATION/> helped pave the path for cognitivelinguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <TARGET_CITATION/> in the prediction of association norms .","1http://stephenroller.com/research/ emnlp13cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). <CITATION/> helped pave the path for cognitivelinguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <TARGET_CITATION/> in the prediction of association norms . <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). 1http://stephenroller.com/research/ emnlp13",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,fb62673397115e06defd5e0f6586e3eea1809b44,Richard Harshman Indexing by Latent Semantic Analysis,1990,S. Deerwester; S. Dumais; G. Furnas; T. Landauer
77,W06-3309,N04-1015,[4],related work,"Although not the first to employ a generative approach to directly model content , the seminal work of <TARGET_CITATION/> is a noteworthy point of reference and comparison ."," Although not the first to employ a generative approach to directly model content , the seminal work of <TARGET_CITATION/> is a noteworthy point of reference and comparison . Although not the first to employ a generative approach to directly model content, the seminal work of <CITATION/> is a noteworthy point of reference and comparison.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,984efc8932edb635d09ec1a5fd8fc1d1ceccad45,"Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",2004,R. Barzilay; Lillian Lee
78,D10-1123,D08-1009,[0],conclusion,"Others include selectional preferences , transitivity <TARGET_CITATION/> , mutual exclusion , symmetry , etc. .","We run our techniques on a large set of relations to output a first repository of typed functional relations. We release this list for further use by the research community.2 Future Work: Functionality is one of the several properties a relation can possess. Others include selectional preferences , transitivity <TARGET_CITATION/> , mutual exclusion , symmetry , etc. . Others include selectional preferences, transitivity <CITATION/>, mutual exclusion, symmetry, etc.. We release this list for further use by the research community.2 Future Work: Functionality is one of the several properties a relation can possess. We run our techniques on a large set of relations to output a first repository of typed functional relations.",fc88dbb1459e1a757f808c374cbd61abb2b84db3,Identifying Functional Relations in Web Text,2010,Thomas Lin; Mausam; Oren Etzioni,cf3ba53a5030b8dd6ec65101b6f5a9b8e4d06f80,Scaling Textual Inference to the Web,2008,Stefan Schoenmackers; Oren Etzioni; Daniel S. Weld
79,J00-2004,P99-1027,[0],method," crosslanguage information retrieval <TARGET_CITATION/> ,  multilingual document filtering <CITATION/> ,  computerassisted language learning <CITATION/> ,  certain machineassisted translation tools <CITATION/> ,  concordancing for bilingual lexicography <CITATION/> ,","Empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications. Applications where word order is not essential include crosslanguage information retrieval <TARGET_CITATION/> ,  multilingual document filtering <CITATION/> ,  computerassisted language learning <CITATION/> ,  certain machineassisted translation tools <CITATION/> ,  concordancing for bilingual lexicography <CITATION/> ,  crosslanguage information retrieval <CITATION/>,  multilingual document filtering <CITATION/>,  computerassisted language learning <CITATION/>,  certain machineassisted translation tools <CITATION/>,  concordancing for bilingual lexicography <CITATION/>,Applications where word order is not essential includeEmpirically estimated models of translational equivalence among word types can play a central role in both kinds of applications.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,0b41459e7d8dd2183c359aa2f296b0477261a6ea,Should we Translate the Documents or the Queries in Cross-language Information Retrieval?,1999,J. Scott McCarley
80,W04-1610,External_62905,[0],related work,"For example , <TARGET_CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .","include decision tree learning and Bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in <CITATION/>, respectively. The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents. For example , <TARGET_CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces . For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces. The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents. include decision tree learning and Bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in <CITATION/>, respectively.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,using clustering to boost text classificationquot,2001,Y C Fang; S Parthasarathy; F Schwartz
82,J02-3002,External_27921,[0],,"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/> , neural networks <CITATION/> , and maximumentropy modeling <TARGET_CITATION/> .","Another wellacknowledged shortcoming of rulebased systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/> , neural networks <CITATION/> , and maximumentropy modeling <TARGET_CITATION/> . Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/>, neural networks <CITATION/>, and maximumentropy modeling <CITATION/>. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Another wellacknowledged shortcoming of rulebased systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,6edceaf0fada3588ee5f036e944c1a00661df77a,A Maximum Entropy Approach to Identifying Sentence Boundaries,1997,Jeffrey C. Reynar; A. Ratnaparkhi
83,W06-1639,External_6360,[0],introduction,or quotation of messages in emails or postings ( see <TARGET_CITATION/> but cfXXX <CITATION/> ) .,"Indeed, in other settings (e.g., a moviediscussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (I second that!'') or quotation of messages in emails or postings ( see <TARGET_CITATION/> but cfXXX <CITATION/> ) . or quotation of messages in emails or postings (see <CITATION/> but cfXXX <CITATION/>). tween two speakers, such as explicit assertions (I second that!'') Indeed, in other settings (e.g., a moviediscussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,b479d6a38a8e8f4cf86427a756b3ad0f04bb04ab,A Preliminary Investigation into Sentiment Analysis of Informal Political Discourse,2006,Tony Mullen; Robert Malouf
84,D13-1115,P10-1126,[0],related work,The first work to do this with topic models is <TARGET_CITATION/> ) .,"<CITATION/> introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCAbased model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is <TARGET_CITATION/> ) . The first work to do this with topic models is <CITATION/>. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. <CITATION/> introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCAbased model and others on association norm prediction, held out feature prediction, and word similarity.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1,How Many Words Is a Picture Worth? Automatic Caption Generation for News Images,2010,Yansong Feng; Mirella Lapata
85,W04-0910,P01-1019,[4],,The language chosen for semantic representation is a flat semantics along the line of <TARGET_CITATION/> .,"The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The language chosen for semantic representation is a flat semantics along the line of <TARGET_CITATION/> . The language chosen for semantic representation is a flat semantics along the line of <CITATION/>. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,e006247c9584f39593bed908827cca40b74cdf66,An Algebra for Semantic Construction in Constraint-based Grammars,2001,Ann A. Copestake; A. Lascarides; D. Flickinger
86,J01-4001,External_1853,[0],,"Tetreault 's contribution features comparative evaluation involving the author 's own centeringbased pronoun resolution algorithm called the LeftRight Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm <TARGET_CITATION/> , BFP <CITATION/> , and Strube 's 5list approach <CITATION/> .","He also argues that evaluation of anaphora resolution systems should take into account several factors beyond simple accuracy of resolution. In particular, both developeroriented (e.g., related to the selection of optimal resolution factors) and applicationoriented (e.g., related to the requirement of the application, as in the case of information extraction, where a proper name antecedent is needed) evaluation metrics should be considered. Tetreault 's contribution features comparative evaluation involving the author 's own centeringbased pronoun resolution algorithm called the LeftRight Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm <TARGET_CITATION/> , BFP <CITATION/> , and Strube 's 5list approach <CITATION/> . Tetreault's contribution features comparative evaluation involving the author's own centeringbased pronoun resolution algorithm called the LeftRight Centering algorithm (LRC) as well as three other pronoun resolution methods: Hobbs's naive algorithm <CITATION/>, BFP <CITATION/>, and Strube's 5list approach <CITATION/>. In particular, both developeroriented (e.g., related to the selection of optimal resolution factors) and applicationoriented (e.g., related to the requirement of the application, as in the case of information extraction, where a proper name antecedent is needed) evaluation metrics should be considered. He also argues that evaluation of anaphora resolution systems should take into account several factors beyond simple accuracy of resolution.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,61c0eaf156647ffad23921a65e3d7b3296f7afd5,Resolving pronoun references,1986,Jerry R. Hobbs
87,D10-1101,P07-1056,[5],conclusion,"For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation <TARGET_CITATION/> , perform in comparison to our approach .","Our CRFbased approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation <TARGET_CITATION/> , perform in comparison to our approach . For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation <CITATION/>, perform in comparison to our approach. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. Our CRFbased approach also yields promising results in the crossdomain setting.",4d135641931a6efce82bd9c1d69d86e08d3cd28d,Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields,2010,Niklas Jakob; Iryna Gurevych,d895647b4a80861703851ef55930a2627fe19492,"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",2007,John Blitzer; Mark Dredze; Fernando C Pereira
89,D12-1084,W11-1208,[3],,"Provided with the candidate fragment elements , we previously <TARGET_CITATION/> used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para  ) phrase .","Smoothing (step 2) is done for each word by taking the average score of it and its four neighbor words. All the word alignments (excluding stopwords) with positive scores are selected as candidate fragment elements. Provided with the candidate fragment elements , we previously <TARGET_CITATION/> used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para  ) phrase . Provided with the candidate fragment elements, we previously <CITATION/> used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a (para) phrase. All the word alignments (excluding stopwords) with positive scores are selected as candidate fragment elements. Smoothing (step 2) is done for each word by taking the average score of it and its four neighbor words.",a59a0185e02bf46b9f03274da718e87a24e7b8a8,Using Discourse Information for Paraphrase Extraction,2012,Michaela Regneri; Rui Wang,2a2a7b8a93ca9e7dad6c2223a6ebac7f33616869,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,2011,Rui Wang; Chris Callison-Burch
90,P13-3018,External_90448,[0],introduction,"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( <CITATION/> ; Grainger , et al. , 1991 ; <TARGET_CITATION/> ) .","Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( <CITATION/> ; Grainger , et al. , 1991 ; <TARGET_CITATION/> ) . There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (<CITATION/>; Grainger, et al., 1991; <CITATION/>). Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,ab2d6e3f1d713df4dbd70fa94274271a105ec18b,Morphological and orthographic similarity in visual word recognition.,1995,Etta Drews; P. Zwitserlood
91,P97-1063,External_6772,[4],method,This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models <TARGET_CITATION/> .,"Just as easily, we can model links that coincide with entries in a preexisting translation lexicon separatelyfrom those that do not. This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models <TARGET_CITATION/> . This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models <CITATION/>. from those that do not. Just as easily, we can model links that coincide with entries in a preexisting translation lexicon separately",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,the mathematics of statistical machine translation parameter estimationquot,1993,P F Brown; V J Della Pietra; S A Della Pietra; R L Mercer
92,K15-1003,P02-1017,[0],introduction,"One important example is the constituentcontext model ( CCM ) of <TARGET_CITATION/> , which was specifically designed to capture the linguistic observation made by <CITATION/> that there are regularities to the contexts in which constituents appear .","But with less information, it becomes necessary to devise ways of making better use of the information that is available. In general, this means constructing inductive biases that take advantage of unannotated data to train probabilistic models. One important example is the constituentcontext model ( CCM ) of <TARGET_CITATION/> , which was specifically designed to capture the linguistic observation made by <CITATION/> that there are regularities to the contexts in which constituents appear . One important example is the constituentcontext model (CCM) of <CITATION/>, which was specifically designed to capture the linguistic observation made by <CITATION/> that there are regularities to the contexts in which constituents appear. In general, this means constructing inductive biases that take advantage of unannotated data to train probabilistic models. But with less information, it becomes necessary to devise ways of making better use of the information that is available.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,77021fb48704b860fa850dd103b79db4dcf920ee,A Generative Constituent-Context Model for Improved Grammar Induction,2002,D. Klein; Christopher D. Manning
93,W06-2933,External_38733,[4],experiments,"Japanese <TARGET_CITATION/> , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .","If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. Typical examples are Bulgarian <CITATION/>, Chinese <CITATION/>, Danish <CITATION/>, and Swedish <CITATION/>. Japanese <TARGET_CITATION/> , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances . Japanese <CITATION/>, despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. Typical examples are Bulgarian <CITATION/>, Chinese <CITATION/>, Danish <CITATION/>, and Swedish <CITATION/>. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,,stylebook for the japanese treebank in verbmobil verbmobilreport 240 seminar f¨ur sprachwissenschaft,2000,Y Kawata; J Bartels
95,W06-1104,External_20447,[0],,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <TARGET_CITATION/> , informationbased <CITATION/> or distributional <CITATION/> ."," Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <TARGET_CITATION/> , informationbased <CITATION/> or distributional <CITATION/> . Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,0e3e3c3d8ae5cb7c4636870d69967c197484d3bb,Verb Semantics and Lexical Selection,1994,Zhibiao Wu; Martha Palmer
96,D12-1037,D11-1125,[0],introduction,"Some methods are based on likelihood <CITATION/> , error rate <CITATION/> , margin <CITATION/> and ranking <TARGET_CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one .","where f and e (e') are source and target sentences, respectively. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. Some methods are based on likelihood <CITATION/> , error rate <CITATION/> , margin <CITATION/> and ranking <TARGET_CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one . Some methods are based on likelihood <CITATION/>, error rate <CITATION/>, margin <CITATION/> and ranking <CITATION/>, and among which minimum error rate training (MERT) <CITATION/> is the most popular one. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. where f and e (e') are source and target sentences, respectively.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,a13d46125ef505d4e687e25ded74b794efc18323,Tuning as Ranking,2011,Mark Hopkins; Jonathan May
97,K15-1003,D14-1107,[2],,"We follow <TARGET_CITATION/> in allowing a small set of generic , linguisticallyplausible unary and binary grammar rules .","The direction of the slash operator gives the behavior of the function. A category (s\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\) with a noun phrase (np) that serves as its subject. We follow <TARGET_CITATION/> in allowing a small set of generic , linguisticallyplausible unary and binary grammar rules . We follow <CITATION/> in allowing a small set of generic, linguisticallyplausible unary and binary grammar rules. A category (s\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\) with a noun phrase (np) that serves as its subject.The direction of the slash operator gives the behavior of the function.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,d4e87e2e64da8d72a9dc91a16a60e7fdaf28f00e,A* CCG Parsing with a Supertag-factored Model,2014,M. Lewis; Mark Steedman
98,P07-1068,External_4260,[1],introduction,( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs <TARGET_CITATION/> ) .,"6For simplicity, OTHERS is viewed as an NE type here.ture for NPi whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs <TARGET_CITATION/> ) . (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see <CITATION/>). ture for NPi whose value is the most likely NE type. 6For simplicity, OTHERS is viewed as an NE type here.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
99,J90-3003,External_24319,[4],introduction,"Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subjectpredicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in <TARGET_CITATION/> ( henceforth G&G ) .","<CITATION/> claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subjectpredicate boundary in Waiters who remember well II serve orders correctly. Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subjectpredicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in <TARGET_CITATION/> ( henceforth G&G ) . Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating II the remaining green vegetables, where the subjectpredicate boundary finds no prosodic correspondent.4 The most explicit version of this approach is the analysis presented in <CITATION/> (henceforth G&G). Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subjectpredicate boundary in Waiters who remember well II serve orders correctly. <CITATION/> claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,3185f75000efa42a0ce91fecf9e03de3d5c66bcf,Performance structures: A psycholinguistic and linguistic appraisal,1983,J. Gee; F. Grosjean
102,J01-4001,External_13670,[0],,"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> .","The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> . Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,fa9125d18d3c0d565c207b61ec95b830fc3ede18,Robust reference resolution with limited knowledge: High precision genre-specific approach for English and Polish,2000,R. Mitkov; Malgorzata Stys
103,D08-1034,External_24680,[4],experiments,"We use the same data setting with <CITATION/> , however a bit different from <TARGET_CITATION/> .","fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with <CITATION/> , however a bit different from <TARGET_CITATION/> . We use the same data setting with <CITATION/>, however a bit different from <CITATION/>.The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. fid.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,ee4c4fe7fd24125531a0e9eafb6d110cf3c27398,Automatic Semantic Role Labeling for Chinese Verbs,2005,Nianwen Xue; Martha Palmer
104,P11-1134,N10-1031,[1],,"They proved to be useful in a number of NLP applications such as natural language generation <CITATION/> , multidocument summarization <CITATION/> , automatic evaluation of MT <TARGET_CITATION/> , and TE <CITATION/> .","1http://www.statmt.org/wmt10/Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation <CITATION/> , multidocument summarization <CITATION/> , automatic evaluation of MT <TARGET_CITATION/> , and TE <CITATION/> . They proved to be useful in a number of NLP applications such as natural language generation <CITATION/>, multidocument summarization <CITATION/>, automatic evaluation of MT <CITATION/>, and TE <CITATION/>. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. 1http://www.statmt.org/wmt10/",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,3b224df5048eeac8f6707dea5446cc5811611383,Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level,2010,Michael J. Denkowski; A. Lavie
105,W06-2807,External_36047,[0],introduction,"Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history <TARGET_CITATION/> .","We consider it as a good approximation of the version control' concept as shown above. In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history <TARGET_CITATION/> . Moreover, a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history <CITATION/>. In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. We consider it as a good approximation of the version control' concept as shown above.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,1f4081abc0c973ff5d7830655ac3341ad28ecdc2,The Wiki Way: Quick Collaboration on the Web,2001,Bo Leuf; Ward Cunningham
106,D09-1143,W05-0601,[5],conclusion,"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <TARGET_CITATION/> or shallow semantic trees , <CITATION/> .","For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <TARGET_CITATION/> or shallow semantic trees , <CITATION/> . Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNetbased features <CITATION/> or shallow semantic trees, <CITATION/>. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task.",a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi,91c0bfb0b7ecd4cad4fc815862ad17e3f318eb39,Effective use of WordNet Semantics via Kernel-Based Learning,2005,Roberto Basili; Marco Cammisa; Alessandro Moschitti
107,D13-1115,External_23881,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,43f8e62d8e0f7599f6b928fea28cc1315234c8ad,Perceptual Inference Through Global Lexical Similarity,2012,Brendan T. Johns; Michael N. Jones
108,W10-3814,D08-1023,[5],conclusion,"Future research should apply the work of <TARGET_CITATION/> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multinonterminal grammars .","Hierarchical phrasebased MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntaxaugmented MT with its thousands of nonterminals, and made even worse by its joint sourceandtarget extension. Future research should apply the work of <TARGET_CITATION/> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multinonterminal grammars . Future research should apply the work of <CITATION/>, who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multinonterminal grammars. This problem is exacerbated by syntaxaugmented MT with its thousands of nonterminals, and made even worse by its joint sourceandtarget extension. Hierarchical phrasebased MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",1d14215e704bc262ce7ab600aa49530b6f0599fc,New Parameterizations and Features for PSCFG-Based Machine Translation,2010,Andreas Zollmann; S. Vogel,bef0b80cbe5ba692aecba09a4557bbe1e02b0e8a,Probabilistic Inference for Machine Translation,2008,Phil Blunsom; M. Osborne
109,J92-1004,External_36647,[2],,"We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse <TARGET_CITATION/> To produce these  Nbest '' alternatives , we make use of a standard A * search algorithm <CITATION/> .","A simple wordpair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse <TARGET_CITATION/> To produce these  Nbest '' alternatives , we make use of a standard A * search algorithm <CITATION/> . We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse <CITATION/> To produce these Nbest'' alternatives, we make use of a standard A* search algorithm <CITATION/>. If the parse failed, then the sentence was rejected. A simple wordpair grammar constrained the search space.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,6600ffacc96755cee573953313c53af69418c05e,Integration of speech recognition and natural language processing in the MIT VOYAGER system,1991,V. Zue; James R. Glass; D. Goodine; H. Leung; M. Phillips; J. Polifroni; S. Seneff
110,W06-1705,W03-0806,[0],related work,<TARGET_CITATION/>,"Parallelising or distributing processing has been suggested before. Clark and Curran's (2004) work is in parallelising an implementation of loglinear parsing on the Wall Street Journal Corpus, whereas we focus on partofspeech tagging of a far larger and more varied web corpus, a technique more widely considered a prerequisite for corpus linguistics research. <TARGET_CITATION/>Clark and Curran's (2004) work is in parallelising an implementation of loglinear parsing on the Wall Street Journal Corpus, whereas we focus on partofspeech tagging of a far larger and more varied web corpus, a technique more widely considered a prerequisite for corpus linguistics research. Parallelising or distributing processing has been suggested before.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran
111,J00-2014,External_28137,[0],introduction,"OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surfacelevel constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of  simple constraints '' <TARGET_CITATION/> is of course an empirical question .","Such a generalization may fail in contexts where it is overruled by a higherranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surfacelevel constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of  simple constraints '' <TARGET_CITATION/> is of course an empirical question . OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surfacelevel constraints that partially mask one another.1 Whether this is always possible under an appropriate definition of simple constraints'' <CITATION/> is of course an empirical question.As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. Such a generalization may fail in contexts where it is overruled by a higherranked requirement of the language (or of the underlying form).",962381e601b37b50cd2a1ae387a1159f1c9209e6,Book Reviews: Optimality Theory,2000,Jason Eisner,fbca315ee6a13d609be222304911ce9ff2505389,Efficient Generation in Primitive Optimality Theory,1997,Jason Eisner
112,J97-4003,External_105,[0],introduction,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of <TARGET_CITATION/> , ch .","A passivization lexical rule. written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of <TARGET_CITATION/> , ch . Consider, for example, the lexical rule in Figure 2, which encodes a passive lexical rule like the one presented by Pollard and Sag (1987, 215) in terms of the setup of Pollard and Sag (1994, ch. written as fully specified relations between words, rather, only what is supposed to be changed is specified. A passivization lexical rule.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,headdriven phrase structure grammar,1994,Carl Pollard; Ivan Sag
113,D13-1115,External_32478,[4],experiments,This result is consistent with other works using this model with these features <TARGET_CITATION/> .,Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the textonly model (twotailed ttest). This result is consistent with other works using this model with these features <TARGET_CITATION/> . This result is consistent with other works using this model with these features <CITATION/>. The 2D models employing feature norms and association norms do significantly better than the textonly model (twotailed ttest). Table 1 shows our results for each of our selected models with our compositionality evaluation.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
114,W06-1104,External_3502,[4],experiments,"<TARGET_CITATION/> reported a correlation of r = .9026.10 The results are not directly comparable , because he only used nounnoun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .","is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. <TARGET_CITATION/> reported a correlation of r = .9026.10 The results are not directly comparable , because he only used nounnoun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . <CITATION/> reported a correlation of r=.9026.10 The results are not directly comparable, because he only used nounnoun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. is statistically significant at p < .05.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,265be00bf112c6cb2fa3e8176bff8394a114dbde,Using Information Content to Evaluate Semantic Similarity in a Taxonomy,1995,P. Resnik
115,P13-3018,External_90447,[0],related work,Similar observation for surface word frequency was also observed by <TARGET_CITATION/> where it has been claimed that words having low surface frequency tends to decompose .,"It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations. <CITATION/> with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by <TARGET_CITATION/> where it has been claimed that words having low surface frequency tends to decompose . Similar observation for surface word frequency was also observed by <CITATION/> where it has been claimed that words having low surface frequency tends to decompose. <CITATION/> with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,lexical representation of derivational relation,1980,D Bradley
117,W06-1639,W06-3808,[0],related work,Previous sentimentanalysis work in different domains has considered interdocument similarity <TARGET_CITATION/> or explicit,"We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentimentanalysis work in different domains has considered interdocument similarity <TARGET_CITATION/> or explicit Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitRelationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,6fec21a78eb9279c87cc89ef7efa0acf22ff4abd,Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization,2006,A. Goldberg; Xiaojin Zhu
118,J92-1004,H91-1014,[2],,"However , the method we are currently using in the ATIS domain <TARGET_CITATION/> represents our most promising approach to this problem .","There are no separate semantic rules off to the side; rather, the semantic information is encoded directly as names attached to nodes in the tree. Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain <TARGET_CITATION/> represents our most promising approach to this problem . However, the method we are currently using in the ATIS domain <CITATION/> represents our most promising approach to this problem. Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. There are no separate semantic rules off to the side; rather, the semantic information is encoded directly as names attached to nodes in the tree.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,4911a09ce25987e260c4a8c7773a703335977c99,Development and Preliminary Evaluation of the MIT ATIS System,1991,S. Seneff; James R. Glass; D. Goddeau; D. Goodine; L. Hirschman; H. Leung; M. Phillips; J. Polifroni; V. Zue
119,W06-2807,External_98154,[0],introduction,"Henceforth the collaborative traits of blogs and wikis <TARGET_CITATION/> emphasize annotation , comment , and strong editing .","From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an opera aperta' (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis <TARGET_CITATION/> emphasize annotation , comment , and strong editing . Henceforth the collaborative traits of blogs and wikis <CITATION/> emphasize annotation, comment, and strong editing. From a more pessimistic one, an author may feel to have lost power in this openness. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an opera aperta' (open work), as Eco would define it (1962).",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,f9f9f50b962e505d52895724ca4491f5bd83dcb8,Genre Under Construction: The Diary on the Internet,2005,L. Mcneill
120,N01-1006,P00-1036,[0],,The ICA system <TARGET_CITATION/> aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .,"The additional memory space required to store the lists of pointers associated with these rules is about 450 MB, which is a rather large requirement to add to a system.l2.1.2 The ICA Approach The ICA system <TARGET_CITATION/> aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance . The ICA system <CITATION/> aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance. 2.1.2 The ICA ApproachThe additional memory space required to store the lists of pointers associated with these rules is about 450 MB, which is a rather large requirement to add to a system.l",c52f80f056a2de8f503bf912e8025413ec2111ec,Transformation Based Learning in the Fast Lane,2001,G. Ngai; Radu Florian,4a27822c8718bcfdd01fd5cc9e75dd1df9f9add5,Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers,2000,Mark Hepple
121,W00-1017,External_46366,[0],introduction,"To this end , several toolkits for building spoken dialogue systems have been developed <TARGET_CITATION/> .","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <CITATION/>. One of the next research goals is to make these systems taskportable, that is, to simplify the process of porting to another task domain. To this end , several toolkits for building spoken dialogue systems have been developed <TARGET_CITATION/> . To this end, several toolkits for building spoken dialogue systems have been developed <CITATION/>. One of the next research goals is to make these systems taskportable, that is, to simplify the process of porting to another task domain. The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <CITATION/>.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,e0349d8338e05366aac5c881c0f59e33b2e7bc7a,EUROPA: a generic framework for developing spoken dialogue systems.,1999,M. Sasajima; T. Yano; Y. Kono
123,W06-1639,External_32379,[0],introduction,"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) .","In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) . Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,directionbased text interpretation as an information access refinement,1992,M Hearst
124,W06-1104,External_3502,[0],,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <CITATION/> , informationbased <TARGET_CITATION/> or distributional <CITATION/> ."," Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <CITATION/> , informationbased <TARGET_CITATION/> or distributional <CITATION/> . Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,265be00bf112c6cb2fa3e8176bff8394a114dbde,Using Information Content to Evaluate Semantic Similarity in a Taxonomy,1995,P. Resnik
126,D08-1007,J05-4002,[4],method,The advantage of tuning similarity to the application of interest has been shown previously by <TARGET_CITATION/> .,"For example, nouns that can be the object of eat will also occur as the subject of taste and contain. Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated. The advantage of tuning similarity to the application of interest has been shown previously by <TARGET_CITATION/> . The advantage of tuning similarity to the application of interest has been shown previously by <CITATION/>. Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated. For example, nouns that can be the object of eat will also occur as the subject of taste and contain.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,429952086b36f1bf61816efb5fbfd449143469be,Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity,2005,Julie Weeds; David J. Weir
129,P07-1068,External_5983,[1],introduction,"We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in <CITATION/> , motivated by its success in the related tasks of word sense disambiguation <TARGET_CITATION/> and NE classification <CITATION/> .","To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependencybased thesaurus, which is constructed using a distributional approach combined with an informationtheoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in <CITATION/> , motivated by its success in the related tasks of word sense disambiguation <TARGET_CITATION/> and NE classification <CITATION/> . We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in <CITATION/>, motivated by its success in the related tasks of word sense disambiguation <CITATION/> and NE classification <CITATION/>. Learning algorithms. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependencybased thesaurus, which is constructed using a distributional approach combined with an informationtheoretic definition of similarity.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,944cba683d10d8c1a902e05cd68e32a9f47b372e,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,1995,David Yarowsky
130,P02-1001,External_876,[0],introduction,"A central technique is to define a joint relation as a noisychannel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 <TARGET_CITATION/> .","Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a. We offer a theorem that highlights the broad applicability of these modeling techniques.4 If f(input, output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a E E U LCBERCB, b E A U LCBERCB) using concatenation, probabilistic union +p, and probabilistic closure *p. For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules <CITATION/>, (3) by compilation of decision trees <CITATION/>, (4) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation (Gerdemann and van <CITATION/>),5 (5) by conditionalization of a joint relation as discussed below. A central technique is to define a joint relation as a noisychannel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 <TARGET_CITATION/> . A central technique is to define a joint relation as a noisychannel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 <CITATION/>. We offer a theorem that highlights the broad applicability of these modeling techniques.4 If f(input, output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a E E U LCBERCB, b E A U LCBERCB) using concatenation, probabilistic union +p, and probabilistic closure *p. For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules <CITATION/>, (3) by compilation of decision trees <CITATION/>, (4) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation (Gerdemann and van <CITATION/>),5 (5) by conditionalization of a joint relation as discussed below. Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,0bf2c2ca87256956b3e51bec4845c4fa28d4de7b,Speech Recognition by Composition of Weighted Finite Automata,1996,Fernando C Pereira; M. Riley
131,D08-1004,P04-1035,[2],method,We use the same set of binary features as in previous work on this dataset <TARGET_CITATION/> .,"A CRF is just another conditional loglinear model:where f() extracts a feature vector from a classified  document,  are the corresponding weights of those features, and Z(x) def  Ey u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset <TARGET_CITATION/> . We use the same set of binary features as in previous work on this dataset <CITATION/>. where f() extracts a feature vector from a classified  document,  are the corresponding weights of those features, and Z(x) def  Ey u(x, y) is a normalizer. A CRF is just another conditional loglinear model:",14e2aec7e25d8880a851a547cf8d27a9721f8e6c,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,2008,Omar Zaidan; Jason Eisner,167e1359943b96b9e92ee73db1df69a1f65d731d,A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts,2004,B. Pang; Lillian Lee
132,W06-1639,External_7807,[2],method,"Our classification framework , directly inspired by <TARGET_CITATION/> , integrates both perspectives , optimizing its labeling of speech segments based on both individual speechsegment classification scores and preferences for groups of speech segments to receive the same label .","The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation. As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. Our classification framework , directly inspired by <TARGET_CITATION/> , integrates both perspectives , optimizing its labeling of speech segments based on both individual speechsegment classification scores and preferences for groups of speech segments to receive the same label . Our classification framework, directly inspired by <CITATION/>, integrates both perspectives, optimizing its labeling of speech segments based on both individual speechsegment classification scores and preferences for groups of speech segments to receive the same label. As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,0eedbab3ae55fd6a4e7bbc75fcc261293384f883,Learning from Labeled and Unlabeled Data using Graph Mincuts,2001,Avrim Blum; Shuchi Chawla
134,J02-3002,J97-2002,[4],conclusion,"For instance , <TARGET_CITATION/> report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .","The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , <TARGET_CITATION/> report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words . For instance, <CITATION/> report that the SATZ system (decision tree variant) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16,000 words. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,30154464f549643e825ccf60072a17a3e55291d3,To Appear in Computational Linguistics Adaptive Multilingual Sentence Boundary Disambiguation,2004,D. Palmer; Marti A. Hearst
135,P11-1134,P01-1067,[5],conclusion,"One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by <TARGET_CITATION/> .","Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by <TARGET_CITATION/> . One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by <CITATION/>. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9,A Syntax-based Statistical Translation Model,2001,Kenji Yamada; Kevin Knight
136,N10-1084,External_64566,[0],related work,"Later works , such as <TARGET_CITATION/> , further made use of partofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by <CITATION/>. Later works , such as <TARGET_CITATION/> , further made use of partofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method . Later works, such as <CITATION/>, further made use of partofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method. The first lexical substitution method was proposed by <CITATION/>. The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,,a method of linguistic steganography based on coladdressallyverified synonym,2004,Igor A Bolshakov
137,J86-1002,External_33232,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , <TARGET_CITATION/> , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."," A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , <TARGET_CITATION/> , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,a02a928cc45539c03ceb30ff565cbec710546db7,"Organization and operation of a connected speech understanding system at lexical, syntactic and semantic levels",1976,J. Haton; J. Pierrel
138,E03-1002,External_63,[4],experiments,The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSNFreq > 200 ) and five recent statistical parsers <TARGET_CITATION/> .,The Tags model achieves performance which is better than any previously published results on parsing with a nonlexicalized model. The Tags model also does much better than the only other broad coverage neural network parser <CITATION/>. The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSNFreq > 200 ) and five recent statistical parsers <TARGET_CITATION/> . The bottom panel of table 1 lists the results for the chosen lexicalized model (SSNFreq>200) and five recent statistical parsers <CITATION/>. The Tags model also does much better than the only other broad coverage neural network parser <CITATION/>. The Tags model achieves performance which is better than any previously published results on parsing with a nonlexicalized model.,adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,844db702be4bc149b06b822b47247e15f5894cc3,Discriminative Reranking for Natural Language Parsing,2000,M. Collins; Terry Koo
139,W03-0806,E03-1071,[0],,"The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines <TARGET_CITATION/> .","The Python interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler. Finally, since Python can produce standalone executables directly, it will be possible to create distributable code that does not require the entire infrastructure or Python interpreter to be installed. The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines <TARGET_CITATION/> . The basic Python reflection has already been implemented and used for large scale experiments with POS tagging, using pyMPI (a message passing interface library for Python) to coordinate experiments across a cluster of over 100 machines <CITATION/>. Finally, since Python can produce standalone executables directly, it will be possible to create distributable code that does not require the entire infrastructure or Python interpreter to be installed. The Python interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,1358bc877f106ffd066796f66ed6f8242b99d2d1,Investigating GIS and Smoothing for Maximum Entropy Taggers,2003,J. Curran; S. Clark
140,P97-1063,External_20588,[0],,"This imbalance foils thresholding strategies , clever as they might be <TARGET_CITATION/> .","Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be <TARGET_CITATION/> . This imbalance foils thresholding strategies, clever as they might be <CITATION/>. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,d609fd5e328a06a67f883c08e609bc57583100f0,Building Probabilistic Models for Natural Language,1996,Stanley F. Chen
142,P11-1134,External_43455,[2],experiments,We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool <TARGET_CITATION/> to measure the relatedness between words in the dataset .,VerbOcean has been used to extract 18232 pairs of verbs connected by the strongerthan'' relation (e.g. kill'' strongerthan injure''). Wikipedia (WIKI). We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool <TARGET_CITATION/> to measure the relatedness between words in the dataset . We performed Latent Semantic Analysis (LSA) over Wikipedia using the jLSI tool <CITATION/> to measure the relatedness between words in the dataset. Wikipedia (WIKI). VerbOcean has been used to extract 18232 pairs of verbs connected by the strongerthan'' relation (e.g. kill'' strongerthan injure'').,0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,,jlsi a tool for latent semantic indexing software available at httptccitcitresearchtextectoolsresourcesjlsihtml,2007,Claudio Giuliano
143,D12-1027,D09-1141,[4],related work,"For example , our previous work <TARGET_CITATION/> experimented with various techniques for combining a small bitext for a resourcepoor language ( Indonesian or Spanish , pretending that Spanish is resourcepoor ) with a much larger bitext for a related resourcerich language ( Malay or Portuguese ) ; the target language of all bitexts was English .","Unlike this work, which heavily relied on languagespecific rules, our approach is statistical, and largely languageindependent; moreover, our improvements are much more sizable. A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages. For example , our previous work <TARGET_CITATION/> experimented with various techniques for combining a small bitext for a resourcepoor language ( Indonesian or Spanish , pretending that Spanish is resourcepoor ) with a much larger bitext for a related resourcerich language ( Malay or Portuguese ) ; the target language of all bitexts was English . For example, our previous work <CITATION/> experimented with various techniques for combining a small bitext for a resourcepoor language (Indonesian or Spanish, pretending that Spanish is resourcepoor) with a much larger bitext for a related resourcerich language (Malay or Portuguese); the target language of all bitexts was English. A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages. Unlike this work, which heavily relied on languagespecific rules, our approach is statistical, and largely languageindependent; moreover, our improvements are much more sizable.",2f8fe4694ff20ebc64cd50a9492799023ae52955,Source Language Adaptation for Resource-Poor Machine Translation,2012,Pidong Wang; Preslav Nakov; H. Ng,d8c09faf290f902367391c1c592117f703d71778,Improved Statistical Machine Translation for Resource-Poor Languages Using Related Resource-Rich Languages,2009,Preslav Nakov; H. Ng
144,W06-1104,External_1645,[0],,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <TARGET_CITATION/> , ontologybased <CITATION/> , informationbased <CITATION/> or distributional <CITATION/> ."," Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <TARGET_CITATION/> , ontologybased <CITATION/> , informationbased <CITATION/> or distributional <CITATION/> . Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,76e4e034c20bea86edcc6e71bbaddb47fafeecbc,Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone,1986,M. Lesk
145,D13-1115,P12-1015,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,917fbd64a435cb33e0e5b4cd73fe830db7b166db,Distributional Semantics in Technicolor,2012,Elia Bruni; Gemma Boleda; Marco Baroni; N. Tran
146,W06-1104,I05-1067,[0],related work,<TARGET_CITATION/> replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .,"However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task. Furthermore, semantic relatedness is an intuitive concept and being forced to assign finegrained continuous values is felt to overstrain the test subjects. <TARGET_CITATION/> replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German . <CITATION/> replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. Furthermore, semantic relatedness is an intuitive concept and being forced to assign finegrained continuous values is felt to overstrain the test subjects. However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,2ebe88fab46e53e980b5aef28c414c04d133fc6f,Using the Structure of a Conceptual Network in Computing Semantic Relatedness,2005,Iryna Gurevych
147,P00-1012,External_5515,[2],method,"One approach to this more general problem , taken by the  Nitrogen ' generator <TARGET_CITATION/> , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .","The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the  Nitrogen ' generator <TARGET_CITATION/> , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . One approach to this more general problem, taken by the Nitrogen' generator <CITATION/>, takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model. The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.",a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf,,generation that exploits corpusbased statistical knowledge,1998,Irene Langkilde; Kevin Knight
149,J09-4010,N03-1020,[2],method,"13 We also employed sequencebased measures using the ROUGE tool set <TARGET_CITATION/> , with similar results to those obtained with the wordbyword measures .","To this effect, we considered the methods whose coverage exceeds some minimum (e.g., 10%), and chose the method(s) which could adequately answer the largest number of queries in the data set (based on coverage combined with Fscore and precision). Table 5 presents the coverage and unique/best coverage of each method (the percentage of queries covered only by this method or for which this method produces a better reply than other methods), and the average and standard deviation of the precision and Fscore obtained by each method (calculated over the requests that are covered). 13 We also employed sequencebased measures using the ROUGE tool set <TARGET_CITATION/> , with similar results to those obtained with the wordbyword measures . 13 We also employed sequencebased measures using the ROUGE tool set <CITATION/>, with similar results to those obtained with the wordbyword measures.Table 5 presents the coverage and unique/best coverage of each method (the percentage of queries covered only by this method or for which this method produces a better reply than other methods), and the average and standard deviation of the precision and Fscore obtained by each method (calculated over the requests that are covered).To this effect, we considered the methods whose coverage exceeds some minimum (e.g., 10%), and chose the method(s) which could adequately answer the largest number of queries in the data set (based on coverage combined with Fscore and precision).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,c63bb976dc0d3a897f3b0920170a4c573ef904c6,Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics,2003,Chin-Yew Lin; E. Hovy
150,W06-3309,External_74942,[0],conclusion,"Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition <TARGET_CITATION/> .","First, Gaussian modeling adds an extra degree of freedom during training, by capturing secondorder statistics. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition <TARGET_CITATION/> . Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition <CITATION/>. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. First, Gaussian modeling adds an extra degree of freedom during training, by capturing secondorder statistics.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,,cuhtk conversational telephone speech transcription system,2004,Gunnar Evermann; H Y Chan; Mark J F Gales; Thomas Hain; Xunying Liu; David Mrva; Lan Wang; Phil Woodland
152,N04-2004,External_164,[4],introduction,"The typical solution to the redundancy problem is to group verbs according to their argument realization patterns <TARGET_CITATION/> , possibly arranged in an inheritance hierarchy .","The lexicon explicitly specifies the different subcategorization frames of a verb, e.g., the causative frame, the causative instrumental frame, the inchoative frame, etc.. The major drawback of this approach, however, is the tremendous amount of redundancy in the lexiconfor example, the class of prototypical transitive verbs where the agent appears as the subject and the theme as the direct object must all duplicate this pattern. The typical solution to the redundancy problem is to group verbs according to their argument realization patterns <TARGET_CITATION/> , possibly arranged in an inheritance hierarchy . The typical solution to the redundancy problem is to group verbs according to their argument realization patterns <CITATION/>, possibly arranged in an inheritance hierarchy. The major drawback of this approach, however, is the tremendous amount of redundancy in the lexiconfor example, the class of prototypical transitive verbs where the agent appears as the subject and the theme as the direct object must all duplicate this pattern. The lexicon explicitly specifies the different subcategorization frames of a verb, e.g., the causative frame, the causative instrumental frame, the inchoative frame, etc..",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,6cbc1eb25f4ab29a613418b3b0740e74141a0f17,English Verb Classes and Alternations: A Preliminary Investigation,1993,B. Levin
154,W00-1312,External_46911,[0],,Another technique is automatic discovery of translations from parallel or nonparallel corpora <TARGET_CITATION/> .,"Translation of numbers can be solved using simple rules. Transliteration, a technique that guesses the likely translations of a word based on pronunciation, can be readily used in translating proper nouns. Another technique is automatic discovery of translations from parallel or nonparallel corpora <TARGET_CITATION/> . Another technique is automatic discovery of translations from parallel or nonparallel corpora <CITATION/>. Transliteration, a technique that guesses the likely translations of a word based on pronunciation, can be readily used in translating proper nouns. Translation of numbers can be solved using simple rules.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,finding terminology translations from nonparallel corporaquot,1997,P Fung; K Mckeown
155,W04-1805,External_44706,[2],method,ASARES is presented in detail in <TARGET_CITATION/> .,"The method used for the acquisition of NV pairs relies mainly on ASARES, a pattern inference tool. ASARES is presented in detail in <TARGET_CITATION/> . ASARES is presented in detail in <CITATION/>. The method used for the acquisition of NV pairs relies mainly on ASARES, a pattern inference tool.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,0439f0942c85d4855b1dcd0546ec946f0c83752a,Learning Semantic Lexicons from a Part-of-Speech and Semantically Tagged Corpus Using Inductive Logic Programming,2003,V. Claveau; P. Sébillot; Cécile Fabre; P. Bouillon
157,W04-1805,External_95820,[0],related work,A number of applications have relied on distributional analysis <TARGET_CITATION/> in order to build classes of semantically related terms ., A number of applications have relied on distributional analysis <TARGET_CITATION/> in order to build classes of semantically related terms . A number of applications have relied on distributional analysis <CITATION/> in order to build classes of semantically related terms.,f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,ac514ee52f8a1d2c3b2a85260cce2d947928db13,Structure mathématique du langage : L'analyse distributionnelle : Nouvelle approche dans l'analyse de discours en psychiatrie,1997,M. C. Noël-Jorand; M. Reinert; S. Giudicelli; D. Dassa
159,J97-4003,External_24531,[4],introduction,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by <TARGET_CITATION/> , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .","A passivization lexical rule. written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by <TARGET_CITATION/> , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch . Consider, for example, the lexical rule in Figure 2, which encodes a passive lexical rule like the one presented by Pollard and Sag (1987, 215) in terms of the setup of Pollard and Sag (1994, ch. written as fully specified relations between words, rather, only what is supposed to be changed is specified. A passivization lexical rule.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,777570c140f6f995385f4d5a3b6bd902dfab3f5d,Review of Prolog and natural-language analysis: CSLI lecture notes 10 by Fernando C. N. Pereira and Stuart M. Shieber. Center for the Study of Language and Information 1987.,1988,P. Saint-Dizier
160,J09-4010,C04-1128,[4],,"Two applications that , like helpdesk , deal with question  answer pairs are : summarization of email threads <TARGET_CITATION/> , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000 ;","However, the personalization component of the casebased reasoning approach was rulebased (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpusbased approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that , like helpdesk , deal with question  answer pairs are : summarization of email threads <TARGET_CITATION/> , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000 ; Two applications that, like helpdesk, deal with questionanswer pairs are: summarization of email threads <CITATION/>, and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000;With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpusbased approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). However, the personalization component of the casebased reasoning approach was rulebased (e.g., rules were applied to substitute names of individuals and companies in texts).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,531afc35be2748bfa0fd5a6050ca5b82ebca1797,Detection of Question-Answer Pairs in Email Conversations,2004,Lokesh Shrestha; K. McKeown
161,D13-1115,External_2129,[0],related work,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) ."," The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) . The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,c6a8aef1bf134294482d8088f982d5643347d2ff,Describing objects by their attributes,2009,Ali Farhadi; Ian Endres; Derek Hoiem; D. Forsyth
162,Q13-1020,P09-1088,[4],,"In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar <TARGET_CITATION/> 5 .","However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different Utrees and production rules. Thus, using STSG, we can build more specific Utrees for translation. In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar <TARGET_CITATION/> 5 . In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar <CITATION/>5. Thus, using STSG, we can build more specific Utrees for translation. However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different Utrees and production rules.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,46e06cf27255dedcd40a99751dbc14d7fd44f80e,A Gibbs Sampler for Phrasal Synchronous Grammar Induction,2009,Phil Blunsom; Trevor Cohn; Chris Dyer; M. Osborne
163,W01-1510,External_60,[0],introduction,"There are several grammars developed in the FBLTAG formalism , including the XTAG English grammar , a largescale grammar for English ( The XTAG Research <TARGET_CITATION/> ) .","only by derived trees (i.e., parse trees) but also by derivation trees. A derivation tree is a structural description in LTAG and represents the history of combinations of elementary trees. There are several grammars developed in the FBLTAG formalism , including the XTAG English grammar , a largescale grammar for English ( The XTAG Research <TARGET_CITATION/> ) . There are several grammars developed in the FBLTAG formalism, including the XTAG English grammar, a largescale grammar for English (The XTAG Research <CITATION/>). A derivation tree is a structural description in LTAG and represents the history of combinations of elementary trees. only by derived trees (i.e., parse trees) but also by derivation trees.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,6fff864e0622d6dd84122a02c6bf990f0a390f8c,A Lexicalized Tree Adjoining Grammar for English,1990,Abeillé; Kathleen Bishop; Sharon Cote; Yves Schabes
164,A00-2022,P99-1061,[4],conclusion,"Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of <TARGET_CITATION/> , who report large speedups from the elimination of disjunction processing during unification .","By comparison to previous work in unificationbased parsing we have demonstrated that proand retroactive packing are wellsuited to achieve optimal packing; furthermore, experimental results obtained with a publiclyavailable HPSG processing platform confirm that ambiguity packing can greatly reduce average parse complexity for this type of grammars. In related work, <CITATION/> describes an approach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments. Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of <TARGET_CITATION/> , who report large speedups from the elimination of disjunction processing during unification . Although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of <CITATION/>, who report large speedups from the elimination of disjunction processing during unification. In related work, <CITATION/> describes an approach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments. By comparison to previous work in unificationbased parsing we have demonstrated that proand retroactive packing are wellsuited to achieve optimal packing; furthermore, experimental results obtained with a publiclyavailable HPSG processing platform confirm that ambiguity packing can greatly reduce average parse complexity for this type of grammars.",63c12c1142f2cad9fe03182cc14756a58be0990e,Ambiguity Packing in Constraint-based Parsing Practical Results,2000,S. Oepen; John A. Carroll,f6e44ce874581102565dc0bde0cf295a5067d642,A Bag of Useful Techniques for Efficient and Robust Parsing,1999,B. Kiefer; Hans-Ulrich Krieger; John A. Carroll; Robert Malouf
166,A00-1016,A97-1001,[2],experiments,"The speech and language processing architecture is based on that of the SRI CommandTalk system ( <TARGET_CITATION/> ; Stent et a. , 1999 ) .","State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system ( <TARGET_CITATION/> ; Stent et a. , 1999 ) . The speech and language processing architecture is based on that of the SRI CommandTalk system (<CITATION/>; Stent et a., 1999). A visual display gives direct feedback on some of these parameters.State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed).",229eebe84ed75649e83ebebdd283f248a913aaef,A Compact Architecture for Dialogue Management Based on Scripts and Meta-Outputs,2000,Manny Rayner; Beth Ann Hockey; Frankie James,b35076035130ea7683fb30b3314957fe9684bde3,CommandTalk: A Spoken-Language Interface for Battlefield Simulations,1997,Robert C. Moore; J. Dowding; H. Bratt; J. Gawron; Y. Gorfu; Adam Cheyer
167,P11-1134,N10-1146,[4],experiments,"Second , in line with the findings of <TARGET_CITATION/> , the results obtained over the MTderived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .","First, we notice that dealing with MTderived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of <TARGET_CITATION/> , the results obtained over the MTderived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . Second, in line with the findings of <CITATION/>, the results obtained over the MTderived corpus are equal to those we achieve over the original RTE3 dataset (i.e. 63.50%). This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. First, we notice that dealing with MTderived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,4876b9d79886960e034a5d52adcdad640b363c76,Syntactic/Semantic Structures for Textual Entailment Recognition,2010,Yashar Mehdad; Alessandro Moschitti; Fabio Massimo Zanzotto
168,W06-1104,External_61155,[4],experiments,"Therefore , intersubject correlation is lower than the results obtained by <TARGET_CITATION/> .","that (car  vehicle) are highly related, a strong connection between (parts  speech) may only be established by a certain group. Due to the corpusbased approach, many domainspecific concept pairs are introduced into the test set. Therefore , intersubject correlation is lower than the results obtained by <TARGET_CITATION/> . Therefore, intersubject correlation is lower than the results obtained by <CITATION/>. Due to the corpusbased approach, many domainspecific concept pairs are introduced into the test set. that (car  vehicle) are highly related, a strong connection between (parts  speech) may only be established by a certain group.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3b5b95038c6b065f29649c1b11ea3e7855c00a53,Thinking beyond the nouns - computing semantic relatedness across parts of speech,2006,Iryna Gurevych
169,N04-2004,External_3105,[0],,There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <CITATION/> ; Rappaport <TARGET_CITATION/> ) .,"The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <CITATION/> ; Rappaport <TARGET_CITATION/> ) . There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structurerepresentations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity (<CITATION/>; Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>).",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,,building verb meanings,1998,Malka Rappaport Hovav; Beth Levin
170,J00-4001,A94-1028,[0],method,"For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes <TARGET_CITATION/> .","Thus, any NLP tool (e.g., partofspeech taggers, parsers, etc.) can provide similar measures. The appropriate analysislevel style markers have to be defined according to the methodology used by the tool in order to analyze the text. For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes <TARGET_CITATION/> . For example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes <CITATION/>. The appropriate analysislevel style markers have to be defined according to the methodology used by the tool in order to analyze the text. Thus, any NLP tool (e.g., partofspeech taggers, parsers, etc.) can provide similar measures.",bad78f839e2b566c4d62558827d03b9aa1c57e48,Automatic Text Categorization In Terms Of Genre and Author,2000,E. Stamatatos; N. Fakotakis; G. Kokkinakis,c50104bd642473b3b628bd75b9898a4d287d2c5b,Robust Text Processing in Automated Information Retrieval,1994,T. Strzalkowski
171,D08-1039,W97-1014,[4],method,The resulting training procedure is analogous to the one presented in <TARGET_CITATION/> .,"where A(f, e, e0) is a relative weight accumulator over the parallel corpus:The function (, ) denotes the Kronecker delta. The resulting training procedure is analogous to the one presented in <TARGET_CITATION/> . The resulting training procedure is analogous to the one presented in <CITATION/>. The function (, ) denotes the Kronecker delta. where A(f, e, e0) is a relative weight accumulator over the parallel corpus:",aa1dc69e299eaef7911a35afb56ca079864325c2,Triplet Lexicon Models for Statistical Machine Translation,2008,Sasa Hasan; Juri Ganitkevitch; H. Ney; Jesús Andrés-Ferrer,c0dc20ada590a912134ef7147ae12ea599b5e82d,Word Triggers and the EM Algorithm,1997,Christoph Tillmann; H. Ney
172,J92-1004,External_36647,[2],,"successfully parses , or until a quitting criterion is reached , such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions <TARGET_CITATION/> , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .","Hence it is a tight upper bound on the true score for the rest of the sentence. The recognizer can continue to propose hypotheses until one successfully parses , or until a quitting criterion is reached , such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions <TARGET_CITATION/> , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . successfully parses, or until a quitting criterion is reached, such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions <CITATION/>, the tightly coupled system allows the parser to discard partial theories that have no way of continuing. The recognizer can continue to propose hypotheses until oneHence it is a tight upper bound on the true score for the rest of the sentence.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,6600ffacc96755cee573953313c53af69418c05e,Integration of speech recognition and natural language processing in the MIT VOYAGER system,1991,V. Zue; James R. Glass; D. Goodine; H. Leung; M. Phillips; J. Polifroni; S. Seneff
173,Q13-1020,W06-3119,[4],related work,<TARGET_CITATION/> substituted the nonterminal X in hierarchical phrasebased model by extended syntactic categories .,"<CITATION/> utilized a transformationbased method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Treebank resources and focus on generating effective unsupervised tree structures for treebased translation models. <TARGET_CITATION/> substituted the nonterminal X in hierarchical phrasebased model by extended syntactic categories . <CITATION/> substituted the nonterminal X in hierarchical phrasebased model by extended syntactic categories. Compared to their work, we do not rely on any Treebank resources and focus on generating effective unsupervised tree structures for treebased translation models. <CITATION/> utilized a transformationbased method to learn a sequence of monolingual tree transformations for translation.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,56bc078f0b7b4c6112001af12527b3f7fcf4f021,Syntax Augmented Machine Translation via Chart Parsing,2006,Andreas Zollmann; Ashish Venugopal
174,J01-4001,External_21667,[0],,"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <TARGET_CITATION/> , which was difficult both to represent and to process , and which required considerable human input .","The drive toward corpusbased robust NLP solutions further stimulated interest in alternative and/or dataenriched approaches. Last, but not least, applicationdriven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <TARGET_CITATION/> , which was difficult both to represent and to process , and which required considerable human input . Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. Last, but not least, applicationdriven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. The drive toward corpusbased robust NLP solutions further stimulated interest in alternative and/or dataenriched approaches.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,toward a computational theory of definite anaphora comprehension in english,1979,Candace Sidner
175,W06-2807,External_36047,[0],,"The paradigm is  write many , read many '' <TARGET_CITATION/> .","In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself. Generally, people avoid commenting, preferring to edit each document. The paradigm is  write many , read many '' <TARGET_CITATION/> . The paradigm is write many, read many'' <CITATION/>. Generally, people avoid commenting, preferring to edit each document. In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,1f4081abc0c973ff5d7830655ac3341ad28ecdc2,The Wiki Way: Quick Collaboration on the Web,2001,Bo Leuf; Ward Cunningham
176,P10-2059,External_5300,[2],introduction,The Praat tool was used <TARGET_CITATION/> .,"In addition to this, the acoustic signals are segmented into words, syllables and prosodic phrases, and annotated with POStags, phonological and phonetic transcriptions, pitch and intonation contours. Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see <CITATION/>. The Praat tool was used <TARGET_CITATION/> . The Praat tool was used <CITATION/>. Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see <CITATION/>. In addition to this, the acoustic signals are segmented into words, syllables and prosodic phrases, and annotated with POStags, phonological and phonetic transcriptions, pitch and intonation contours.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,praat doing phonetics by computer retrieved,2009,Paul Boersma; David Weenink
177,J06-2002,External_3884,[0],introduction,"2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( <TARGET_CITATION/> ; also Section 8.1 of the present article ) .","We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of 2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( <TARGET_CITATION/> ; also Section 8.1 of the present article ) . 2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article).This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form ofWe shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,d7689bea78120d1eb2efbc09506d9ae98452d59e,Two theories about adjectives,2013,H. Kamp
178,W03-0806,E03-1071,[1],experiments,The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <TARGET_CITATION/> .,"Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <TARGET_CITATION/> . The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <CITATION/>. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,1358bc877f106ffd066796f66ed6f8242b99d2d1,Investigating GIS and Smoothing for Maximum Entropy Taggers,2003,J. Curran; S. Clark
179,W06-1639,H05-1068,[5],method,"Default parameters were used , although experimentation with different parameter settings is an important direction for future work <TARGET_CITATION/> .","In our experiments, we employed the wellknown classifier SVMght to obtain individualdocument classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis <CITATION/>, the input to SVMght consisted of normalized presenceoffeature (rather than frequencyoffeature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used , although experimentation with different parameter settings is an important direction for future work <TARGET_CITATION/> . Default parameters were used, although experimentation with different parameter settings is an important direction for future work <CITATION/>. The ind value 5SVMlight is available at svmlight.joachims.org. In our experiments, we employed the wellknown classifier SVMght to obtain individualdocument classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis <CITATION/>, the input to SVMght consisted of normalized presenceoffeature (rather than frequencyoffeature) vectors.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,fb8ae13ec7dbbf410f1117e613418dfa4d7e3b61,Optimizing to Arbitrary NLP Metrics using Ensemble Selection,2005,Art Munson; Claire Cardie; R. Caruana
180,K15-1002,D13-1057,[2],,Our work is inspired by the latent leftlinking model in <TARGET_CITATION/> and the ILP formulation from <CITATION/> .,This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent leftlinking model in <TARGET_CITATION/> and the ILP formulation from <CITATION/> . Our work is inspired by the latent leftlinking model in <CITATION/> and the ILP formulation from <CITATION/>. This section describes our joint coreference resolution and mention head detection framework.,f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,be2f82cfd32c41d6493dc3ffb414de27b4f9e15b,A Constrained Latent Variable Model for Coreference Resolution,2013,Kai-Wei Chang; Rajhans Samdani; D. Roth
181,W06-3309,External_8545,[0],introduction,"Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) <CITATION/> , and the availability of software that leverages this knowledge  MetaMap <CITATION/> for concept identification and SemRep <TARGET_CITATION/> for relation extraction  provide a foundation for studying the role of semantics in various tasks .","Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine(NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) <CITATION/> , and the availability of software that leverages this knowledge  MetaMap <CITATION/> for concept identification and SemRep <TARGET_CITATION/> for relation extraction  provide a foundation for studying the role of semantics in various tasks . Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. (NLM), which also serves as a readily available corpus of abstracts for our experiments. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,30bafa2d2f9dbe9e89c0efae1e4571809d383328,The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text,2003,Thomas C. Rindflesch; M. Fiszman
182,J00-2001,External_80545,[0],,"The names given to the components vary ; they have been called  strategic '' and  tactical '' components <CITATION/> 1 ,  planning '' and  realization '' <TARGET_CITATION/> , or simply  what to say '' versus  how to say it '' <CITATION/> .","Research in natural language generation has generally separated the task into distinct text planning and linguistic components. The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. The names given to the components vary ; they have been called  strategic '' and  tactical '' components <CITATION/> 1 ,  planning '' and  realization '' <TARGET_CITATION/> , or simply  what to say '' versus  how to say it '' <CITATION/> . The names given to the components vary; they have been called strategic'' and tactical'' components <CITATION/>1, planning'' and realization'' <CITATION/>, or simply what to say'' versus how to say it'' <CITATION/>. The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language. Research in natural language generation has generally separated the task into distinct text planning and linguistic components.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,,generating natural language linder pragmatic constraints lawrence erlbaum,1988,Eduard H Hovy
183,P10-4003,External_53558,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,15842698b7d2536c6fe3648ab0f0f002b79f1aed,Towards Modelling and Using Common Ground in Tutorial Dialogue,2007,Mark Buckley; Magdalena Wolska
184,P10-4003,W07-1207,[2],experiments,We use the TRIPS dialogue parser <TARGET_CITATION/> to parse the utterances ., We use the TRIPS dialogue parser <TARGET_CITATION/> to parse the utterances . We use the TRIPS dialogue parser <CITATION/> to parse the utterances.,1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,c346d9f793bfcbdc31f549fab3feb0266ea33cb8,Deep Linguistic Processing for Spoken Dialogue Systems,2007,James F. Allen; M. Dzikovska; Mehdi Manshadi; Mary D. Swift
185,J09-4010,External_42504,[2],method,"In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system <TARGET_CITATION/> .","Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of requestresponse pairs would have to be used. Further, userbased evaluations of the output produced by our system require the subjects to read relatively long request response emails, which quickly becomes tedious. In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system <TARGET_CITATION/> . In order to address these limitations in a practical way, we conducted a small user study where we asked four judges (graduate students from the Faculty of Information Technology at Monash University) to assess the responses generated by our system <CITATION/>. Further, userbased evaluations of the output produced by our system require the subjects to read relatively long request response emails, which quickly becomes tedious. Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order for a user study to be representative, a fairly large sample involving several hundreds of requestresponse pairs would have to be used.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,evaluation of a largescale email response system,2007,Y Marom; I Zukerman
186,W00-1017,P99-1026,[2],,"The understanding module utilizes ISSS ( Incremental Significantutterance Sequence Search ) <TARGET_CITATION/> , which is an integrated parsing and discourse processing method .","The language understanding module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the result of understanding and discourse information are represented by a frame (i.e., attributevalue pairs). The understanding module utilizes ISSS ( Incremental Significantutterance Sequence Search ) <TARGET_CITATION/> , which is an integrated parsing and discourse processing method . The understanding module utilizes ISSS (Incremental Significantutterance Sequence Search) <CITATION/>, which is an integrated parsing and discourse processing method. The language understanding module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the result of understanding and discourse information are represented by a frame (i.e., attributevalue pairs).",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,b9317d1aa658d94f18fe7cbad6c8ab3ac64b73f9,Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems,1999,Mikio Nakano; Noboru Miyazaki; Jun-ichi Hirasawa; Kohji Dohsaka; T. Kawabata
187,W01-1510,External_60,[2],experiments,"We applied our system to the XTAG English grammar ( The XTAG Research <TARGET_CITATION/> ) 3 , which is a largescale FBLTAG grammar for English .","The RenTAL system is implemented in LiLFeS <CITATION/>2. LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system <CITATION/>. We applied our system to the XTAG English grammar ( The XTAG Research <TARGET_CITATION/> ) 3 , which is a largescale FBLTAG grammar for English . We applied our system to the XTAG English grammar (The XTAG Research <CITATION/>)3, which is a largescale FBLTAG grammar for English.LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system <CITATION/>. The RenTAL system is implemented in LiLFeS <CITATION/>2.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,6fff864e0622d6dd84122a02c6bf990f0a390f8c,A Lexicalized Tree Adjoining Grammar for English,1990,Abeillé; Kathleen Bishop; Sharon Cote; Yves Schabes
188,P11-1134,W09-0441,[0],,"After the extraction , pruning techniques <TARGET_CITATION/> can be applied to increase the precision of the extracted paraphrases .","One of the proposed methods to extract paraphrases relies on a pivotbased approach using phrase alignments in a bilingual parallel corpus <CITATION/>. With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction , pruning techniques <TARGET_CITATION/> can be applied to increase the precision of the extracted paraphrases . After the extraction, pruning techniques <CITATION/> can be applied to increase the precision of the extracted paraphrases. With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. One of the proposed methods to extract paraphrases relies on a pivotbased approach using phrase alignments in a bilingual parallel corpus <CITATION/>.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,58b543783ed43edaa57b1b3a3c4ea81b5b23aad9,"Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric",2009,M. Snover; Nitin Madnani; B. Dorr; R. Schwartz
189,D09-1053,N04-4006,[0],conclusion,"In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and errordriven learning ) , which have been well studied in statistical language modeling for speech and natural language applications <TARGET_CITATION/> , to ranking models for Web search applications ."," In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and errordriven learning ) , which have been well studied in statistical language modeling for speech and natural language applications <TARGET_CITATION/> , to ranking models for Web search applications . In this paper, we extend two classes of model adaptation methods (i.e., model interpolation and errordriven learning), which have been well studied in statistical language modeling for speech and natural language applications <CITATION/>, to ranking models for Web search applications.",70751c07b6497a61216c845d4048b1ddb4792349,Model Adaptation via Model Interpolation and Boosting for Web Search Ranking,2009,Jianfeng Gao; Qiang Wu; C. Burges; K. Svore; Yi Su; N. Khan; Shalin S Shah; Hongyan Zhou,9822558280cd661fcf07230ccfa5328d582e078d,Language Model Adaptation with MAP Estimation and the Perceptron Algorithm,2004,M. Bacchiani; Brian Roark; M. Saraçlar
190,W03-0806,External_20441,[0],experiments,GATE goes beyond earlier systems by using a componentbased infrastructure <TARGET_CITATION/> which the GUI is built on top of .,"Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) <CITATION/> and the Alembic Workbench <CITATION/>) as well as NLP tools and resources that can be manipulated from the GUI. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors <CITATION/>. GATE goes beyond earlier systems by using a componentbased infrastructure <TARGET_CITATION/> which the GUI is built on top of . GATE goes beyond earlier systems by using a componentbased infrastructure <CITATION/> which the GUI is built on top of. For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors <CITATION/>. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) <CITATION/> and the Alembic Workbench <CITATION/>) as well as NLP tools and resources that can be manipulated from the GUI.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,066581bf5c169cf4d621104bd204812316520546,Software architecture for language engineering,2000,
192,P10-4003,W06-1803,[2],experiments,The system uses a knowledge base implemented in the KM representation language <TARGET_CITATION/> to represent the state of the world ., The system uses a knowledge base implemented in the KM representation language <TARGET_CITATION/> to represent the state of the world . The system uses a knowledge base implemented in the KM representation language <CITATION/> to represent the state of the world.,1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,eb2daad100184fcbabd311e66b516e51052f2863,Interpretation and Generation in a Knowledge-Based TutorialSystem,2006,M. Dzikovska; Charles B. Callaway; Elaine Farrow
194,J97-4003,External_41778,[0],introduction,descriptionlevel lexical rules ( DLRs ; <TARGET_CITATION/> ) .5 2.2.1 MetaLevel Lexical Rules .,"While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed, the metalevel lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the . descriptionlevel lexical rules ( DLRs ; <TARGET_CITATION/> ) .5 2.2.1 MetaLevel Lexical Rules . descriptionlevel lexical rules (DLRs; Meurers 1995).5 2.2.1 MetaLevel Lexical Rules. Two formalizations of lexical rules as used by HPSG linguists have been proposed, the metalevel lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the .While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,towards a semantics for lexical rules as used in hpsg,1995,Detmar Meurers
195,J03-3004,External_3000,[0],introduction,"All EBMT systems , from the initial proposal by <TARGET_CITATION/> to the recent collection of <CITATION/> , are premised on the availability of subsentential alignments derived from the input bitext .","2. Deriving Translation Resources from WebBased MT Systems All EBMT systems , from the initial proposal by <TARGET_CITATION/> to the recent collection of <CITATION/> , are premised on the availability of subsentential alignments derived from the input bitext . All EBMT systems, from the initial proposal by <CITATION/> to the recent collection of <CITATION/>, are premised on the availability of subsentential alignments derived from the input bitext. Deriving Translation Resources from WebBased MT Systems2.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,bc43f6bccb18a5a4892daa8e66756e0a684e7f5c,A framework of a mechanical translation between Japanese and English by analogy principle,1984,M. Nagao
197,J97-4003,External_18901,[4],related work,"In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; <TARGET_CITATION/> ;","Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; <TARGET_CITATION/> ; In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,featurebased inheritance networks for computational lexicons,1992,Hans-Ulrich Krieger; John Nerbonne
198,P07-1068,External_2967,[1],introduction,"These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the firstsense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics <TARGET_CITATION/> .","Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).(5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NPZ is a hyponym of w in WordNet, using only the first WordNet sense of NPZ.1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the firstsense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics <TARGET_CITATION/> . These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs.2 (6) INDUCED CLASS: Since the firstsense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., <CITATION/>). (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NPZ is a hyponym of w in WordNet, using only the first WordNet sense of NPZ.1 If so, we create a WN CLASS feature with w as its value. Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,dbfd191afbbc8317577cbc44afe7156df546e143,Automatic Acquisition of Hyponyms from Large Text Corpora,1992,Marti A. Hearst
200,E03-1002,P01-1010,[4],experiments,The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSNFreq > 200 ) and five recent statistical parsers <TARGET_CITATION/> .,The Tags model achieves performance which is better than any previously published results on parsing with a nonlexicalized model. The Tags model also does much better than the only other broad coverage neural network parser <CITATION/>. The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSNFreq > 200 ) and five recent statistical parsers <TARGET_CITATION/> . The bottom panel of table 1 lists the results for the chosen lexicalized model (SSNFreq>200) and five recent statistical parsers <CITATION/>. The Tags model also does much better than the only other broad coverage neural network parser <CITATION/>. The Tags model achieves performance which is better than any previously published results on parsing with a nonlexicalized model.,adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,d73a70359f568ab32943a74f7891a27257847b3e,What is the Minimal Set of Fragments that Achieves Maximal Parse Accuracy?,2001,R. Bod
201,D09-1056,W07-2041,[0],related work,"Nevertheless , the full document text is present in most systems , sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <TARGET_CITATION/>  .","The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. Nevertheless , the full document text is present in most systems , sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <TARGET_CITATION/>  . Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. The most basic is a Bag of Words (BoW) representation of the document text.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,03b6cd177ccf8d73c05a5898ad23d74a049612f3,IRST-BP: Web People Search Using Name Entities,2007,Octavian Popescu; B. Magnini
202,P00-1007,W98-1117,[5],conclusion,"In a similar vain to <TARGET_CITATION/> et al. ( 1999 ) , the method extends an existing flat shallowparsing method to handle composite structures .","In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use wordtypes, such as a special tag for beverbs, or for prepositions like of' which attaches mainly to nouns <CITATION/>. In a similar vain to <TARGET_CITATION/> , the method extends an existing flat shallowparsing method to handle composite structures . In a similar vain to <CITATION/>, the method extends an existing flat shallowparsing method to handle composite structures. An additional possibility is to use wordtypes, such as a special tag for beverbs, or for prepositions like of' which attaches mainly to nouns <CITATION/>. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,2f21c600925122bfd7e31135a6dca51f90ee9cfa,A Maximum-Entropy Partial Parser for Unrestricted Text,1998,Wojciech Skut; T. Brants
203,P07-1068,H05-1003,[0],introduction,"As a result , researchers have readopted the oncepopular knowledgerich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs <TARGET_CITATION/> , their semantic similarity as computed using WordNet ( e.g. , <CITATION/> ) or Wikipedia <CITATION/> , and the contextual role played by an NP ( see <CITATION/> ) .","While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have readopted the oncepopular knowledgerich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs <TARGET_CITATION/> , their semantic similarity as computed using WordNet ( e.g. , <CITATION/> ) or Wikipedia <CITATION/> , and the contextual role played by an NP ( see <CITATION/> ) . As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>). In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,b2589af6b6150f80f6a492ad0d519362dd981270,Using Semantic Relations to Refine Coreference Decisions,2005,Heng Ji; David Westbrook; R. Grishman
204,P08-1101,P07-1106,[3],experiments,"We built a twostage baseline system , using the perceptron segmentation model from our previous work <TARGET_CITATION/> and the perceptron POS tagging model from <CITATION/> ."," We built a twostage baseline system , using the perceptron segmentation model from our previous work <TARGET_CITATION/> and the perceptron POS tagging model from <CITATION/> . We built a twostage baseline system, using the perceptron segmentation model from our previous work <CITATION/> and the perceptron POS tagging model from <CITATION/>.",3594af2ebf510609651bf282dfea65c8e837b1a7,Joint Word Segmentation and POS Tagging Using a Single Perceptron,2008,Yue Zhang; S. Clark,c7607b791d5df3b45dc49a1dab48d8fae07ebd4b,Chinese Segmentation with a Word-Based Perceptron Algorithm,2007,Yue Zhang; S. Clark
206,W02-0309,External_80942,[0],introduction,"Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neoclassical compounding <TARGET_CITATION/> .","The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents. Hence, enumerating morphological variants in a semiautomatically generated lexicon, such as proposed for French <CITATION/>, turns out to be infeasible, at least for German and related languages. Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neoclassical compounding <TARGET_CITATION/> . Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neoclassical compounding <CITATION/>. Hence, enumerating morphological variants in a semiautomatically generated lexicon, such as proposed for French <CITATION/>, turns out to be infeasible, at least for German and related languages.The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,,the semantic structure of neoclassical compounds,1988,A McCray; A Browne; D Moore
208,D10-1052,P09-1037,[2],,"To model d ( FWi  1 , S  T ) , d ( FWi +1 , S  T ) , i.e. whether Li , S  T and Ri , S  T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of <TARGET_CITATION/> .","The maximality ensures the uniqueness of L and R.jections of the function word and the neighboring phrase are adjacent or separated by an intervening phrase. To model d ( FWi  1 , S  T ) , d ( FWi +1 , S  T ) , i.e. whether Li , S  T and Ri , S  T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of <TARGET_CITATION/> . To model d(FWi1,ST), d(FWi+1,ST), i.e. whether Li,ST and Ri,ST extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of <CITATION/>. jections of the function word and the neighboring phrase are adjacent or separated by an intervening phrase. The maximality ensures the uniqueness of L and R.",c97c609c34db8787505d83dfa03077d4813c7f19,Discriminative Word Alignment with a Function Word Reordering Model,2010,Hendra Setiawan; Christopher Dyer; P. Resnik,7c8e20532d6c0932b471f678ec28580b06c38cdd,Topological Ordering of Function Words in Hierarchical Phrase-based Translation,2009,Hendra Setiawan; Min-Yen Kan; Haizhou Li; P. Resnik
210,D08-1034,External_29717,[0],introduction,Semantic Role labeling ( SRL ) was first defined in <TARGET_CITATION/> ., Semantic Role labeling ( SRL ) was first defined in <TARGET_CITATION/> . Semantic Role labeling (SRL) was first defined in <CITATION/>.,74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,c274b8aac56e49e65a3827c570b2496b14429166,Automatic Labeling of Semantic Roles,2000,D. Gildea; Dan Jurafsky
211,W06-2807,External_59540,[0],,AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML <TARGET_CITATION/> .,The Ruby on <CITATION/> framework permits us to quickly develop web applications without rewriting common functions and classes. We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface. AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML <TARGET_CITATION/> . AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML <CITATION/>.We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface. The Ruby on <CITATION/> framework permits us to quickly develop web applications without rewriting common functions and classes.,48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,ajax a new approach to web applications url httpwwwadaptivepathcompublicationsessays archives000385php retrieved the 22nd of december,2005,Jesse James Garrett
212,J01-4001,External_17373,[0],,"The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC6 and MUC7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in <TARGET_CITATION/> .","From simple cooccurrence rules <CITATION/> through training decision trees to identify anaphorantecedent pairs <CITATION/> to genetic algorithms to optimize the resolution factors <CITATION/>, the successful performance of more and more modern approaches was made possible by the availability of suitable corpora. While the shift toward knowledgepoor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC6 and MUC7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in <TARGET_CITATION/> . The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>. While the shift toward knowledgepoor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. From simple cooccurrence rules <CITATION/> through training decision trees to identify anaphorantecedent pairs <CITATION/> to genetic algorithms to optimize the resolution factors <CITATION/>, the successful performance of more and more modern approaches was made possible by the availability of suitable corpora.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,recognizing referential links an information extraction perspective,1997,Megumi Kameyama
213,P97-1063,External_19463,[4],,"The most detailed evaluation of link tokens to date was performed by <TARGET_CITATION/> , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."," The most detailed evaluation of link tokens to date was performed by <TARGET_CITATION/> , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards . The most detailed evaluation of link tokens to date was performed by <CITATION/>, who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,e35ef9b56f086fca76b9d7465e61cc227cc6c452,Line ‘Em Up: Advances in Alignment Technology and their Impact on Translation Support Tools,1998,E. Macklovitch; M. Hannan
214,E03-1002,A00-2018,[4],,"Loglinear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers <TARGET_CITATION/> .","The method is a form of multilayered artificial neural network called Simple Synchrony Networks <CITATION/>. The outputs of this network are probability estimates computed with a loglinear model (also known as a maximum entropy model), as is done in <CITATION/>. Loglinear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers <TARGET_CITATION/> . Loglinear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers <CITATION/>. The outputs of this network are probability estimates computed with a loglinear model (also known as a maximum entropy model), as is done in <CITATION/>. The method is a form of multilayered artificial neural network called Simple Synchrony Networks <CITATION/>.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
216,W02-0309,External_45869,[0],introduction,"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( <TARGET_CITATION/> ; J  appinen and Niemist  o , 1988 ; <CITATION/> ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved ."," Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( <TARGET_CITATION/> ; J  appinen and Niemist  o , 1988 ; <CITATION/> ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved . Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval (IR) system <CITATION/>, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,,responsa an operational fulltext retrieval system with linguistic components for large corpora,1990,Y Choueka
217,W04-0910,External_40272,[0],,"For shuffling paraphrases , french alternations are partially described in <TARGET_CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .","Intercategorial synonymy not involving a derivational morphology link has been little studied and resources are lacking. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases , french alternations are partially described in <TARGET_CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs . For shuffling paraphrases, french alternations are partially described in <CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. Intercategorial synonymy not involving a derivational morphology link has been little studied and resources are lacking.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,7ab72bcaf8e576ea716185a60fad70ee9a0aede9,Alternations and Verb Semantic Classes for French: Analysis and Class Formation,1999,P. Saint-Dizier
218,N04-2004,External_4340,[0],,"A more recent approach , advocated by Rappaport <CITATION/> , describes a basic set of event templates corresponding to Vendler 's event classes <TARGET_CITATION/> : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )","(2) He sweeps the floor clean. [ [ DO(he, sweeps(the floor)) ] CAUSE [ BECOME [ clean(the floor) ] ] ] Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean. A more recent approach , advocated by Rappaport <CITATION/> , describes a basic set of event templates corresponding to Vendler 's event classes <TARGET_CITATION/> : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment ) A more recent approach, advocated by Rappaport <CITATION/>, describes a basic set of event templates corresponding to Vendler's event classes <CITATION/>: (3) a. [ x ACT<MANNER> ] (activity) b. [ x <STATE> ] (state) c. [ BECOME [ x <STATE> ] ] (achievement) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] (accomplishment)[ [ DO(he, sweeps(the floor)) ] CAUSE [ BECOME [ clean(the floor) ] ] ] Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean. (2) He sweeps the floor clean.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,3638a2a28ead10a1b91ba3afd536ceb92ad7c6fc,Verbs and Times,1957,Z. Vendler
219,J03-3004,External_30768,[0],introduction,<TARGET_CITATION/> combines lexical and dependency mappings to form his generalizations .,"Of course, many other researchers also try to extract generalized templates. <CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. <TARGET_CITATION/> combines lexical and dependency mappings to form his generalizations . <CITATION/> combines lexical and dependency mappings to form his generalizations. <CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Of course, many other researchers also try to extract generalized templates.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,7b972262e01dacfcb8a96d68b9b986cf0c883371,A Method for Extracting Translation Patterns from Translation Examples,1993,Hideo Watanabe
220,W04-0910,P01-1019,[0],,"Thus for instance , <TARGET_CITATION/> describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","Semantic grammars'' already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , <TARGET_CITATION/> describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar ( LFG ) with a glue semantics . Thus for instance, <CITATION/> describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar (LFG) with a glue semantics. Semantic grammars'' already exist which describe not only the syntax but also the semantics of natural language.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,e006247c9584f39593bed908827cca40b74cdf66,An Algebra for Semantic Construction in Constraint-based Grammars,2001,Ann A. Copestake; A. Lascarides; D. Flickinger
221,D10-1052,P09-1037,[3],,The reordering models we describe follow our previous work using function word models for translation <TARGET_CITATION/> ., The reordering models we describe follow our previous work using function word models for translation <TARGET_CITATION/> . The reordering models we describe follow our previous work using function word models for translation <CITATION/>.,c97c609c34db8787505d83dfa03077d4813c7f19,Discriminative Word Alignment with a Function Word Reordering Model,2010,Hendra Setiawan; Christopher Dyer; P. Resnik,7c8e20532d6c0932b471f678ec28580b06c38cdd,Topological Ordering of Function Words in Hierarchical Phrase-based Translation,2009,Hendra Setiawan; Min-Yen Kan; Haizhou Li; P. Resnik
222,E03-1005,P02-1034,[1],introduction,"And <CITATION/> argues for  keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in <TARGET_CITATION/> who use exactly the same set of ( all ) tree fragments as proposed in <CITATION/> .","While the models of <CITATION/> restricted the fragments to the locality of headwords, later models showed the importance of including context from higher nodes in the tree <CITATION/>. The importance of including nonheadwords has become uncontroversial <CITATION/>. And <CITATION/> argues for  keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in <TARGET_CITATION/> who use exactly the same set of ( all ) tree fragments as proposed in <CITATION/> . And <CITATION/> argues for keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in <CITATION/> who use exactly the same set of (all) tree fragments as proposed in <CITATION/>. The importance of including nonheadwords has become uncontroversial <CITATION/>. While the models of <CITATION/> restricted the fragments to the locality of headwords, later models showed the importance of including context from higher nodes in the tree <CITATION/>.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,fe638b5610475d4524684fb2c2b7b08c119c8700,"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron",2002,M. Collins; Nigel P. Duffy
223,J09-4010,J05-3002,[0],method,"In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization <TARGET_CITATION/> .","This task can be cast as extractive multidocument summarization. Unlike a document reuse approach, sentencelevel approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization <TARGET_CITATION/> . In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization <CITATION/>. Unlike a document reuse approach, sentencelevel approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. This task can be cast as extractive multidocument summarization.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,21a7b165b2e09cfc166bbbc69fef0732d6d99d5c,Sentence Fusion for Multidocument News Summarization,2005,R. Barzilay; K. McKeown
224,D11-1138,W11-2102,[2],experiments,criteria and data used in our experiments are based on the work of <TARGET_CITATION/> .,"The reordering cost, evaluation1Our rules are similar to those from <CITATION/>. criteria and data used in our experiments are based on the work of <TARGET_CITATION/> . criteria and data used in our experiments are based on the work of <CITATION/>. 1Our rules are similar to those from <CITATION/>.The reordering cost, evaluation",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,6de941e8c9099e71658b34a77d2bcb8f856656a8,A Lightweight Evaluation Framework for Machine Translation Reordering,2011,David Talbot; H. Kazawa; Hiroshi Ichikawa; Jason Katz-Brown; Masakazu Seno; F. Och
225,K15-1002,N06-2015,[2],experiments,"We present experiments on the two standard coreference resolution datasets , ACE2004 ( NIST , 2004 ) and OntoNotes5 .0 <TARGET_CITATION/> ."," We present experiments on the two standard coreference resolution datasets , ACE2004 ( NIST , 2004 ) and OntoNotes5 .0 <TARGET_CITATION/> . We present experiments on the two standard coreference resolution datasets, ACE2004 (NIST, 2004) and OntoNotes5.0 <CITATION/>.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,e54d8b07ef659f9ee2671441c4355e414e408836,OntoNotes: The 90% Solution,2006,E. Hovy; Mitchell P. Marcus; Martha Palmer; L. Ramshaw; R. Weischedel
226,J09-4010,P06-1093,[4],method, Only qualitative observations of the responses were reported ( no formal evaluation was performed ) <TARGET_CITATION/> .,In <CITATION/> we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. Only qualitative observations of the responses were reported ( no formal evaluation was performed ) <TARGET_CITATION/> .  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. These systems addressed the evaluation issue as follows.In <CITATION/> we identified several systems that resemble ours in that they provide answers to queries.,a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,df57f5bada8f1e70a934300886f0a0f4ec86927c,Automatic Generation of Domain Models for Call-Centers from Noisy Transcriptions,2006,Shourya Roy; L. V. Subramaniam
227,E03-1005,External_16080,[0],introduction,"And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 <TARGET_CITATION/> .","For the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability.Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 <TARGET_CITATION/> . And subderivations headed by A1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1/a1 <CITATION/>. Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a. For the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa,Efficient Algorithms for Parsing the DOP Model,1996,J. Goodman
228,W06-2933,W02-2016,[2],introduction, Support vector machines for mapping histories to parser actions <TARGET_CITATION/> ., A deterministic algorithm for building labeled projective dependency graphs <CITATION/>.  Historybased feature models for predicting the next parser action <CITATION/>. Support vector machines for mapping histories to parser actions <TARGET_CITATION/> .  Support vector machines for mapping histories to parser actions <CITATION/>.  Historybased feature models for predicting the next parser action <CITATION/>.  A deterministic algorithm for building labeled projective dependency graphs <CITATION/>.,f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,f6c043a4c9bb41fb155bc3485e80bb079ef61cb9,Japanese Dependency Analysis using Cascaded Chunking,2002,Taku Kudo; Yuji Matsumoto
229,E03-1005,External_16080,[0],introduction,"<TARGET_CITATION/> ) developed a polynomial time PCFGreduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .","Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques <CITATION/>, or by Viterbi nbest search <CITATION/>, or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002). Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. <TARGET_CITATION/> ) developed a polynomial time PCFGreduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar . Goodman (1996, 1998) developed a polynomial time PCFGreduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques <CITATION/>, or by Viterbi nbest search <CITATION/>, or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa,Efficient Algorithms for Parsing the DOP Model,1996,J. Goodman
232,P02-1001,External_8687,[0],introduction,Such approaches have been tried recently in restricted cases <TARGET_CITATION/> .,"This allows meaningful parameter tying: if certain arcs such asu:i *, *, and a:ae o:e * share a contextual vowelfronting'' feature, then their weights rise and fall together with the strength of that feature. The resulting machine must be normalized, either perstate or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases <TARGET_CITATION/> . Such approaches have been tried recently in restricted cases <CITATION/>. The resulting machine must be normalized, either perstate or globally, to obtain a joint or a conditional distribution as desired. This allows meaningful parameter tying: if certain arcs such asu:i *, *, and a:ae o:e * share a contextual vowelfronting'' feature, then their weights rise and fall together with the strength of that feature.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,expectation semirings flexible em for finitestate transducers,2001,Jason Eisner
233,J90-3003,External_73668,[0],introduction,"The relation between discourse and prosodic phrasing has been examined in some detail by <TARGET_CITATION/> , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .","like the claws of a crab II thrown out on each side.f. Two years II have passed since then. The relation between discourse and prosodic phrasing has been examined in some detail by <TARGET_CITATION/> , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse . The relation between discourse and prosodic phrasing has been examined in some detail by <CITATION/>, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse. f. Two years II have passed since then. like the claws of a crab II thrown out on each side.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,39d16512f4797d34f1072ad21ebb0686ff997be0,Textual Aspects of Prosody in Swedish,1982,G. Bruce
234,W06-2933,External_6725,[4],experiments,"By contrast , Turkish <TARGET_CITATION/> exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .","The results for Arabic <CITATION/> are characterized by low root accuracyas well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast , Turkish <TARGET_CITATION/> exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) . By contrast, Turkish <CITATION/> exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). The results for Arabic <CITATION/> are characterized by low root accuracy",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,ab626a437807ffb7660d335a0c00342c2d7adf85,The Annotation Process in the Turkish Treebank,2003,N. Atalay; Kemal Oflazer; Bilge Say
235,J04-3001,External_15318,[0],,The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV <TARGET_CITATION/> .,"There are two types of selection algorithms: committeebased and single learner. A committeebased selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV <TARGET_CITATION/> . The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV <CITATION/>. A committeebased selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). There are two types of selection algorithms: committeebased and single learner.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,c101259742dcd6fb2fcf51c7995522e632db2744,Selective Sampling Using the Query by Committee Algorithm,1997,Y. Freund; H. Seung; E. Shamir; Naftali Tishby
236,P11-1134,External_4216,[2],,"Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit <TARGET_CITATION/> .","In order to build EnglishSpanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations SpanishEnglish parallel corpora released for the WMT101. We run TreeTagger <CITATION/> for tokenization, and used the Giza++ <CITATION/> to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit <TARGET_CITATION/> . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit <CITATION/>. We run TreeTagger <CITATION/> for tokenization, and used the Giza++ <CITATION/> to align the tokenized corpora at the word level. In order to build EnglishSpanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations SpanishEnglish parallel corpora released for the WMT101.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,4ee2eab4c298c1824a9fb8799ad8eed21be38d21,Moses: Open Source Toolkit for Statistical Machine Translation,2007,Philipp Koehn; Hieu T. Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; N. Bertoldi; Brooke Cowan; Wade Shen; C. Moran; Richard Zens; Chris Dyer; Ondrej Bojar; Alexandra Constantin; Evan Herbst
237,J92-1004,H89-1017,[0],introduction,"Representative systems are described in <CITATION/> , De <TARGET_CITATION/> .","In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in <CITATION/> , De <TARGET_CITATION/> . Representative systems are described in <CITATION/>, De <CITATION/>.Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,6ca5729fb789b50385d9c38627f2947ff7db86dd,The MINDS System: Using Context and Dialog to Enhance Speech Recognition,1989,S. Young
238,J90-3003,External_48686,[2],introduction,"Our rules for phonological word formation are adopted , for the most part , from G & G , <TARGET_CITATION/> , and the account of monosyllabic destressing in <CITATION/> .","V NP PP We+saw spring+up among+the+trees The location rules identify possible boundary sites by first deriving phonological words from the lexical items in a parse tree, and then grouping the phonological words into larger phonological phrases. The boundaries that separate phonological phrases are the candidates for prosodic phrase boundaries, since the prosodic phrases of speech consist of one or more phonological phrases. Our rules for phonological word formation are adopted , for the most part , from G & G , <TARGET_CITATION/> , and the account of monosyllabic destressing in <CITATION/> . Our rules for phonological word formation are adopted, for the most part, from G &G, <CITATION/>, and the account of monosyllabic destressing in <CITATION/>. The boundaries that separate phonological phrases are the candidates for prosodic phrase boundaries, since the prosodic phrases of speech consist of one or more phonological phrases. V NP PP We+saw spring+up among+the+trees The location rules identify possible boundary sites by first deriving phonological words from the lexical items in a parse tree, and then grouping the phonological words into larger phonological phrases.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,ef154b63de83e78357ddb867dff83c91d94b63e7,Prosodic structure and spoken word recognition,1987,F. Grosjean; J. Gee
239,J05-3003,External_59044,[0],,"As a generalization , <TARGET_CITATION/> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .","Lexicographers were allowed to extrapolate from the citations found, a procedurewhich is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , <TARGET_CITATION/> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . As a generalization, <CITATION/> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. which is bound to be less certain than the assignment of frames based entirely on existing examples. Lexicographers were allowed to extrapolate from the citations found, a procedure",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,8d85c207d12109c746dbcb59278c742c46af6c5f,From dictionary to corpus to self-organizing dictionary: learning valency associations in the face of variation and change,2001,Ted Briscoe
240,W06-3813,External_9163,[0],related work,Such systems extract information from some types of syntactic units ( clauses in <TARGET_CITATION/> ; noun phrases in <CITATION/> ) .,"Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>. In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Such systems extract information from some types of syntactic units ( clauses in <TARGET_CITATION/> ; noun phrases in <CITATION/> ) . Such systems extract information from some types of syntactic units (clauses in <CITATION/>; noun phrases in <CITATION/>). In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,b6e63d7636801300a6c2fa63d3410bf849ce2e36,Semantic Interpretation of Nominalizations,1996,R. Hull; F. Gomez
241,W06-1104,External_933,[0],,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <CITATION/> , informationbased <TARGET_CITATION/> or distributional <CITATION/> ."," Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <CITATION/> , informationbased <TARGET_CITATION/> or distributional <CITATION/> . Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,6b64e068a8face2540fc436af40dbcd2b0912bbf,Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy,1997,Jay J. Jiang; D. Conrath
242,P11-1134,External_14644,[0],introduction,"Besides WordNet , the RTE literature documents the use of a variety of lexical information sources <TARGET_CITATION/> .","Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet , the RTE literature documents the use of a variety of lexical information sources <TARGET_CITATION/> . Besides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,41aa047d94c91674cccad645759fd4e06ad8f6be,"Recognizing textual entailment: Rational, evaluation and approaches",2009,Ido Dagan; Bill Dolan; B. Magnini; D. Roth
243,J09-4010,N03-1004,[4],,"The question answering system developed by <TARGET_CITATION/> belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade subcategory ) .","A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by <TARGET_CITATION/> belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade subcategory ) . The question answering system developed by <CITATION/> belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke's cascade subcategory). Instead it uses a voting mechanism to select the answer given by the majority of methods. A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,8e83d501742f56cdf35cbdb08f21fd4ffc6406c5,"In Question Answering, Two Heads Are Better Than One",2003,Jennifer Chu-Carroll; Krzysztof Czuba; J. Prager; Abraham Ittycheriah
244,W04-1610,External_87687,[0],related work,"More recently , <CITATION/> has performed a good survey of document categorization ; recent works can also be found in <TARGET_CITATION/> .","For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces. A good study comparing document categorization algorithms can be found in <CITATION/>. More recently , <CITATION/> has performed a good survey of document categorization ; recent works can also be found in <TARGET_CITATION/> . More recently, <CITATION/> has performed a good survey of document categorization; recent works can also be found in <CITATION/>. A good study comparing document categorization algorithms can be found in <CITATION/>. For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,6bfdcd1e6cb8b93cd571b98514990bb91fe7a4ea,A Family of Additive Online Algorithms for Category Ranking,2003,K. Crammer; Y. Singer
245,J00-4001,External_30353,[0],,Discriminant analysis has been employed by researchers in automatic text genre detection <TARGET_CITATION/> since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .,"The Mahalanobis distance d of a vector x from a mean vector itt, is given by the formula:where C, is the covariance matrix of x. Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid. Discriminant analysis has been employed by researchers in automatic text genre detection <TARGET_CITATION/> since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables . Discriminant analysis has been employed by researchers in automatic text genre detection <CITATION/> since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables.where C, is the covariance matrix of x. Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid. The Mahalanobis distance d of a vector x from a mean vector itt, is given by the formula:",bad78f839e2b566c4d62558827d03b9aa1c57e48,Automatic Text Categorization In Terms Of Genre and Author,2000,E. Stamatatos; N. Fakotakis; G. Kokkinakis,,using registerdiversified corpora for general language studies,1993,Douglas Biber
246,D13-1115,External_32478,[3],introduction,"This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity <TARGET_CITATION/> .","The model we rely on was originally developed by<CITATION/> and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity <TARGET_CITATION/> . This model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity <CITATION/>. <CITATION/> and is based on a generalization of Latent Dirichlet Allocation. The model we rely on was originally developed by",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
247,J06-2002,External_522,[0],introduction,"In other words , existing treatments of gradables in GRE fail to take the  efficiency of language '' into account ( <TARGET_CITATION/> ; see our Section 2 ) .","Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it. Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations LCBlarge, chihuahuaRCB (which denotes the empty set) and LCBlarge,alsatianRCB (the set of all alsatians) useless. In other words , existing treatments of gradables in GRE fail to take the  efficiency of language '' into account ( <TARGET_CITATION/> ; see our Section 2 ) . In other words, existing treatments of gradables in GRE fail to take the efficiency of language'' into account (Barwise and Perry 1983; see our Section 2).Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations LCBlarge, chihuahuaRCB (which denotes the empty set) and LCBlarge,alsatianRCB (the set of all alsatians) useless. Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,319baebb5894d843d63a913647ec9a52801ba1b5,Situations and Attitudes.,1986,N. Cocchiarella; J. Barwise; J. Perry
249,W00-1312,External_4381,[2],,"Following <TARGET_CITATION/> , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) ."," Following <TARGET_CITATION/> , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) . Following <CITATION/>, the IR system ranks documents according to the probability that a document D is relevant given the query Q, P(D is R IQ).",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,a hidden markov model information retrieval systemquot,1999,D Miller; T Leek; R Schwartz
250,J05-3003,External_4377,[0],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; <TARGET_CITATION/> ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; <TARGET_CITATION/> ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexicalfunctional syntax,2001,Joan Bresnan
252,D08-1007,J03-3005,[1],method,"Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to <TARGET_CITATION/> 's conditional probability scores for pseudodisambiguation of ( v , n , n  ) triples : Pr ( v  n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than cooccurrence frequency f ( v , n ) .","For example, a feature for a verbobject pair might be, the verb is eat and the object is lowercase.'' In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to <TARGET_CITATION/> 's conditional probability scores for pseudodisambiguation of ( v , n , n  ) triples : Pr ( v  n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than cooccurrence frequency f ( v , n ) . Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to <CITATION/>'s conditional probability scores for pseudodisambiguation of (v, n, n) triples: Pr(vn) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than cooccurrence frequency f(v, n). In this representation, features for one predicate will be completely independent from those for every other predicate. For example, a feature for a verbobject pair might be, the verb is eat and the object is lowercase.''",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,5dfed29550d75cca99019aa52d40038dcb23b3cb,Using the Web to Obtain Frequencies for Unseen Bigrams,2003,Frank Keller; Mirella Lapata
254,K15-1001,P03-1021,[2],experiments,This was done by MERT optimization <TARGET_CITATION/> towards postedits under the TER target metric .,PEtest was held out for testing the algorithms' progress on unseen data. PEdev was used to obtain w* to define the utility model. This was done by MERT optimization <TARGET_CITATION/> towards postedits under the TER target metric . This was done by MERT optimization <CITATION/> towards postedits under the TER target metric. PEdev was used to obtain w* to define the utility model. PEtest was held out for testing the algorithms' progress on unseen data.,571e0df1b33cf9e232f9c96428618b530fdd6be1,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,2015,Artem Sokolov; S. Riezler; Shay B. Cohen,1f12451245667a85d0ee225a80880fc93c71cc8b,Minimum Error Rate Training in Statistical Machine Translation,2003,F. Och
255,J05-3003,External_61276,[0],,"She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden <TARGET_CITATION/> .","However, their evaluation does not examine the extracted subcategorization frames but rather the argumentadjunct distinctions posited by their system. The largest lexical evaluation we know of is that of Schulte im <CITATION/> for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden <TARGET_CITATION/> . She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden <CITATION/>. The largest lexical evaluation we know of is that of Schulte im <CITATION/> for German. However, their evaluation does not examine the extracted subcategorization frames but rather the argumentadjunct distinctions posited by their system.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,duden—das stilworterbuch duden—the style dictionary,2001,editor Dudenredaktion
257,J05-3003,A97-1052,[4],,"<TARGET_CITATION/> report on manually analyzing an openclass vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .","which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, <CITATION/> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. <TARGET_CITATION/> report on manually analyzing an openclass vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . <CITATION/> report on manually analyzing an openclass vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. As a generalization, <CITATION/> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. which is bound to be less certain than the assignment of frames based entirely on existing examples.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,9b77ca011896f79d9014704aaa63ecf4ffb3485c,Automatic Extraction of Subcategorization from Corpora,1997,Ted Briscoe; John A. Carroll
258,E03-1005,A00-2018,[4],conclusion,This is roughly an 11 % relative reduction in error rate over <TARGET_CITATION/> PCFGreduction reported in Table 1 .,"But while the accuracy of SLDOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LSDOP continues to increase and converges to LikelihoodDOP. The highest accuracy is obtained by SLDOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over <TARGET_CITATION/> PCFGreduction reported in Table 1 . This is roughly an 11% relative reduction in error rate over <CITATION/> and Bods PCFGreduction reported in Table 1. The highest accuracy is obtained by SLDOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. But while the accuracy of SLDOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LSDOP continues to increase and converges to LikelihoodDOP.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
259,P10-2059,External_88748,[0],introduction,"Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <TARGET_CITATION/> .","<CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <TARGET_CITATION/> . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <CITATION/>. <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,2a24e3fc6febb51bf2d78a10aac28e4125aa82ab,Contextual recognition of head gestures,2005,Louis-Philippe Morency; C. Sidner; Christopher Lee; Trevor Darrell
261,P10-4003,External_22831,[3],experiments,"The diagnoser , based on <TARGET_CITATION/> ) , outputs a diagnosis which consists of lists of correct , contradictory and nonmentioned objects and relations from the student 's answer .","Different remediation strategies need to be used depending on whether the student made a factual error (i.e., they misread the diagram and the bulb is not in a closed path) or produced an incorrect explanation (i.e., the bulb is indeed in a closed path, but they failed to mention that a battery needs to be in the same closed path for the bulb to light). The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser , based on <TARGET_CITATION/> ) , outputs a diagnosis which consists of lists of correct , contradictory and nonmentioned objects and relations from the student 's answer . The diagnoser, based on <CITATION/>, outputs a diagnosis which consists of lists of correct, contradictory and nonmentioned objects and relations from the student's answer. The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. Different remediation strategies need to be used depending on whether the student made a factual error (i.e., they misread the diagram and the bulb is not in a closed path) or produced an incorrect explanation (i.e., the bulb is indeed in a closed path, but they failed to mention that a battery needs to be in the same closed path for the bulb to light).",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,linking semantic and knowledge representations in a multidomain dialogue system,2008,Myroslava O Dzikovska; James F Allen; Mary D Swift
262,J97-4003,External_41778,[0],introduction,The formalization of DLRs provided by <TARGET_CITATION/> defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .,"Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up <CITATION/>. One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. The formalization of DLRs provided by <TARGET_CITATION/> defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 . The formalization of DLRs provided by <CITATION/> defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up <CITATION/>.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,towards a semantics for lexical rules as used in hpsg,1995,Detmar Meurers
263,P06-1012,W02-1006,[2],experiments,"Similar to our previous work <CITATION/> , we used the supervised WSD approach described in <TARGET_CITATION/> for our experiments , using the naive Bayes algorithm as our classifier ."," Similar to our previous work <CITATION/> , we used the supervised WSD approach described in <TARGET_CITATION/> for our experiments , using the naive Bayes algorithm as our classifier . Similar to our previous work <CITATION/>, we used the supervised WSD approach described in <CITATION/> for our experiments, using the naive Bayes algorithm as our classifier.",796f991064b5cb119df30945f77f1f46cf9c401b,Estimating Class Priors in Domain Adaptation for Word Sense Disambiguation,2006,Yee Seng Chan; H. Ng,4c2a642effd543babace0c565b48cadcb6fce14f,An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation,2002,Yoong Keok Lee; H. Ng
264,D14-1076,D13-1047,[4],experiments,"Similar to <TARGET_CITATION/> , our summarization system is , which consists of three key components : an initial sentence preselection module to select some important sentence candidates ; the above compression model to generate nbest compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences ."," Similar to <TARGET_CITATION/> , our summarization system is , which consists of three key components : an initial sentence preselection module to select some important sentence candidates ; the above compression model to generate nbest compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences . Similar to <CITATION/>, our summarization system is , which consists of three key components: an initial sentence preselection module to select some important sentence candidates; the above compression model to generate nbest compressions for each sentence; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences.",d830e732d4158807321bb8c60a6239f2b41f7ff3,Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees,2014,Chen Li; Yang Liu; Fei Liu; Lin Zhao; F. Weng,1b98ecc801d4435fd57c6ef87b4c24cf6b3b58ce,Document Summarization via Guided Sentence Compression,2013,Chen Li; Fei Liu; F. Weng; Yang Liu
265,J05-3003,External_13798,[0],related work,"The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <TARGET_CITATION/> .","The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. <CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <TARGET_CITATION/> . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <CITATION/>. <CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce,Natural Language Parsing as Statistical Pattern Recognition,1994,David M. Magerman
266,D11-1138,J08-4003,[2],experiments,transitionbased dependency parsing framework <TARGET_CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8 .,For our experiments we focus on two dependency parsers. Transitionbased: An implementation of the transitionbased dependency parsing framework <TARGET_CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8 . transitionbased dependency parsing framework <CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8.  Transitionbased: An implementation of theFor our experiments we focus on two dependency parsers.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,053f1cf10ced2321c1853f307075f0a6a83b6840,Algorithms for Deterministic Incremental Dependency Parsing,2008,Joakim Nivre
269,W06-3309,External_42488,[0],introduction,<TARGET_CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .,"(NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. <TARGET_CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . <CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. (NLM), which also serves as a readily available corpus of abstracts for our experiments.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,72de308314b88b53aef6cb86bd8390f334a6bd24,Categorization of Sentence Types in Medical Abstracts,2003,Lawrence K. McKnight; P. Srinivasan
270,W00-1017,External_46371,[0],,"This recognizer incrementally outputs word hypotheses as soon as they are found in the bestscored path in the forward search <TARGET_CITATION/> using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .","As the recognition engine, either VoiceRex, developed by NTT <CITATION/>, or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan <CITATION/>. This recognizer incrementally outputs word hypotheses as soon as they are found in the bestscored path in the forward search <TARGET_CITATION/> using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses . This recognizer incrementally outputs word hypotheses as soon as they are found in the bestscored path in the forward search <CITATION/> using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan <CITATION/>. As the recognition engine, either VoiceRex, developed by NTT <CITATION/>, or HTK from Entropic Research can be used.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,8f260472831ee4b53ca061363c3fff918a622905,Implementation of coordinative nodding behavior on spoken dialogue systems,1998,Jun-ichi Hirasawa; Noboru Miyazaki; Mikio Nakano; T. Kawabata
271,W00-1017,External_2950,[0],introduction,The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> ., The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> . The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <CITATION/>.,143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,596b5cc2773973515567d860dfcf25ebadd371ed,The Philips automatic train timetable information system,1995,H. Aust; M. Oerder; F. Seide; Volker Steinbiss
272,J09-4010,External_42507,[0],,"We posit that this would not have a significant effect on the results , in particular for MMLbased classification techniques , such as Decision Graphs <TARGET_CITATION/> .","The standard approach for combining precision and recall is to compute their harmonic mean, Fscore, as we have done in our16 In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. We posit that this would not have a significant effect on the results , in particular for MMLbased classification techniques , such as Decision Graphs <TARGET_CITATION/> . We posit that this would not have a significant effect on the results, in particular for MMLbased classification techniques, such as Decision Graphs <CITATION/>.16 In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. The standard approach for combining precision and recall is to compute their harmonic mean, Fscore, as we have done in our",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,73f1d17df0e1232da9e2331878a802a941f351c6,Decision Graphs - An Extension of Decision Trees,1993,Jonathan J. Oliver
273,E03-1005,P02-1034,[0],introduction,"<TARGET_CITATION/> showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in <CITATION/> on the WSJ .","<CITATION/> used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence <CITATION/>. <TARGET_CITATION/> showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in <CITATION/> on the WSJ . <CITATION/> showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in <CITATION/> on the WSJ. Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence <CITATION/>. <CITATION/> used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,fe638b5610475d4524684fb2c2b7b08c119c8700,"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron",2002,M. Collins; Nigel P. Duffy
274,W01-1510,External_15845,[4],introduction,Tateisi et al. also translated LTAG into HPSG <TARGET_CITATION/> .,"trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG <TARGET_CITATION/> . Tateisi et al. also translated LTAG into HPSG <CITATION/>. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. trees, and map them to LTAG derivation trees.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,d999ba173cf76a8242c94ecb2776b5c9c7ecf27a,Translating the XTAG English grammar to HPSG,1998,Yuka Tateisi; Kentaro Torisawa; Yusuke Miyao; Junichi Tsujii
277,W06-1639,External_6364,[0],related work,"There has also been work focused upon determining the political leaning ( e.g. ,  liberal '' vs.  conservative '' ) of a document or author , where most previouslyproposed methods make no direct use of relationships between the documents to be classified ( the  unlabeled '' texts ) <TARGET_CITATION/> .","Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>. There has also been work focused upon determining the political leaning ( e.g. ,  liberal '' vs.  conservative '' ) of a document or author , where most previouslyproposed methods make no direct use of relationships between the documents to be classified ( the  unlabeled '' texts ) <TARGET_CITATION/> . There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,extracting policy positions from political texts using words as data american political science review,2003,M Laver; K Benoit; J Garry
278,J86-1002,External_13881,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , <TARGET_CITATION/> , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."," A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , <TARGET_CITATION/> , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,the harpy speech understanding system in lea,1980,B Lowerre; R Reddy
280,J05-3003,External_1558,[0],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] <TARGET_CITATION/> , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] <TARGET_CITATION/> , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,3f6786dcd33e0474210a74e479f9eb53e849fa06,Tree-Adjoining Grammars and Genetic Programming,2015,A. Brabazon; M. O’Neill; S. McGarraghy
281,D12-1037,External_84061,[1],introduction,"In this paper , inspired by KNNSVM <TARGET_CITATION/> , we propose a local training method , which trains sentencewise weights instead of a single weight , to address the above two problems .","Therefore, there exists no single weight W which simultaneously obtains e11 and e21 as translation for f1 and f2 via Equation (1). However, we can achieve this with two weights: (1, 1) for f1 and (1, 1) for f2. In this paper , inspired by KNNSVM <TARGET_CITATION/> , we propose a local training method , which trains sentencewise weights instead of a single weight , to address the above two problems . In this paper, inspired by KNNSVM <CITATION/>, we propose a local training method, which trains sentencewise weights instead of a single weight, to address the above two problems. However, we can achieve this with two weights: (1, 1) for f1 and (1, 1) for f2. Therefore, there exists no single weight W which simultaneously obtains e11 and e21 as translation for f1 and f2 via Equation (1).",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,ceb0e1a86dc35e21ce5f0524c8476f15e1b08988,SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition,2006,Haotong Zhang; A. Berg; M. Maire; Jitendra Malik
282,W04-1805,External_4723,[4],related work,"1990 ) , on linguisitic acquisition ( by the use of PartofSpeech filters handcrafted by a linguist ) <CITATION/> or , more frequently , on a combination of the two ( <TARGET_CITATION/> , for example ) .","On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, However, our interpretation of LFs in this work is much looser, since we admitted verbs that would not be considered to be members of true collocations as Melcuk et al. (1984 1999) define them, i.e. groups of lexical units that share a restricted cooccurrence relationship. 1990 ) , on linguisitic acquisition ( by the use of PartofSpeech filters handcrafted by a linguist ) <CITATION/> or , more frequently , on a combination of the two ( <TARGET_CITATION/> , for example ) . 1990), on linguisitic acquisition (by the use of PartofSpeech filters handcrafted by a linguist) <CITATION/> or, more frequently, on a combination of the two (<CITATION/>, for example). Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, However, our interpretation of LFs in this work is much looser, since we admitted verbs that would not be considered to be members of true collocations as Melcuk et al. (1984 1999) define them, i.e. groups of lexical units that share a restricted cooccurrence relationship. On the other hand, other work has been carried out in order to acquire collocations.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,wordsketch extraction and display of significant collocations for lexicography,2001,Adam Kilgarriff; David Tugwell
283,J00-2014,External_52731,[0],,"<TARGET_CITATION/> report excellent partofspeech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .","Second, weights are an annoyance when writing grammars by hand. In some cases rankings may work well enough. <TARGET_CITATION/> report excellent partofspeech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences . <CITATION/> report excellent partofspeech tagging results using a handcrafted approach that is close to OT.3 More speculatively, imagine an OT grammar for stylistic revision of parsed sentences. In some cases rankings may work well enough. Second, weights are an annoyance when writing grammars by hand.",962381e601b37b50cd2a1ae387a1159f1c9209e6,Book Reviews: Optimality Theory,2000,Jason Eisner,9dad7b5ba8309259994908baa28660ad41845dc4,Comparing a Linguistic and a Stochastic Tagger,1997,C. Samuelsson; Atro Voutilainen
284,W04-0910,External_9563,[0],,"Thus for instance , <TARGET_CITATION/> describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","Semantic grammars'' already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , <TARGET_CITATION/> describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar ( LFG ) with a glue semantics . Thus for instance, <CITATION/> describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar (LFG) with a glue semantics. Semantic grammars'' already exist which describe not only the syntax but also the semantics of natural language.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,9605ca18895dc212861244495807f6696655882b,An Open Source Grammar Development Environment and Broad-coverage English Grammar Using HPSG,2000,Ann A. Copestake; D. Flickinger
285,J05-3003,External_12085,[0],,"While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , <TARGET_CITATION/> questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the languagespecific constituent structure level .","XCOMP and XADJ are open functions not requiring an internal SUBJ. The subject is instead specified externally in the matrix phrase: The judge wants [XCOMP to open an inquiry]. While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , <TARGET_CITATION/> questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the languagespecific constituent structure level . While many linguistic theories state subcategorization requirements in terms of phrase structure (CFG categories), <CITATION/> questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the languagespecific constituent structure level. The subject is instead specified externally in the matrix phrase: The judge wants [XCOMP to open an inquiry]. XCOMP and XADJ are open functions not requiring an internal SUBJ.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexical functional grammar volume 34 of syntax and semantics,2001,Mary Dalrymple
287,Q13-1020,P11-1001,[1],experiments,"11 From <TARGET_CITATION/> , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .","We built an s2t translation system with the achieved Utrees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From <TARGET_CITATION/> , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . 11 From <CITATION/>, we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large.We built an s2t translation system with the achieved Utrees after the 1000th iteration.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,feb8011d6bcd06bfa2ae2e50517a8d5b283b7d71,A Word-Class Approach to Labeling PSCFG Rules for Machine Translation,2011,Andreas Zollmann; S. Vogel
288,J87-3002,J87-3001,[0],introduction,In this paper we focus on the exploitation of the LDOCE grammar coding system ; <TARGET_CITATION/> describe further research in Cambridge utilising different types of information available in LDOCE .,"Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled core' vocabulary in defining the words throughout the dictionary. (<CITATION/> contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system ; <TARGET_CITATION/> describe further research in Cambridge utilising different types of information available in LDOCE . In this paper we focus on the exploitation of the LDOCE grammar coding system; <CITATION/> describe further research in Cambridge utilising different types of information available in LDOCE. (<CITATION/> contains further description and discussion of LDOCE.) Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled core' vocabulary in defining the words throughout the dictionary.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5,Processing Dictionary Definitions with Phrasal Pattern Hierarchies,1987,H. Alshawi
289,W06-1104,External_61155,[1],experiments,<TARGET_CITATION/> observed that some annotators were not familiar with the exact definition of semantic relatedness .,"Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. <TARGET_CITATION/> observed that some annotators were not familiar with the exact definition of semantic relatedness . <CITATION/> observed that some annotators were not familiar with the exact definition of semantic relatedness. Thus, they were not supervised during the experiment. Test subjects were invited via email to participate in the experiment.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3b5b95038c6b065f29649c1b11ea3e7855c00a53,Thinking beyond the nouns - computing semantic relatedness across parts of speech,2006,Iryna Gurevych
290,P11-1134,External_33255,[0],introduction,"All current approaches to monolingual TE , either syntactically oriented <CITATION/> , or applying logical inference <CITATION/> , or adopting transformationbased techniques <TARGET_CITATION/> , incorporate different types of lexical knowledge to support textual inference .","Section 6 concludes the paper, and outlines the directions of our future research.2 Lexical resources for TE and CLTE All current approaches to monolingual TE , either syntactically oriented <CITATION/> , or applying logical inference <CITATION/> , or adopting transformationbased techniques <TARGET_CITATION/> , incorporate different types of lexical knowledge to support textual inference . All current approaches to monolingual TE, either syntactically oriented <CITATION/>, or applying logical inference <CITATION/>, or adopting transformationbased techniques <CITATION/>, incorporate different types of lexical knowledge to support textual inference. 2 Lexical resources for TE and CLTESection 6 concludes the paper, and outlines the directions of our future research.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,2cc4810d05d8d734a2353f28e76ec6801ac8d8be,Tree edit distance for textual entailment,2007,Milen Kouylekov; B. Magnini
292,W02-0309,External_18980,[0],conclusion,"There has been some controversy , at least for simple stemmers <CITATION/> , about the effectiveness of morphological analysis for document retrieval <TARGET_CITATION/> ."," There has been some controversy , at least for simple stemmers <CITATION/> , about the effectiveness of morphological analysis for document retrieval <TARGET_CITATION/> . There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,22ea035da318c728015400551ad642160e1a23a9,How effective is suffixing?,1991,D. Harman
293,E03-1002,P93-1005,[2],method,"in historybased models <TARGET_CITATION/> , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .","structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in historybased models <TARGET_CITATION/> , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i . in historybased models <CITATION/>, the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d,_1, which is called the derivation history at step i. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1993,Ezra Black; F. Jelinek; J. Lafferty; David M. Magerman; R. Mercer; S. Roukos
294,J97-4003,External_61990,[0],introduction,The reader is referred to <TARGET_CITATION/> for a more detailed discussion of our use of constraint propagation .32 We illustrate the result of constraint propagation with our example grammar .,"Once we have computed c, we use it to make the extended lexical entry more specific. This technique closely resembles the offline constraint propagation technique described by <CITATION/>. The reader is referred to <TARGET_CITATION/> for a more detailed discussion of our use of constraint propagation .32 We illustrate the result of constraint propagation with our example grammar . The reader is referred to <CITATION/> for a more detailed discussion of our use of constraint propagation.32 We illustrate the result of constraint propagation with our example grammar. This technique closely resembles the offline constraint propagation technique described by <CITATION/>. Once we have computed c, we use it to make the extended lexical entry more specific.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,offline constraint propagation for efficient hpsg processing,1996,Detmar Meurers; Guido Minnen
295,P02-1001,External_1232,[0],introduction,"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisychannel decoding , ' including classic models for speech recognition <CITATION/> and machine translation <TARGET_CITATION/> .","An artificial example will appear in §2. The availability of toolkits for this weighted case (<CITATION/>; van <CITATION/>) promises to unify much of statistical NLP. Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisychannel decoding , ' including classic models for speech recognition <CITATION/> and machine translation <TARGET_CITATION/> . Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisychannel decoding,' including classic models for speech recognition <CITATION/> and machine translation <CITATION/>. The availability of toolkits for this weighted case (<CITATION/>; van <CITATION/>) promises to unify much of statistical NLP. An artificial example will appear in §2.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,translation with finitestate devices,1998,Kevin Knight; Yaser Al-Onaizan
296,J09-4010,External_36090,[0],,"Such technologies require significant human input , and are difficult to create and maintain <TARGET_CITATION/> .","The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms, such as expert systems <CITATION/> and casebased reasoning <CITATION/>. Such technologies require significant human input , and are difficult to create and maintain <TARGET_CITATION/> . Such technologies require significant human input, and are difficult to create and maintain <CITATION/>. The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms, such as expert systems <CITATION/> and casebased reasoning <CITATION/>.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,knowledge harvesting articulation and delivery the hewlettpackard journal,1998,K A Delic; D Lahaix
297,K15-1003,W14-1615,[0],method,We can define PCAT using a probabilistic grammar <TARGET_CITATION/> .,"For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward crosslinguisticallyplausible categories. To formalize the notion of what it means for a category to be more plausible'', we extend the category generator of our previous work, which we will call PCAT. We can define PCAT using a probabilistic grammar <TARGET_CITATION/> . We can define PCAT using a probabilistic grammar <CITATION/>. To formalize the notion of what it means for a category to be more plausible'', we extend the category generator of our previous work, which we will call PCAT. For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward crosslinguisticallyplausible categories.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,05bbf7436bfcb1e49e1132c16f25706b61de6c93,Weakly-Supervised Bayesian Learning of a CCG Supertagger,2014,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith
300,W01-1510,C00-1060,[0],introduction,"There have been many studies on parsing techniques <CITATION/> , ones on disambiguation models <TARGET_CITATION/> , and ones on programming/grammardevelopment environ ","Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques <CITATION/> , ones on disambiguation models <TARGET_CITATION/> , and ones on programming/grammardevelopment environ There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environStrongly equivalent grammars enable the sharing of ideas developed in each formalism. Our concern is, however, not limited to the sharing of grammars and lexicons.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,97fce4f41691b8707b12421320601f42b8cf134f,A Hybrid Japanese Parser with Hand-crafted Grammar and Statistics,2000,H. Kanayama; Kentaro Torisawa; Yutaka Mitsuishi; Junichi Tsujii
301,W06-3309,External_74941,[0],introduction,"For example , <TARGET_CITATION/> experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in Fscore .","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/>, information retrieval <CITATION/>, information extraction <CITATION/>, and question answering. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example , <TARGET_CITATION/> experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in Fscore . For example, <CITATION/> experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in Fscore. Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/>, information retrieval <CITATION/>, information extraction <CITATION/>, and question answering.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,,semiautomatic indexing of full text biomedical articles,2005,Clifford W Gay; Mehmet Kayaalp; Alan R Aronson
302,W04-1805,External_94861,[2],,"The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles <TARGET_CITATION/> .","CompuTerm 2004 3rd International Workshop on Computational Terminology 43 To construct this test set, we have focused our attention on ten domainspecific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer), serveur (server), systeme (system), utilisateur (user). The terms have been identified as the most specific to our corpus by a program developed by <CITATION/> and called TER1vloSTAT. The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles <TARGET_CITATION/> . The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles <CITATION/>. The terms have been identified as the most specific to our corpus by a program developed by <CITATION/> and called TER1vloSTAT. CompuTerm 2004 3rd International Workshop on Computational Terminology 43 To construct this test set, we have focused our attention on ten domainspecific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer), serveur (server), systeme (system), utilisateur (user).",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,two methods for extracting quotspecificquot singleword terms from specialized corpora,2004,Chantal Lemay; Marie-Claude L'Homme; Patrick Drouin
303,W06-1705,External_25124,[5],method,"Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by <TARGET_CITATION/> ) and shingling techniques described by <CITATION/> ."," Site based corpus annotation in which the user can specify a web site to annotate  Domain based corpus annotation in which the user specifies a content domain (with the use of keywords) to annotate  Crawler based corpus annotation more general web based corpus annotation in which crawlers are used to locate web pagesFrom a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentenceunits, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by <TARGET_CITATION/> ) and shingling techniques described by <CITATION/> . Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by <CITATION/> and shingling techniques described by <CITATION/>. From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentenceunits, but anaphoric annotation needs paragraphs or larger).  Site based corpus annotation in which the user can specify a web site to annotate  Domain based corpus annotation in which the user specifies a content domain (with the use of keywords) to annotate  Crawler based corpus annotation more general web based corpus annotation in which crawlers are used to locate web pages",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,facilitating the compilation and dissemination of adhoc web corpora,2004,W H Fletcher
305,J05-3003,External_13799,[0],related work,<TARGET_CITATION/> attempts to improve on the approach of <CITATION/> by passing raw text through a stochastic tagger and a finitestate parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they cooccur .,The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. <CITATION/> employ an additional statistical method based on loglinear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. <TARGET_CITATION/> attempts to improve on the approach of <CITATION/> by passing raw text through a stochastic tagger and a finitestate parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they cooccur . <CITATION/> attempts to improve on the approach of <CITATION/> by passing raw text through a stochastic tagger and a finitestate parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they cooccur. <CITATION/> employ an additional statistical method based on loglinear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.,ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,automatic acquisition of a large subcategorisation dictionary from corpora,1993,Christopher Manning
306,W02-0309,External_45868,[0],introduction,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> .","When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> . From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexicosemantic aspects of dederivation and decomposition <CITATION/>. This is particularly true for the medical domain. When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,,morphosemantic parsing of medical expressions,1998,R Baud; C Lovis; A-M Rassinoux; J-R Scherrer
309,J08-1003,External_90779,[1],introduction,"Problems such as these have motivated research on more abstract , dependencybased parser evaluation <TARGET_CITATION/> .","3. Because a treebased gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as handcrafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract , dependencybased parser evaluation <TARGET_CITATION/> . Problems such as these have motivated research on more abstract, dependencybased parser evaluation <CITATION/>. Because a treebased gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as handcrafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser.3.",9c8e756fda6c46d9f78430ee4f7bbce66b168921,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,2008,A. Cahill; Michael Burke; Ruth O'Donovan; S. Riezler; Josef van Genabith; Andy Way,29899547c993f30a9afcc3514ef55358d45d6b97,Speed and Accuracy in Shallow and Deep Stochastic Parsing,2004,R. Kaplan; S. Riezler; Tracy Holloway King; John T. Maxwell; A. Vasserman; Dick Crouch
310,P11-1134,N03-1017,[0],,They are widely used in MT as a way to figure out how to translate input in one language into output in another language <TARGET_CITATION/> .,"Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language <TARGET_CITATION/> . They are widely used in MT as a way to figure out how to translate input in one language into output in another language <CITATION/>. Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,a4b828609b60b06e61bea7a4029cc9e1cad5df87,Statistical Phrase-Based Translation,2003,Philipp Koehn; F. Och; Daniel Marcu
311,J04-3001,External_336,[0],,Further details about the properties of entropy can be found in textbooks on information theory <TARGET_CITATION/> .,"That is,where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function. Further details about the properties of entropy can be found in textbooks on information theory <TARGET_CITATION/> . Further details about the properties of entropy can be found in textbooks on information theory <CITATION/>. where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function. That is,",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,7dbdb4209626fd92d2436a058663206216036e68,Elements of Information Theory,2005,T. Cover; Joy A. Thomas
312,J06-2002,External_40449,[0],experiments,"<TARGET_CITATION/> , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .","Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication. We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. <TARGET_CITATION/> , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) . <CITATION/>, for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner). We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented. Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,a32c486987fb5df4d8dc9133180d51cee899478a,Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions,1995,R. Dale; Ehud Reiter
314,D12-1037,P03-1021,[1],introduction,"Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one .","where f and e (e') are source and target sentences, respectively. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one . Some methods are based on likelihood <CITATION/>, error rate <CITATION/>, margin <CITATION/> and ranking <CITATION/>, and among which minimum error rate training (MERT) <CITATION/> is the most popular one. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. where f and e (e') are source and target sentences, respectively.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,1f12451245667a85d0ee225a80880fc93c71cc8b,Minimum Error Rate Training in Statistical Machine Translation,2003,F. Och
315,W06-3813,External_15776,[1],,"This idea was inspired by <TARGET_CITATION/> , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .","The type of knowledge that it accumulates consists of previously analysed pairs, together with the semantic relation assigned, and a syntacticsemantic graph centered on each word in a sentence which appears as the main element in a processed pair. To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P. This idea was inspired by <TARGET_CITATION/> , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) . This idea was inspired by <CITATION/>, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles (case relations). To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P. The type of knowledge that it accumulates consists of previously analysed pairs, together with the semantic relation assigned, and a syntacticsemantic graph centered on each word in a sentence which appears as the main element in a processed pair.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,c3f5b4d2c81d48e7d7298b2d8f636d385c14a580,Pattern matching for case analysis: a computational definition of closeness,1993,S. Delisle; T. Copeck; Stan Szpakowicz; Ken Barker
317,P07-1068,N04-1037,[0],introduction,"While these approaches have been reasonably successful ( see <CITATION/> ) , <TARGET_CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","In the past decade, knowledgelean approaches have significantly influenced research in noun phrase (NP) coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document. In knowledgelean approaches, coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process (e.g., <CITATION/>). While these approaches have been reasonably successful ( see <CITATION/> ) , <TARGET_CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In knowledgelean approaches, coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process (e.g., <CITATION/>). In the past decade, knowledgelean approaches have significantly influenced research in noun phrase (NP) coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,4e12c32770feaadb167a2b57d0388a75339544cd,The (Non)Utility of Predicate-Argument Frequencies for Pronoun Interpretation,2004,A. Kehler; D. Appelt; Lara Taylor; A. Simma
318,W01-1510,External_9270,[4],introduction,A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speedup against an existing LTAG parser <TARGET_CITATION/> .,"In this paper, we show that the strongly equivalent grammars enable the sharing of parsing techniques'', which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research <CITATION/>), which is a largescale FBLTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speedup against an existing LTAG parser <TARGET_CITATION/> . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speedup against an existing LTAG parser <CITATION/>. We apply our system to the latest version of the XTAG English grammar (The XTAG Research <CITATION/>), which is a largescale FBLTAG grammar. In this paper, we show that the strongly equivalent grammars enable the sharing of parsing techniques'', which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,efficient ltag parsing using hpsg parsers,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Jun’ichi Tsujii
320,P13-3018,External_63932,[0],introduction,Such questions are typically answered by designing appropriate priming experiments <TARGET_CITATION/> or other lexical decision tasks .,"That is, whether lexical representations in the mental lexicon are word based or are they organized along morphological lines. For example, whether a word such as unimaginable'' is stored in the mental lexicon as a whole word or do we break it up un'' , imagine'' and able'', understand the meaning of each of these constituent and then recombine the units to comprehend the whole word. Such questions are typically answered by designing appropriate priming experiments <TARGET_CITATION/> or other lexical decision tasks . Such questions are typically answered by designing appropriate priming experiments <CITATION/> or other lexical decision tasks. For example, whether a word such as unimaginable'' is stored in the mental lexicon as a whole word or do we break it up un'' , imagine'' and able'', understand the meaning of each of these constituent and then recombine the units to comprehend the whole word. That is, whether lexical representations in the mental lexicon are word based or are they organized along morphological lines.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,df408a7acf643fead01848986fec63f26da09bbb,Morphology and meaning in the English mental lexicon.,1994,W. Marslen-Wilson; L. Tyler; Rachelle Waksler; Lianne Older
321,N04-2004,External_3810,[4],,The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program <TARGET_CITATION/> .,My theory of verbal argument structure can be implemented in a unified morphosyntactic parsing model that interleaves syntactic and semantic parsing. The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program <TARGET_CITATION/> . The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky's Minimalist Program <CITATION/>.My theory of verbal argument structure can be implemented in a unified morphosyntactic parsing model that interleaves syntactic and semantic parsing.,1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,629bff0a02d0bf030c73c690b022a66a95e72cca,Derivational Minimalism,1996,E. Stabler
322,J87-3002,External_11433,[0],,<TARGET_CITATION/> et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .,"The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, IS, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to IS if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. <TARGET_CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . <CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, IS, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to IS if the verb is felt to be semantically two place rather than one place, such as know versus appear.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,exploiting a large dictionary data base,1982,Archibal Michiels
323,J87-3002,External_43915,[0],introduction,"The research described below is taking place in the context of three collaborative projects <TARGET_CITATION/> to develop a generalpurpose , wide coverage morphological and syntactic analyser for English .","These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Realtime parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects <TARGET_CITATION/> to develop a generalpurpose , wide coverage morphological and syntactic analyser for English . The research described below is taking place in the context of three collaborative projects <CITATION/> to develop a generalpurpose, wide coverage morphological and syntactic analyser for English. Realtime parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,a parser for generalised phrase structure grammars,1986,John Phillips; Henry Thompson
325,J06-2002,External_522,[4],introduction,"Viewed in this way , gradable adjectives are an extreme example of the  efficiency of language '' <TARGET_CITATION/> : Far from meaning something concrete like  larger than 8 cm ''  a concept that would have very limited applicability  or even something more general like  larger than the average N , '' a word like large is applicable across a wide range of different situations .","2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article).the adjective to be semantically equivalent to the superlative form (and, analogously, the comparative): The n large mice = The largest n mice The large mice = The largest mice The large mouse = The largest mouse. Viewed in this way , gradable adjectives are an extreme example of the  efficiency of language '' <TARGET_CITATION/> : Far from meaning something concrete like  larger than 8 cm ''  a concept that would have very limited applicability  or even something more general like  larger than the average N , '' a word like large is applicable across a wide range of different situations . Viewed in this way, gradable adjectives are an extreme example of the efficiency of language'' <CITATION/>: Far from meaning something concrete like larger than 8 cm''a concept that would have very limited applicabilityor even something more general like larger than the average N,'' a word like large is applicable across a wide range of different situations.the adjective to be semantically equivalent to the superlative form (and, analogously, the comparative): The n large mice = The largest n mice The large mice = The largest mice The large mouse = The largest mouse. 2 The reader is asked to focus on any reasonable size measurement, for example, the maximal horizontal or vertical distance, or some combination of dimensions (Kamp 1975; also Section 8.1 of the present article).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,319baebb5894d843d63a913647ec9a52801ba1b5,Situations and Attitudes.,1986,N. Cocchiarella; J. Barwise; J. Perry
327,W06-1104,External_31966,[0],introduction,"It is defined on different kinds of textual units , e.g. documents , parts of a document ( e.g. words and their surrounding context ) , words or concepts <TARGET_CITATION/> .2 Linguistic distance between words is inverse to their semantic similarity or relatedness .","Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units , e.g. documents , parts of a document ( e.g. words and their surrounding context ) , words or concepts <TARGET_CITATION/> .2 Linguistic distance between words is inverse to their semantic similarity or relatedness . It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts <CITATION/>.2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3411ee1422bca9cb481ff8c6f0821724539cd500,Computing similarity distances between rankings,2013,Farzad Farnoud; O. Milenkovic; Gregory J. Puleo; Lili Su
328,W03-0806,External_44164,[0],introduction,"However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data <TARGET_CITATION/> ) .","For example, 10 million words of the American National Corpus <CITATION/> will have manually corrected POS tags, a tenfold increase over the Penn Treebank <CITATION/>, currently used for training POS taggers. This will require more efficient learning algorithms and implementations. However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data <TARGET_CITATION/> ) . However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data <CITATION/>). This will require more efficient learning algorithms and implementations. For example, 10 million words of the American National Corpus <CITATION/> will have manually corrected POS tags, a tenfold increase over the Penn Treebank <CITATION/>, currently used for training POS taggers.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,english gigaword corpus catalogue number ldc2003t05,2003,Linguistic Data Consortium
329,W02-1601,C00-2131,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,42cbcb49516ad980694d3ae49624e27c0087fbce,Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-based Translation,2000,Hideo Watanabe; S. Kurohashi; E. Aramaki
330,D12-1037,P02-1038,[0],introduction,"<TARGET_CITATION/> introduced the loglinear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :"," <TARGET_CITATION/> introduced the loglinear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem : <CITATION/> introduced the loglinear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem:",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,37fadfb6d60e83e24c72d8a90da5644b39d6e8f0,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,2002,F. Och; H. Ney
331,J92-1004,External_84962,[0],introduction,"Representative systems are described in <CITATION/> , De <TARGET_CITATION/> .","In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in <CITATION/> , De <TARGET_CITATION/> . Representative systems are described in <CITATION/>, De <CITATION/>.Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,the interaction of word recognition and linguistic processing in speech understandingquot,1990,H Niemann
334,P10-4003,External_20473,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,15cbdb2ed9723c1b9ad6292cb4cc7bb25f93ecb3,Developing pedagogically effective tutorial dialogue tactics: experiments and a testbed,2007,K. VanLehn; Pamela W. Jordan; D. Litman
336,W06-1639,External_4710,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,maxmargin markov networks,2003,B Taskar; C Guestrin; D Koller
337,W04-1610,External_91194,[1],,"It compares favorably to other stemming or root extraction algorithms ( <TARGET_CITATION/> ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El <CITATION/>). It compares favorably to other stemming or root extraction algorithms <TARGET_CITATION/> , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . It compares favorably to other stemming or root extraction algorithms (<CITATION/>), with a performance of over 97% for extracting the correct root in web documents, and it addresses the challenge of the Arabic broken plural and hollow verbs. In this work, we use the Arabic root extraction technique in (El <CITATION/>). This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,49af3e80343eb80c61e727ae0c27541628c7c5e2,Introduction to Modern Information Retrieval,1983,Gerard Salton; Michael McGill
338,A00-2036,External_5861,[5],conclusion,"We perceive that these results can be extended to other language models that properly embed bilexical contextfree grammars , as for instance the more general historybased models used in <TARGET_CITATION/> .","In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical contextfree grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical contextfree grammars , as for instance the more general historybased models used in <TARGET_CITATION/> . We perceive that these results can be extended to other language models that properly embed bilexical contextfree grammars, as for instance the more general historybased models used in <CITATION/>. Our results hold for bilexical contextfree grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). In this paper we have provided an original mathematical argument in favour of this thesis.",01c9ea0ebd7b342fa28de34293687d3b038ebd4a,Left-To-Right Parsing and Bilexical Context-Free Grammars,2000,M. Nederhof; G. Satta,54c846ee00c6132d70429cc279e8577f63ed05e4,A Linear Observed Time Statistical Parser Based on Maximum Entropy Models,1997,A. Ratnaparkhi
339,J06-2002,External_3886,[0],introduction,"Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , <TARGET_CITATION/> ) .","To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of(3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm) Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , <TARGET_CITATION/> ) . Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cfXXX, Kennedy 1999). (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm)To see this, consider a domain of three mice, sized 5, 8, and 10 cm.2 Here one can speak of",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,793a2711ebd6db67dc6926be7192dbb6dd0b1294,"Review of C. Kennedy, Projecting the adjective. The syntax and semantics of gradability and comparison",1999,P. Hendriks
340,J90-3003,External_7124,[1],experiments,"Many investigators <TARGET_CITATION/> have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a texttospeech synthesizer. Existing texttospeech systems perform well on word pronunciation and short sentences,12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators <TARGET_CITATION/> have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . Many investigators <CITATION/> have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech. Existing texttospeech systems perform well on word pronunciation and short sentences,12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a texttospeech synthesizer.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,8114586ed6d50eb2c74c11d52621c7cee780ee2d,Synthesis of speech from unrestricted text,1976,J. Allen
343,D08-1007,External_4260,[2],experiments,"We gather similar words using <TARGET_CITATION/> ) , mining similar verbs from a comparablesized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use <CITATION/> 's approach to obtaining webcounts .","We convert <CITATION/>'s similaritysmoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1). We also test an MI model inspired by <CITATION/>: We gather similar words using <TARGET_CITATION/> ) , mining similar verbs from a comparablesized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use <CITATION/> 's approach to obtaining webcounts . We gather similar words using <CITATION/>, mining similar verbs from a comparablesized parsed corpus, and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use <CITATION/>'s approach to obtaining webcounts. We also test an MI model inspired by <CITATION/>:We convert <CITATION/>'s similaritysmoothed probability to MI by replacing the empirical Pr(nlv) in Equation (2) with the smoothed PrSIM from Equation (1).",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
344,J86-1002,External_2536,[0],experiments,"Thus , for example , it can acquire a  script '' such as the one for going to a restaurant as defined in <TARGET_CITATION/> .","Though the implemented system is limited to matrixoriented problems, the theoretical system is capable of learning a wide range of problem types. The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples. Thus , for example , it can acquire a  script '' such as the one for going to a restaurant as defined in <TARGET_CITATION/> . Thus, for example, it can acquire a script'' such as the one for going to a restaurant as defined in <CITATION/>. The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples. Though the implemented system is limited to matrixoriented problems, the theoretical system is capable of learning a wide range of problem types.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,scripts plans goals and understanding lawrence erlbaum associates,1977,R Schank; R Abelson
345,P11-1134,External_3409,[0],introduction,"ones , DIRT <TARGET_CITATION/> , VerbOcean <CITATION/> , FrameNet <CITATION/> , and Wikipedia <CITATION/> .","Besides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>. These include, just to mention the most popular ones , DIRT <TARGET_CITATION/> , VerbOcean <CITATION/> , FrameNet <CITATION/> , and Wikipedia <CITATION/> . ones, DIRT <CITATION/>, VerbOcean <CITATION/>, FrameNet <CITATION/>, and Wikipedia <CITATION/>. These include, just to mention the most popularBesides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,c384d9ee4fd8657b26a8165244eb4ad73df4f492,DIRT – Discovery of Inference Rules from Text,2001,Dekang Lin; Patrick Pantel
346,J86-1002,External_21716,[4],,There is some literature on procedure acquisition such as the LISP synthesis work described in <CITATION/> and the PROLOG synthesis method of <TARGET_CITATION/> .,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in <CITATION/> and the PROLOG synthesis method of <TARGET_CITATION/> . There is some literature on procedure acquisition such as the LISP synthesis work described in <CITATION/> and the PROLOG synthesis method of <CITATION/>. That is, the current system learns procedures rather than data structures. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,3e36f22685b8d3db73532d3104b325cea5288a66,Algorithmic Program Debugging,1983,E. Shapiro
347,P10-4003,External_22830,[2],experiments,"The system uses a domainspecific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE <TARGET_CITATION/> generation system to produce the appropriate text .","The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domainspecific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE <TARGET_CITATION/> generation system to produce the appropriate text . The system uses a domainspecific content planner to produce input to the surface realizer based on the strategy decision, and a FUF/SURGE <CITATION/> generation system to produce the appropriate text. The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,controlling content realization with functional unification grammars in,1992,Michael Elhadad; Jacques Robin
348,J87-3002,P84-1095,[0],,"However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see <TARGET_CITATION/> for further discussion ) .","The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields  head word, pronunciation, grammar codes, definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see <TARGET_CITATION/> for further discussion ) . However, each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require (see <CITATION/> for further discussion). The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields  head word, pronunciation, grammar codes, definitions, examples, and so forth.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,cada5a781ec0ed58f10e1ad30c0044e7421c44ab,"Machine-Readable Dictionaries, Lexical Data Bases and the Lexical System",1984,N. Calzolari
349,D10-1101,P07-1056,[4],experiments,"Our results also confirm the insights gained by <TARGET_CITATION/> , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .","In isolation, training only on the cars data yields the second highest results on the movies and webservices datasets and the highest results regarding FMeasure on the cameras data. However, the results of the cars + cameras training data combination indicate that the cameras data does not contribute any additional information during the learning, since the results on both the movies and the webservices datasets are lower than when training only on the cameras data. Our results also confirm the insights gained by <TARGET_CITATION/> , who observed that in crossdomain polarity analysis adding more training data is not always beneficial . Our results also confirm the insights gained by <CITATION/>, who observed that in crossdomain polarity analysis adding more training data is not always beneficial. However, the results of the cars + cameras training data combination indicate that the cameras data does not contribute any additional information during the learning, since the results on both the movies and the webservices datasets are lower than when training only on the cameras data. In isolation, training only on the cars data yields the second highest results on the movies and webservices datasets and the highest results regarding FMeasure on the cameras data.",4d135641931a6efce82bd9c1d69d86e08d3cd28d,Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields,2010,Niklas Jakob; Iryna Gurevych,d895647b4a80861703851ef55930a2627fe19492,"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",2007,John Blitzer; Mark Dredze; Fernando C Pereira
350,W06-2807,External_91607,[2],,We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve <TARGET_CITATION/> .,We believe that ownership has an important role and we do not want to force our users to take anonattributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve <TARGET_CITATION/> . We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve <CITATION/>. nonattributive copyright licence to their work. We believe that ownership has an important role and we do not want to force our users to take a,48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,cc944b0a929d4ca6ce8a47fc84cadae38cfe8c27,Free Culture: How Big Media Uses Technology and the Law to Lock Down Culture and Control Creativity,2004,Lawrence Lessig
351,W04-1610,External_94611,[1],,"It compares favorably to other stemming or root extraction algorithms ( <TARGET_CITATION/> ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El <CITATION/>). It compares favorably to other stemming or root extraction algorithms <TARGET_CITATION/> , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . It compares favorably to other stemming or root extraction algorithms (<CITATION/>), with a performance of over 97% for extracting the correct root in web documents, and it addresses the challenge of the Arabic broken plural and hollow verbs. In this work, we use the Arabic root extraction technique in (El <CITATION/>). This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,towards an arabic information retrieval system,1999,Y Houmame
352,D13-1115,External_8378,[0],related work,"In a similar vein , <TARGET_CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task .","<CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. <CITATION/> furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , <TARGET_CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task . In a similar vein, <CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task. <CITATION/> furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,42f3a746319ca6e6a4448b84ad31459eeaa5bce1,Combining Feature Norms and Text Data with Topic Models,None,M. Steyvers
353,P00-1007,W99-0707,[5],conclusion,"As <TARGET_CITATION/> show , lexical information improves on NP and VP chunking as well .","The presented method concerns primarily with phrases, which can be represented by a tree structure. It is not aimed at handling dependencies, which require heavy use of lexical information (<CITATION/>, for PP attachment). As <TARGET_CITATION/> show , lexical information improves on NP and VP chunking as well . As <CITATION/> show, lexical information improves on NP and VP chunking as well. It is not aimed at handling dependencies, which require heavy use of lexical information (<CITATION/>, for PP attachment). The presented method concerns primarily with phrases, which can be represented by a tree structure.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,ba93f0b457de350e5bb8fcc9e647c85eaa046991,Memory-Based Shallow Parsing,1999,Walter Daelemans; Sabine Buchholz; Jorn Veenstra
354,E03-1005,External_6640,[0],introduction,"But while Bod 's estimator obtains stateoftheart results on the WSJ , comparable to <CITATION/> , Bonnema et al. 's estimator performs worse and is comparable to <TARGET_CITATION/> .","This paper presents the first published results with Goodman's PCFGreductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFGreductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod 's estimator obtains stateoftheart results on the WSJ , comparable to <CITATION/> , Bonnema et al. 's estimator performs worse and is comparable to <TARGET_CITATION/> . But while Bod's estimator obtains stateoftheart results on the WSJ, comparable to <CITATION/>, Bonnema et al.'s estimator performs worse and is comparable to <CITATION/>. We show that these PCFGreductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). This paper presents the first published results with Goodman's PCFGreductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,3764baa7465201f054083d02b58fa75f883c4461,A New Statistical Parser Based on Bigram Lexical Dependencies,1996,M. Collins
355,D10-1038,P08-1095,[0],conclusion,In multiparty discussion people usually mention each other 's name for the purpose of disentanglement <TARGET_CITATION/> .,The best baseline Speaker' in Table 4 also favours this claim. Another possibly critical feature is the mention of names'. In multiparty discussion people usually mention each other 's name for the purpose of disentanglement <TARGET_CITATION/> . In multiparty discussion people usually mention each other's name for the purpose of disentanglement <CITATION/>. Another possibly critical feature is the mention of names'. The best baseline Speaker' in Table 4 also favours this claim.,1b1b027c1ac1aa720d763ec516b65c1e208c0011,Exploiting Conversation Structure in Unsupervised Topic Segmentation for Emails,2010,Shafiq R. Joty; G. Carenini; Gabriel Murray; R. Ng,c70c697425c3f23ca42d97a58380ff0cf3a1644b,You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement,2008,Micha Elsner; Eugene Charniak
356,P97-1063,External_19461,[0],method,"Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based <TARGET_CITATION/> .","The arrow connecting vk and uki in Figure 1 represents an indirect association, since the association between vk and uki arises only by virtue of the association between each of them and uk . Models of translational equivalence that are ignorant of indirect associations have a tendency ... to be confused by collocates'' <CITATION/>. Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based <TARGET_CITATION/> . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based <CITATION/>. Models of translational equivalence that are ignorant of indirect associations have a tendency ... to be confused by collocates'' <CITATION/>. The arrow connecting vk and uki in Figure 1 represents an indirect association, since the association between vk and uki arises only by virtue of the association between each of them and uk .",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,a geometric approach to mapping bitext correspondencequot,1996,I D Melamed
357,J04-3001,N01-1023,[0],related work,"The work of <TARGET_CITATION/> , Osborne , et al. ( 2003 ) suggests that cotraining can be helpful for statistical parsing .","This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining <CITATION/>, in which two sufficiently different learners help each other learn by labeling training data for one another. The work of <TARGET_CITATION/> , Osborne , et al. ( 2003 ) suggests that cotraining can be helpful for statistical parsing . The work of <CITATION/> and Steedman, Osborne, et al. (2003) suggests that cotraining can be helpful for statistical parsing. Another technique for making better use of unlabeled data is cotraining <CITATION/>, in which two sufficiently different learners help each other learn by labeling training data for one another. This approach assumes that there are enough existing labeled data to train the individual parsers.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,f0c90a5bc53027d76b24854209a4cdb1bd75dd2f,Applying Co-Training Methods to Statistical Parsing,2001,Anoop Sarkar
358,W01-1510,External_9268,[4],experiments,"TNT refers to the HPSG parser <TARGET_CITATION/> , C++ implementation of the twophase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .","Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2, lem refers to the LTAG parser <CITATION/>, ANSI C implementation of the twophase parsing algorithm that performs the head corner parsing (van <CITATION/>) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser <TARGET_CITATION/> , C++ implementation of the twophase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . TNT refers to the HPSG parser <CITATION/>, C++ implementation of the twophase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). In Table 2, lem refers to the LTAG parser <CITATION/>, ANSI C implementation of the twophase parsing algorithm that performs the head corner parsing (van <CITATION/>) without features (phase 1), and then executes feature unification (phase 2). Table 2 shows the average parsing time with the LTAG and HPSG parsers.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,1518f674f1a27f011d61f18521004d6a86573838,An HPSG parser with CFG filtering,2000,Kentaro Torisawa; K. Nishida; Yusuke Miyao; Junichi Tsujii
359,J00-2001,P89-1025,[0],,"1 ° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that  a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( <TARGET_CITATION/> , 203 ) .","The planner first checks all of its toplevel plans to see which have effects that match the goal. Each matching plan's preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan's body. 1 ° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that  a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( <TARGET_CITATION/> , 203 ) . 1° The body of a plan can be an action or sequence of actions, a goal or sequence 9 Moore and Paris also note that a generation system must maintain the kinds of information outlined by Grosz and Sidner'' (Moore and Paris 1989, 203). Each matching plan's preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan's body. The planner first checks all of its toplevel plans to see which have effects that match the goal.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,4fb573e6ead5b1986b6afddd5148b16f36f7735a,Planning Text for Advisory Dialogues,1989,Johanna D. Moore; Cécile Paris
360,J00-3002,E99-1009,[0],,"Surveys and articles on the topic include <CITATION/> , de <TARGET_CITATION/> .","Here we are concerned with categorial proof nets <CITATION/> as the fundamental structures of proof in categorial logic, in the same sense that linear proof nets were originally introduced by <CITATION/> as the fundamental structures of proof in linear logic. (Cutfree) proof nets exhibit no spurious ambiguity and play the role in categorial grammar that parse trees play in phrase structure grammar. Surveys and articles on the topic include <CITATION/> , de <TARGET_CITATION/> . Surveys and articles on the topic include <CITATION/>, de <CITATION/>. (Cutfree) proof nets exhibit no spurious ambiguity and play the role in categorial grammar that parse trees play in phrase structure grammar. Here we are concerned with categorial proof nets <CITATION/> as the fundamental structures of proof in categorial logic, in the same sense that linear proof nets were originally introduced by <CITATION/> as the fundamental structures of proof in linear logic.",1cf5e98f01f6aa2ccd2a68682c49cb93d152f152,Incremental processing and acceptability,2000,G. Morrill,abe8bfd3b44e9c87b16b2593ecec75083b1ca7c1,Geometry of Lexico-Syntactic Interaction,1999,G. Morrill
361,W14-2106,P04-1085,[4],related work,Another line of research that is correlated with ours is recognition of agreement/disagreement <TARGET_CITATION/> and classification of stances <CITATION/> in online forums .,Our work contributes a new principled method for building annotated corpora for online interactions. The corpus and guidelines will also be shared with the research community. Another line of research that is correlated with ours is recognition of agreement/disagreement <TARGET_CITATION/> and classification of stances <CITATION/> in online forums . Another line of research that is correlated with ours is recognition of agreement/disagreement <CITATION/> and classification of stances <CITATION/> in online forums. The corpus and guidelines will also be shared with the research community. Our work contributes a new principled method for building annotated corpora for online interactions.,2b8b59a74d815a70bbb31892ce484510480be6fe,Analyzing Argumentative Discourse Units in Online Interactions,2014,Debanjan Ghosh; S. Muresan; Nina Wacholder; Mark Aakhus; M. Mitsui,18d079a6d72e3f0b0c9214f597b6b178265b05ee,Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies,2004,Michel Galley; K. McKeown; Julia Hirschberg; Elizabeth Shriberg
362,E03-1002,A00-2018,[4],,The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history <TARGET_CITATION/> .,"The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history <TARGET_CITATION/> . The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history <CITATION/>. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
363,D08-1034,External_63171,[0],introduction,"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering <CITATION/> , Information Extraction <TARGET_CITATION/> , and Machine Translation <CITATION/> .","The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering <CITATION/> , Information Extraction <TARGET_CITATION/> , and Machine Translation <CITATION/> . Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering <CITATION/>, Information Extraction <CITATION/>, and Machine Translation<CITATION/>. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,,using predicateargument structures for information extraction,2003,Mihai Surdeanu; Sanda Harabagiu; John Williams; Paul Aarseth
364,W06-3813,External_5559,[0],related work,"Most approaches rely on VerbNet <TARGET_CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/> .","The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. Most approaches rely on VerbNet <TARGET_CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/> . Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/>. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,,classbased construction of a verb lexicon in,2000,Karin Kipper; Hoa Trang Dang; Martha Palmer
365,J90-3003,External_14830,[0],introduction,"In previous work <CITATION/> , we described an experimental texttospeech system that determined prosodic phrasing for the Olive  Liberman synthesizer <TARGET_CITATION/> ."," In previous work <CITATION/> , we described an experimental texttospeech system that determined prosodic phrasing for the Olive  Liberman synthesizer <TARGET_CITATION/> . In previous work <CITATION/>, we described an experimental texttospeech system that determined prosodic phrasing for the OliveLiberman synthesizer <CITATION/>.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,,texttospeechan overview,1985,S P Olive; M Y Liberman
366,J03-3004,External_42692,[0],introduction,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; <TARGET_CITATION/> ) .7 As an example , consider the translation into French of the house collapsed .","When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; <TARGET_CITATION/> ) .7 As an example , consider the translation into French of the house collapsed . Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cfXXX Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,inducing translation templates for examplebased machine translation,1999,Michael Carl
367,J97-4003,External_65596,[4],related work,"This approach is taken , for example , in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( <TARGET_CITATION/> , 31 ) .","Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( <TARGET_CITATION/> , 31 ) . This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). Another common approach to lexical rules is to encode them as unary phrase structure rules.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,the compleat lkb,1993,Ann Copestake
368,J97-4003,W96-0303,[0],introduction,"27 <TARGET_CITATION/> argue that semiproductivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .","Interaction predicates encoding lexical rule interaction for the natural classes of lexical entries in the lexicon.The way these predicates interconnect is represented in Figure 19. 27 <TARGET_CITATION/> argue that semiproductivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry . 27 <CITATION/> argue that semiproductivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry. The way these predicates interconnect is represented in Figure 19.Interaction predicates encoding lexical rule interaction for the natural classes of lexical entries in the lexicon.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,53a39651778980d82cd992efa741ccbc076e1fcf,Controlling the Application of Lexical Rules,1996,Ted Briscoe; Ann A. Copestake
369,P10-4003,External_34147,[5],conclusion,Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain <TARGET_CITATION/> .,"In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed <CITATION/>. Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain <TARGET_CITATION/> . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain <CITATION/>. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed <CITATION/>. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,44ef7251f9709f74f4d3ae8891e5d31f2343a685,Cohesion and Learning in a Tutorial Spoken Dialog System,2006,Arthur Ward; D. Litman
370,E03-1004,External_19017,[2],experiments,These automatic transformations are based on linguistic rules <TARGET_CITATION/> .,"During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one. These automatic transformations are based on linguistic rules <TARGET_CITATION/> . These automatic transformations are based on linguistic rules <CITATION/>. During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.",55559a2ee9693969d30237534ac290f4b0077a3a,Czech-English Dependency Tree-based Machine Translation,2003,Martin Cmejrek; J. Curín; Jirí Havelka,bc4926f2299eeb33d0e7d49cfbdcf052e501b652,Automatic Procedures in Tectogrammatical Tagging,2000,Alena Böhmová
371,J09-4010,External_12994,[4],method," Only an automatic evaluation was performed , which relied on having model responses <TARGET_CITATION/> .","These systems addressed the evaluation issue as follows. Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. Only an automatic evaluation was performed , which relied on having model responses <TARGET_CITATION/> .  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>.  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. These systems addressed the evaluation issue as follows.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,bridging the lexical chasm statistical approaches to answerfinding,2000,A Berger; R Caruana; D Cohn; D Freitag; V Mittal
373,J86-1002,External_33230,[5],conclusion," use of low level knowledge from the speech recognition phase ,  use of high level knowledge about the domain in particular and the dialogue task in general ,  a  continue '' facility and an  autoloop '' facility as described by <CITATION/> ,  a  conditioning '' facility as described by <TARGET_CITATION/> ,  implementation of new types of paraphrasing ,  checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and  examining interspeaker dialogue patterns .","However, there are many possible extensions that could be examined in the future and added to the implementation if the investigation indicates that it would create a yet more usable system. These include the following: use of low level knowledge from the speech recognition phase ,  use of high level knowledge about the domain in particular and the dialogue task in general ,  a  continue '' facility and an  autoloop '' facility as described by <CITATION/> ,  a  conditioning '' facility as described by <TARGET_CITATION/> ,  implementation of new types of paraphrasing ,  checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and  examining interspeaker dialogue patterns .  use of low level knowledge from the speech recognition phase,  use of high level knowledge about the domain in particular and the dialogue task in general,  a continue'' facility and an autoloop'' facility as described by <CITATION/>,  a conditioning'' facility as described by <CITATION/>,  implementation of new types of paraphrasing,  checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and  examining interspeaker dialogue patterns.These include the following:However, there are many possible extensions that could be examined in the future and added to the implementation if the investigation indicates that it would create a yet more usable system.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,c253d9e71dbd92bea0c45401d50e5f4a8f7096a9,Computer control via limited natural language,1985,P. Fink; A. Sigmon; A. Biermann
374,K15-1002,W09-1119,[0],related work,"<TARGET_CITATION/> present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of nonlocal features , and integration of external knowledge .","<CITATION/> propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. <TARGET_CITATION/> present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of nonlocal features , and integration of external knowledge . <CITATION/> present detailed studies on the task of named entity recognition, which discusses and compares different methods on multiple aspects including chunk representation, inference method, utility of nonlocal features, and integration of external knowledge. However, the phrases detected are not necessarily mentions that we need to discover. <CITATION/> propose to select certain rules based on a given corpus, to identify base noun phrases.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,aa9efc8b2737eac0675ba5abb5feab8305482c12,Design Challenges and Misconceptions in Named Entity Recognition,2009,Lev-Arie Ratinov; D. Roth
375,W02-0309,External_3337,[0],introduction,mers <TARGET_CITATION/> demonstrably improve retrieval performance .,"The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator. mers <TARGET_CITATION/> demonstrably improve retrieval performance . mers <CITATION/> demonstrably improve retrieval performance. For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator. The efforts required for performing morphological analysis vary from language to language.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,6b3853f08c482fe1bfbe39d656d50a8c73976f3c,Development of a stemming algorithm,1968,J. B. Lovins
376,J06-2002,External_40451,[0],experiments,"Cases like this would be covered if the decisiontheoretic property of Pareto optimality <TARGET_CITATION/> was used as the sole criterion : Formally , an object r E C has a Paretooptimal combination of Values V iff there is no other x E C such that","Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decisiontheoretic property of Pareto optimality <TARGET_CITATION/> was used as the sole criterion : Formally , an object r E C has a Paretooptimal combination of Values V iff there is no other x E C such that Cases like this would be covered if the decisiontheoretic property of Pareto optimality <CITATION/> was used as the sole criterion: Formally, an object r E C has a Paretooptimal combination of Values V iff there is no other x E C such thatThe NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension:Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,d705919b21c9e21ee6ee042d0eeb4c91265e7ee2,Welfare economics and social choice theory,1980,A. Feldman
377,J92-1004,External_19518,[0],,A formula for the test set perplexity <TARGET_CITATION/> is :13,"We built a subset grammar from the 791 parsed training sentences, and then used this grammar to test coverage and perplexity on the unseen test sentences. The grammar could parse 100% of the training sentences and 84% of the test sentences. A formula for the test set perplexity <TARGET_CITATION/> is :13 A formula for the test set perplexity <CITATION/> is:13The grammar could parse 100% of the training sentences and 84% of the test sentences. We built a subset grammar from the 791 parsed training sentences, and then used this grammar to test coverage and perplexity on the unseen test sentences.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,4edeaf8c87ba2d17a6fbc95683af9a7d12262c0e,Automatic Speech Recognition: The Development of the SPHINX System,2013,Kai-Fu Lee; R. Reddy
378,J09-4010,External_1877,[2],method,"This method follows a traditional Information Retrieval paradigm <TARGET_CITATION/> , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .","As stated herein, we studied two documentbased methods: Document Retrieval and Document Prediction. 3.1.1 Document Retrieval (DocRet). This method follows a traditional Information Retrieval paradigm <TARGET_CITATION/> , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query . This method follows a traditional Information Retrieval paradigm <CITATION/>, where a query is represented by the content terms it contains, and the system retrieves from the corpus a set of documents that best match this query. 3.1.1 Document Retrieval (DocRet). As stated herein, we studied two documentbased methods: Document Retrieval and Document Prediction.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,0b94a8bfd4f4ef52d81a3cd7bda2a20bf1e412e3,"Book Review of: Introduction to modern information retrieval by C.G. Chowddhury, 3rd ed.",2010,S. Fitz-Gerald; B. Wiggins
380,A00-1031,External_13541,[4],introduction,The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in <TARGET_CITATION/> .,"The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus <CITATION/> and the Penn Treebank <CITATION/>. The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in <TARGET_CITATION/> . The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in <CITATION/>. Additionally, we present results of the tagger on the NEGRA corpus <CITATION/> and the Penn Treebank <CITATION/>. The aim of this paper is to give a detailed account of the techniques used in TnT.",9bd13ac82c011a91542df1a705cf46bb787764cb,TnT – A Statistical Part-of-Speech Tagger,2000,T. Brants,,a maximum entropy model for partofspeech tagging,1996,Adwait Ratnaparkhi
381,J05-3003,A97-1052,[0],experiments,"<TARGET_CITATION/> , by comparison , employ 163 distinct predefined frames .","5 To recap, if two verbs have the same subcategorization requirements (e.g., give([subj, obj, obj2]), send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once.from PennII and 221 from PennIII. <TARGET_CITATION/> , by comparison , employ 163 distinct predefined frames . <CITATION/>, by comparison, employ 163 distinct predefined frames.from PennII and 221 from PennIII. 5 To recap, if two verbs have the same subcategorization requirements (e.g., give([subj, obj, obj2]), send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,9b77ca011896f79d9014704aaa63ecf4ffb3485c,Automatic Extraction of Subcategorization from Corpora,1997,Ted Briscoe; John A. Carroll
382,D09-1067,D08-1007,[5],conclusion,"<CITATION/> have showed that WordNetbased approaches do not always outperform simple frequencybased models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach <TARGET_CITATION/> .","In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to WordNet classes. <CITATION/> have showed that WordNetbased approaches do not always outperform simple frequencybased models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach <TARGET_CITATION/> . <CITATION/> have showed that WordNetbased approaches do not always outperform simple frequencybased models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach <CITATION/>. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to WordNet classes. In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.",1cdfaff988038c2f9cd16b39922a2a3a0b4f320b,Improving Verb Clustering with Automatically Acquired Selectional Preferences,2009,Lin Sun; A. Korhonen,94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel
383,P02-1001,External_4179,[0],,"f on demand <TARGET_CITATION/> can pay off here , since only part of f may be needed subsequently . )","In the extreme, if each input string is fully observed (not the case if the input is bound by composition to the output of a onetomany FST), one can succeed by restricting g to each input string in turn; this amounts to manually dividing f(x, y) by g(x).10Traditionally log(strength) values are called weights, but this paper uses weight'' to mean something else. f on demand <TARGET_CITATION/> can pay off here , since only part of f may be needed subsequently . ) f on demand <CITATION/> can pay off here, since only part of f may be needed subsequently.) 10Traditionally log(strength) values are called weights, but this paper uses weight'' to mean something else.In the extreme, if each input string is fully observed (not the case if the input is bound by composition to the output of a onetomany FST), one can succeed by restricting g to each input string in turn; this amounts to manually dividing f(x, y) by g(x).",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,a rational design for a weighted finitestate transducer library,1998,Mehryar Mohri; Fernando C N Pereira; Michael Riley
384,J00-3002,External_76559,[0],,"An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in <TARGET_CITATION/> but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .","By a result of <CITATION/>, the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus. Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design. An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in <TARGET_CITATION/> but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction . An approach (also based on regulation of the succession of rule application) to the associated problem of spurious ambiguity is given in <CITATION/> but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction.Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design. By a result of <CITATION/>, the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus.",1cf5e98f01f6aa2ccd2a68682c49cb93d152f152,Incremental processing and acceptability,2000,G. Morrill,ef9085becc58af6338607eb5c7710e0cd67c7431,Parsing and Derivational Equivalence,1989,Mark Hepple; G. Morrill
385,J00-4002,J90-1001,[4],,This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in <TARGET_CITATION/> .,"Before going on to more complex cases, we will also show how to deal with intrasentential anaphora, including reflexives, and binding of a pronoun by a quantifier. The relevant equivalence is: This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in <TARGET_CITATION/> . This equivalence is doing essentially the same job as Pereira's pronoun abstraction schema in <CITATION/>. The relevant equivalence is:Before going on to more complex cases, we will also show how to deal with intrasentential anaphora, including reflexives, and binding of a pronoun by a quantifier.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,8a1053957e26b4fccd9105ab94835e6d06cd659d,Categorial Semantics and Scoping,1990,Fernando C Pereira
386,D12-1027,D09-1141,[2],method,"Finally , we experiment with a method for combining phrase tables proposed in <TARGET_CITATION/> .","In order to counterbalance this, we repeat the smaller IndonesianEnglish bitext enough times so that we can make the number of sentences it contains roughly the same as for the Indonesian''English bitext; then we concatenate the two bitexts and we train an SMT system on the resulting bitext. Sophisticated phrase table combination. Finally , we experiment with a method for combining phrase tables proposed in <TARGET_CITATION/> . Finally, we experiment with a method for combining phrase tables proposed in <CITATION/>. Sophisticated phrase table combination. In order to counterbalance this, we repeat the smaller IndonesianEnglish bitext enough times so that we can make the number of sentences it contains roughly the same as for the Indonesian''English bitext; then we concatenate the two bitexts and we train an SMT system on the resulting bitext.",2f8fe4694ff20ebc64cd50a9492799023ae52955,Source Language Adaptation for Resource-Poor Machine Translation,2012,Pidong Wang; Preslav Nakov; H. Ng,d8c09faf290f902367391c1c592117f703d71778,Improved Statistical Machine Translation for Resource-Poor Languages Using Related Resource-Rich Languages,2009,Preslav Nakov; H. Ng
387,W14-1815,D13-1011,[4],related work,"Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <TARGET_CITATION/> ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .","The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as <CITATION/> which produces weather reports from structured data or <CITATION/> which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <TARGET_CITATION/> ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <CITATION/> (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as <CITATION/> which produces weather reports from structured data or <CITATION/> which generates descriptions of objects from images.",dcd0e19d450a0d43b0d8b32415bb731f5838e593,Natural Language Generation with Vocabulary Constraints,2014,Benjamin Swanson; Elif Yamangil; Eugene Charniak,532f79e083ba308c3cb41c73622facd3684f529f,Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation,2013,Dekai Wu; Karteek Addanki; Markus Saers; Meriem Beloucif
388,J09-4010,External_1877,[2],method,"For the cases where retrieval took place , we used Fscore ( van Rijsbergen 1979 ; <TARGET_CITATION/> ) to determine the similarity between the response from the topranked document and the real response ( the formulas for Fscore and its contributing factors , recall and precision , appear in Section 4.2 ) .","The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on requestresponse pairs yields even more retrieved documents. For the cases where retrieval took place , we used Fscore ( van Rijsbergen 1979 ; <TARGET_CITATION/> ) to determine the similarity between the response from the topranked document and the real response ( the formulas for Fscore and its contributing factors , recall and precision , appear in Section 4.2 ) . For the cases where retrieval took place, we used Fscore (van Rijsbergen 1979; Salton and McGill 1983) to determine the similarity between the response from the topranked document and the real response (the formulas for Fscore and its contributing factors, recall and precision, appear in Section 4.2). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on requestresponse pairs yields even more retrieved documents. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,0b94a8bfd4f4ef52d81a3cd7bda2a20bf1e412e3,"Book Review of: Introduction to modern information retrieval by C.G. Chowddhury, 3rd ed.",2010,S. Fitz-Gerald; B. Wiggins
389,J03-3004,External_18134,[4],conclusion,"These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an ngram target language model ) , in effect forms a multiengine MT system as described by <TARGET_CITATION/> .","We have presented an EBMT system based on the marker hypothesis that uses post hoc validation and correction via the Web.11 Over 218,000 NPs and VPs were extracted automatically from the PennII Treebank using just 59 of its 29,000 rule types. These phrases were then translated automatically by three online MT systems. These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an ngram target language model ) , in effect forms a multiengine MT system as described by <TARGET_CITATION/> . These translations gave rise to a number of automatically constructed linguistic resources: (1) the original (source,target) phrasal translation pairs, (2) the marker lexicon, (3) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system, seeded with input from multiple translation systems, with a postvalidation process via the Web (amounting to an ngram target language model), in effect forms a multiengine MT system as described by <CITATION/>.These phrases were then translated automatically by three online MT systems. We have presented an EBMT system based on the marker hypothesis that uses post hoc validation and correction via the Web.11 Over 218,000 NPs and VPs were extracted automatically from the PennII Treebank using just 59 of its 29,000 rule types.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,8148705411af1d333b79f270b62d8b75f9af1451,Three Heads are Better than One,1994,R. Frederking; S. Nirenburg
390,D11-1138,External_8741,[0],introduction,"This includes work on generalized expectation <CITATION/> , posterior regularization <CITATION/> and constraint driven learning <TARGET_CITATION/> .","We call our algorithm augmentedloss training as it optimizes multiple losses to augment the traditional supervised parser loss. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation <CITATION/> , posterior regularization <CITATION/> and constraint driven learning <TARGET_CITATION/> . This includes work on generalized expectation <CITATION/>, posterior regularization <CITATION/> and constraint driven learning <CITATION/>. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. We call our algorithm augmentedloss training as it optimizes multiple losses to augment the traditional supervised parser loss.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,175c5e5828eb7be4f5a76f351886a388d1d290b4,Structured Output Learning with Indirect Supervision,2010,Ming-Wei Chang; Vivek Srikumar; Dan Goldwasser; D. Roth
391,W06-3813,External_27696,[1],introduction,"The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent <TARGET_CITATION/> .","If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph. We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent <TARGET_CITATION/> . The list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent <CITATION/>. We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,fee1b8b15216f52a641fd9b72eba311dff57da40,Systematic construction of a versatile case system,1997,Ken Barker; T. Copeck; Stan Szpakowicz; S. Delisle
392,J05-3003,External_13650,[4],related work,"Unlike our approach , those of <TARGET_CITATION/> include a substantial initial correction and cleanup of the PennII trees .","Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles like'' coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of <TARGET_CITATION/> include a substantial initial correction and cleanup of the PennII trees . Unlike our approach, those of <CITATION/> include a substantial initial correction and cleanup of the PennII trees. The algorithm handles like'' coordination and exploits the traces used in the treebank in order to interpret LDDs. Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,d4943720cc393626ed1ff87e6bad9622e69cb284,Extracting Tree Adjoining Grammars from Bracketed Corpora,2009,Fei Xia; Chung-hye Han; A. Joshi; Martha Palmer; C. Prolo; Anoop Sarkar
393,N01-1006,P00-1036,[2],method,"2The algorithm was implemented by the the authors , following the description in <TARGET_CITATION/> ."," B (r) = fs 2 Sjp,(s) = true and C[s] =6t, and C[s] = T [s]g the samples on which the rule applies and changes the classification from correct to incorrect; similarly, bad(r) = jB(r)j. Given a newly learned rule b that is to be applied to S, the goal is to identify the rules r for which at least one of the sets G (r) , B (r) is modified by the application of rule b. Obviously, if both sets are not modified when applying rule b, then the value of the objective function for rule r remains unchanged. 2The algorithm was implemented by the the authors , following the description in <TARGET_CITATION/> . 2The algorithm was implemented by the the authors, following the description in <CITATION/>. t, and C[s] = T [s]g the samples on which the rule applies and changes the classification from correct to incorrect; similarly, bad(r) = jB(r)j. Given a newly learned rule b that is to be applied to S, the goal is to identify the rules r for which at least one of the sets G (r) , B (r) is modified by the application of rule b. Obviously, if both sets are not modified when applying rule b, then the value of the objective function for rule r remains unchanged.  B (r) = fs 2 Sjp,(s) = true and C[s] =6",c52f80f056a2de8f503bf912e8025413ec2111ec,Transformation Based Learning in the Fast Lane,2001,G. Ngai; Radu Florian,4a27822c8718bcfdd01fd5cc9e75dd1df9f9add5,Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers,2000,Mark Hepple
394,W14-1815,W12-2016,[1],related work,"Our motivation for generation of material for language education exists in work such as <TARGET_CITATION/> , which deal with automatic generation of classic fill in the blank questions .","Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output. Examples such as <CITATION/> use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Our motivation for generation of material for language education exists in work such as <TARGET_CITATION/> , which deal with automatic generation of classic fill in the blank questions . Our motivation for generation of material for language education exists in work such as <CITATION/>, which deal with automatic generation of classic fill in the blank questions. Examples such as <CITATION/> use template filling techniques guided by quantified notions of humor or how catchy a phrase is. Computational creativity is another subfield of NLG that often does not fix an a priori meaning in its output.",dcd0e19d450a0d43b0d8b32415bb731f5838e593,Natural Language Generation with Vocabulary Constraints,2014,Benjamin Swanson; Elif Yamangil; Eugene Charniak,5140a32042673d1efabb70b463abcd4e49577e7a,Generating Diagnostic Multiple Choice Comprehension Cloze Questions,2012,Jack Mostow; Hyeju Jang
395,D09-1056,External_20235,[0],related work,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> .","The most used feature for the Web People Search task, however, are NEs. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> . In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents see for instance <CITATION/>. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. The most used feature for the Web People Search task, however, are NEs.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,aa7c5eee3942fb420f6e45c95c919b77401dd35a,"Automatic Entity Disambiguation : Benefits to NER , Relation Extraction , Link Analysis , and Inference",None,Matthias Blume
397,D13-1115,External_14422,[0],related work,"Some works abstract perception via the usage of symbolic logic representations <TARGET_CITATION/> , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>. Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using highlevel representations instead. Some works abstract perception via the usage of symbolic logic representations <TARGET_CITATION/> , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Some works abstract perception via the usage of symbolic logic representations <CITATION/>, while others choose to employ concepts elicited from psycholinguistic and cognition studies. Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using highlevel representations instead. The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,55e18e0dde592258882134d2dceeb86122b366ab,Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language,2014,David L. Chen; Joohyun Kim; R. Mooney
398,P97-1063,External_6470,[0],introduction,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <TARGET_CITATION/> ."," Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <TARGET_CITATION/> . Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,a statistical approach to machine translationquot,1990,P F Brown; J Cocke; S Della Pietra; V Della Pietra; F Jelinek; R Mercer; P Roossin
399,J06-2002,External_69922,[0],,One area of current interest concerns the lefttoright arrangement of premodifying adjectives within an NP <TARGET_CITATION/> .,"An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the lefttoright arrangement of premodifying adjectives within an NP <TARGET_CITATION/> . One area of current interest concerns the lefttoright arrangement of premodifying adjectives within an NP <CITATION/>. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things.An inference phase, during which the list L is transformed; 3.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,4eec0a1ce645151c4acc89b151814b3deb53af43,Ordering Among Premodifiers,1999,James Shaw; V. Hatzivassiloglou
400,P97-1063,External_7175,[0],method,Models of translational equivalence that are ignorant of indirect associations have  a tendency ... to be confused by collocates '' <TARGET_CITATION/> .,"Then vk and uk+i will also cooccur more often than expected by chance. The arrow connecting vk and uki in Figure 1 represents an indirect association, since the association between vk and uki arises only by virtue of the association between each of them and uk . Models of translational equivalence that are ignorant of indirect associations have  a tendency ... to be confused by collocates '' <TARGET_CITATION/> . Models of translational equivalence that are ignorant of indirect associations have a tendency ... to be confused by collocates'' <CITATION/>. The arrow connecting vk and uki in Figure 1 represents an indirect association, since the association between vk and uki arises only by virtue of the association between each of them and uk . Then vk and uk+i will also cooccur more often than expected by chance.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,robust word alignment for machine aided translationquot,1993,I Dagan; K Church; SZ W Gale
401,N12-1010,W11-2036,[3],,"Note that although our feature set was drawn primarily from our prior uncertainty detection experiments <TARGET_CITATION/> , we have also experimented with other features , including stateoftheart acousticprosodic features used in the last Interspeech Challenges <CITATION/> and made freely available in the openSMILE Toolkit <CITATION/> .","deviation running totals and averages for all features Lexical and Dialogue Features dialogue name and turn number question name and question depth ITSPOKErecognized lexical items in turn ITSPOKElabeled turn (in)correctness incorrect runs  User Identifier Features: gender and pretest score Note that although our feature set was drawn primarily from our prior uncertainty detection experiments <TARGET_CITATION/> , we have also experimented with other features , including stateoftheart acousticprosodic features used in the last Interspeech Challenges <CITATION/> and made freely available in the openSMILE Toolkit <CITATION/> . Note that although our feature set was drawn primarily from our prior uncertainty detection experiments <CITATION/>, we have also experimented with other features, including stateoftheart acousticprosodic features used in the last Interspeech Challenges <CITATION/> and made freely available in the openSMILE Toolkit <CITATION/>.  Lexical and Dialogue Features dialogue name and turn number question name and question depth ITSPOKErecognized lexical items in turn ITSPOKElabeled turn (in)correctness incorrect runs  User Identifier Features: gender and pretest scoredeviation running totals and averages for all features",0712bab671864259bd180bf58ec679f38f28d203,Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System,2012,Katherine Forbes-Riley; D. Litman; Heather Friedberg; J. Drummond,d7d9b5f8e0f73d95fe9bc78e526091407c0b14c4,Examining the Impacts of Dialogue Content and System Automation on Affect Models in a Spoken Tutorial Dialogue System,2011,J. Drummond; D. Litman
402,K15-1002,D08-1031,[2],,"For this mentionpair coreference model  ( u , v ) , we use the same set of features used in <TARGET_CITATION/> .","Here, yu,v = 1 iff mentions u,v are directly linked. Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are coreferred. For this mentionpair coreference model  ( u , v ) , we use the same set of features used in <TARGET_CITATION/> . For this mentionpair coreference model (u,v), we use the same set of features used in <CITATION/>. Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are coreferred. Here, yu,v = 1 iff mentions u,v are directly linked.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,454bfea18aebae023e9a716503e3a9956dcea8b4,Understanding the Value of Features for Coreference Resolution,2008,Eric Bengtson; D. Roth
403,E03-1005,External_63,[4],conclusion,"Compared to the reranking technique in <TARGET_CITATION/> , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .","The highest accuracy is obtained by SLDOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over <CITATION/> and Bods PCFGreduction reported in Table 1. Compared to the reranking technique in <TARGET_CITATION/> , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . Compared to the reranking technique in <CITATION/>, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. This is roughly an 11% relative reduction in error rate over <CITATION/> and Bods PCFGreduction reported in Table 1. The highest accuracy is obtained by SLDOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,844db702be4bc149b06b822b47247e15f5894cc3,Discriminative Reranking for Natural Language Parsing,2000,M. Collins; Terry Koo
404,N01-1006,P00-1036,[4],,"The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformationbased learner , and the ICA system <TARGET_CITATION/> .","However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformationbased learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformationbased learner , and the ICA system <TARGET_CITATION/> . The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformationbased learner, and the ICA system <CITATION/>. In this paper, we present a novel and realistic method for speeding up the training time of a transformationbased learner without sacrificing performance. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.",c52f80f056a2de8f503bf912e8025413ec2111ec,Transformation Based Learning in the Fast Lane,2001,G. Ngai; Radu Florian,4a27822c8718bcfdd01fd5cc9e75dd1df9f9add5,Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers,2000,Mark Hepple
405,W06-1639,External_71382,[0],related work,"There has also been work focused upon determining the political leaning ( e.g. ,  liberal '' vs.  conservative '' ) of a document or author , where most previouslyproposed methods make no direct use of relationships between the documents to be classified ( the  unlabeled '' texts ) <TARGET_CITATION/> .","Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>. There has also been work focused upon determining the political leaning ( e.g. ,  liberal '' vs.  conservative '' ) of a document or author , where most previouslyproposed methods make no direct use of relationships between the documents to be classified ( the  unlabeled '' texts ) <TARGET_CITATION/> . There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,4b8c9030fa4bd571c12c2c25d1aa1e044f3f0972,Cultural Orientation: Classifying Subjective Documents by Cociation Analysis,2004,Miles Efron
406,N04-2004,External_29661,[0],,"It projects a functional head , voice <TARGET_CITATION/> , whose specifier is the external argument .","(9) vDO [+dynamic, inchoative] = DO vb [+dynamic, +inchoative] = BECOME vBE [dynamic] = BEThe light verb vDO licenses an atelic noninchoative event, and is compatible with verbal roots expressing activity. It projects a functional head , voice <TARGET_CITATION/> , whose specifier is the external argument . It projects a functional head, voice <CITATION/>, whose specifier is the external argument.The light verb vDO licenses an atelic noninchoative event, and is compatible with verbal roots expressing activity. (9) vDO [+dynamic, inchoative] = DO vb [+dynamic, +inchoative] = BECOME vBE [dynamic] = BE",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,,the event argument and the semantics of voice,1994,Angelika Kratzer
407,P00-1004,P96-1021,[0],introduction,"Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation <CITATION/> , as have been bilingual stochastic grammars <TARGET_CITATION/> .","In the simplest form, translations are stored and reused for the translation of new input. This approach, known as translation memory, examplebased or casebased translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammarbased translation paradigms <CITATION/>. Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation <CITATION/> , as have been bilingual stochastic grammars <TARGET_CITATION/> . Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation <CITATION/>, as have been bilingual stochastic grammars <CITATION/>. This approach, known as translation memory, examplebased or casebased translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammarbased translation paradigms <CITATION/>. In the simplest form, translations are stored and reused for the translation of new input.",9ddf3d2e52789255fc4d9692cffff95af3b10628,Translation with Cascaded Finite State Transducers,2000,S. Vogel; H. Ney,4711ff01d8eff9b9d10deeb3b68f366f7944c208,A Polynomial-Time Algorithm for Statistical Machine Translation,1996,Dekai Wu
408,W06-2933,External_6508,[4],experiments,"This is noticeable for German <CITATION/> and Portuguese <TARGET_CITATION/> , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B  ohmov  a et al. , 2003 ) , Dutch ( van der <CITATION/> ) and Slovene <CITATION/> , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .","Japanese <CITATION/>, despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of nonprojective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German <CITATION/> and Portuguese <TARGET_CITATION/> , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B  ohmov  a et al. , 2003 ) , Dutch ( van der <CITATION/> ) and Slovene <CITATION/> , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . This is noticeable for German <CITATION/> and Portuguese <CITATION/>, which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech <CITATION/>, Dutch (van der <CITATION/>) and Slovene <CITATION/>, where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. A second observation is that a high proportion of nonprojective structures leads to fragmentation in the parser output, reflected in lower precision for roots. Japanese <CITATION/>, despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,,floresta sint´actica” a treebank for portuguese,2002,S Afonso; E Bick; R Haber; D Santos
409,J06-2002,External_3885,[0],introduction,"3 The degree of precision of the measurement ( <TARGET_CITATION/> , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .","As a result, all other properties that turn up in the NP are already in the list L when size is added. Suppose the target is c4: 3 The degree of precision of the measurement ( <TARGET_CITATION/> , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size . 3 The degree of precision of the measurement (James et al. 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. Suppose the target is c4:As a result, all other properties that turn up in the NP are already in the list L when size is added.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,modern engineering mathematics second edition,1996,Glyn James; David Burley; Dick Clements; Phil Dyke; John Searl; Jerry Wright
410,W03-0806,H01-1017,[0],,There have already been several attempts to develop distributed NLP systems for dialogue systems <TARGET_CITATION/> and speech recognition <CITATION/> .,Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems <TARGET_CITATION/> and speech recognition <CITATION/> . There have already been several attempts to develop distributed NLP systems for dialogue systems <CITATION/> and speech recognition <CITATION/>. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.,7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,b7abd7c7632af2de58f09f7f8c7f0fa085e18315,Dialogue Interaction with the DARPA Communicator Infrastructure: The Development of Useful Software,2001,Samuel Bayer; Christine Doran; Bryan George
411,P97-1063,External_19464,[0],introduction,"Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools ( e.g. <TARGET_CITATION/> ) , concordancing for bilingual lexicography <CITATION/> , computerassisted language learning , corpus linguistics ( Melby .","Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools <TARGET_CITATION/> , concordancing for bilingual lexicography <CITATION/> , computerassisted language learning , corpus linguistics ( Melby . Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including crummy'' MT on the World Wide Web <CITATION/>, certain machineassisted translation tools (e.g. <CITATION/>), concordancing for bilingual lexicography <CITATION/>, computerassisted language learning, corpus linguistics (Melby. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,using bitextual alignment for translation validation the transcheck systemquot,1994,E Macklovitch
412,A00-1020,W99-0104,[3],introduction,"SWIZZLE is a multilingual enhancement of COCKTAIL <TARGET_CITATION/> , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information ' .","Our claim is that by adding the wealth of coreferential features provided by multilingual data, new powerful heuristics for coreference resolution can be developed that outperform monolingual coreference resolution systems. For both languages, we resolved coreference by using SWIZZLE, our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL <TARGET_CITATION/> , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information ' . SWIZZLE is a multilingual enhancement of COCKTAIL <CITATION/>, a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information'. For both languages, we resolved coreference by using SWIZZLE, our implementation of a bilingual coreference resolver. Our claim is that by adding the wealth of coreferential features provided by multilingual data, new powerful heuristics for coreference resolution can be developed that outperform monolingual coreference resolution systems.",76894392818a9a360feaf2f1a797bbe1eaac82b0,Multilingual Coreference Resolution,2000,S. Harabagiu; Steven J. Maiorano,df9490b518e726a96879f16c92d46e9d4f883c1b,Knowledge-Lean Coreference Resolution and its Relation to Textual Cohesion and Coherence,1999,S. Harabagiu; Steven J. Maiorano
413,W00-1312,External_7133,[4],related work,"( <TARGET_CITATION/> ; Hull and ( 3refenstette , 1996 ) .","Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived. Another common approach is term translation, e.g., via a bilingual lexicon. ( <TARGET_CITATION/> ; Hull and ( 3refenstette , 1996 ) . (<CITATION/>; Hull and (3refenstette, 1996). Another common approach is term translation, e.g., via a bilingual lexicon. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,phrasal translation and query expansion techniques for crosslanguage information retrievalquot,1997,L Ballesteros; W B Croft
414,J97-4003,External_105,[0],introduction,"de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <TARGET_CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements .","email: LCBdm,minnenRCB@sfs.nphil.unituebingen. de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <TARGET_CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements . de URL: http://www.sfs.nphil.unituebingen.de/sfb /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements. nphil.unituebingen.email: LCBdm,minnenRCB@sfs.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,headdriven phrase structure grammar,1994,Carl Pollard; Ivan Sag
415,J03-3004,External_46911,[0],introduction,"<TARGET_CITATION/> attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .","All EBMT systems, from the initial proposal by <CITATION/> to the recent collection of <CITATION/>, are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 <CITATION/> attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the (source, target) words have a similar distribution. <TARGET_CITATION/> attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . <CITATION/> attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 <CITATION/> attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the (source, target) words have a similar distribution. All EBMT systems, from the initial proposal by <CITATION/> to the recent collection of <CITATION/>, are premised on the availability of subsentential alignments derived from the input bitext.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,finding terminology translations from nonparallel corporaquot,1997,P Fung; K Mckeown
416,N04-2004,External_29662,[4],,The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program <TARGET_CITATION/> .,My theory of verbal argument structure can be implemented in a unified morphosyntactic parsing model that interleaves syntactic and semantic parsing. The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program <TARGET_CITATION/> . The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky's Minimalist Program <CITATION/>.My theory of verbal argument structure can be implemented in a unified morphosyntactic parsing model that interleaves syntactic and semantic parsing.,1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,18d881efd694d0feb01bff2a6269198232cdaa6e,A Recognizer for Minimalist Grammars,2000,H. Harkema
417,A00-1024,W98-1111,[1],experiments,"Each component will return a confidence measure of the reliability of its prediction , c.f. <TARGET_CITATION/> .","unknown word. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.. Each component will return a confidence measure of the reliability of its prediction , c.f. <TARGET_CITATION/> . Each component will return a confidence measure of the reliability of its prediction, c.f. <CITATION/>. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.. unknown word.",caa11f45ef1d8cdd6683a34b22407ec0f2c55d77,Categorizing Unknown Words: Using Decision Trees to Identify Names and Misspellings,2000,J. Toole,90d1fcdc271d222eef2c930964b3aad7bc75fd14,Language Identification With Confidence Limits,1999,D. Elworthy
418,K15-1002,D08-1031,[0],introduction,"In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads <TARGET_CITATION/> .","In the example above, the first they'' refers to Multinational companies investing in China'' and the second They'' refers to Domestic manufacturers, who are also suffering''. In both cases, the mention heads are sufficient to support the decisions: ''they'' refers to ''companies'', and ''They'' refers to ''manufacturers''. In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads <TARGET_CITATION/> . In fact, most of the features3 implemented in existing coreference resolution systems rely solely on mention heads <CITATION/>. In both cases, the mention heads are sufficient to support the decisions: ''they'' refers to ''companies'', and ''They'' refers to ''manufacturers''. In the example above, the first they'' refers to Multinational companies investing in China'' and the second They'' refers to Domestic manufacturers, who are also suffering''.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,454bfea18aebae023e9a716503e3a9956dcea8b4,Understanding the Value of Features for Coreference Resolution,2008,Eric Bengtson; D. Roth
419,J86-1002,External_33229,[4],,The problem of handling illformed input has been studied by <TARGET_CITATION/> .,"While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. The problem of handling illformed input has been studied by <TARGET_CITATION/> . The problem of handling illformed input has been studied by <CITATION/>. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,the nomad system expectationbased detection and correction of errors during understanding of syntactically illformed text,1983,R Granger
420,D09-1056,W07-2024,[0],related work,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> .","The most used feature for the Web People Search task, however, are NEs. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> . In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents see for instance <CITATION/>. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. The most used feature for the Web People Search task, however, are NEs.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,90f8b83210aba36bf43792f370670191a62b020d,CU-COMSEM: Exploring Rich Features for Unsupervised Web Personal Name Disambiguation,2007,Ying Chen; James H. Martin
421,J97-4003,External_1396,[0],introduction,"This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system <TARGET_CITATION/> .","Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.' This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system <TARGET_CITATION/> . This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as, for example, adopted in the LKB system <CITATION/>. Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.' Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,the representation of lexical semantic information cognitive science research paper csrp 280,1992,Ann Copestake
422,J00-2003,J97-4001,[0],introduction,Typical lettertosound rule sets are those described by <TARGET_CITATION/> .,"They also constitute a formal model of universal computation <CITATION/>. Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern. Typical lettertosound rule sets are those described by <TARGET_CITATION/> . Typical lettertosound rule sets are those described by <CITATION/>. Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern. They also constitute a formal model of universal computation <CITATION/>.",18ff4f15416e34d9a56142e6f5d491567934e4fb,A multistrategy approach to improving pronunciation by analogy,2000,Y. Marchand; R. Damper,2a792467c88452a0013c2b61aecfb23cf178e015,Algorithms for Grapheme-Phoneme Translation for English and French: Applications for Database Searches and Speech Synthesis,1997,M. Divay; Anthony J. Vitale
423,W06-3309,External_67158,[2],method,"<TARGET_CITATION/> describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand betweencovariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .","For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the fourdimensional vectors that maximizes the separation of the Gaussians (corresponding to the HMM states). <TARGET_CITATION/> describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand betweencovariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space . <CITATION/> describe an efficient algorithm (of linear complexity in the number of training sentences) for computing the LDA transform matrix, which entails computing the withinand betweencovariance matrices of the classes, and using Singular Value Decomposition (SVD) to compute the eigenvectors of the new space. In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the fourdimensional vectors that maximizes the separation of the Gaussians (corresponding to the HMM states). For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,,modern applied statistics with splus,1994,William N Venables; Brian D Ripley
425,J04-3001,External_5769,[0],related work,"Some examples include text categorization <TARGET_CITATION/> , base noun phrase chunking <CITATION/> , partofspeech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation <CITATION/> , and word sense disambiguation <CITATION/> .","In addition to PPattachment, as discussed in this article, sample selection has been successfully applied to other classificationapplications. Some examples include text categorization <TARGET_CITATION/> , base noun phrase chunking <CITATION/> , partofspeech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation <CITATION/> , and word sense disambiguation <CITATION/> . Some examples include text categorization <CITATION/>, base noun phrase chunking <CITATION/>, partofspeech tagging (Engelson Dagan 1996), spelling confusion set disambiguation <CITATION/>, and word sense disambiguation <CITATION/>. applications. In addition to PPattachment, as discussed in this article, sample selection has been successfully applied to other classification",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,b69e0cce79eb288ffb43ad7ae3b99b8dea9ac5ac,Heterogeneous Uncertainty Sampling for Supervised Learning,1994,D. Lewis; J. Catlett
426,Q13-1020,D09-1037,[1],method,"Differently , <TARGET_CITATION/> designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .","For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently , <TARGET_CITATION/> designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment . <CITATION/> designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,dc605e05765f4948328525d1c60aeca58970afe1,A Bayesian Model of Syntax-Directed Tree to String Grammar Induction,2009,Trevor Cohn; Phil Blunsom
427,D13-1115,External_9732,[0],related work,They use a Bag of Visual Words ( BoVW ) model <TARGET_CITATION/> to create a bimodal vocabulary describing documents .,"As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is <CITATION/>. They use a Bag of Visual Words ( BoVW ) model <TARGET_CITATION/> to create a bimodal vocabulary describing documents . They use a Bag of Visual Words (BoVW) model <CITATION/> to create a bimodal vocabulary describing documents. The first work to do this with topic models is <CITATION/>. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,distinctive image features from scaleinvariant keypoints,2004,David G Lowe
429,P97-1063,External_15612,[0],,"This imbalance foils thresholding strategies , clever as they might be <TARGET_CITATION/> .","Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be <TARGET_CITATION/> . This imbalance foils thresholding strategies, clever as they might be <CITATION/>. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,4fe2a45babab10c1bfae05d2464363f4e52bbaf9,A Program for Aligning Sentences in Bilingual Corpora,1993,W. Gale; Kenneth Ward Church
430,J86-1002,C80-1027,[4],,The problem of handling illformed input has been studied by <TARGET_CITATION/> .,"While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. The problem of handling illformed input has been studied by <TARGET_CITATION/> . The problem of handling illformed input has been studied by <CITATION/>. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,956ceae23bd0f5ae598eeb7d4b58c2cdc99aa8fa,Linguistic Analysis of Natural Language Communication With Computers,1980,B. H. Thompson
431,W06-1705,External_26069,[0],related work,"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp <CITATION/> , KWiCFinder <CITATION/> and the Linguist 's Search Engine <TARGET_CITATION/> .","<CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler. <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp <CITATION/> , KWiCFinder <CITATION/> and the Linguist 's Search Engine <TARGET_CITATION/> . Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp <CITATION/>, KWiCFinder <CITATION/> and the Linguist's Search Engine <CITATION/>. <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. <CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,9e1be80488c07093d5c3b334c713f15e1181a38c,Linggle: a Web-scale Linguistic Search Engine for Words in Context,2013,Joanne Boisson; Ting-hui Kao; Jian-Cheng Wu; Tzu-Hsi Yen; Jason J. S. Chang
432,D09-1056,External_97098,[0],related work,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> .","The most used feature for the Web People Search task, however, are NEs. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> . In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents see for instance <CITATION/>. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. The most used feature for the Web People Search task, however, are NEs.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,,disambiguation algorithm for people search on the web in,2007,Dmitri V Kalashnikov; Stella Chen; Rabia Nuray; Sharad Mehrotra; Naveen Ashish
433,D13-1115,External_2129,[0],related work,"More recently , <CITATION/> show that visual attribute classifiers , which have been immensely successful in object recognition <TARGET_CITATION/> , act as excellent substitutes for feature","<CITATION/> show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. <CITATION/> show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently , <CITATION/> show that visual attribute classifiers , which have been immensely successful in object recognition <TARGET_CITATION/> , act as excellent substitutes for feature More recently, <CITATION/> show that visual attribute classifiers, which have been immensely successful in object recognition <CITATION/>, act as excellent substitutes for feature<CITATION/> show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. <CITATION/> show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,c6a8aef1bf134294482d8088f982d5643347d2ff,Describing objects by their attributes,2009,Ali Farhadi; Ian Endres; Derek Hoiem; D. Forsyth
434,W06-3813,External_48312,[0],related work,"In other methods , lexical resources are specifically tailored to meet the requirements of the domain <TARGET_CITATION/> or the system <CITATION/> .","Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>. In other methods , lexical resources are specifically tailored to meet the requirements of the domain <TARGET_CITATION/> or the system <CITATION/> . In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,,classifying the semantic relations in nouncompounds via a domain specific hierarchy,2001,Barbara Rosario; Marti Hearst
435,W06-3813,External_29717,[0],related work,Such systems extract information from some types of syntactic units ( clauses in <TARGET_CITATION/> ; noun phrases in <CITATION/> ) .,"Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>. In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Such systems extract information from some types of syntactic units ( clauses in <TARGET_CITATION/> ; noun phrases in <CITATION/> ) . Such systems extract information from some types of syntactic units (clauses in <CITATION/>; noun phrases in <CITATION/>). In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,c274b8aac56e49e65a3827c570b2496b14429166,Automatic Labeling of Semantic Roles,2000,D. Gildea; Dan Jurafsky
436,J92-1004,External_91850,[2],,The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database backend <TARGET_CITATION/> .,The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database <CITATION/>. The second version (RM) concerns the Resource Management task <CITATION/> that has been popular within the DARPA community in recent years. The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database backend <TARGET_CITATION/> . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database backend <CITATION/>. The second version (RM) concerns the Resource Management task <CITATION/> that has been popular within the DARPA community in recent years. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database <CITATION/>.,ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,the voyager speech understanding system preliminary development and evaluationquot,1990,V Zue; J Glass; D Goodine; H Leung; M Phillips; J Polifroni; S Seneff
437,D11-1138,P07-1050,[2],experiments,"We use the nonprojective kbest MST algorithm to generate kbest lists <TARGET_CITATION/> , where k = 8 for the experiments in this paper ."," Graphbased: An implementation of graphbased parsing algorithms with an arcfactored parameterization <CITATION/>. We use the nonprojective kbest MST algorithm to generate kbest lists <TARGET_CITATION/> , where k = 8 for the experiments in this paper . We use the nonprojective kbest MST algorithm to generate kbest lists <CITATION/>, where k = 8 for the experiments in this paper. based parsing algorithms with an arcfactored parameterization <CITATION/>.  Graphbased: An implementation of graph",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,486b414f58d588dfdc9fc761f88d8c904a3b036f,K-best Spanning Tree Parsing,2007,Keith B. Hall
440,W06-2933,P05-1013,[2],introduction, Graph transformations for recovering nonprojective structures <TARGET_CITATION/> ., Historybased feature models for predicting the next parser action <CITATION/>.  Support vector machines for mapping histories to parser actions <CITATION/>. Graph transformations for recovering nonprojective structures <TARGET_CITATION/> .  Graph transformations for recovering nonprojective structures <CITATION/>. Support vector machines for mapping histories to parser actions <CITATION/>.  Historybased feature models for predicting the next parser action <CITATION/>.,f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,b92c0e898b1fee243864176e18a2b50105be3e54,Pseudo-Projective Dependency Parsing,2005,Joakim Nivre; Jens Nilsson
441,K15-1003,P02-1017,[0],related work,<TARGET_CITATION/> 's CCM is an unlabeled bracketing model that generates the span of partofspeech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each nonconstituent ) ., <TARGET_CITATION/> 's CCM is an unlabeled bracketing model that generates the span of partofspeech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each nonconstituent ) . <CITATION/>'s CCM is an unlabeled bracketing model that generates the span of partofspeech tags that make up each constituent and the pair of tags surrounding each constituent span (as well as the spans and contexts of each nonconstituent).,39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,77021fb48704b860fa850dd103b79db4dcf920ee,A Generative Constituent-Context Model for Improved Grammar Induction,2002,D. Klein; Christopher D. Manning
442,W14-1609,W09-1119,[1],introduction,"This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications <TARGET_CITATION/> .","Given a onetoone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest postmerge loglikelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications <TARGET_CITATION/> . This process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications <CITATION/>. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest postmerge loglikelihood, doing so until all classes have been merged. Given a onetoone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.",d53d878cf1a3f0bed5d9c68c925994cb72f47304,Lexicon Infused Phrase Embeddings for Named Entity Resolution,2014,Alexandre Passos; Vineet Kumar; A. McCallum,aa9efc8b2737eac0675ba5abb5feab8305482c12,Design Challenges and Misconceptions in Named Entity Recognition,2009,Lev-Arie Ratinov; D. Roth
444,J05-3003,External_317,[0],introduction,"Aside from the extraction of theoryneutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG <TARGET_CITATION/> .","<CITATION/> argues that, aside from missing domainspecific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theoryneutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG <TARGET_CITATION/> . Aside from the extraction of theoryneutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG <CITATION/>. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. <CITATION/> argues that, aside from missing domainspecific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,d48edcd14d6d5163e96d61a2867c0a6d2b64feda,Automated Extraction of TAGs from the Penn Treebank,2000,Johnathan M. Chen; Vijay K. Shanker
445,J00-1003,P98-2147,[4],,1 The representation in <TARGET_CITATION/> is even more compact than ours for grammars that are not selfembedding .,"Replace each transition in Ap of the form (q, A q') by (a copy of) automaton A, in a straightforward way. This means that new etransitions connect q to the start state of A, and the final states of A, to q'. 1 The representation in <TARGET_CITATION/> is even more compact than ours for grammars that are not selfembedding . 1 The representation in <CITATION/> is even more compact than ours for grammars that are not selfembedding. This means that new etransitions connect q to the start state of A, and the final states of A, to q'.Replace each transition in Ap of the form (q, A q') by (a copy of) automaton A, in a straightforward way.",b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof,c5abbc0dab9cb8ee63c49b4c44e00fb34ea2fadc,Dynamic Compilation of Weighted Context-Free Grammars,1998,Mehryar Mohri; Fernando Pereira
446,J97-4003,External_24532,[4],introduction,29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body <TARGET_CITATION/> .,"Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compiletime. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body <TARGET_CITATION/> . 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body <CITATION/>.As a result, the literal can be removed from the body ofIntuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compiletime.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,2b3952bf23e83025c519c1f0b173f02bdb34e9ed,Transformation of Logic Programs: Foundations and Techniques,1994,A. Pettorossi; M. Proietti
447,D10-1052,P07-1090,[3],,The reordering models we describe follow our previous work using function word models for translation <TARGET_CITATION/> ., The reordering models we describe follow our previous work using function word models for translation <TARGET_CITATION/> . The reordering models we describe follow our previous work using function word models for translation <CITATION/>.,c97c609c34db8787505d83dfa03077d4813c7f19,Discriminative Word Alignment with a Function Word Reordering Model,2010,Hendra Setiawan; Christopher Dyer; P. Resnik,4db12fdbc657443e50c969e73e9a616d1eaa9fb6,Ordering Phrases with Function Words,2007,Hendra Setiawan; Min-Yen Kan; Haizhou Li
449,W02-1601,External_69139,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,37de4fc0762eddd02274a65b1ca2103a8c0fa118,Example-based machine translation based on the synchronous SSTC annotation schema,1999,M. H. Al-Adhaileh; E. Tang
450,W06-1705,J03-3001,[0],introduction,"In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( <CITATION/> : 56 ) unless the web is used as a corpus <TARGET_CITATION/> .","The motivation for increasingly large data sets remains the same. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( <CITATION/> : 56 ) unless the web is used as a corpus <TARGET_CITATION/> . In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (<CITATION/>: 56) unless the web is used as a corpus <CITATION/>. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. The motivation for increasingly large data sets remains the same.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,e965483087d63b9ca957fa04c6e639d945b3a49b,Introduction to the Special Issue on the Web as Corpus,2003,A. Kilgarriff; G. Grefenstette
452,N01-1012,W98-0708,[0],,See <TARGET_CITATION/> for a discussion .,"Subpredicates inherit all the thematic roles not listed in their definitions from their parent predicates. The number and nature of the thematic roles depend on the generic predicates and subpredicates, and not on some general criteria regardless of each predicate <CITATION/>. See <TARGET_CITATION/> for a discussion . See <CITATION/> for a discussion.The number and nature of the thematic roles depend on the generic predicates and subpredicates, and not on some general criteria regardless of each predicate <CITATION/>. Subpredicates inherit all the thematic roles not listed in their definitions from their parent predicates.",8c4b26ee4b838e204c71fe36f467fbbcbdd72652,An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet,2001,F. Gomez,39b402c924dda1be8ab8611ccd5a3e5ed0f1ee8b,Linking WordNet Verb Classes to Semantic Interpretation,1998,F. Gomez
453,A00-1014,W00-0310,[0],,Figure 2 ( a ) shows the framebased semantic representation for the utterance  What time is Analyze This playing 2 See <TARGET_CITATION/> for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concepttospeech generation .,"MIMIC utilizes a nonrecursive framebased semantic representation commonly used in spoken dialogue systems (e.g. <CITATION/>), which represents an utterance as a set of attributevalue pairs. Figure 2 ( a ) shows the framebased semantic representation for the utterance  What time is Analyze This playing 2 See <TARGET_CITATION/> for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concepttospeech generation . Figure 2(a) shows the framebased semantic representation for the utterance What time is Analyze This playing 2 See <CITATION/> for how MIMIC's dialoguelevel knowledge is used to override default prosodic assignments for concepttospeech generation.MIMIC utilizes a nonrecursive framebased semantic representation commonly used in spoken dialogue systems (e.g. <CITATION/>), which represents an utterance as a set of attributevalue pairs.",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,a8d242c50b07bec0473b3a53bf625efe8ae6f56b,Using Dialogue Representations for Concept-to-Speech Generation,2000,C. Nakatani; Jennifer Chu-Carroll
454,K15-1002,External_16514,[4],experiments,"For Berkeley system , we use the reported results from <TARGET_CITATION/> .","12We do not provide results from Berkeley and HOTCoref on ACE2004 dataset as they do not directly support ACE input. Results for HOTCoref are slightly different from the results reported in <CITATION/>. For Berkeley system , we use the reported results from <TARGET_CITATION/> . For Berkeley system, we use the reported results from <CITATION/>.Results for HOTCoref are slightly different from the results reported in <CITATION/>. 12We do not provide results from Berkeley and HOTCoref on ACE2004 dataset as they do not directly support ACE input.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,28eb033eee5f51c5e5389cbb6b777779203a6778,"A Joint Model for Entity Analysis: Coreference, Typing, and Linking",2014,Greg Durrett; D. Klein
456,W06-3309,External_20682,[5],conclusion,Such a component would serve as the first stage of a clinical question answering system <TARGET_CITATION/> or summarization system <CITATION/> .,"The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsicallyvalid measure of performance. Such a component would serve as the first stage of a clinical question answering system <TARGET_CITATION/> or summarization system <CITATION/> . Such a component would serve as the first stage of a clinical question answering system <CITATION/> or summarization system <CITATION/>. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsicallyvalid measure of performance. The true utility of content models is to structure abstracts that have no structure to begin with.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,58e3df962aeea5ece0857ab6e37870857a54ee07,Knowledge Extraction for Clinical Question Answering: Preliminary Results,2005,Dina Demner-Fushman; Jimmy J. Lin
457,W06-1705,External_25126,[0],related work,"In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for dataintensive NLP and textmining for eScience <TARGET_CITATION/> .","These core foci of our work represent crucial innovations lacking in prior research. In particular, representativeness and replicability are key research concerns to enhance the reliability of web data for corpora. In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for dataintensive NLP and textmining for eScience <TARGET_CITATION/> . In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for dataintensive NLP and textmining for eScience <CITATION/>. In particular, representativeness and replicability are key research concerns to enhance the reliability of web data for corpora. These core foci of our work represent crucial innovations lacking in prior research.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,supporting text mining for escience the challenges for gridenabled natural language processing,2005,J Carroll; R Evans; E Klein
458,J87-3002,External_40337,[4],,<TARGET_CITATION/> comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .,"On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. <CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. <TARGET_CITATION/> comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated . <CITATION/> comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated. <CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,7f5ca029a899488cf1d80c6da4dbc57abd0f1106,Sentence types and complement types in English,1971,R. Wilkinson
459,D08-1034,External_24680,[4],experiments,"To prove that our method is effective , we also make a comparison between the performances of our system and <TARGET_CITATION/> .","This made the interdependence of core arguments can be directly explored from the extraction of semantic context features. So the ARGX sub task is improved. To prove that our method is effective , we also make a comparison between the performances of our system and <TARGET_CITATION/> . To prove that our method is effective, we also make a comparison between the performances of our system and <CITATION/>. So the ARGX sub task is improved. This made the interdependence of core arguments can be directly explored from the extraction of semantic context features.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,ee4c4fe7fd24125531a0e9eafb6d110cf3c27398,Automatic Semantic Role Labeling for Chinese Verbs,2005,Nianwen Xue; Martha Palmer
460,Q13-1020,External_63993,[4],related work,The obtained SCFG is further used in a phrasebased and hierarchical phrasebased system <TARGET_CITATION/> .,"This work differs from the above work in that we design a novel Bayesian model to induce unsupervised Utrees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrasebased and hierarchical phrasebased system <TARGET_CITATION/> . The obtained SCFG is further used in a phrasebased and hierarchical phrasebased system <CITATION/>. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised Utrees, and prior knowledge can be encoded into the model more freely and effectively.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,,hierarchical phrasebased translation,2007,David Chiang
461,J97-4003,C96-1076,[0],introduction,32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints <TARGET_CITATION/> can be used to circumvent constraint propagation .,Applying constraint propagation to the extended lexical entry of Figure 17 yields the result shown in Figure 23. The information common to all solutions to the interaction call is lifted up into the lexical entry and becomes available upon lexical lookup. 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints <TARGET_CITATION/> can be used to circumvent constraint propagation . 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints <CITATION/> can be used to circumvent constraint propagation. The information common to all solutions to the interaction call is lifted up into the lexical entry and becomes available upon lexical lookup.Applying constraint propagation to the extended lexical entry of Figure 17 yields the result shown in Figure 23.,d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,0090349749a32f5d419d111770bbaed54aeef0e7,Modularizing Contexted Constraints,1996,John Griffith
462,Q13-1020,External_13771,[4],related work,"<TARGET_CITATION/> , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .","Our previous work <CITATION/> designed an EMbased method to construct unsupervised trees for treebased translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised Utrees, and prior knowledge can be encoded into the model more freely and effectively. <TARGET_CITATION/> ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised Utrees, and prior knowledge can be encoded into the model more freely and effectively. Our previous work <CITATION/> designed an EMbased method to construct unsupervised trees for treebased translation models.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,5c087c11ff147dc2304719554a479ff30cd4a914,Bayesian Synchronous Grammar Induction,2008,Phil Blunsom; Trevor Cohn; M. Osborne
463,D08-1034,C04-1100,[0],introduction,"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering <TARGET_CITATION/> , Information Extraction <CITATION/> , and Machine Translation <CITATION/> .","The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering <TARGET_CITATION/> , Information Extraction <CITATION/> , and Machine Translation <CITATION/> . Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering <CITATION/>, Information Extraction <CITATION/>, and Machine Translation<CITATION/>. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,b103a58878730b9e3bcdf311ca20428981152835,Question Answering Based on Semantic Structures,2004,S. Narayanan; S. Harabagiu
465,W06-1639,External_2304,[0],related work,Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> .,Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitinterdocument references in the form of hyperlinks <CITATION/>. Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> . Notable early papers on graphbased semisupervised learning include <CITATION/>. interdocument references in the form of hyperlinks <CITATION/>. Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicit,dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,49b8dff62cccc26023c876460234bf29084a382f,Transductive Learning via Spectral Graph Partitioning,2003,T. Joachims
466,E09-1100,External_1012,[4],method,"Due to using a global model like CRFs , our previous work in <TARGET_CITATION/> reported the best results over the evaluated corpora of Bakeoff2 until now7 .","It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs , our previous work in <TARGET_CITATION/> reported the best results over the evaluated corpora of Bakeoff2 until now7 . Due to using a global model like CRFs, our previous work in <CITATION/> reported the best results over the evaluated corpora of Bakeoff2 until now7. As full features are used, the former and the latter provide the similar performance. It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence.",b575da5d912b55d1661db71affd3d116425037ce,Character-Level Dependencies in Chinese: Usefulness and Learning,2009,Zhao Hai,966622e873057115d47cdeb4dde90b63386afe0f,Exploiting Unlabeled Text with Different Unsupervised Segmentation Criteria for Chinese Word Segmentation,2008,Zhao Hai; Chunyu Kit
467,D09-1144,J03-2004,[0],conclusion,"As already mentioned in the literature , see for example <TARGET_CITATION/> , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation ."," As already mentioned in the literature , see for example <TARGET_CITATION/> , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation . As already mentioned in the literature, see for example <CITATION/>, knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation, information extraction, question answering or machine translation.",cb40ceb4e33a05d7469e332936a32b009cfbf191,Automatic Acquisition of the Argument-Predicate Relations from a Frame-Annotated Corpus,2009,Ekaterina Ovchinnikova; T. Alexandrov; Tonio Wandmacher,f932caac89709a716a7d3e6632caf9f34d709518,A Probabilistic Account of Logical Metonymy,2003,Mirella Lapata; A. Lascarides
470,J04-3001,External_218,[0],related work,"Another technique for making better use of unlabeled data is cotraining <TARGET_CITATION/> , in which two sufficiently different learners help each other learn by labeling training data for one another .","They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining <TARGET_CITATION/> , in which two sufficiently different learners help each other learn by labeling training data for one another . Another technique for making better use of unlabeled data is cotraining <CITATION/>, in which two sufficiently different learners help each other learn by labeling training data for one another. This approach assumes that there are enough existing labeled data to train the individual parsers. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,combining labeled and unlabeled data with cotraining,1998,Avrim Blum; Tom Mitchell
472,P97-1063,External_15612,[0],introduction,"Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools ( e.g. <CITATION/> ) , concordancing for bilingual lexicography <TARGET_CITATION/> , computerassisted language learning , corpus linguistics ( Melby .","Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools ( e.g. <CITATION/> ) , concordancing for bilingual lexicography <TARGET_CITATION/> , computerassisted language learning , corpus linguistics ( Melby . Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including crummy'' MT on the World Wide Web <CITATION/>, certain machineassisted translation tools (e.g. <CITATION/>), concordancing for bilingual lexicography <CITATION/>, computerassisted language learning, corpus linguistics (Melby. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,4fe2a45babab10c1bfae05d2464363f4e52bbaf9,A Program for Aligning Sentences in Bilingual Corpora,1993,W. Gale; Kenneth Ward Church
473,J09-1003,External_8084,[3],,"Following our previous work <TARGET_CITATION/> , the input to information ordering is an unordered set of informationbearing items represented as CF lists ."," Following our previous work <TARGET_CITATION/> , the input to information ordering is an unordered set of informationbearing items represented as CF lists . Following our previous work <CITATION/>, the input to information ordering is an unordered set of informationbearing items represented as CF lists.",41c12a46d6275d7919ff82622fa43f5b735696eb,Evaluating Centering for Information Ordering Using Corpora,2009,Nikiforos Karamanis; C. Mellish; Massimo Poesio; J. Oberlander,81124d7b0b0d69f6bd08e159e7fd75ce55dd103c,Stochastic Text Structuring Using the Principle of Continuity,2002,Nikiforos Karamanis; H. Manurung
474,W02-0309,External_44558,[0],conclusion,"There has been some controversy , at least for simple stemmers <CITATION/> , about the effectiveness of morphological analysis for document retrieval <TARGET_CITATION/> ."," There has been some controversy , at least for simple stemmers <CITATION/> , about the effectiveness of morphological analysis for document retrieval <TARGET_CITATION/> . There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,6f7237801b84a674c5530f279787d2cf9d922f3e,Stemming Algorithms: A Case Study for Detailed Evaluation,1996,David A. Hull
475,J09-4010,External_1877,[0],,"This situation suggests a responseautomation approach that follows the document retrieval paradigm <TARGET_CITATION/> , where a new request is matched with existing response documents ( emails ) .","In our work, we focus on the first two of these situations, where either complete existing responses or parts of responses are reused to address a new request. The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response. This situation suggests a responseautomation approach that follows the document retrieval paradigm <TARGET_CITATION/> , where a new request is matched with existing response documents ( emails ) . This situation suggests a responseautomation approach that follows the document retrieval paradigm <CITATION/>, where a new request is matched with existing response documents (emails). The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response. In our work, we focus on the first two of these situations, where either complete existing responses or parts of responses are reused to address a new request.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,0b94a8bfd4f4ef52d81a3cd7bda2a20bf1e412e3,"Book Review of: Introduction to modern information retrieval by C.G. Chowddhury, 3rd ed.",2010,S. Fitz-Gerald; B. Wiggins
476,P11-1134,H05-1047,[0],introduction,"All current approaches to monolingual TE , either syntactically oriented <CITATION/> , or applying logical inference ( Tatu and <TARGET_CITATION/> ) , or adopting transformationbased techniques <CITATION/> , incorporate different types of lexical knowledge to support textual inference .","Section 6 concludes the paper, and outlines the directions of our future research.2 Lexical resources for TE and CLTE All current approaches to monolingual TE , either syntactically oriented <CITATION/> , or applying logical inference ( Tatu and <TARGET_CITATION/> ) , or adopting transformationbased techniques <CITATION/> , incorporate different types of lexical knowledge to support textual inference . All current approaches to monolingual TE, either syntactically oriented <CITATION/>, or applying logical inference <CITATION/>, or adopting transformationbased techniques <CITATION/>, incorporate different types of lexical knowledge to support textual inference. 2 Lexical resources for TE and CLTESection 6 concludes the paper, and outlines the directions of our future research.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,7e0d29ab6f59d35547bdb48c0139958dc75023a2,A Semantic Approach to Recognizing Textual Entailment,2005,M. Tatu; D. Moldovan
479,J87-3002,External_43911,[0],,"<TARGET_CITATION/>:472 ) , but these are the only ones which are explicit in the LDOCE coding system .","(5) *John believed Mary that the Earth is round.Clearly, there are other syntactic and semantic tests for this distinction, (see eg. <TARGET_CITATION/>:472 ) , but these are the only ones which are explicit in the LDOCE coding system . <CITATION/>:472), but these are the only ones which are explicit in the LDOCE coding system. Clearly, there are other syntactic and semantic tests for this distinction, (see eg. (5) *John believed Mary that the Earth is round.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,1b4c0c5b3eae6aa57cdebeb6ee2d411687b6c255,Syntactic Argumentation and the Structure of English,1979,David M. Perlmutter; S. Soames
480,W01-1510,External_105,[0],introduction,This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar ( FBLTAG1 ) <CITATION/> and HeadDriven Phrase Structure Grammar ( HPSG ) <TARGET_CITATION/> by a method of grammar conversion ., This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar ( FBLTAG1 ) <CITATION/> and HeadDriven Phrase Structure Grammar ( HPSG ) <TARGET_CITATION/> by a method of grammar conversion . This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar (FBLTAG1) <CITATION/> and HeadDriven Phrase Structure Grammar (HPSG) <CITATION/> by a method of grammar conversion.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,headdriven phrase structure grammar,1994,Carl Pollard; Ivan Sag
481,J00-4002,External_32773,[0],,"These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs <TARGET_CITATION/> , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .","But little of this detail is essential to our main aims: a wide range of grammatical formalisms and interpreters would be compatible with the basic assumptions of the contextual interpretation mechanism, assuming only that the same grammatical description is used in both the analysis and generation direction. What is required is that QLFs are, as here, expressed in a typed higherorder logic, augmented with constructs representing the interpretation of contextdependent elements (pronouns, ellipsis, focus, etc.). These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs <TARGET_CITATION/> , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) . These constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution (unlike, say, the metavariables of standard QLFs [Alshawi and Crouch 1992], or the labels of UDRS [Reyle 1996], which are motivated entirely by the mechanisms that operate on them after grammatical processing). What is required is that QLFs are, as here, expressed in a typed higherorder logic, augmented with constructs representing the interpretation of contextdependent elements (pronouns, ellipsis, focus, etc.). But little of this detail is essential to our main aims: a wide range of grammatical formalisms and interpreters would be compatible with the basic assumptions of the contextual interpretation mechanism, assuming only that the same grammatical description is used in both the analysis and generation direction.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,e87314e4e465ff21001e04a38b8fcdfeadae714e,Monotonic Semantic Interpretation,1992,H. Alshawi; Dick Crouch
482,W06-1705,External_36201,[0],related work,"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp <CITATION/> , KWiCFinder <CITATION/> and the Linguist 's Search Engine <TARGET_CITATION/> .","<CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler. <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp <CITATION/> , KWiCFinder <CITATION/> and the Linguist 's Search Engine <TARGET_CITATION/> . Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp <CITATION/>, KWiCFinder <CITATION/> and the Linguist's Search Engine <CITATION/>. <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. <CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,af58493ccfbfb15e9d1aceed4c5cd49824a99d74,THE LINGUIST'S SEARCH ENGINE: GETTING STARTED GUIDE,2003,P. Resnik; Aaron Elkiss
483,P97-1063,External_19461,[0],introduction,"Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools ( e.g. <TARGET_CITATION/> ) , concordancing for bilingual lexicography <CITATION/> , computerassisted language learning , corpus linguistics ( Melby .","Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools <TARGET_CITATION/> , concordancing for bilingual lexicography <CITATION/> , computerassisted language learning , corpus linguistics ( Melby . Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including crummy'' MT on the World Wide Web <CITATION/>, certain machineassisted translation tools (e.g. <CITATION/>), concordancing for bilingual lexicography <CITATION/>, computerassisted language learning, corpus linguistics (Melby. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,a geometric approach to mapping bitext correspondencequot,1996,I D Melamed
484,J86-1002,External_75527,[0],experiments,"An offtheshelf speech recognition device , a Nippon Electric Corporation DP200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) <TARGET_CITATION/> .","The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An offtheshelf speech recognition device , a Nippon Electric Corporation DP200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) <TARGET_CITATION/> . An offtheshelf speech recognition device, a Nippon Electric Corporation DP200, was added to an existing natural language processing system, the Natural Language Computer (NLC) <CITATION/>. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,e024f119f78e33662d4bb4ccd8def4d15c96b127,Toward Natural Language Computation I,1980,A. Biermann; B. Ballard
485,D13-1115,P11-1096,[0],introduction,"Some efforts have tackled tasks such as automatic image caption generation <CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <TARGET_CITATION/> .","Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <TARGET_CITATION/> . Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,5f8a964074755b8845856341db98d31a5dc9884e,Simple supervised document geolocation with geodesic grids,2011,Benjamin Wing; Jason Baldridge
486,P00-1007,External_9757,[4],,"The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <TARGET_CITATION/> , and became a common testbed ."," The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <TARGET_CITATION/> , and became a common testbed . The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 (Table 1), same as used by <CITATION/>, and became a common testbed.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,,statistical decisiontree models for parsing,1995,David M Magerman
487,J05-3003,A97-1052,[0],related work,"<TARGET_CITATION/> predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX <CITATION/> and ANLT <CITATION/> dictionaries and adding around 30 frames found by manual inspection .","The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following <CITATION/>. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verbsubcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. <TARGET_CITATION/> predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX <CITATION/> and ANLT <CITATION/> dictionaries and adding around 30 frames found by manual inspection . <CITATION/> predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX <CITATION/> and ANLT <CITATION/> dictionaries and adding around 30 frames found by manual inspection. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verbsubcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following <CITATION/>.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,9b77ca011896f79d9014704aaa63ecf4ffb3485c,Automatic Extraction of Subcategorization from Corpora,1997,Ted Briscoe; John A. Carroll
488,W06-1639,External_6728,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,fc168ffb0eaa4074e91ca481e9788e554e7ae616,Iterative Classification in Relational Data,2000,Jennifer Neville; David D. Jensen
489,P02-1001,External_35216,[2],,<TARGET_CITATION/> give a sufficiently general finitestate framework to allow this : weights may fall in any set K ( instead of R ) .,"The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset H must be (&E  P(), &E  P() val()) E R>0 x V , from which (1) is trivial to compute. <TARGET_CITATION/> give a sufficiently general finitestate framework to allow this : weights may fall in any set K ( instead of R ) . <CITATION/> give a sufficiently general finitestate framework to allow this: weights may fall in any set K (instead of R). We will enforce an invariant: the weight of any pathset H must be (&E  P(), &E  P() val()) E R>0 x V , from which (1) is trivial to compute. The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,d26844d389df6aa01152bbe31832ff4ab73f5b01,Rational series and their languages,1988,J. Berstel; C. Reutenauer
490,W01-1510,External_78607,[0],introduction,FBLTAG <TARGET_CITATION/> is an extension of the LTAG formalism .,Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled x onto an internal node of another tree with the same symbol x (Figure 4). FBLTAG <TARGET_CITATION/> is an extension of the LTAG formalism . FBLTAG <CITATION/> is an extension of the LTAG formalism. Adjunction grafts an auxiliary tree with the root node and foot node labeled x onto an internal node of another tree with the same symbol x (Figure 4). Substitution replaces a substitution node with another initial tree (Figure 3).,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,54cf4641d00b8a177c3b98a6e07d48620dc91388,Feature Structures Based Tree Adjoining Grammars,1988,K. Vijay-Shanker; A. Joshi
491,W00-1312,External_16322,[4],method,There are several variations of such a method <TARGET_CITATION/> .,"A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method <TARGET_CITATION/> . There are several variations of such a method <CITATION/>. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. A second method is to structure the translated query, separating the translations for one term from translations for other terms.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,using structured queries for disambiguation in crosslanguage information retrievalquot,1997,D A Hull
492,J97-4003,External_41778,[0],introduction,"12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in <TARGET_CITATION/> .","The translation of the lexical rule into a predicate is trivial. The result is displayed description language. 12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in <TARGET_CITATION/> . 12 In order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in <CITATION/>. The result is displayed description language.The translation of the lexical rule into a predicate is trivial.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,towards a semantics for lexical rules as used in hpsg,1995,Detmar Meurers
493,J90-3003,External_73669,[1],experiments,"Many investigators <TARGET_CITATION/> have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a texttospeech synthesizer. Existing texttospeech systems perform well on word pronunciation and short sentences,12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators <TARGET_CITATION/> have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . Many investigators <CITATION/> have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech. Existing texttospeech systems perform well on word pronunciation and short sentences,12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a texttospeech synthesizer.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,4f395f3538beaa300880c727484ec7f079c0bbca,From Sad to Glad : Emotional Computer Voices,None,Janet E. Cahn
495,E03-1007,External_6772,[2],experiments,We performed translation experiments with an implementation of the IBM4 translation model <TARGET_CITATION/> .,"Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution. We performed translation experiments with an implementation of the IBM4 translation model <TARGET_CITATION/> . We performed translation experiments with an implementation of the IBM4 translation model <CITATION/>. This shows that our method was successful in producing a more focused lexicon probability distribution. Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4.",339ec71f2e0907ba376a3c8e7b7a89c592be3fdd,Using POS Information for SMT into Morphologically Rich Languages,2003,Nicola Ueffing; H. Ney,,the mathematics of statistical machine translation parameter estimationquot,1993,P F Brown; V J Della Pietra; S A Della Pietra; R L Mercer
496,W02-1601,C88-1013,[0],introduction,"In this paper , a flexible annotation schema called Structured StringTree Correspondence ( SSTC ) <TARGET_CITATION/> will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .","There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences (translations) between layers of representation structures <CITATION/>. In this paper , a flexible annotation schema called Structured StringTree Correspondence ( SSTC ) <TARGET_CITATION/> will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two . In this paper, a flexible annotation schema called Structured StringTree Correspondence (SSTC) <CITATION/> will be introduced to capture a natural language text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences (translations) between layers of representation structures <CITATION/>. There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,545e6410b23db2b0c6d3178430f61cb472a41e5e,Representation Trees and String-Tree Correspondences,1988,C. Boitet; Yusoff Zaharin
497,J02-3002,External_12989,[0],,"For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC7 ) <TARGET_CITATION/> ."," frequent proper name  abbreviation (as opposed to regular word)These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC7 ) <TARGET_CITATION/> . For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times (NYT) corpus that was supplied as training data for the 7th Message Understanding Conference (MUC7) <CITATION/>. These four lists can be acquired completely automatically from raw (unlabeled) texts.  frequent proper name  abbreviation (as opposed to regular word)",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,overview of muc7”,1998,Nancy Chinchor
499,W06-1639,N03-2012,[0],related work,"More sophisticated approaches have been proposed <TARGET_CITATION/> , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments <CITATION/> .","Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. More sophisticated approaches have been proposed <TARGET_CITATION/> , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments <CITATION/> . More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,0d833ccbc824e81891e55baa2da6318e9651956a,Detection Of Agreement vs. Disagreement In Meetings: Training With Unlabeled Data,2003,D. Hillard; Mari Ostendorf; Elizabeth Shriberg
501,W00-1312,External_24245,[2],,The onesided ttest <TARGET_CITATION/> at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .,"The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C. Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries. The onesided ttest <TARGET_CITATION/> at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant . The onesided ttest <CITATION/> at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant. Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries. The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,using statistical testing in the evaluation of retrieval experimentsquot,1993,D Hull
502,W02-0309,External_649,[0],introduction,mers <TARGET_CITATION/> demonstrably improve retrieval performance .,"The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator. mers <TARGET_CITATION/> demonstrably improve retrieval performance . mers <CITATION/> demonstrably improve retrieval performance. For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator. The efforts required for performing morphological analysis vary from language to language.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,a651bb7cc7fc68ece0cc66ab921486d163373385,An algorithm for suffix stripping,1997,M. Porter
503,J09-4010,External_60689,[0],introduction,It is therefore no surprise that early attempts at response automation were knowledgedriven <TARGET_CITATION/> .,"1 http://customercare.telephonyonline.com/ar/telecom next generation customer.circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledgedriven <TARGET_CITATION/> . It is therefore no surprise that early attempts at response automation were knowledgedriven <CITATION/>. circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. 1 http://customercare.telephonyonline.com/ar/telecom next generation customer.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml,1995,A Barr; S Tessler
504,A00-1009,A97-1039,[0],,The framework was originally developed for the realization of deepsyntactic structures in NLG <TARGET_CITATION/> .,"8 History of the Framework and Comparison with Other SystemsThe framework represents a generalization of several predecessor NLG systems based on MeaningText Theory: FoG <CITATION/>, LFS <CITATION/>, and JOYCE <CITATION/>. The framework was originally developed for the realization of deepsyntactic structures in NLG <TARGET_CITATION/> . The framework was originally developed for the realization of deepsyntactic structures in NLG <CITATION/>. The framework represents a generalization of several predecessor NLG systems based on MeaningText Theory: FoG <CITATION/>, LFS <CITATION/>, and JOYCE <CITATION/>. 8 History of the Framework and Comparison with Other Systems",6602edbc2f35e085dc4ee0361da214c4f14c5a07,A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing,2000,Benoit Lavoie; R. Kittredge; Tanya Korelsky; Owen Rambow,6e4fbe7b485045b61d468a2ebc4df81c0d5ea52d,A Fast and Portable Realizer for Text Generation Systems,1997,Benoit Lavoie; O. Rainbow
505,W02-1601,External_29486,[0],,"These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are nonstandard , e.g. crossed dependencies <TARGET_CITATION/> .","The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective <CITATION/>. These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are nonstandard , e.g. crossed dependencies <TARGET_CITATION/> . These features are very much desired in the design of an annotation scheme, in particular for the treatment of linguistic phenomena, which are nonstandard, e.g. crossed dependencies <CITATION/>.The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,,handling crossed dependencies with the stcg,1995,E K Tang; Y Zaharin
506,P00-1001,External_5424,[0],introduction,Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are <TARGET_CITATION/> ) .,"In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and wellformed utterance. Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are <TARGET_CITATION/> ) . Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are <CITATION/>). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and wellformed utterance. In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995).",,a holistic lexiconbased approach to opinion mining,2000,,,deterministic parsing of syntactic nonfluencies,1983,D Hindle
508,W00-1017,External_1973,[5],conclusion,There have been several efforts aimed at developing a domainindependent method for generating responses from a frame representation of user requests <TARGET_CITATION/> .,"If the generation functions employ sophisticated dialogue strategies, the system can perform complicated dialogues that are not just question answering. WIT, however, does not provide taskindependent facilities that make it easier to employ such dialogue strategies. There have been several efforts aimed at developing a domainindependent method for generating responses from a frame representation of user requests <TARGET_CITATION/> . There have been several efforts aimed at developing a domainindependent method for generating responses from a frame representation of user requests <CITATION/>. WIT, however, does not provide taskindependent facilities that make it easier to employ such dialogue strategies. If the generation functions employ sophisticated dialogue strategies, the system can perform complicated dialogues that are not just question answering.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,c2505e3f0e19d50bdd418ca9becf8cbb08f61dc1,"GUS, A Frame-Driven Dialog System",1986,D. Bobrow; R. Kaplan; M. Kay; D. Norman; Henry S. Thompson; T. Winograd
509,E03-1007,External_3692,[2],,The maximum entropy approach <TARGET_CITATION/> presents a powerful framework for the combination of several knowledge sources ., The maximum entropy approach <TARGET_CITATION/> presents a powerful framework for the combination of several knowledge sources . The maximum entropy approach <CITATION/> presents a powerful framework for the combination of several knowledge sources.,339ec71f2e0907ba376a3c8e7b7a89c592be3fdd,Using POS Information for SMT into Morphologically Rich Languages,2003,Nicola Ueffing; H. Ney,fb486e03369a64de2d5b0df86ec0a7b55d3907db,A Maximum Entropy Approach to Natural Language Processing,1996,Adam L. Berger; S. D. Pietra; V. D. Pietra
511,J09-4010,External_45353,[2],,We then use the program Snob <TARGET_CITATION/> to cluster these experiences .,"We train the system by clustering the experiences'' of the responsegeneration methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively). We then use the program Snob <TARGET_CITATION/> to cluster these experiences . We then use the program Snob <CITATION/> to cluster these experiences. We train the system by clustering the experiences'' of the responsegeneration methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations (7) and (8), respectively).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,3c56e086eb04ebc4a26c760df7d1fa842c931ccc,Statistical and Inductive Inference by Minimum Message Length (Information Science and Statistics),2005,C. S. Wallace
513,A00-1025,P98-1034,[2],,"Our implementation of the NPbased QA system uses the Empire noun phrase finder , which is described in detail in <TARGET_CITATION/> .","The filter operates on the ordered list of summary extracts for a particular question and produces a list of answer hypotheses, one for each noun phrase (NP) in the extracts in the lefttoright order in which they appeared. The NPbased QA System. Our implementation of the NPbased QA system uses the Empire noun phrase finder , which is described in detail in <TARGET_CITATION/> . Our implementation of the NPbased QA system uses the Empire noun phrase finder, which is described in detail in <CITATION/>. The NPbased QA System. The filter operates on the ordered list of summary extracts for a particular question and produces a list of answer hypotheses, one for each noun phrase (NP) in the extracts in the lefttoright order in which they appeared.",7acb7b3e7ad16adbc68626cddd4bcba515b86e23,Examining the Role of Statistical and Linguistic Knowledge Sources in a General-Knowledge Question-Answering System,2000,Claire Cardie; Vincent Ng; D. Pierce; C. Buckley,f5bb34e38e3403054d4396fc48882f02eae1ffcc,Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification,1998,Claire Cardie; D. Pierce
515,W14-2106,W13-4006,[1],,"In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement <TARGET_CITATION/> .","Compared to the all unigrams baseline, the MIbased unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6). The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification. In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement <TARGET_CITATION/> . In addition, we consider several types of lexical features (LexF) inspired by previous work on agreement and disagreement <CITATION/>.The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification. Compared to the all unigrams baseline, the MIbased unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).",2b8b59a74d815a70bbb31892ce484510480be6fe,Analyzing Argumentative Discourse Units in Online Interactions,2014,Debanjan Ghosh; S. Muresan; Nina Wacholder; Mark Aakhus; M. Mitsui,05f06634d0557530bfc56b45e74aa203f56cea91,Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue,2013,Amita Misra; M. Walker
516,P10-2059,External_66677,[2],,All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme <TARGET_CITATION/> ., All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme <TARGET_CITATION/> . All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme <CITATION/>.,b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,the mumin coding scheme for the annotation of feedback turn management and sequencing multimodal corpora for modelling human multimodal behaviour,2007,Jens Allwood; Loredana Cerrato; Kristiina Jokinen; Costanza Navarretta; Patrizia Paggio
517,J09-1005,External_21987,[1],,"Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by <TARGET_CITATION/> .","This approach has two main challenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it can only measure the lexical fixedness of idiomatic combinations, and so could not apply to literal combinations. We thus interpret this property statistically in the following way: We expect a lexically fixed verb+noun combination to appear much more frequently than its variants in general. Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by <TARGET_CITATION/> . Specifically, we examine the strength of association between the verb and the noun constituent of a combination (the target expression or its lexical variants) as an indirect cue to its idiomaticity, an approach inspired by <CITATION/>. We thus interpret this property statistically in the following way: We expect a lexically fixed verb+noun combination to appear much more frequently than its variants in general. This approach has two main challenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it can only measure the lexical fixedness of idiomatic combinations, and so could not apply to literal combinations.",dd1924e94968c0e19129e9b82eb42bb540a36f67,Unsupervised Type and Token Identification of Idiomatic Expressions,2009,A. Fazly; Paul Cook; S. Stevenson,,automatic identification of noncompositional phrases,1999,Dekang Lin
518,J86-1002,External_21718,[4],,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/> , assertional statements as in <CITATION/> , or semantic nets as in <TARGET_CITATION/> .","It self activates to bias recognition toward historically observed patterns but is not otherwise observable. The VNLCE processor may be considered to be a learning system of the tradition described, for example, in <CITATION/>. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/> , assertional statements as in <CITATION/> , or semantic nets as in <TARGET_CITATION/> . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>. The VNLCE processor may be considered to be a learning system of the tradition described, for example, in <CITATION/>. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,learning structural descriptions from examples in,1975,P Winston
520,D09-1003,External_4260,[0],conclusion,<TARGET_CITATION/> for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .,"We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition. The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates. <TARGET_CITATION/> for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples . <CITATION/> for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples. The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates. We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition.",ae29b936d437a93ad259ee008ba56fe82ab4db61,Semi-supervised Semantic Role Labeling Using the Latent Words Language Model,2009,K. Deschacht; Marie-Francine Moens,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
521,P00-1007,External_555,[0],introduction,"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences ( e.g. , <TARGET_CITATION/> ) ."," A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences <TARGET_CITATION/> . A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences (e.g., <CITATION/>).",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,0ffa423a5283396c88ff3d4033d541796bd039cc,"Three Generative, Lexicalised Models for Statistical Parsing",1997,M. Collins
522,A00-1014,P99-1024,[4],,"The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; <TARGET_CITATION/> ) ) .","5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results <CITATION/>, which we leave for future work.rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1. The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; <TARGET_CITATION/> ) ) . The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems (e.g., (Bennacef et at., 1996; <CITATION/>)). rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1. 5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results <CITATION/>, which we leave for future work.",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,7eccf66ff8218b1de41205ef80354cd5d755f5b9,The CommandTalk Spoken Dialogue System,1999,Amanda Stent; John Doweling; J. Gawron; Elizabeth Owen Bratt; Robert C. Moore
523,E03-1002,External_62,[2],experiments,7A11 our results are computed with the evalb program following the nowstandard criteria in <TARGET_CITATION/> .,Momentum was applied throughout training. Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. 7A11 our results are computed with the evalb program following the nowstandard criteria in <TARGET_CITATION/> . 7A11 our results are computed with the evalb program following the nowstandard criteria in <CITATION/>.Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. Momentum was applied throughout training.,adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,,headdriven statistical models for natural language parsing,1999,M Collins
524,J02-3002,External_1066,[4],,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] <TARGET_CITATION/> , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] <TARGET_CITATION/> , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992], Brill's [Brill 1995a], and MaxEnt [Ratnaparkhi 1996]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. This requires resolving sentence boundaries before tagging. tagger operates on text spans that form a sentence.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,robust partofspeech tagging using a hidden markov model computer speech and language,1992,Julian Kupiec
526,Q13-1020,W06-1606,[2],experiments,The system is implemented based on <TARGET_CITATION/> .,"To create the baseline system, we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased (HPB) system, and a syntaxaugmented MT (SAMT) 11 system <CITATION/> respectively. The translation system used for testing the effectiveness of our Utrees is our inhouse stringtotree system (abbreviated as s2t). The system is implemented based on <TARGET_CITATION/> . The system is implemented based on <CITATION/>. The translation system used for testing the effectiveness of our Utrees is our inhouse stringtotree system (abbreviated as s2t). To create the baseline system, we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased (HPB) system, and a syntaxaugmented MT (SAMT) 11 system <CITATION/> respectively.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,cb59fada125478c0302c6874aa13a83ab0ac62f1,SPMT: Statistical Machine Translation with Syntactified Target Language Phrases,2006,D. Marcu; Wei Wang; Abdessamad Echihabi; Kevin Knight
527,W06-1639,External_6429,[0],related work,"Also relevant is work on the general problems of dialogact tagging <CITATION/> , citation analysis <CITATION/> , and computational rhetorical analysis <TARGET_CITATION/> .","Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Also relevant is work on the general problems of dialogact tagging <CITATION/> , citation analysis <CITATION/> , and computational rhetorical analysis <TARGET_CITATION/> . Also relevant is work on the general problems of dialogact tagging <CITATION/>, citation analysis <CITATION/>, and computational rhetorical analysis <CITATION/>. More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,9379e97cc4cfb5616811fe6a86b8e04f9531d23a,The Theory and Practice of Discourse Parsing and Summarization,2000,Daniel Marcu
530,P10-2059,W09-3936,[0],introduction,"<CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical , syntactic and prosodic cues , while <TARGET_CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues .","Related are also the studies by Rieks op den <CITATION/>: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of maptask dialogues, also targeted in this paper. <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical , syntactic and prosodic cues , while <TARGET_CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues . <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. Work has also been done on prosody and gestures in the specific domain of maptask dialogues, also targeted in this paper. Related are also the studies by Rieks op den <CITATION/>: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,1e0c04eb8222fa7fe3afb982c0c8c038dec44b11,Turn-Yielding Cues in Task-Oriented Dialogue,2009,Agustin Gravano; Julia Hirschberg
531,J05-3003,External_5585,[2],,"We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank <TARGET_CITATION/> and Penn Chinese Treebank <CITATION/> , extracting widecoverage , probabilistic LFG grammar","The work reported here is part of the core components for bootstrapping this approach. In the shorter term, we intend to make the extracted subcategorization lexicons from PennII and PennIII available as a downloadable publicdomain research resource. We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank <TARGET_CITATION/> Chinese Treebank <CITATION/> , extracting widecoverage , probabilistic LFG grammar We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank <CITATION/> and Penn Chinese Treebank <CITATION/>, extracting widecoverage, probabilistic LFG grammarIn the shorter term, we intend to make the extracted subcategorization lexicons from PennII and PennIII available as a downloadable publicdomain research resource. The work reported here is part of the core components for bootstrapping this approach.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,0a3713b0e86b282b1821e29688d3b28a7bfbc3e5,The TIGER Treebank,2001,Sabine Brants; S. Dipper; Silvia Hansen; Wolfgang Lezius; George Smith
532,D10-1100,D09-1143,[2],,"We use the structures previously used by <TARGET_CITATION/> , and propose one new structure .","Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the discrete'' structures followed by the kernel we used. We use the structures previously used by <TARGET_CITATION/> , and propose one new structure . We use the structures previously used by <CITATION/>, and propose one new structure. Now we present the discrete'' structures followed by the kernel we used. Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task.",330d82cedd1567515d42163b766197944adf6647,Automatic Detection and Classification of Social Events,2010,Apoorv Agarwal; Owen Rambow,a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi
533,J87-3002,External_4910,[4],introduction,"Two exceptions to this generalisation are the Linguistic String Project <CITATION/> and the IBM CRITIQUE ( formerly EPISTLE ) Project <TARGET_CITATION/> ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .","Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. <CITATION/>) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project <CITATION/> and the IBM CRITIQUE ( formerly EPISTLE ) Project <TARGET_CITATION/> ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . Two exceptions to this generalisation are the Linguistic String Project <CITATION/> and the IBM CRITIQUE (formerly EPISTLE) Project <CITATION/>; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. <CITATION/>) consult relatively small lexicons, typically generated by hand. Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,9c6e68a6d704d0d9518a807584a469f72a4c66c9,Word Formation in Natural Language Processing Systems,1983,Roy J. Byrd
534,J01-4001,External_85092,[0],,"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; <TARGET_CITATION/> ; Mitkov 1996 , 1998b ) .","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; <TARGET_CITATION/> ; Mitkov 1996 , 1998b ) . A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,8299a3ba1677224927f4f152664b5b59520c5838,CogNIAC: high precision coreference with limited knowledge and linguistic resources,1997,B. Baldwin
535,W02-1601,C88-1013,[0],,"Towards this aim , a flexible annotation structure called Structured StringTree Correspondence ( SSTC ) was introduced in <TARGET_CITATION/> to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the subcorrespondences recorded as part of a SSTC .","It is well known that many linguistic constructions are not projective (e.g. scrambling, cross serial dependencies, etc.). Hence, it is very much desired to define the correspondence in a way to be able to handle the nonstandard cases (e.g. nonprojective correspondence), see Figure 1. Towards this aim , a flexible annotation structure called Structured StringTree Correspondence ( SSTC ) was introduced in <TARGET_CITATION/> to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the subcorrespondences recorded as part of a SSTC . Towards this aim, a flexible annotation structure called Structured StringTree Correspondence (SSTC) was introduced in <CITATION/> to record the string of terms, its associated representation structure and the mapping between the two, which is expressed by the subcorrespondences recorded as part of a SSTC.Hence, it is very much desired to define the correspondence in a way to be able to handle the nonstandard cases (e.g. nonprojective correspondence), see Figure 1. It is well known that many linguistic constructions are not projective (e.g. scrambling, cross serial dependencies, etc.).",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,545e6410b23db2b0c6d3178430f61cb472a41e5e,Representation Trees and String-Tree Correspondences,1988,C. Boitet; Yusoff Zaharin
536,J90-3003,External_14830,[2],experiments,We have built an experimental texttospeech system that uses our analysis of prosody to generate phrase boundaries for the Olive  Liberman synthesizer <TARGET_CITATION/> ., We have built an experimental texttospeech system that uses our analysis of prosody to generate phrase boundaries for the Olive  Liberman synthesizer <TARGET_CITATION/> . We have built an experimental texttospeech system that uses our analysis of prosody to generate phrase boundaries for the OliveLiberman synthesizer <CITATION/>.,678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,,texttospeechan overview,1985,S P Olive; M Y Liberman
537,N04-2004,External_29664,[2],introduction,"In this paper , I present a computational implementation of Distributed Morphology <TARGET_CITATION/> , a nonlexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .","Nevertheless, the lexicalist approach does not capture productive morphological processes that pervade natural language, for example, flat.V  flatten.ADJ or hammer.N  hammer.V; most frameworks for computational semantics fail to capture the deeper derivational relationship between morphologicallyrelated terms. For languages with rich derivational morphology, this problem is often critical: the standard architectural view of morphological analysis as a preprocessor presents difficulties in handling semantically meaningful affixes. In this paper , I present a computational implementation of Distributed Morphology <TARGET_CITATION/> , a nonlexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation . In this paper, I present a computational implementation of Distributed Morphology <CITATION/>, a nonlexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation. For languages with rich derivational morphology, this problem is often critical: the standard architectural view of morphological analysis as a preprocessor presents difficulties in handling semantically meaningful affixes. Nevertheless, the lexicalist approach does not capture productive morphological processes that pervade natural language, for example, flat.V  flatten.ADJ or hammer.N  hammer.V; most frameworks for computational semantics fail to capture the deeper derivational relationship between morphologicallyrelated terms.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,5f6dad0c28fc700ed495dd2281ab0e2d60d54c46,Distributed morphology and the pieces of inflection,1993,M. Halle; A. Marantz
538,J06-2002,External_2473,[4],,"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead <TARGET_CITATION/> ."," Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead <TARGET_CITATION/> . Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead <CITATION/>.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,efficient contextsensitive generation of referring expressions,2002,Emiel Krahmer; Mari¨et Theune
539,J92-1004,H89-2018,[2],,Subsequent processing by the natural language and response generation components was done automatically by the computer <TARGET_CITATION/> .,"excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural language and response generation components was done automatically by the computer <TARGET_CITATION/> . Subsequent processing by the natural language and response generation components was done automatically by the computer <CITATION/>. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. excluded.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,097a9aa2f9bc0feb5e0026b1b354a28d033b6ccb,The Collection and Preliminary Analysis of a Spontaneous Speech Database,1989,V. Zue; Nancy A. Daly-Kelly; James R. Glass; D. Goodine; H. Leung; M. Phillips; J. Polifroni; S. Seneff; M. Soclof
540,J02-3002,P99-1021,[2],,"This is implemented as a cascade of simple strategies , which were briefly described in <TARGET_CITATION/> .","Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a documentcentered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in <TARGET_CITATION/> . This is implemented as a cascade of simple strategies, which were briefly described in <CITATION/>.We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a documentcentered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,13682fc6de7288f92cab99798e187ad4ad7a1a24,A Knowledge-free Method for Capitalized Word Disambiguation,1999,Andrei Mikheev
542,D09-1087,P08-1067,[5],conclusion,Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <TARGET_CITATION/> for self training .,"We show for the first time that selftraining is able to significantly improve the performance of a PCFGLA parser, a single generative parser, on both small and large amounts of labeled training data. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by overfitting. Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <TARGET_CITATION/> for self training . Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <CITATION/> for self training. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by overfitting. We show for the first time that selftraining is able to significantly improve the performance of a PCFGLA parser, a single generative parser, on both small and large amounts of labeled training data.",5bfd8d40bc071fffaf93685a46974b122ee4239d,Self-Training PCFG Grammars with Latent Annotations Across Languages,2009,Zhongqiang Huang; M. Harper,1ba700aec9f23ecb76622f2202badf25f6ad896e,Forest Reranking: Discriminative Parsing with Non-Local Features,2008,Liang Huang
543,W06-2807,External_91606,[0],related work,Intermedia is no more developed and nobody of us had the opportunity to try it <TARGET_CITATION/> .,"Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace. In fact, they were used expecially in academic writing with some success. Intermedia is no more developed and nobody of us had the opportunity to try it <TARGET_CITATION/> . Intermedia is no more developed and nobody of us had the opportunity to try it <CITATION/>. In fact, they were used expecially in academic writing with some success. Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,1994,George P Landow
544,W02-0309,External_44559,[0],introduction,"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( <CITATION/> ; J  appinen and Niemist  o , 1988 ; <TARGET_CITATION/> ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved ."," Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( <CITATION/> ; J  appinen and Niemist  o , 1988 ; <TARGET_CITATION/> ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved . Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval (IR) system <CITATION/>, since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,acddd8959a22b55050de23b0810428b80cf100c5,Viewing stemming as recall enhancement,1996,Wessel Kraaij; Renée Pohlmann
545,W01-1510,External_332,[0],introduction,ment <TARGET_CITATION/> .,"There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environ1In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. ment <TARGET_CITATION/> . ment <CITATION/>. 1In this paper, we use the term LTAG to refer to FBLTAG, if not confusing.There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environ",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,29145415ff9199be44accb0abc1689f9fbe8086e,Typing as a means for validating feature structures,1999,Anoop Sarkar; S. Wintner
546,P10-2059,External_66679,[0],introduction,"Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <TARGET_CITATION/> .","<CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <TARGET_CITATION/> . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <CITATION/>. <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,c88c8d2eb34cf2a685fe5cbdcddeb4c68df0df0f,A conversation robot using head gesture recognition as para-linguistic information,2004,S. Fujie; Y. Ejiri; Kei Nakajima; Y. Matsusaka; Tetsunori Kobayashi
547,D13-1115,External_32478,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
548,D08-1007,P03-1001,[2],method,"We also made use of the personname/instance pairs automatically extracted by <TARGET_CITATION/> .2 This data provides counts for pairs such as  Edwin Moses , hurdler '' and  William Farley , industrialist . ''","If wesee that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc.. We also made use of the personname/instance pairs automatically extracted by <TARGET_CITATION/> .2 This data provides counts for pairs such as  Edwin Moses , hurdler '' and  William Farley , industrialist . '' We also made use of the personname/instance pairs automatically extracted by <CITATION/>.2 This data provides counts for pairs such as Edwin Moses, hurdler'' and William Farley, industrialist.'' see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc.. If we",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,dfcece84048fc3fab7f7d5e23415b6e5d58f139f,Offline Strategies for Online Question Answering: Answering Questions Before They Are Asked,2003,Michael Fleischman; E. Hovy; Abdessamad Echihabi
549,W02-1601,External_69138,[0],,<TARGET_CITATION/> presented an approach for constructing a BKB based on the SSSTC .,"The proposed SSSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures. SSSTC very well suited for the construction of a BKB, which is needed for the EBMT applications. <TARGET_CITATION/> presented an approach for constructing a BKB based on the SSSTC . <CITATION/> presented an approach for constructing a BKB based on the SSSTC. SSSTC very well suited for the construction of a BKB, which is needed for the EBMT applications. The proposed SSSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,75981495263f7a7fbfa18de3302c40f743ce3173,Converting a bilingual dictionary into a bilingual knowledge bank based on the synchronous SSTC,2001,E. Tang; M. H. Al-Adhaileh
551,W03-0806,External_218,[0],,"Also , advanced methods often require many training iterations , for example active learning <CITATION/> and cotraining <TARGET_CITATION/> .","Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also , advanced methods often require many training iterations , for example active learning <CITATION/> and cotraining <TARGET_CITATION/> . Also, advanced methods often require many training iterations, for example active learning <CITATION/> and cotraining <CITATION/>. Efficient training is required because the amount of data available for training will increase significantly. Efficiency is required both in training and processing.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,combining labeled and unlabeled data with cotraining,1998,Avrim Blum; Tom Mitchell
552,D13-1115,External_32478,[3],related work,"<TARGET_CITATION/> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .","cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. <TARGET_CITATION/> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . <CITATION/> furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>).",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
553,P00-1006,External_3692,[1],method,It can be shown <TARGET_CITATION/> that the use of this model with maximum likelihood parameter estimation is justified on informationtheoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .,"2<CITATION/> reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. It can be shown <TARGET_CITATION/> that the use of this model with maximum likelihood parameter estimation is justified on informationtheoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events . It can be shown <CITATION/> that the use of this model with maximum likelihood parameter estimation is justified on informationtheoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values.3 There is no requirement that the components of f represent disjoint or statistically independent events. However, since the models tested apparently differed in other aspects, it is hard to determine how much of this gain can be attributed to the use of ME. 2<CITATION/> reports a greater perplexity reduction (23% versus 10%) over a baseline trigram language model due the use of ME versus linear word triggers.",d60cf1a4c7a35f859e2e203d27f3ec71994e2e3e,A Maximum Entropy/Minimum Divergence Translation Model,2000,George F. Foster,fb486e03369a64de2d5b0df86ec0a7b55d3907db,A Maximum Entropy Approach to Natural Language Processing,1996,Adam L. Berger; S. D. Pietra; V. D. Pietra
554,J00-2013,External_1120,[0],,"Shortly after the publication of The Sound Pattern of English <CITATION/> , Kornai points out ,  <CITATION/> demonstrated that the contextsensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finitestate transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in <TARGET_CITATION/> . ''","In this regard, Kornai's own chapter on vectorized finitestate automata describes an extremely efficient patternmatching engine, around which the NewsMonitor system is built. This system extracts relational information, such as who is where'' or who bought what'', from issues of the Wall Street Journal (source code and sample data are included on the CDROM). Shortly after the publication of The Sound Pattern of English <CITATION/> , Kornai points out ,  <CITATION/> demonstrated that the contextsensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finitestate transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in <TARGET_CITATION/> . '' Shortly after the publication of The Sound Pattern of English <CITATION/>, Kornai points out, <CITATION/> demonstrated that the contextsensitive machinery of SPE ... [could] be replaced by a much simpler one, based on finitestate transducers (FSTs); the same conclusion was reached independently by Kaplan and Kay, whose work remained an underground classic until it was finally published in <CITATION/>.'' This system extracts relational information, such as who is where'' or who bought what'', from issues of the Wall Street Journal (source code and sample data are included on the CDROM). In this regard, Kornai's own chapter on vectorized finitestate automata describes an extremely efficient patternmatching engine, around which the NewsMonitor system is built.",751dff01cfd06821c2ecfa1f02f5380276a82ffe,Book Reviews: Extended Finite State Models of Language,1996,András Kornai,edf80266d1ecfed3937833574b59c94b88aedee4,Regular Models of Phonological Rule Systems,1994,R. Kaplan; M. Kay
556,N10-1084,External_36166,[0],related work,<TARGET_CITATION/> ) and <CITATION/> attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .,"<CITATION/> all belong to the syntactic transformation category. After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. <TARGET_CITATION/> ) and <CITATION/> attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method . <CITATION/> attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools. <CITATION/> all belong to the syntactic transformation category.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,d64dc949b8ceae642e8fcad7ccd29ee523116371,Natural language processing for information assurance and security: an overview and implementations,2001,M. Atallah; C. McDonough; V. Raskin; S. Nirenburg
558,J05-3003,External_198,[0],,The subcategorization requirements expressed by semantic forms are enforced at fstructure level through completeness and coherence wellformedness conditions on fstructure <TARGET_CITATION/> : An fstructure is locally complete iff it contains all the governable grammatical functions that its predicate governs .,"of phrase structural position. In LFG, the subcategorization requirements of a particular predicate are expressed by its semantic form: FOCUS((r SUBJ)(r OBLon)) in Figure 1. The subcategorization requirements expressed by semantic forms are enforced at fstructure level through completeness and coherence wellformedness conditions on fstructure <TARGET_CITATION/> : An fstructure is locally complete iff it contains all the governable grammatical functions that its predicate governs . The subcategorization requirements expressed by semantic forms are enforced at fstructure level through completeness and coherence wellformedness conditions on fstructure <CITATION/>: An fstructure is locally complete iff it contains all the governable grammatical functions that its predicate governs. In LFG, the subcategorization requirements of a particular predicate are expressed by its semantic form: FOCUS((r SUBJ)(r OBLon)) in Figure 1. of phrase structural position.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,e17117dbee804d7d177d8eb9fadf0bda1ebc4d22,Lexical Functional Grammar A Formal System for Grammatical Representation,2004,Ronald M. Kaplan
559,W01-1510,External_78607,[0],introduction,This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar ( FBLTAG1 ) <TARGET_CITATION/> and HeadDriven Phrase Structure Grammar ( HPSG ) <CITATION/> by a method of grammar conversion ., This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar ( FBLTAG1 ) <TARGET_CITATION/> Phrase Structure Grammar ( HPSG ) <CITATION/> by a method of grammar conversion . This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar (FBLTAG1) <CITATION/> and HeadDriven Phrase Structure Grammar (HPSG) <CITATION/> by a method of grammar conversion.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,54cf4641d00b8a177c3b98a6e07d48620dc91388,Feature Structures Based Tree Adjoining Grammars,1988,K. Vijay-Shanker; A. Joshi
560,W06-2807,External_91606,[2],introduction,"Following the example of <TARGET_CITATION/> , we will call the autonomous units of a hypertext lexias ( from  lexicon ' ) , a word coined by <CITATION/> .","We consider hypertexts as parents of blogs and wikis. Our aim is to use the analysis of hypertexts for interesting insights, useful for blogs and wikis too. Following the example of <TARGET_CITATION/> , we will call the autonomous units of a hypertext lexias ( from  lexicon ' ) , a word coined by <CITATION/> . Following the example of <CITATION/>, we will call the autonomous units of a hypertext lexias (from lexicon'), a word coined by <CITATION/>. Our aim is to use the analysis of hypertexts for interesting insights, useful for blogs and wikis too.We consider hypertexts as parents of blogs and wikis.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,1994,George P Landow
561,J92-1004,External_4900,[0],,<TARGET_CITATION/> .,"These include agreement constraints, semantic restrictions,Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subjecttagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ''(which article), do you think I should read (t1)?'') <TARGET_CITATION/> . <CITATION/>. Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subjecttagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ''(which article), do you think I should read (t1)?'') These include agreement constraints, semantic restrictions,",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,on whmovementquot in formal syntax edited by,1977,Noam Chomsky
562,D08-1004,W02-1011,[2],method,We use the same set of binary features as in previous work on this dataset <TARGET_CITATION/> .,"A CRF is just another conditional loglinear model:where f() extracts a feature vector from a classified  document,  are the corresponding weights of those features, and Z(x) def  Ey u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset <TARGET_CITATION/> . We use the same set of binary features as in previous work on this dataset <CITATION/>. where f() extracts a feature vector from a classified  document,  are the corresponding weights of those features, and Z(x) def  Ey u(x, y) is a normalizer. A CRF is just another conditional loglinear model:",14e2aec7e25d8880a851a547cf8d27a9721f8e6c,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,2008,Omar Zaidan; Jason Eisner,12d0353ce8b41b7e5409e5a4a611110aee33c7bc,Thumbs up? Sentiment Classification using Machine Learning Techniques,2002,B. Pang; Lillian Lee; Shivakumar Vaithyanathan
563,W04-1610,External_35944,[0],experiments,"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) <TARGET_CITATION/> , minimum description length principal <CITATION/> , and the X2 statistic .","Crossvalidation, using feature selectionFeature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) <TARGET_CITATION/> , minimum description length principal <CITATION/> , and the X2 statistic . Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) <CITATION/>, minimum description length principal <CITATION/>, and the X2 statistic. Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Crossvalidation, using feature selection",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,automatic indexing based on bayesian inference networksquot,1993,K Tzeras; S Hartman
564,W06-1639,External_63492,[0],related work,"Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit <TARGET_CITATION/> ."," Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit <TARGET_CITATION/> . Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,64030e7c4a39e30248a0d4728af3e009fb72768e,Multidimensional text analysis for eRulemaking,2006,Namhee Kwon; Stuart W. Shulman; E. Hovy
565,D11-1138,D10-1069,[4],experiments,<TARGET_CITATION/> observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .,"Another application of the augmentedloss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. <TARGET_CITATION/> observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . <CITATION/> observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank. Consider, for example, the case of questions. Another application of the augmentedloss framework is to improve parser domain portability in the presence of partially labeled data.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,6c9b8a8d8b615d7b70a08f8bfaa66d65c8b93d3a,Uptraining for Accurate Deterministic Question Parsing,2010,Slav Petrov; Pi-Chuan Chang; Michael Ringgaard; H. Alshawi
566,D08-1007,External_4260,[0],experiments,"<TARGET_CITATION/> ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat .","In particular, the weights on the verb cooccurrence features (Section 3.3.1) provide a highquality, argumentspecific similarityranking of other verb contexts. The DSP parameters for eat, for example, place high weight on features like Pr(nlbraise), Pr(nlration), and Pr(nlgarnish). <TARGET_CITATION/> ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat . <CITATION/>'s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14), because these have similar subjects to eat. The DSP parameters for eat, for example, place high weight on features like Pr(nlbraise), Pr(nlration), and Pr(nlgarnish). In particular, the weights on the verb cooccurrence features (Section 3.3.1) provide a highquality, argumentspecific similarityranking of other verb contexts.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
567,J03-3004,J87-3009,[0],introduction, Learnability <TARGET_CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization ( Sch  aler 1996 ),"Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: Learnability <TARGET_CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization ( Sch  aler 1996 )  Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization <CITATION/>More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,6c5ee06bf5fb7066a3a8c5128c5538b4888cda0b,The Self-Extending Phrasal Lexicon,1987,U. Zernik; M. Dyer
568,W06-2807,External_98155,[0],introduction,"For example , a  web page ' is more similar to an infinite canvas than a written page <TARGET_CITATION/> .","For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation  i.e. literary criticism  unlike in the previous times <CITATION/>. Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web <CITATION/>. For example , a  web page ' is more similar to an infinite canvas than a written page <TARGET_CITATION/> . For example, a web page' is more similar to an infinite canvas than a written page <CITATION/>. Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web <CITATION/>. For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation  i.e. literary criticism  unlike in the previous times <CITATION/>.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,6d0ce443cee21915038992df97c707dce792ef83,Understanding Comics: The Invisible Art,2018,Byu Scholarsarchive; A. Manning; Scott McCloud
571,J92-1004,External_21532,[4],,This approach resembles the work by <TARGET_CITATION/> on selectional restrictions .,"In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by <TARGET_CITATION/> on selectional restrictions . This approach resembles the work by <CITATION/> on selectional restrictions. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,grammaticallybased automatic word class formationquot,1975,L Hirschman; R Grishman; N Sager
572,W06-2933,External_5585,[4],experiments,"This is noticeable for German <TARGET_CITATION/> and Portuguese <CITATION/> , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B  ohmov  a et al. , 2003 ) , Dutch ( van der <CITATION/> ) and Slovene <CITATION/> , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .","Japanese <CITATION/>, despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances. A second observation is that a high proportion of nonprojective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German <TARGET_CITATION/> , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B  ohmov  a et al. , 2003 ) , Dutch ( van der <CITATION/> ) and Slovene <CITATION/> , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . This is noticeable for German <CITATION/> and Portuguese <CITATION/>, which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech <CITATION/>, Dutch (van der <CITATION/>) and Slovene <CITATION/>, where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. A second observation is that a high proportion of nonprojective structures leads to fragmentation in the parser output, reflected in lower precision for roots. Japanese <CITATION/>, despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,0a3713b0e86b282b1821e29688d3b28a7bfbc3e5,The TIGER Treebank,2001,Sabine Brants; S. Dipper; Silvia Hansen; Wolfgang Lezius; George Smith
573,W01-1510,External_10229,[0],introduction,LTAG <TARGET_CITATION/> is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera , LTAG <TARGET_CITATION/> is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera LTAG <CITATION/> is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,5cd28d8da08176bc22eb3a33fa4a68d282ef0cd8,Parsing Strategies with ‘Lexicalized’ Grammars: Application to Tree Adjoining Grammars,1988,Yves Schabes; Abeillé; A. Joshi
574,P11-1134,W10-0734,[2],experiments,"Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by <TARGET_CITATION/> .","The dataset used for our experiments is an EnglishSpanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by <TARGET_CITATION/> . Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 (MTurk), adopting the methodology proposed by <CITATION/>. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). The dataset used for our experiments is an EnglishSpanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,9ada8a9538e2566e2c93accc71000d9a6ad18df0,Creating a Bi-lingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush,2010,Matteo Negri; Yashar Mehdad
575,W04-1610,External_69209,[3],related work,"This work is a continuation of that initiated in <TARGET_CITATION/> , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .","The present work evaluates the performance on Arabic documents of the Nave Bayes algorithm (NB) one of the simplest algorithms applied to English document categorization <CITATION/>. The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in <TARGET_CITATION/> , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . This work is a continuation of that initiated in <CITATION/>, which reports an overall NB classification correctness of 75.6%, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories (the data set is collected from different Arabic portals). The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. The present work evaluates the performance on Arabic documents of the Nave Bayes algorithm (NB) one of the simplest algorithms applied to English document categorization <CITATION/>.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,toward an arabic web page classifierquot master project,2001,M Yahyaoui
576,Q13-1020,External_10658,[4],related work,Our previous work <TARGET_CITATION/> designed an EMbased method to construct unsupervised trees for treebased translation models .,"In this study, we move in a new direction to build a treebased translation model with effective unsupervised Utree structures. For unsupervised tree structure induction, <CITATION/> adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic prereordering. Our previous work <TARGET_CITATION/> designed an EMbased method to construct unsupervised trees for treebased translation models . Our previous work <CITATION/> designed an EMbased method to construct unsupervised trees for treebased translation models. For unsupervised tree structure induction, <CITATION/> adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic prereordering. In this study, we move in a new direction to build a treebased translation model with effective unsupervised Utree structures.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,,treebased translation without using parse trees,2012,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong
577,Q13-1020,D07-1078,[2],experiments,"Then , we binarize the English parse trees using the head binarization approach <TARGET_CITATION/> and use the resulting binary parse trees to build another s2t system .","We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser <CITATION/>. Then , we binarize the English parse trees using the head binarization approach <TARGET_CITATION/> and use the resulting binary parse trees to build another s2t system . Then, we binarize the English parse trees using the head binarization approach <CITATION/> and use the resulting binary parse trees to build another s2t system. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser <CITATION/>. We then obtain the composed rules by composing two or three adjacent minimal rules.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,d07db07e9f98e3dd97544bd835619357683fc936,Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy,2007,Wei Wang; Kevin Knight; D. Marcu
578,J06-2002,External_3882,[0],,This is the strongest version of the sorites paradox <TARGET_CITATION/> .,"Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between observationally indifferent'' entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox <TARGET_CITATION/> . This is the strongest version of the sorites paradox <CITATION/>. A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between observationally indifferent'' entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,0030f3846dac9b3f41a20fff3ba254a8868f9723,Constructionalization and the Sorites Paradox,2020,S. Flach
580,P11-1134,N10-1146,[0],introduction,"Crosslingual Textual Entailment ( CLTE ) has been proposed by <TARGET_CITATION/> as an extension of Textual Entailment <CITATION/> that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T ."," Crosslingual Textual Entailment ( CLTE ) has been proposed by <TARGET_CITATION/> as an extension of Textual Entailment <CITATION/> that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T . Crosslingual Textual Entailment (CLTE) has been proposed by <CITATION/> as an extension of Textual Entailment <CITATION/> that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,4876b9d79886960e034a5d52adcdad640b363c76,Syntactic/Semantic Structures for Textual Entailment Recognition,2010,Yashar Mehdad; Alessandro Moschitti; Fabio Massimo Zanzotto
581,D13-1115,External_7146,[0],introduction,"Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning <TARGET_CITATION/> .","The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textualinformation alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning <TARGET_CITATION/> . Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. The language grounding problem has come in many different flavors with just as many different approaches. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textualinformation alone is insufficient for a complete understanding of language.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,ee710964af56cdbe5f2d494343b06898cd3b87f1,Learning Language Semantics from Ambiguous Supervision,2007,Rohit J. Kate; R. Mooney
583,W06-1705,External_25124,[0],related work,"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp <CITATION/> , KWiCFinder <TARGET_CITATION/> and the Linguist 's Search Engine <CITATION/> .","<CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler. <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp <CITATION/> , KWiCFinder <TARGET_CITATION/> and the Linguist 's Search Engine <CITATION/> . Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp <CITATION/>, KWiCFinder <CITATION/> and the Linguist's Search Engine <CITATION/>. <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. <CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,facilitating the compilation and dissemination of adhoc web corpora,2004,W H Fletcher
587,W06-3309,External_1054,[0],introduction,"Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , <TARGET_CITATION/> .","Building on the work of <CITATION/> in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cfXXX <CITATION/>. Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , <TARGET_CITATION/> . Discriminative approaches (especially SVMs) have been shown to be very effective for many supervised classification tasks; see, for example, <CITATION/>. Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Building on the work of <CITATION/> in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cfXXX <CITATION/>.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,90929a6aa901ba958eb4960aeeb594c752e08369,On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes,2001,A. Ng; Michael I. Jordan
588,D08-1007,J03-3005,[2],experiments,"Also , the <TARGET_CITATION/> approach will be undefined if the pair is unobserved on the web .","not be able to provide a score for each example. The similaritysmoothed examples will be undefined if SIMS(w) is empty. Also , the <TARGET_CITATION/> approach will be undefined if the pair is unobserved on the web . Also, the <CITATION/> approach will be undefined if the pair is unobserved on the web. The similaritysmoothed examples will be undefined if SIMS(w) is empty. not be able to provide a score for each example.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,5dfed29550d75cca99019aa52d40038dcb23b3cb,Using the Web to Obtain Frequencies for Unseen Bigrams,2003,Frank Keller; Mirella Lapata
589,J09-4010,External_60689,[4],,"The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms , such as expert systems <TARGET_CITATION/> and casebased reasoning <CITATION/> ."," The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms , such as expert systems <TARGET_CITATION/> and casebased reasoning <CITATION/> . The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms, such as expert systems <CITATION/> and casebased reasoning <CITATION/>.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml,1995,A Barr; S Tessler
590,D08-1007,External_244,[2],method,We measure this association using pointwise Mutual Information ( MI ) <TARGET_CITATION/> .,"To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text. To create the positives, we automatically parse a large corpus, and then extract the predicateargument pairs that have a statistical association in this data. We measure this association using pointwise Mutual Information ( MI ) <TARGET_CITATION/> . We measure this association using pointwise Mutual Information (MI) <CITATION/>. To create the positives, we automatically parse a large corpus, and then extract the predicateargument pairs that have a statistical association in this data. To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,9e2caa39ac534744a180972a30a320ad0ae41ea3,"Word Association Norms, Mutual Information, and Lexicography",1989,Kenneth Ward Church; Patrick Hanks
591,W06-3813,External_1008,[0],related work,Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <TARGET_CITATION/> ., Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <TARGET_CITATION/> . Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.,f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,547f23597f9ec8a93f66cedaa6fbfb73960426b1,The Berkeley FrameNet Project,1998,Collin F. Baker; C. Fillmore; John B. Lowe
592,J02-3002,External_5854,[2],,"There are two corpora normally used for evaluation in a number of textprocessing tasks : the Brown corpus <TARGET_CITATION/> and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank <CITATION/> .","In this case apart from the error rate (which corresponds to precision or accuracy as 1error rate) we also measure the system's coverage or recall2.1 Corpora for Evaluation There are two corpora normally used for evaluation in a number of textprocessing tasks : the Brown corpus <TARGET_CITATION/> and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank <CITATION/> . There are two corpora normally used for evaluation in a number of textprocessing tasks: the Brown corpus <CITATION/> and the Wall Street Journal (WSJ) corpus, both part of the Penn Treebank <CITATION/>. 2.1 Corpora for EvaluationIn this case apart from the error rate (which corresponds to precision or accuracy as 1error rate) we also measure the system's coverage or recall",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,729316fbded86763104f3412cadf98f00a9a3993,FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR,1983,W. Francis; H. Kucera; Andrew Mackie
593,W06-1639,External_17783,[0],introduction,or quotation of messages in emails or postings ( see <CITATION/> but cfXXX <TARGET_CITATION/> ) .,"Indeed, in other settings (e.g., a moviediscussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (I second that!'') or quotation of messages in emails or postings ( see <CITATION/> but cfXXX <TARGET_CITATION/> ) . or quotation of messages in emails or postings (see <CITATION/> but cfXXX <CITATION/>). tween two speakers, such as explicit assertions (I second that!'') Indeed, in other settings (e.g., a moviediscussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,22291dcd8b5255b6d0d51a86138ee5bf57b34550,Mining newsgroups using networks arising from social behavior,2003,R. Agrawal; S. Rajagopalan; R. Srikant; Yirong Xu
594,D13-1115,External_32478,[0],method,<TARGET_CITATION/> extend LDA to allow for the inference of document and topic distributions in a multimodal corpus ., <TARGET_CITATION/> extend LDA to allow for the inference of document and topic distributions in a multimodal corpus . <CITATION/> extend LDA to allow for the inference of document and topic distributions in a multimodal corpus.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
596,J92-1004,External_5589,[2],,"The search algorithm is the standard Viterbi search <TARGET_CITATION/> , except that the match involves a networktonetwork alignment problem rather than sequencetosequence .","The recognizer for these systems is the SUMMIT system <CITATION/>, which uses a segmentalbased framework and includes an auditory model in the frontend processing. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search <TARGET_CITATION/> , except that the match involves a networktonetwork alignment problem rather than sequencetosequence . The search algorithm is the standard Viterbi search <CITATION/>, except that the match involves a networktonetwork alignment problem rather than sequencetosequence. The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The recognizer for these systems is the SUMMIT system <CITATION/>, which uses a segmentalbased framework and includes an auditory model in the frontend processing.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,145c0b53514b02bdc3dadfb2e1cea124f2abd99b,Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,1967,A. Viterbi
597,J02-3002,W99-0612,[0],,"Since then this idea has been applied to several tasks , including word sense disambiguation <CITATION/> and namedentity recognition <TARGET_CITATION/> .","It has been applied not only to the identification of proper names, as described in this article, but also to their classification <CITATION/>. <CITATION/> showed that words strongly tend to exhibit only one sense in a document or discourse (one sense per discourse''). Since then this idea has been applied to several tasks , including word sense disambiguation <CITATION/> and namedentity recognition <TARGET_CITATION/> . Since then this idea has been applied to several tasks, including word sense disambiguation <CITATION/> and namedentity recognition <CITATION/>. <CITATION/> showed that words strongly tend to exhibit only one sense in a document or discourse (one sense per discourse''). It has been applied not only to the identification of proper names, as described in this article, but also to their classification <CITATION/>.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,5ee96af7e166dd833fcfc43dec290115fac4e7d2,Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence,1999,Silviu Cucerzan; David Yarowsky
598,J09-4010,External_380,[2],method,7 We employed the LIBSVM package <TARGET_CITATION/> .,"During the6 For SentPred we also experimented with grammatical and sentencebased syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person <CITATION/>, but the simple binary bagoflemmas representation yielded similar results. 7 We employed the LIBSVM package <TARGET_CITATION/> . 7 We employed the LIBSVM package <CITATION/>.6 For SentPred we also experimented with grammatical and sentencebased syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person <CITATION/>, but the simple binary bagoflemmas representation yielded similar results. During the",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,libsvm a library for support vector machines software available at httpwwwcsientuedutw cjlinlibsvm,2001,C-C Chang; C-J Lin
599,P07-1068,P04-1018,[4],introduction,"coreference performance on perfect mentions ( e.g. , Incorporate the two knowledge sources in a <TARGET_CITATION/> ) ; and for those that do report percoreference resolver .","More importantly, the ACE participants SC is OTHERS, and YES otherwise. This KS could do not evaluate the role of induced SC knowledge be useful for ACE coreference, since ACE is conin coreference resolution: many of them evaluate cerned with resolving only NPs that are mentions. coreference performance on perfect mentions ( e.g. , Incorporate the two knowledge sources in a <TARGET_CITATION/> ) ; and for those that do report percoreference resolver . coreference performance on perfect mentions (e.g., Incorporate the two knowledge sources in a <CITATION/>); and for those that do report percoreference resolver. This KS could do not evaluate the role of induced SC knowledge be useful for ACE coreference, since ACE is conin coreference resolution: many of them evaluate cerned with resolving only NPs that are mentions. More importantly, the ACE participants SC is OTHERS, and YES otherwise.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,4f8dd94c1a1454cc34475a4f533e137c7e4afd8d,A Mention-Synchronous Coreference Resolution Algorithm Based On the Bell Tree,2004,Xiaoqiang Luo; Abraham Ittycheriah; Hongyan Jing; N. Kambhatla; S. Roukos
602,J86-1002,External_33231,[4],,Another dialogue acquisition system has been developed by <TARGET_CITATION/> .,"The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by <CITATION/> where program flowcharts were constructed from traces of their behaviors. However, the flowcharts'' in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by <TARGET_CITATION/> . Another dialogue acquisition system has been developed by <CITATION/>. However, the flowcharts'' in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by <CITATION/> where program flowcharts were constructed from traces of their behaviors.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,the dialogue designing dialogue system dissertation,1984,T-P Ho
603,J05-3003,External_12085,[0],,Lexical functional grammar <TARGET_CITATION/> is a member of the family of constraintbased grammars ., Lexical functional grammar <TARGET_CITATION/> is a member of the family of constraintbased grammars . Lexical functional grammar <CITATION/> is a member of the family of constraintbased grammars.,ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexical functional grammar volume 34 of syntax and semantics,2001,Mary Dalrymple
604,D11-1138,N09-1028,[4],experiments,1Our rules are similar to those from <TARGET_CITATION/> .,"We evaluate our results on an evaluation set of 6338 examples of similarly created reordering data. The reordering cost, evaluation 1Our rules are similar to those from <TARGET_CITATION/> . 1Our rules are similar to those from <CITATION/>.The reordering cost, evaluationWe evaluate our results on an evaluation set of 6338 examples of similarly created reordering data.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,7d7a1f1412a0b078b29278b94cf4c5ac75f36d53,Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages,2009,P. Xu; Jaeho Kang; Michael Ringgaard; F. Och
605,W06-1104,H05-1077,[0],related work,"In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im <TARGET_CITATION/> ) .","<CITATION/> annotated a larger set of word pairs (353), too. They used a 010 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im <TARGET_CITATION/> ) . In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im <CITATION/>). They used a 010 range of relatedness scores, but did not give further details about their experimental setup. <CITATION/> annotated a larger set of word pairs (353), too.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,2ccf14b257c0e68563121a9551ed24070e3f89a6,Identifying Semantic Relations and Functional Properties of Human Verb Associations,2005,Sabine Schulte im Walde; Alissa Melinger
607,P10-4003,External_8072,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,da4e2c1b96d4f962ddce7c19a77c5131f594f07d,A Natural Language Tutorial Dialogue System for Physics,2006,Pamela W. Jordan; Maxim Makatchev; Umarani Pappuswamy; K. VanLehn; Patricia L. Albacete
608,W06-1705,External_25125,[0],related work,The use of the web as a corpus for teaching and research on language has been proposed a number of times <TARGET_CITATION/> and received a special issue of the journal Computational Linguistics <CITATION/> .,"Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times <TARGET_CITATION/> and received a special issue of the journal Computational Linguistics <CITATION/> . The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,concordancing the web with kwicfinder third north american,2001,W H Fletcher
609,P10-2059,External_88747,[0],introduction,Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see <TARGET_CITATION/> for an overview ) ., Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see <TARGET_CITATION/> for an overview ) . Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see <CITATION/> for an overview).,b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,edcbe6dc3b8f94eadb8999db8b7ef203ac0d4712,Linguistic functions of head movements in the context of speech,2000,Evelyn Z. McClave
610,D12-1037,External_21949,[4],related work,"Our method resorts to some translation examples , which is similar as examplebased translation or translation memory <TARGET_CITATION/> .","Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the test sentences. Our method resorts to some translation examples , which is similar as examplebased translation or translation memory <TARGET_CITATION/> . Our method resorts to some translation examples, which is similar as examplebased translation or translation memory <CITATION/>. One of the advantages is that it can adapt the weights for each of the test sentences. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,,examplebased decoding for statistical machine translation,2003,Taro Watanabe; Eiichiro Sumita
611,K15-1002,External_16514,[4],experiments,"Baseline Systems We choose three publicly available stateoftheart endtoend coreference systems as our baselines : Stanford system <CITATION/> , Berkeley system <TARGET_CITATION/> and HOTCoref system ( Bj  orkelund and <CITATION/> ) .","The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE2004 and OntoNotes5.0 datasets. Baseline Systems We choose three publicly available stateoftheart endtoend coreference systems as our baselines : Stanford system <CITATION/> , Berkeley system <TARGET_CITATION/> and HOTCoref system ( Bj  orkelund and <CITATION/> ) . Baseline Systems We choose three publicly available stateoftheart endtoend coreference systems as our baselines: Stanford system <CITATION/>, Berkeley system <CITATION/> and HOTCoref system <CITATION/>. 3.1.1 can be verified empirically on both ACE2004 and OntoNotes5.0 datasets. The nonoverlapping mention head assumption in Sec.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,28eb033eee5f51c5e5389cbb6b777779203a6778,"A Joint Model for Entity Analysis: Coreference, Typing, and Linking",2014,Greg Durrett; D. Klein
612,J86-1002,External_21719,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and <TARGET_CITATION/> ) ."," A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and <TARGET_CITATION/> ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,the hwim speech understanding system in lea,1980,J Wolf; W Woods
614,D09-1056,External_17037,[0],related work,Other representations use the link structure <TARGET_CITATION/> or generate graph representations of the extracted features <CITATION/> .,"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Other representations use the link structure <TARGET_CITATION/> or generate graph representations of the extracted features <CITATION/> . Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <CITATION/>. Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,a74f15169ef03ddd7f09dbf255aed58663e25e8b,Unsupervised Name Disambiguation via Social Network Similarity,2005,B. Malin
615,W02-0309,External_34413,[0],introduction,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> .","When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> . From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexicosemantic aspects of dederivation and decomposition <CITATION/>. This is particularly true for the medical domain. When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,f338eab7bf084f4a6b74a45fb0c076cc524dd960,The Use of Morphosemantic Regularities in the Medical Vocabulary for Automatic Lexical Coding,1984,S. Wolff
616,W02-0309,External_649,[0],conclusion,"There has been some controversy , at least for simple stemmers <TARGET_CITATION/> , about the effectiveness of morphological analysis for document retrieval <CITATION/> ."," There has been some controversy , at least for simple stemmers <TARGET_CITATION/> , about the effectiveness of morphological analysis for document retrieval <CITATION/> . There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,a651bb7cc7fc68ece0cc66ab921486d163373385,An algorithm for suffix stripping,1997,M. Porter
617,D10-1100,D09-1143,[4],conclusion,This revalidates the observation of <TARGET_CITATION/> that phrase structure representations and dependency representations add complimentary value to the learning task .,"Our experiments show that as a result of how language expresses the relevant information, dependencybased structures are best suited for encoding this information. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependencybased structures perform the best. This revalidates the observation of <TARGET_CITATION/> that phrase structure representations and dependency representations add complimentary value to the learning task . This revalidates the observation of <CITATION/> that phrase structure representations and dependency representations add complimentary value to the learning task. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependencybased structures perform the best. Our experiments show that as a result of how language expresses the relevant information, dependencybased structures are best suited for encoding this information.",330d82cedd1567515d42163b766197944adf6647,Automatic Detection and Classification of Social Events,2010,Apoorv Agarwal; Owen Rambow,a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi
618,W06-2933,W04-2407,[2],method,The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by <CITATION/> and extended to labeled dependency parsing by <TARGET_CITATION/> ., The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by <CITATION/> and extended to labeled dependency parsing by <TARGET_CITATION/> . The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by <CITATION/> and extended to labeled dependency parsing by <CITATION/>.,f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,b2fab36801910e460dc60524f731f5f0438553b4,Memory-Based Dependency Parsing,2004,Joakim Nivre; Johan Hall; Jens Nilsson
619,D08-1034,External_41700,[2],,The semantic categories of verbs and other words are extracted from the Semantic Knowledgebase of Contemporary Chinese <TARGET_CITATION/> .,"layer of the constituent in focus, the number of constituents in the ascending part of the path subtracted by the number of those in the descending part of path, e.g. if the path is PPBNFVPVP VV, the feature extracted by this template will be 1. SemCat (semantic category) of predicate, SemCat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word. The semantic categories of verbs and other words are extracted from the Semantic Knowledgebase of Contemporary Chinese <TARGET_CITATION/> . The semantic categories of verbs and other words are extracted from the Semantic Knowledgebase of Contemporary Chinese <CITATION/>. SemCat (semantic category) of predicate, SemCat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word. layer of the constituent in focus, the number of constituents in the ascending part of the path subtracted by the number of those in the descending part of path, e.g. if the path is PPBNFVPVP VV, the feature extracted by this template will be 1.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,,the specification of the semantic knowledgebase of contemporary chinese,2003,Hui Wang; Weidong Zhan; Shiwen Yu
620,J04-3001,External_18341,[2],,"Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a contextfree variant of treeadjoining grammars <CITATION/> , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism <TARGET_CITATION/> , and Collins 's Model 2 parser ( 1997 ) .","Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a contextfree variant of treeadjoining grammars <CITATION/> , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism <TARGET_CITATION/> , and Collins 's Model 2 parser ( 1997 ) . Moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models: one based on a contextfree variant of treeadjoining grammars <CITATION/>, the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism <CITATION/>, and Collins's Model 2 parser (1997). In this section, we investigate whether these observations hold true for training statistical parsing models as well. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,stochastic lexicalized contextfree grammar,1993,Yves Schabes; Richard Waters
621,J97-4003,External_69618,[0],introduction,"6 The PartialVP Topicalization Lexical Rule proposed by <TARGET_CITATION/> , 10 ) is a linguistic example .","4 This interpretation of the signature is sometimes referred to as closed world <CITATION/>. 5 An indepth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The PartialVP Topicalization Lexical Rule proposed by <TARGET_CITATION/> , 10 ) is a linguistic example . 6 The PartialVP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. 5 An indepth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 4 This interpretation of the signature is sometimes referred to as closed world <CITATION/>.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,partialvp and splitnp topicalization in german an hpsg analysis,1994,Erhard Hinrichs; Tsuneko Nakazawa
622,P13-3018,External_90443,[2],method,We measure the inter annotator agreement using the Fleiss Kappa <TARGET_CITATION/> measure ( x ) where the agreement lies around 0.79 .,"Each linguist has received 2000 verb pairs along with their respective example sentences. Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping. We measure the inter annotator agreement using the Fleiss Kappa <TARGET_CITATION/> measure ( x ) where the agreement lies around 0.79 . We measure the inter annotator agreement using the Fleiss Kappa <CITATION/> measure (x) where the agreement lies around 0.79. Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping. Each linguist has received 2000 verb pairs along with their respective example sentences.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,the measurement of interrater agreement statistical methods for rates and proportions2212–236,1981,Joseph L Fleiss; Bruce Levin; Myunghee Cho Paik
624,J01-4001,External_6228,[0],,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <TARGET_CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <TARGET_CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Charniak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form <CITATION/>; and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a, 2001b). Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,centeringinthelarge computing referential discourse segments,1997,Udo Hahn; Michael Strube
625,P00-1007,External_7068,[4],conclusion,"It is not aimed at handling dependencies , which require heavy use of lexical information ( <TARGET_CITATION/> , for PP attachment ) .","Analogously, the denominator in MBSL would be Freq(Y). The presented method concerns primarily with phrases, which can be represented by a tree structure. It is not aimed at handling dependencies , which require heavy use of lexical information ( <TARGET_CITATION/> , for PP attachment ) . It is not aimed at handling dependencies, which require heavy use of lexical information (<CITATION/>, for PP attachment). The presented method concerns primarily with phrases, which can be represented by a tree structure. Analogously, the denominator in MBSL would be Freq(Y).",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,bdaf232c561f1f50e88b1d24097e214890b37e8b,Structural Ambiguity and Lexical Relations,1991,Donald Hindle; Mats Rooth
626,D11-1138,N10-1120,[0],introduction,"This includes work on question answering <CITATION/> , sentiment analysis <TARGET_CITATION/> , MT reordering <CITATION/> , and many other tasks .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <CITATION/> , sentiment analysis <TARGET_CITATION/> , MT reordering <CITATION/> , and many other tasks . This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,da5cd00115f7ec108de8eebf071c5f3f19807df4,Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables,2010,Tetsuji Nakagawa; Kentaro Inui; S. Kurohashi
628,D14-1130,External_10048,[3],related work,Our experimental design with professional bilingual translators follows our previous work <TARGET_CITATION/> ) comparing scratch translation to postedit .,"The process study most similar to ours is that of <CITATION/>, who compared scratch, postedit, and simple interactive modes. However, he used undergraduate, nonprofessional subjects, and did not consider retuning. Our experimental design with professional bilingual translators follows our previous work <TARGET_CITATION/> ) comparing scratch translation to postedit . Our experimental design with professional bilingual translators follows our previous work <CITATION/> comparing scratch translation to postedit. However, he used undergraduate, nonprofessional subjects, and did not consider retuning. The process study most similar to ours is that of <CITATION/>, who compared scratch, postedit, and simple interactive modes.",2ef7e4d918c8f70a01a9a59746764b7003afdb40,Human Effort and Machine Learnability in Computer Aided Translation,2014,Spence Green; Sida I. Wang; Jason Chuang; Jeffrey Heer; Sebastian Schuster; Christopher D. Manning,,the efficacy of human postediting for language translation,2013,S Green; J Heer; C D Manning
629,J05-3003,External_4377,[0],,Lexical functional grammar <TARGET_CITATION/> is a member of the family of constraintbased grammars ., Lexical functional grammar <TARGET_CITATION/> is a member of the family of constraintbased grammars . Lexical functional grammar <CITATION/> is a member of the family of constraintbased grammars.,ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexicalfunctional syntax,2001,Joan Bresnan
630,J02-3002,External_24096,[0],,"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers <TARGET_CITATION/> , neural networks <CITATION/> , and maximumentropy modeling <CITATION/> .","Another wellacknowledged shortcoming of rulebased systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers <TARGET_CITATION/> , neural networks <CITATION/> , and maximumentropy modeling <CITATION/> . Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/>, neural networks <CITATION/>, and maximumentropy modeling <CITATION/>. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Another wellacknowledged shortcoming of rulebased systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,some applications of treebased modeling to speech and language indexing”,1989,Michael D Riley
632,J06-2002,External_2473,[0],introduction,We will examine the worstcase complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects <TARGET_CITATION/> ., We will examine the worstcase complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects <TARGET_CITATION/> . We will examine the worstcase complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects <CITATION/>.,0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,efficient contextsensitive generation of referring expressions,2002,Emiel Krahmer; Mari¨et Theune
633,J09-4010,External_45353,[2],method,"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion <TARGET_CITATION/> .","Hence, we keep their representation at a low level of abstraction (bagoflemmas). The idea behind the DocPred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion <TARGET_CITATION/> . In our case, the clustering is performed by the program Snob, which implements mixture modeling combined with model selection based on the Minimum Message Length (MML) criterion <CITATION/>. The idea behind the DocPred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. Hence, we keep their representation at a low level of abstraction (bagoflemmas).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,3c56e086eb04ebc4a26c760df7d1fa842c931ccc,Statistical and Inductive Inference by Minimum Message Length (Information Science and Statistics),2005,C. S. Wallace
634,W00-1312,External_11751,[4],related work,"The third approach to crosslingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) <CITATION/> , or the General Vector space model ( GVSM ) , <TARGET_CITATION/> .","Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <CITATION/>. <CITATION/> studied the issue of disambiguation for monolingual M. The third approach to crosslingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) <CITATION/> , or the General Vector space model ( GVSM ) , <TARGET_CITATION/> . The third approach to crosslingual retrieval is to map queries and documents to some intermediate representation, e.g latent semantic indexing (LSI) <CITATION/>, or the General Vector space model (GVSM), <CITATION/>. <CITATION/> studied the issue of disambiguation for monolingual M.Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <CITATION/>.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,translingual information retrieval a comparative evaluationquot,1997,J Carbonell; Y Yang; R Frederlcing; R Brown; Y Geng; D Lee
635,W03-0806,C96-2187,[0],experiments,Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) <TARGET_CITATION/> and the Alembic Workbench <CITATION/> ) as well as NLP tools and resources that can be manipulated from the GUI .,There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) <TARGET_CITATION/> and the Alembic Workbench <CITATION/> ) as well as NLP tools and resources that can be manipulated from the GUI . Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) <CITATION/> and the Alembic Workbench <CITATION/>) as well as NLP tools and resources that can be manipulated from the GUI. There are a number of generalised NLP systems in the literature.,7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,7321ae1b6dc03f4575b9f1acee6c69b1072e45f8,GATE-a General Architecture for Text Engineering,1996,H. Cunningham; Yorick Wilks; R. Gaizauskas
636,D13-1115,External_9236,[2],experiments,"The keypoints are clustered into 5,000 visual codewords ( centroids ) using kmeans clustering <TARGET_CITATION/> , and images are then quantized over the 5,000 codewords .","It is faster and more forgiving than the commonly known SIFT algorithm. We compute SURF keypoints for every image in our data set using SimpleCV3 and randomly sample 1% of the keypoints. The keypoints are clustered into 5,000 visual codewords ( centroids ) using kmeans clustering <TARGET_CITATION/> , and images are then quantized over the 5,000 codewords . The keypoints are clustered into 5,000 visual codewords (centroids) using kmeans clustering <CITATION/>, and images are then quantized over the 5,000 codewords. We compute SURF keypoints for every image in our data set using SimpleCV3 and randomly sample 1% of the keypoints. It is faster and more forgiving than the commonly known SIFT algorithm.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,webscale kmeans clustering,2010,D Sculley
637,P00-1007,W98-1117,[0],introduction,Another approach for partial parsing was presented by <TARGET_CITATION/> .,"The output of NP and VP chunking was used as an input to grammatical relation inference. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. Another approach for partial parsing was presented by <TARGET_CITATION/> . Another approach for partial parsing was presented by <CITATION/>. The inferences process is cascaded, and a clear improvement was obtained by passing results across cascades. The output of NP and VP chunking was used as an input to grammatical relation inference.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,2f21c600925122bfd7e31135a6dca51f90ee9cfa,A Maximum-Entropy Partial Parser for Unrestricted Text,1998,Wojciech Skut; T. Brants
638,P07-1068,J01-4003,[0],introduction,"In knowledgelean approaches , coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process ( e.g. , <TARGET_CITATION/> ) .","In the past decade, knowledgelean approaches have significantly influenced research in noun phrase (NP) coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document. In knowledgelean approaches , coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process <TARGET_CITATION/> . In knowledgelean approaches, coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process (e.g., <CITATION/>). In the past decade, knowledgelean approaches have significantly influenced research in noun phrase (NP) coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,c70156cd37773f40d3c8a43c6ac3135499fe8891,A Corpus-Based Evaluation of Centering and Pronoun Resolution,2001,Joel R. Tetreault
640,D13-1115,D12-1130,[4],experiments,This result is consistent with other works using this model with these features <TARGET_CITATION/> .,Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the textonly model (twotailed ttest). This result is consistent with other works using this model with these features <TARGET_CITATION/> . This result is consistent with other works using this model with these features <CITATION/>. The 2D models employing feature norms and association norms do significantly better than the textonly model (twotailed ttest). Table 1 shows our results for each of our selected models with our compositionality evaluation.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,d4149dbef949644dad4833012e2def98529c0241,Grounded Models of Semantic Representation,2012,Carina Silberer; Mirella Lapata
641,W04-0910,External_40271,[0],,Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <TARGET_CITATION/> .,"Constructing paraphrastic sets. Depending on the type of paraphrastic means involved, constructing a paraphrastic set (the set of all lexical items related by a paraphrastic link be it parallel, shuffling or definitional) is more or less easy as resources for that specific means may or may not be readily available. Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <TARGET_CITATION/> . Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <CITATION/>. Depending on the type of paraphrastic means involved, constructing a paraphrastic set (the set of all lexical items related by a paraphrastic link be it parallel, shuffling or definitional) is more or less easy as resources for that specific means may or may not be readily available. Constructing paraphrastic sets.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,modlisation et traitement informatique de la synonymi linguisticae investigationes,1997,S Ploux
642,P10-2026,N09-2005,[1],related work,The feature of head word trigger which we apply to the loglinear model is motivated by the triggerbased approach <TARGET_CITATION/> .,"The basic difference of our method from <CITATION/> is that we keep rules that both sides should be relaxedwellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the loglinear model is motivated by the triggerbased approach <TARGET_CITATION/> . The feature of head word trigger which we apply to the loglinear model is motivated by the triggerbased approach <CITATION/>. Besides, our system complexity is not increased because no additional language model is introduced. The basic difference of our method from <CITATION/> is that we keep rules that both sides should be relaxedwellformed dependency structure, not just the target side.",b260ff607d3fd634c0d819d841a7e79c88a04700,Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules,2010,Zhiyang Wang; Yajuan Lü; Qun Liu; Young-Sook Hwang,d1dfeb007ed551aab4f4ec89bf3fbd5a56df6646,Comparison of Extended Lexicon Models in Search and Rescoring for SMT,2009,Sasa Hasan; H. Ney
643,D13-1115,Q13-1003,[0],related work,Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <TARGET_CITATION/> .,"More recently, <CITATION/> show that visual attribute classifiers, which have been immensely successful in object recognition <CITATION/>, act as excellent substitutes for featurenorms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <TARGET_CITATION/> . Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>. norms. More recently, <CITATION/> show that visual attribute classifiers, which have been immensely successful in object recognition <CITATION/>, act as excellent substitutes for feature",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,21b3007f967d39e1346bc91e0fc8b3f16121300c,Grounding Action Descriptions in Videos,2013,Michaela Regneri; Marcus Rohrbach; Dominikus Wetzel; Stefan Thater; B. Schiele; Manfred Pinkal
644,D11-1138,P05-1012,[2],experiments,based parsing algorithms with an arcfactored parameterization <TARGET_CITATION/> .,All feature conjunctions are included. Graphbased: An implementation of graph based parsing algorithms with an arcfactored parameterization <TARGET_CITATION/> . based parsing algorithms with an arcfactored parameterization <CITATION/>.  Graphbased: An implementation of graphAll feature conjunctions are included.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,d3b27746f7a53f2dc5d9b8c2f3d343313622ec36,Online Large-Margin Training of Dependency Parsers,2005,Ryan T. McDonald; K. Crammer; Fernando C Pereira
645,J87-3002,C86-1066,[0],,"No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and <TARGET_CITATION/> ) .","Extending the system to handle nouns, adjectives and adverbs would present no problems of principle. However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system. No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and <TARGET_CITATION/> ) . No attempt has been made to map any closed class entries from LDOCE, as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser (see the Introduction and <CITATION/>). However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system. Extending the system to handle nouns, adjectives and adverbs would present no problems of principle.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,2bb9a366ddf710d678b06c6c84fc97781c886624,A Dictionary and Morphological Analyser for English,1986,G. Russell; S. Pulman; G. Ritchie; A. Black
646,J87-3002,External_15955,[0],introduction,In this paper we focus on the exploitation of the LDOCE grammar coding system ; <TARGET_CITATION/> describe further research in Cambridge utilising different types of information available in LDOCE .,"Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled core' vocabulary in defining the words throughout the dictionary. (<CITATION/> contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system ; <TARGET_CITATION/> describe further research in Cambridge utilising different types of information available in LDOCE . In this paper we focus on the exploitation of the LDOCE grammar coding system; <CITATION/> describe further research in Cambridge utilising different types of information available in LDOCE. (<CITATION/> contains further description and discussion of LDOCE.) Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled core' vocabulary in defining the words throughout the dictionary.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,towards a lexicon support environment for real time parsing,1985,Hiyan Alshawi; Branimir Boguraev; Ted Briscoe
647,J02-3002,A94-1013,[0],,"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/> , neural networks <TARGET_CITATION/> , and maximumentropy modeling <CITATION/> .","Another wellacknowledged shortcoming of rulebased systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/> , neural networks <TARGET_CITATION/> , and maximumentropy modeling <CITATION/> . Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers <CITATION/>, neural networks <CITATION/>, and maximumentropy modeling <CITATION/>. Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Another wellacknowledged shortcoming of rulebased systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,2510c7da837cf4ad083a6aa97a857e524cb4f142,Adaptive Sentence Boundary Disambiguation,1994,D. Palmer; Marti A. Hearst
648,J02-3002,External_62865,[4],,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's <TARGET_CITATION/> , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's <TARGET_CITATION/> , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992], Brill's [Brill 1995a], and MaxEnt [Ratnaparkhi 1996]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. This requires resolving sentence boundaries before tagging. tagger operates on text spans that form a sentence.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging,1995,Eric Brill
649,W04-0910,External_35508,[0],introduction,"Similarly , <TARGET_CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , <TARGET_CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . Similarly, <CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,7c8b067649f09b9260569d6e398c5c1b16fa2712,Automatic paraphrase acquisition from news articles,2002,Yusuke Shinyama; S. Sekine; Kiyoshi Sudo
650,D08-1006,J92-4003,[5],conclusion,"In future work we plan to experiment with richer representations , e.g. including longrange ngrams <CITATION/> , class ngrams <TARGET_CITATION/> , grammatical features <CITATION/> , etc ' .","In our work, the neighborhood' is determined automatically and dynamically as learning proceeds, according to the capabilities of the classifiers used. The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations , e.g. including longrange ngrams <CITATION/> , class ngrams <TARGET_CITATION/> , grammatical features <CITATION/> , etc ' . In future work we plan to experiment with richer representations, e.g. including longrange ngrams <CITATION/>, class ngrams <CITATION/>, grammatical features <CITATION/>, etc'. The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In our work, the neighborhood' is determined automatically and dynamically as learning proceeds, according to the capabilities of the classifiers used.",00dc508fdf5dcbf78bee0ea779aad408830c20e2,Refining Generative Language Models using Discriminative Learning,2008,Ben Sandbank,3de5d40b60742e3dfa86b19e7f660962298492af,Class-Based n-gram Models of Natural Language,1992,P. Brown; V. D. Pietra; P. D. Souza; J. Lai; R. Mercer
653,W11-1410,W10-1502,[3],,"We follow our previous work <TARGET_CITATION/> in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also <CITATION/> ) .","For actual system performance, we evaluate both steps. In selecting features for Korean, we have to account for relatively free word order <CITATION/>. We follow our previous work <TARGET_CITATION/> in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also <CITATION/> ) . We follow our previous work <CITATION/> in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also <CITATION/>). In selecting features for Korean, we have to account for relatively free word order <CITATION/>. For actual system performance, we evaluate both steps.",4ffe7ca323e80dea893899c36de4dc920852878b,Developing Methodology for Korean Particle Error Detection,2011,Markus Dickinson; Ross Israel; Sun-Hee Lee,38b5f6ce69fb4f1877a74c2d1b817e52a862595b,Building a Korean Web Corpus for Analyzing Learner Language,2010,Markus Dickinson; Ross Israel; Sun-Hee Lee
655,J05-3003,External_12085,[0],,"According to <TARGET_CITATION/> , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .","In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS((r SUBJ)(r OBLon)). The argument list can be empty, as in the PRED value for judge in Figure 1. According to <TARGET_CITATION/> , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ . According to <CITATION/>, LFG assumes the following universally available inventory of grammatical functions: SUBJ(ect), OBJ(ect), OBJe, COMP, XCOMP, OBL(ique)e, ADJ(unct), XADJ. The argument list can be empty, as in the PRED value for judge in Figure 1. In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS((r SUBJ)(r OBLon)).",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexical functional grammar volume 34 of syntax and semantics,2001,Mary Dalrymple
657,J97-4003,External_91302,[0],introduction,4 This interpretation of the signature is sometimes referred to as closed world <TARGET_CITATION/> .,"Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world <TARGET_CITATION/> . 4 This interpretation of the signature is sometimes referred to as closed world <CITATION/>. To avoid confusion, we will only use the terminology introduced in the text. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,b4aa242db89cafec2ee3568a147a944ec0cc42e0,Open and Closed World Types in NLP Systems,1995,D. Gerdemann
658,D08-1007,External_19410,[4],method,"Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models <TARGET_CITATION/> .","This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similaritysmoothed models can make use of the regularities across similar verbs, but not the finergrained stringand tokenbased features. Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models <TARGET_CITATION/> . Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models <CITATION/>. Similaritysmoothed models can make use of the regularities across similar verbs, but not the finergrained stringand tokenbased features. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,,inducing a semantically annotated lexicon via embased clustering,1999,M Rooth; S Riezler; D Prescher; G Carroll; F Beil
659,W03-0806,External_89276,[0],,There have already been several attempts to develop distributed NLP systems for dialogue systems <CITATION/> and speech recognition <TARGET_CITATION/> .,Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems <CITATION/> and speech recognition <TARGET_CITATION/> . There have already been several attempts to develop distributed NLP systems for dialogue systems <CITATION/> and speech recognition <CITATION/>. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.,7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,6a1d6ce98931447f3e06d8d69119994c38665b53,A distributed architecture for robust automatic speech recognition,2003,K. Hacioglu; B. Pellom
660,W03-0806,External_4043,[5],conclusion,"For instance , implementing an efficient version of the MXPOST POS tagger <TARGET_CITATION/> will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .","The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components. For instance , implementing an efficient version of the MXPOST POS tagger <TARGET_CITATION/> will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component . For instance, implementing an efficient version of the MXPOST POS tagger <CITATION/> will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component. The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,a maximum entropy partofspeech tagger,1996,Adwait Ratnaparkhi
661,D13-1115,External_724,[0],related work,"cue word and name the first ( or several ) associated words that come to mind ( e.g. , <CITATION/> ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept <TARGET_CITATION/> .","Within the latter category, the two most common representations have been association norms, where subjects are given a1http://stephenroller.com/research/ emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e.g. , <CITATION/> ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept <TARGET_CITATION/> . cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). 1http://stephenroller.com/research/ emnlp13Within the latter category, the two most common representations have been association norms, where subjects are given a",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,b2c04cc369b8f08f399d5fb95ddc884d52cfebd2,Semantic feature production norms for a large set of living and nonliving things,2005,K. McRae; George S. Cree; Mark S. Seidenberg; C. McNorgan
662,J90-3003,P87-1023,[0],introduction,<TARGET_CITATION/> also examine the relation between discourse and prosodic phrasing .,". To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. <TARGET_CITATION/> also examine the relation between discourse and prosodic phrasing . <CITATION/> also examine the relation between discourse and prosodic phrasing. To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. .",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,3dbadff91d174ef937420d61130418138b2749d3,Now Let’s Talk About Now; Identifying Cue Phrases Intonationally,1987,Julia Hirschberg; D. Litman
663,J05-3003,C00-2100,[4],,<TARGET_CITATION/> evaluate 914 Czech verbs against a custommade gold standard and record a token recall of 88 % .,"Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. <TARGET_CITATION/> evaluate 914 Czech verbs against a custommade gold standard and record a token recall of 88 % . <CITATION/> evaluate 914 Czech verbs against a custommade gold standard and record a token recall of 88%. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,1a9bcf583a0a0fb643ab296874e8f29ad509875d,Automatic Extraction of Subcategorization Frames for Czech,2000,Anoop Sarkar; Daniel Zeman
664,J09-4010,External_1877,[2],method,We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and Fscore ( van Rijsbergen 1979 ; <TARGET_CITATION/> ) .,"For each of the crossvalidation folds, the responses generated for the requests in the test split are compared against the actual responses generated by helpdesk operators for these requests. Although this method of assessment is less informative than humanbased evaluations, it enables us to evaluate the performance of our system with substantial amounts of data, and produce representative results for a large corpus such as ours. We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and Fscore ( van Rijsbergen 1979 ; <TARGET_CITATION/> ) . We use two measures from Information Retrieval to determine the quality of an automatically generated response: precision and Fscore (van Rijsbergen 1979; Salton and McGill 1983). Although this method of assessment is less informative than humanbased evaluations, it enables us to evaluate the performance of our system with substantial amounts of data, and produce representative results for a large corpus such as ours. For each of the crossvalidation folds, the responses generated for the requests in the test split are compared against the actual responses generated by helpdesk operators for these requests.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,0b94a8bfd4f4ef52d81a3cd7bda2a20bf1e412e3,"Book Review of: Introduction to modern information retrieval by C.G. Chowddhury, 3rd ed.",2010,S. Fitz-Gerald; B. Wiggins
665,J09-4010,External_45354,[4],,"The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms , such as expert systems <CITATION/> and casebased reasoning <TARGET_CITATION/> ."," The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms , such as expert systems <CITATION/> and casebased reasoning <TARGET_CITATION/> . The automation of helpdesk responses has been previously tackled using mainly knowledgeintensive paradigms, such as expert systems <CITATION/> and casebased reasoning <CITATION/>.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,applying casebased reasoning techniques for enterprise systems,1997,I Watson
666,D08-1034,W04-3212,[0],introduction,<TARGET_CITATION/> did very encouraging work on the feature calibration of semantic role labeling .,"However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. <TARGET_CITATION/> did very encouraging work on the feature calibration of semantic role labeling . <CITATION/> did very encouraging work on the feature calibration of semantic role labeling. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,80c6abf049ff5e280c57d4d28d23f3acdab80ddb,Calibrating Features for Semantic Role Labeling,2004,Nianwen Xue; Martha Palmer
667,W04-1805,External_94889,[0],related work,"This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( <TARGET_CITATION/> , for example ) , does not specify the relationship itself .","A number of applications have relied on distributional analysis <CITATION/> in order to build classes of semantically related terms. This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( <TARGET_CITATION/> , for example ) , does not specify the relationship itself . This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (<CITATION/>, for example), does not specify the relationship itself. A number of applications have relied on distributional analysis <CITATION/> in order to build classes of semantically related terms.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,symbolic word clustering for mediumsized corpora,1996,Benoit Habert; Ellie Naulleau; Adeline Nazarenko
668,W04-0910,External_11973,[0],introduction,And <TARGET_CITATION/> use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .,"For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly, <CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And <TARGET_CITATION/> use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts . And <CITATION/> use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts. Similarly, <CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,a9e21d9adf0a39440fc9f087c66ce512b908e0ea,IDENTIFYING LEXICAL PARAPHRASES FROM A SINGLE CORPUS: A CASE STUDY FOR VERBS,2003,Oren Glickman
669,J97-4003,External_98507,[0],introduction,"Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up <TARGET_CITATION/> .","(Pollard and Sag [1994, 3141, following Flickinger [19871). This idea of preserving properties can be considered an instance of the wellknown frame problem in AT <CITATION/>, and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up <TARGET_CITATION/> . Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up <CITATION/>. This idea of preserving properties can be considered an instance of the wellknown frame problem in AT <CITATION/>, and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. (Pollard and Sag [1994, 3141, following Flickinger [19871).",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,6fe64d5e31abdf3d3673decbafc7d6ddfaab1e24,"On Implementing an HPSG theory - Aspects of the logical architecture, the formalization, and the implementation of head-driven phrase structure grammars",1994,Walt Detmar Meurers
670,W06-1639,External_6360,[0],related work,"There has also been work focused upon determining the political leaning ( e.g. ,  liberal '' vs.  conservative '' ) of a document or author , where most previouslyproposed methods make no direct use of relationships between the documents to be classified ( the  unlabeled '' texts ) <TARGET_CITATION/> .","Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>. There has also been work focused upon determining the political leaning ( e.g. ,  liberal '' vs.  conservative '' ) of a document or author , where most previouslyproposed methods make no direct use of relationships between the documents to be classified ( the  unlabeled '' texts ) <TARGET_CITATION/> . There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,b479d6a38a8e8f4cf86427a756b3ad0f04bb04ab,A Preliminary Investigation into Sentiment Analysis of Informal Political Discourse,2006,Tony Mullen; Robert Malouf
671,J03-3004,External_68637,[0],introduction,"Other similar approaches include those of Cicekli and G  uvenir ( 1996 ) , <TARGET_CITATION/> , inter alia .","<CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. <CITATION/> combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G  uvenir ( 1996 ) , <TARGET_CITATION/> , inter alia . Other similar approaches include those of <CITATION/>, inter alia. <CITATION/> combines lexical and dependency mappings to form his generalizations. <CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,a languageneutral sparsedata algorithm for extracting translation patterns,1999,Kevin McTait; Arturo Trujillo
672,W04-1610,External_69209,[4],conclusion,"To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in <TARGET_CITATION/> ."," To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in <TARGET_CITATION/> . To sum up, this work has been carried out to automatically classify Arabic documents using the NB algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in <CITATION/>.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,toward an arabic web page classifierquot master project,2001,M Yahyaoui
673,A00-2028,P99-1040,[3],conclusion,Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance <TARGET_CITATION/> .,"Even with current accuracy rates, the improved ability to predict problematic dialogues means that it may be possible to field the system without human agent oversight, and we expect to be able to improve these results. The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance <TARGET_CITATION/> . Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance <CITATION/>. The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Even with current accuracy rates, the improved ability to predict problematic dialogues means that it may be possible to field the system without human agent oversight, and we expect to be able to improve these results.",628176f850f7ba7bb0db10c9e458d80b1cd46766,Learning to Predict Problematic Situations in a Spoken Dialogue System: Experiments with How May I Help You?,2000,M. Walker; Irene Langkilde-Geary; Jeremy H. Wright; A. Gorin; D. Litman,22404884bd6e2dc80691b968a1e571f873b0a774,Automatic Detection of Poor Speech Recognition at the Dialogue Level,1999,D. Litman; M. Walker; Michael Kearns
674,J15-3005,J07-3004,[2],introduction,CCGBank <TARGET_CITATION/> is used to train the model .,"We will also investigate the possibility of applying dynamicprogrammingstyle pruning to the chart. We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder. CCGBank <TARGET_CITATION/> is used to train the model . CCGBank <CITATION/> is used to train the model. We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder. We will also investigate the possibility of applying dynamicprogrammingstyle pruning to the chart.",f67ec8d10f04442c55ada6821031cf39e06aaa8e,Discriminative Syntax-Based Word Ordering for Text Generation,2015,Yue Zhang; S. Clark,4c4dcf6655204130f330002a9fb45c4fd436d5ea,CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank,2007,J. Hockenmaier; Mark Steedman
675,W04-1805,W03-1802,[0],related work,"More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : <TARGET_CITATION/> uses derivational morphology ; <CITATION/> use , as a starting point , a number of identical characters .","This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (<CITATION/>, for example), does not specify the relationship itself. Hence, synonyms, cohyponyms, hyperonyms, etc. are not differentiated. More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : <TARGET_CITATION/> uses derivational morphology ; <CITATION/> use , as a starting point , a number of identical characters . More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms: <CITATION/> uses derivational morphology; <CITATION/> use, as a starting point, a number of identical characters. Hence, synonyms, cohyponyms, hyperonyms, etc. are not differentiated. This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (<CITATION/>, for example), does not specify the relationship itself.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,7eb53b3eef6fec46a01caa1be23d071881cb3341,Conceptual Structuring through Term Variations,2003,B. Daille
676,D09-1056,External_60026,[0],related work,Some researchers <TARGET_CITATION/> have explored the use of Wikipedia information to improve the disambiguation process .,"Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <CITATION/>. Some researchers <TARGET_CITATION/> have explored the use of Wikipedia information to improve the disambiguation process . Some researchers <CITATION/> have explored the use of Wikipedia information to improve the disambiguation process. Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <CITATION/>. Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,cb7aaa6e72cacb40750f8926fd67dd6149c6bc00,Named Entity Disambiguation: A Hybrid Statistical and Rule-Based Incremental Approach,2008,Hien T. Nguyen; T. Cao
678,J87-3002,External_29753,[0],,One approach to this problem is that taken by the ASCOT project <TARGET_CITATION/> .,"Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see <CITATION/>, for further comment). One approach to this problem is that taken by the ASCOT project <TARGET_CITATION/> . One approach to this problem is that taken by the ASCOT project <CITATION/>. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see <CITATION/>, for further comment). Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,a critical assessment of the ldoce coding system to appear in,1986,Erik Akkerman
679,W06-2807,External_98159,[0],,"Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture <TARGET_CITATION/> .","nonattributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve <CITATION/>. Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture <TARGET_CITATION/> . Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture <CITATION/>.We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve <CITATION/>. nonattributive copyright licence to their work.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,808b743c2fff5b6c50f404e856c48776b64e9c4f,Free Software Free Society Selected Essays Of,2016,Stallman Book
680,J00-2001,W94-0316,[0],,"McDonald has even argued for extending the model to a large number of components <CITATION/> , and several systems have indeed added an additional component between the planner and the linguistic component <TARGET_CITATION/> .","Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES <CITATION/>, EPICURE <CITATION/>, SPOKESMAN <CITATION/>, Sibun's work on local organization of text <CITATION/>, and COMET <CITATION/> all are organized this way. McDonald has even argued for extending the model to a large number of components <CITATION/> , and several systems have indeed added an additional component between the planner and the linguistic component <TARGET_CITATION/> . McDonald has even argued for extending the model to a large number of components <CITATION/>, and several systems have indeed added an additional component between the planner and the linguistic component <CITATION/>. For example, DIOGENES <CITATION/>, EPICURE <CITATION/>, SPOKESMAN <CITATION/>, Sibun's work on local organization of text <CITATION/>, and COMET <CITATION/> all are organized this way. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,ae73f98183f967c45fbc8678b4bece89625a8421,Building Another Bridge over the Generation Gap,1994,Leo Wanner
681,A00-1031,External_13541,[4],conclusion,"According to current tagger comparisons ( van <CITATION/> ) , and according to a comparsion of the results presented here with those in <TARGET_CITATION/> , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .","They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. According to current tagger comparisons ( van <CITATION/> ) , and according to a comparsion of the results presented here with those in <TARGET_CITATION/> , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here . According to current tagger comparisons (van <CITATION/>), and according to a comparsion of the results presented here with those in <CITATION/>, the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. The architecture remains applicable to a large variety of languages. They do so for several other corpora as well.",9bd13ac82c011a91542df1a705cf46bb787764cb,TnT – A Statistical Part-of-Speech Tagger,2000,T. Brants,,a maximum entropy model for partofspeech tagging,1996,Adwait Ratnaparkhi
682,P13-3018,External_97014,[0],related work,"Similar findings have been proposed by <TARGET_CITATION/> that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .","<CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. <CITATION/> tried to construct a semantic analysis based on prepared'' and unprepared mind''. Similar findings have been proposed by <TARGET_CITATION/> that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . Similar findings have been proposed by <CITATION/> that points out V1 and V2 are paired on the basis of their semantic compatibility, which is subject to syntactic constraints. <CITATION/> tried to construct a semantic analysis based on prepared'' and unprepared mind''. <CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,serial verb construction in marathiquot,1993,R Pandharipande
683,N10-1084,External_46123,[0],related work,The first lexical substitution method was proposed by <TARGET_CITATION/> .,The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms. The first lexical substitution method was proposed by <TARGET_CITATION/> . The first lexical substitution method was proposed by <CITATION/>. The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.,b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,2c33b7ba390c136d20d328f0f86922855accff2f,Hiding the Hidden: A software system for concealing ciphertext as innocuous text,1997,M. Chapman; G. Davida
684,J06-2002,External_87651,[0],,"This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people <TARGET_CITATION/> , sometimes in stark contrast with the intention of the speaker/writer <CITATION/> .","Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people <TARGET_CITATION/> , sometimes in stark contrast with the intention of the speaker/writer <CITATION/> . This can be a hazardous affair, since vague expressions tend to be interpreted in different ways by different people <CITATION/>, sometimes in stark contrast with the intention of the speaker/writer <CITATION/>. We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD. Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,fdfc869318cc2059fa8c25a965e7e918f65760eb,WHAT DO WE MEAN BY USUALLY?,1980,N. Sarkies; J. H. Baron
685,J02-3002,External_13541,[4],,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt <TARGET_CITATION/> ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt <TARGET_CITATION/> ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992], Brill's [Brill 1995a], and MaxEnt [Ratnaparkhi 1996]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. This requires resolving sentence boundaries before tagging. tagger operates on text spans that form a sentence.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,a maximum entropy model for partofspeech tagging,1996,Adwait Ratnaparkhi
686,J06-2002,External_3883,[0],introduction,"<TARGET_CITATION/> ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .","Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? <TARGET_CITATION/> ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking . Hermann and Deutsch (1976; also reported in Levelt 1989) show that greater differences are most likely to be chosen, presumably because they are more striking. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,881a30a7c9e39da5b0f2079e4a7b0eedcc5f63c1,Psychologie der Objektbenennung,1976,T. Herrmann; W. Deutsch
687,A00-1020,External_8592,[0],,"Nevertheless , recent results show that knowledgepoor methods perform with amazing accuracy ( cfXXX <TARGET_CITATION/> ) .","Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge. The acquisition of such knowledge is timeconsuming, difficult, and errorprone. Nevertheless , recent results show that knowledgepoor methods perform with amazing accuracy ( cfXXX <TARGET_CITATION/> ) . Nevertheless, recent results show that knowledgepoor methods perform with amazing accuracy (cfXXX <CITATION/>). The acquisition of such knowledge is timeconsuming, difficult, and errorprone. Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.",76894392818a9a360feaf2f1a797bbe1eaac82b0,Multilingual Coreference Resolution,2000,S. Harabagiu; Steven J. Maiorano,d032db4e8c1cf2b02867f95fa8fe847f5bfdf6ab,Robust pronoun resolution with limited knowledge,1998,R. Mitkov
688,N01-1002,P00-1019,[0],,"Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain <TARGET_CITATION/> .","e.g. prenominal or postnominal, adjectival or as a relative clause. Through corpus annotation, we wish to answer the question of what will be the probability of a given piece of information occupying a given syntactic position (a value of type) on the basis of the semantic and pragmatic properties of that information and relevant NP features, for example, whether a certain color attribute should be expressed by means of a prenominal adjective or a prepositional phrase in a definite NP. Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain <TARGET_CITATION/> . Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of Jordan's work on the COCONUT domain <CITATION/>. Through corpus annotation, we wish to answer the question of what will be the probability of a given piece of information occupying a given syntactic position (a value of type) on the basis of the semantic and pragmatic properties of that information and relevant NP features, for example, whether a certain color attribute should be expressed by means of a prenominal adjective or a prepositional phrase in a definite NP. e.g. prenominal or postnominal, adjectival or as a relative clause.",a463350fc2e38decfe736e0801d465874cef0891,Corpus-based NP Modifier Generation,2001,Hua Cheng; Massimo Poesio; R. Henschel; C. Mellish,b01a5af2b986dfd9252eef0623cb8e28f6956996,Can Nominal Expressions Achieve Multiple Goals?: An Empirical Study,2000,Pamela W. Jordan
689,J04-3001,External_22096,[0],,"Some wellknown approaches include rulebased models <CITATION/> , backedoff models <TARGET_CITATION/> , and a maximumentropy model <CITATION/> .","One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some wellknown approaches include rulebased models <CITATION/> , backedoff models <TARGET_CITATION/> , and a maximumentropy model <CITATION/> . Some wellknown approaches include rulebased models <CITATION/>, backedoff models <CITATION/>, and a maximumentropy model <CITATION/>. Researchers have proposed many computational models for resolving PPattachment ambiguities. One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,prepositional phrase attachment through a backedoff model,1995,M Collins; J Brooks
691,J01-4001,External_6226,[0],,"Tetreault 's contribution features comparative evaluation involving the author 's own centeringbased pronoun resolution algorithm called the LeftRight Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm <CITATION/> , BFP <CITATION/> , and Strube 's 5list approach <TARGET_CITATION/> .","He also argues that evaluation of anaphora resolution systems should take into account several factors beyond simple accuracy of resolution. In particular, both developeroriented (e.g., related to the selection of optimal resolution factors) and applicationoriented (e.g., related to the requirement of the application, as in the case of information extraction, where a proper name antecedent is needed) evaluation metrics should be considered. Tetreault 's contribution features comparative evaluation involving the author 's own centeringbased pronoun resolution algorithm called the LeftRight Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm <CITATION/> , BFP <CITATION/> , and Strube 's 5list approach <TARGET_CITATION/> . Tetreault's contribution features comparative evaluation involving the author's own centeringbased pronoun resolution algorithm called the LeftRight Centering algorithm (LRC) as well as three other pronoun resolution methods: Hobbs's naive algorithm <CITATION/>, BFP <CITATION/>, and Strube's 5list approach <CITATION/>. In particular, both developeroriented (e.g., related to the selection of optimal resolution factors) and applicationoriented (e.g., related to the requirement of the application, as in the case of information extraction, where a proper name antecedent is needed) evaluation metrics should be considered. He also argues that evaluation of anaphora resolution systems should take into account several factors beyond simple accuracy of resolution.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,d50602b6aad3dfa6c098bfea59b8fa5b3eb75edc,Never Look Back: An Alternative to Centering,1998,M. Strube
692,J09-4010,External_52005,[4],,"<TARGET_CITATION/> investigated three approaches to the automatic generation of response emails : text classification , casebased reasoning , and question answering .","This part of their approach resembles our DocRet method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to preexisting templates and returns the templates. <TARGET_CITATION/> investigated three approaches to the automatic generation of response emails : text classification , casebased reasoning , and question answering . <CITATION/> investigated three approaches to the automatic generation of response emails: text classification, casebased reasoning, and question answering. In addition, rather than including actual response sentences in a reply, their system matches response sentences to preexisting templates and returns the templates. This part of their approach resembles our DocRet method, but instead of retrieving entire response documents, they retrieve individual sentences.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,mercure towards an automatic email followup system,2003,G Lapalme; L Kosseim
693,J00-1004,External_28256,[4],method,"This contrasts with one of the traditional approaches <TARGET_CITATION/> to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .","Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving socalled headswitching. This contrasts with one of the traditional approaches <TARGET_CITATION/> to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language . This contrasts with one of the traditional approaches <CITATION/> to posing the translation problem, i.e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving socalled headswitching. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.",355c46c066f29dc91f25d303df6e128bb69858c2,Learning dependency translation models as collections of finite state head transducers,2000,H. Alshawi; Srinivas Bangalore; Shona Douglas,21e69bb8d3a36235a19da1408279290055b373bc,Machine Translation Divergences: A Formal Description and Proposed Solution,1994,B. Dorr
694,W01-1510,External_327,[0],introduction,"In practical context , German , English , and Japanese HPSGbased grammars are developed and used in the Verbmobil project <TARGET_CITATION/> .","There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts <CITATION/>. Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>. In practical context , German , English , and Japanese HPSGbased grammars are developed and used in the Verbmobil project <TARGET_CITATION/> . In practical context, German, English, and Japanese HPSGbased grammars are developed and used in the Verbmobil project <CITATION/>. Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts <CITATION/>.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,verbmobil a translation system for facetoface dialog,1994,M Kay; J Gawron; P Norvig
696,W02-1601,C00-1078,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,426b83ae48f89708d897bcb9df851bec3b3d4e4d,Chart-Based Transfer Rule Application in Machine Translation,2000,Adam Meyers; M. Kosaka; R. Grishman
697,J06-2002,External_87652,[0],experiments,"While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."," While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . While IA is generally thought to be consistent with findings on human language production <CITATION/>, the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,606907f101f6e4ee5fd98c94444e4369254c47bf,The Effects of Redundant Communications on Listeners: When More Is Less.,1982,S. Sonnenschein
698,W06-3813,External_27698,[0],related work,"It helps them build complex knowledge bases by combining components : events , entities and modifiers <TARGET_CITATION/> .","Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components : events , entities and modifiers <TARGET_CITATION/> . It helps them build complex knowledge bases by combining components: events, entities and modifiers <CITATION/>. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. Lists of semantic relations are designed to capture salient domain information.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,,building concept reprezentations from reusable components,1997,Peter Clark; Bruce Porter
699,W01-1510,P98-2144,[0],introduction,"Our group has developed a widecoverage HPSG grammar for Japanese <TARGET_CITATION/> , which is used in a highaccuracy Japanese dependency analyzer <CITATION/> .","Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>. In practical context, German, English, and Japanese HPSGbased grammars are developed and used in the Verbmobil project <CITATION/>. Our group has developed a widecoverage HPSG grammar for Japanese <TARGET_CITATION/> , which is used in a highaccuracy Japanese dependency analyzer <CITATION/> . Our group has developed a widecoverage HPSG grammar for Japanese <CITATION/>, which is used in a highaccuracy Japanese dependency analyzer <CITATION/>.In practical context, German, English, and Japanese HPSGbased grammars are developed and used in the Verbmobil project <CITATION/>. Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,9fbc4d31d844d76bdd33e4663d25758074ecce45,HPSG-Style Underspecified Japanese Grammar with Wide Coverage,1998,Yutaka Mitsuishi; Kentaro Torisawa; Junichi Tsujii
700,W06-1705,External_46139,[0],related work,Word frequency counts in internet search engines are inconsistent and unreliable <TARGET_CITATION/> .,"Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp <CITATION/>, KWiCFinder <CITATION/> and the Linguist's Search Engine <CITATION/>. A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable <TARGET_CITATION/> . Word frequency counts in internet search engines are inconsistent and unreliable <CITATION/>. A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp <CITATION/>, KWiCFinder <CITATION/> and the Linguist's Search Engine <CITATION/>.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,web googles missing pages mystery solved httpaixtalblogspotcom200502webgooglesmissingpagesmysteryhtml accessed,2005,J Veronis
701,J01-4001,External_13660,[0],,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( <TARGET_CITATION/> , 2001b ) .","The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution <TARGET_CITATION/> . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Charniak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form <CITATION/>; and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a, 2001b). Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,evaluating anaphora resolution approaches,1998,Ruslan Mitkov
702,D09-1056,W07-2012,[0],related work,It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <TARGET_CITATION/> .,The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>. Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <TARGET_CITATION/> . It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <CITATION/>. Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>.,a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,fcc301c51ce866dcb7db73d6a3711ceccb8f2aa3,The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task,2007,J. Artiles; Julio Gonzalo; S. Sekine
703,D12-1037,External_14703,[5],experiments,"Actually , if we use LSH technique <TARGET_CITATION/> in retrieval process , the local method can be easily scaled to a larger training data .","This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique <TARGET_CITATION/> in retrieval process , the local method can be easily scaled to a larger training data . Actually, if we use LSH technique <CITATION/> in retrieval process, the local method can be easily scaled to a larger training data.Further, compared to the retrieval, the local training is not the bottleneck. This shows that the local method is efficient.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,e17529924798975856310a75cb3df3066ac7ccfa,Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions,2006,Alexandr Andoni; P. Indyk
704,J05-3003,W93-0109,[0],method,"Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a thatclause and a to + infinitive clause <TARGET_CITATION/> .","The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a thatclause and a to + infinitive clause <TARGET_CITATION/> . Many lexicons, both automatically acquired and manually created, are more fine grained in their approaches to subcategorized clausal arguments, differentiating, for example, between a thatclause and a to + infinitive clause <CITATION/>. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,1a4e9c79bf09d7071895261df0ea762a7e4f389a,The Automatic Acquisition of Frequencies of Verb Subcategorization Frames from Tagged Corpora,2002,Akira Ushioda; David A. Evans; Ted Gibson; A. Waibel
705,D13-1115,External_1533,[0],introduction,"Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions <TARGET_CITATION/> or robot commands <CITATION/> .","The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions <TARGET_CITATION/> or robot commands <CITATION/> . Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. The language grounding problem has come in many different flavors with just as many different approaches.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4b17662f21c6313907a022af20f88616d11620eb,Learning to Interpret Natural Language Navigation Instructions from Observations,2011,David L. Chen; R. Mooney
706,P10-2059,External_63534,[2],introduction,Agreement between two annotation sets is calculated here in terms of Cohen 's kappa <CITATION/> 1 and corrected kappa <TARGET_CITATION/> 2 .,"However, one dialogue was coded independently and in parallel by two expert annotators to measure intercoder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa <CITATION/> 1 and corrected kappa <TARGET_CITATION/> 2 . Agreement between two annotation sets is calculated here in terms of Cohen's kappa <CITATION/>1 and corrected kappa <CITATION/>2. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. However, one dialogue was coded independently and in parallel by two expert annotators to measure intercoder agreement.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,40b3963b07b1b15751afab235cdeaa343bf3342a,"Coefficient Kappa: Some Uses, Misuses, and Alternatives",1981,R. Brennan; Dale J. Prediger
708,W03-0806,W03-0407,[1],experiments,The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <TARGET_CITATION/> .,"Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <TARGET_CITATION/> . The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <CITATION/>. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,906765f7ac46011123cca59de775216f1ee9b451,Bootstrapping POS-taggers using unlabelled data,2003,S. Clark; J. Curran; M. Osborne
709,D13-1115,External_9242,[0],introduction,"Some efforts have tackled tasks such as automatic image caption generation <TARGET_CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <CITATION/> .","Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <TARGET_CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <CITATION/> . Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,8e080b98efbe65c02a116439205ca2344b9f7cd4,Im2Text: Describing Images Using 1 Million Captioned Photographs,2011,Vicente Ordonez; Girish Kulkarni; Tamara L. Berg
711,W06-1705,External_33311,[0],related work,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( <TARGET_CITATION/> , 2004b ) and received a special issue of the journal Computational Linguistics <CITATION/> .","Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times <TARGET_CITATION/> and received a special issue of the journal Computational Linguistics <CITATION/> . The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,google as a corpus tool in,2003,T Robb
712,E03-1005,External_63,[0],introduction,"But while Bod 's estimator obtains stateoftheart results on the WSJ , comparable to <TARGET_CITATION/> , Bonnema et al. 's estimator performs worse and is comparable to <CITATION/> .","This paper presents the first published results with Goodman's PCFGreductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFGreductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod 's estimator obtains stateoftheart results on the WSJ , comparable to <TARGET_CITATION/> , Bonnema et al. 's estimator performs worse and is comparable to <CITATION/> . But while Bod's estimator obtains stateoftheart results on the WSJ, comparable to <CITATION/>, Bonnema et al.'s estimator performs worse and is comparable to <CITATION/>. We show that these PCFGreductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). This paper presents the first published results with Goodman's PCFGreductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,844db702be4bc149b06b822b47247e15f5894cc3,Discriminative Reranking for Natural Language Parsing,2000,M. Collins; Terry Koo
713,W03-0806,P02-1030,[1],experiments,The implementation has been inspired by experience in extracting information from very large corpora <TARGET_CITATION/> and performing experiments on maximum entropy sequence tagging <CITATION/> .,"Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. The implementation has been inspired by experience in extracting information from very large corpora <TARGET_CITATION/> and performing experiments on maximum entropy sequence tagging <CITATION/> . The implementation has been inspired by experience in extracting information from very large corpora <CITATION/> and performing experiments on maximum entropy sequence tagging <CITATION/>. We can also load a lexicon into memory that is shared between all of the components, reducing the memory use. Further, we can use techniques for making string matching and other text processing very fast such as making only one copy of each lexical item or annotation in memory.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,e8066c5522ebfa8f0f08589dcbe5f315bfec90c1,Scaling Context Space,2002,J. Curran; M. Moens
714,D13-1115,P13-1056,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4adca62f888226d3a16654ca499bf2a7d3d11b71,Models of Semantic Representation with Visual Attributes,2013,Carina Silberer; V. Ferrari; Mirella Lapata
715,D08-1004,External_62,[2],,14We parse each sentence with the Collins parser <TARGET_CITATION/> .,"To train our model, we use LBFGS to locally maximize the log of the objective function (1):15 13These are the function words with count > 40 in a random sample of 100 documents, and which were associated with the OI tag transition at more than twice the average rate. We do not use any other lexical 0features that reference x, for fear that they would enable the learner to explain the rationales without changing 0 as desired (see the end of section 5.3). 14We parse each sentence with the Collins parser <TARGET_CITATION/> . 14We parse each sentence with the Collins parser <CITATION/>. We do not use any other lexical 0features that reference x, for fear that they would enable the learner to explain the rationales without changing 0 as desired (see the end of section 5.3).To train our model, we use LBFGS to locally maximize the log of the objective function (1):15 13These are the function words with count > 40 in a random sample of 100 documents, and which were associated with the OI tag transition at more than twice the average rate.",14e2aec7e25d8880a851a547cf8d27a9721f8e6c,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,2008,Omar Zaidan; Jason Eisner,,headdriven statistical models for natural language parsing,1999,M Collins
716,W10-1710,W10-1735,[1],conclusion,"Encouraged by the success of chunkbased verb reordering lattices on ArabicEnglish <TARGET_CITATION/> , we tried to adapt the same approach to the GermanEnglish language pair .","It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunkbased verb reordering lattices on ArabicEnglish <TARGET_CITATION/> , we tried to adapt the same approach to the GermanEnglish language pair . Encouraged by the success of chunkbased verb reordering lattices on ArabicEnglish <CITATION/>, we tried to adapt the same approach to the GermanEnglish language pair. Word reordering between German and English is a complex problem. It would be interesting to compare the relative performance of the two approaches systematically.",b18afb7e1514de13b671bd1a08ab7132e2c45f12,FBK at WMT 2010: Word Lattices for Morphological Reduction and Chunk-Based Reordering,2010,Christian Hardmeier; Arianna Bisazza; Marcello Federico,cce113d0438c69bd03f5788dfded9bf9b39d0784,Chunk-Based Verb Reordering in VSO Sentences for Arabic-English Statistical Machine Translation,2010,Arianna Bisazza; Marcello Federico
717,P13-3018,External_83583,[2],method,We followed the same experimental procedure as discussed in <TARGET_CITATION/> for English polymorphemic words .,This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the noncompositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using 32 native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in <TARGET_CITATION/> for English polymorphemic words . We followed the same experimental procedure as discussed in <CITATION/> for English polymorphemic words. In order to validate such claim we perform a lexical decision experiment using 32 native Bangla speakers with 92 different verb sequences. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the noncompositional verbs which maps to a single expression of meaning.,97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,0c3e2e233a752d09cd29b4fe9bd727bd0c2a1780,Morphological Decomposition and the Reverse Base Frequency Effect,2004,M. Taft
718,W06-1639,W02-1011,[2],method,"In our experiments , we employed the wellknown classifier SVM  ght to obtain individualdocument classification scores , treating Y as the positive class and using plain unigrams as features .5 Following standard practice in sentiment analysis <TARGET_CITATION/> , the input to SVM  ght consisted of normalized presenceoffeature ( rather than frequencyoffeature ) vectors ."," In our experiments , we employed the wellknown classifier SVM  ght to obtain individualdocument classification scores , treating Y as the positive class and using plain unigrams as features .5 Following standard practice in sentiment analysis <TARGET_CITATION/> , the input to SVM  ght consisted of normalized presenceoffeature ( rather than frequencyoffeature ) vectors . In our experiments, we employed the wellknown classifier SVMght to obtain individualdocument classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis <CITATION/>, the input to SVMght consisted of normalized presenceoffeature (rather than frequencyoffeature) vectors.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,12d0353ce8b41b7e5409e5a4a611110aee33c7bc,Thumbs up? Sentiment Classification using Machine Learning Techniques,2002,B. Pang; Lillian Lee; Shivakumar Vaithyanathan
719,J86-1002,J81-2002,[4],,The problem of handling illformed input has been studied by <TARGET_CITATION/> .,"While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. The problem of handling illformed input has been studied by <TARGET_CITATION/> . The problem of handling illformed input has been studied by <CITATION/>. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,d8d9f4a23dd35e0514a6b66cca3a1cd100e2b730,Relaxation Techniques for Parsing Grammatically Ill-Formed Input in Natural Language Understanding Systems,1981,S. Kwasny; N. Sondheimer
720,W04-0910,External_51288,[5],,"To address this problem , we are currently working on developing a metagrammar in the sense of <TARGET_CITATION/> .","For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeille FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of <TARGET_CITATION/> . To address this problem, we are currently working on developing a metagrammar in the sense of <CITATION/>. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeille FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,un outil multilingue de generation de ltag  application au francais et a l’italien,1999,M H Candito
721,J90-3003,External_1210,[0],introduction,"Sentences like 12 , from <TARGET_CITATION/> , are frequently cited .","However, this claim is controversial because of the misa:ignments that occur between the two levels of phrasing. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. Sentences like 12 , from <TARGET_CITATION/> , are frequently cited . Sentences like 12, from <CITATION/>, are frequently cited. For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences. However, this claim is controversial because of the misa:ignments that occur between the two levels of phrasing.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,95c20f35d352f23b19c378c0758b8dc1d7622872,On Aspects of the Theory of Syntax,2021,Anna Maria Di Sciullo
722,D08-1034,W03-1707,[0],introduction,"After the PropBank <TARGET_CITATION/> was built , <CITATION/> have produced more complete and systematic research on Chinese SRL .","They just labeled the predicateargument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank <TARGET_CITATION/> was built , <CITATION/> have produced more complete and systematic research on Chinese SRL . After the PropBank <CITATION/> was built, <CITATION/> have produced more complete and systematic research on Chinese SRL. This paper made the first attempt on Chinese SRL and produced promising results. They just labeled the predicateargument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,e33cf703e6f101b892f559cfa7941531d84af4a3,Annotating the Propositions in the Penn Chinese Treebank,2003,Nianwen Xue; Martha Palmer
723,K15-1003,N07-1018,[2],introduction,"In order to estimate the parameters of our model , we develop a blocked sampler based on that of <TARGET_CITATION/> to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .","Given our desire to train NLP models in lowsupervision scenarios, the possibility of constructing inductive biases out of universal properties of language is enticing: if we can do this well, then it only needs to be done once, and can be applied to any language or domain without adaptation. In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of <TARGET_CITATION/> to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . In order to estimate the parameters of our model, we develop a blocked sampler based on that of <CITATION/> to sample parse trees for sentences in the raw training corpus according to their posterior probabilities. In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. Given our desire to train NLP models in lowsupervision scenarios, the possibility of constructing inductive biases out of universal properties of language is enticing: if we can do this well, then it only needs to be done once, and can be applied to any language or domain without adaptation.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,ece4a51f9d1fab08230a527efbb801c57e0249c5,Bayesian Inference for PCFGs via Markov Chain Monte Carlo,2007,Mark Johnson; T. Griffiths; S. Goldwater
724,J05-3003,External_9757,[0],related work,The extraction procedure utilizes a head percolation table as introduced by <TARGET_CITATION/> in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct .,"As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. <CITATION/> explore a number of related approaches to the extraction of a lexicalized TAG from the PennII Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by <TARGET_CITATION/> in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct . The extraction procedure utilizes a head percolation table as introduced by <CITATION/> in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct. <CITATION/> explore a number of related approaches to the extraction of a lexicalized TAG from the PennII Treebank with the aim of constructing a statistical model for parsing. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,statistical decisiontree models for parsing,1995,David M Magerman
725,J00-2005,External_12160,[0],introduction,Many other such cases are described in Danlos 's book <TARGET_CITATION/> .,"But in a pipelined NLG system, pronominalization decisions are typically made earlier than wordordering decisions; for example in the threestage pipelined architecture presented by <CITATION/>, pronominalization decisions are made in the second stage (microplanning), but word ordering is chosen during the third stage (realization). This means that the microplanner will not be able to make optimal pronominalization decisions in cases where le or la are unambiguous, but l' is not, since it does not know word order and hence whether the pronoun will be abbreviated. Many other such cases are described in Danlos 's book <TARGET_CITATION/> . Many other such cases are described in Danlos's book <CITATION/>. This means that the microplanner will not be able to make optimal pronominalization decisions in cases where le or la are unambiguous, but l' is not, since it does not know word order and hence whether the pronoun will be abbreviated. But in a pipelined NLG system, pronominalization decisions are typically made earlier than wordordering decisions; for example in the threestage pipelined architecture presented by <CITATION/>, pronominalization decisions are made in the second stage (microplanning), but word ordering is chosen during the third stage (realization).",f10e6b08a31d42bd0c6f51808cfa1058d170fd49,Pipelines and size constraints,2000,Ehud Reiter,13470948ea2c24ceb4d309e004f5087f25487d84,The Linguistic Basis of Text Generation,1987,L. Danlos
726,A00-1021,A97-1030,[2],experiments," Before indexing the text , we process it with Textract <TARGET_CITATION/> , which performs lemmatization , and discovers proper names and technical terms .","Some templates do not cause complete replacement of the matched string. For example, the pattern What is the population'' gets replaced by NUMBER$ population''. Before indexing the text , we process it with Textract <TARGET_CITATION/> , which performs lemmatization , and discovers proper names and technical terms .  Before indexing the text, we process it with Textract <CITATION/>, which performs lemmatization, and discovers proper names and technical terms. For example, the pattern What is the population'' gets replaced by NUMBER$ population''.Some templates do not cause complete replacement of the matched string.",4a0010df256640465f5f390ae8753de9ff0090b8,Ranking suspected answers to natural language questions using predictive annotation,2000,Dragomir R. Radev; J. Prager; Valerie Samn,a51be478b6c11f1c61192e563b6f6ae2ef48b9fa,Disambiguation of Proper Names in Text,1997,Nina Wacholder; Yael Ravin; Misook Choi
728,D08-1034,External_24680,[1],introduction,Experiments on Chinese SRL <TARGET_CITATION/> reassured these findings .,"They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL <TARGET_CITATION/> reassured these findings . Experiments on Chinese SRL <CITATION/> reassured these findings. For semantic analysis, developing features that capture the right kind of information is crucial. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,ee4c4fe7fd24125531a0e9eafb6d110cf3c27398,Automatic Semantic Role Labeling for Chinese Verbs,2005,Nianwen Xue; Martha Palmer
729,W03-0806,External_6185,[0],experiments,"An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by <TARGET_CITATION/> that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly <CITATION/> .","Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by <TARGET_CITATION/> that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly <CITATION/> . An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. However, it will be increasingly important as techniques become more complex and corpus sizes grow. Efficiency has not been a focus for NLP research in general.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,b49db3ac26d96b6c5c081dc6c2cc24da93e633f1,Maximum entropy models for natural language ambiguity resolution,1998,A. Ratnaparkhi; Mitchell P. Marcus
730,W06-1104,External_861,[0],,"According to <TARGET_CITATION/> , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .","Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>. The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to <TARGET_CITATION/> , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments . According to <CITATION/>, there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,evaluating wordnetbased measures of semantic distance,2006,Alexander Budanitsky; Graeme Hirst
731,W14-1704,P11-1093,[1],experiments,The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machinelearning methods on error correction tasks <TARGET_CITATION/> .,"ger2 and shallow parser3 <CITATION/>. The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier). The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machinelearning methods on error correction tasks <TARGET_CITATION/> . The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machinelearning methods on error correction tasks <CITATION/>. The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier). ger2 and shallow parser3 <CITATION/>.",1a40105b9eda0cecdd596b5758fb4ad85b7f636b,The Illinois-Columbia System in the CoNLL-2014 Shared Task,2014,Alla Rozovskaya; Kai-Wei Chang; Mark Sammons; D. Roth; Nizar Habash,35c3857b0be77dad3c3ac0d598cd85685d585120,Algorithm Selection and Model Adaptation for ESL Correction Tasks,2011,Alla Rozovskaya; D. Roth
732,W04-1610,External_6677,[0],related work,A good study comparing document categorization algorithms can be found in <TARGET_CITATION/> .,"The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents. For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces. A good study comparing document categorization algorithms can be found in <TARGET_CITATION/> . A good study comparing document categorization algorithms can be found in <CITATION/>. For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces. The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,a reexamination of text categorization methods”,1999,Y Yang; X Liu
733,P02-1001,External_692,[0],,"For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm <TARGET_CITATION/> .","Efficient hardware implementation is also possible via chiplevel parallelism <CITATION/>.  In many cases of interest, Ti is an acyclic graph.20 Then Tar an's method computes w0j for each j in topologically sorted order, thereby finding ti in a linear number of  and  operations. For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm <TARGET_CITATION/> . For HMMs (footnote 11), Ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm <CITATION/>.  In many cases of interest, Ti is an acyclic graph.20 Then Tar an's method computes w0j for each j in topologically sorted order, thereby finding ti in a linear number of  and  operations. Efficient hardware implementation is also possible via chiplevel parallelism <CITATION/>.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,539036ab9e8f038c8a948596e77cc0dfcfa91fb3,An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process,1972,L. Baum
735,J92-1004,External_4542,[2],,Semantic filters can also be used to prevent multiple versions of the same case frame <TARGET_CITATION/> showing up as complements .,"Finally, in the case of passive voice, the CURRENTFOCUS slot is empty at the time the verb is proposed, because the CURRENTFOCUS which was the surfaceform subject has been moved to the floatobject position. In this case, the verb has no information concerning its subject, and so it identifies it as an unbound pronoun. Semantic filters can also be used to prevent multiple versions of the same case frame <TARGET_CITATION/> showing up as complements . Semantic filters can also be used to prevent multiple versions of the same case frame <CITATION/> showing up as complements. In this case, the verb has no information concerning its subject, and so it identifies it as an unbound pronoun. Finally, in the case of passive voice, the CURRENTFOCUS slot is empty at the time the verb is proposed, because the CURRENTFOCUS which was the surfaceform subject has been moved to the floatobject position.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,8c31301fb9f339e2b496a572a7933956d2260154,The Case for the Case,2016,J. Toomasian
736,W04-1805,External_18168,[2],,The terms have been identified as the most specific to our corpus by a program developed by <TARGET_CITATION/> and called TER1vloSTAT .,"These two rates were evaluated using a test sample containing all this information. CompuTerm 2004 3rd International Workshop on Computational Terminology 43 To construct this test set, we have focused our attention on ten domainspecific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer), serveur (server), systeme (system), utilisateur (user). The terms have been identified as the most specific to our corpus by a program developed by <TARGET_CITATION/> and called TER1vloSTAT . The terms have been identified as the most specific to our corpus by a program developed by <CITATION/> and called TER1vloSTAT. CompuTerm 2004 3rd International Workshop on Computational Terminology 43 To construct this test set, we have focused our attention on ten domainspecific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer), serveur (server), systeme (system), utilisateur (user). These two rates were evaluated using a test sample containing all this information.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,termextraction using nontechnical corpora as a point of leverage,2003,Patrick Drouin
738,P02-1001,External_4179,[0],introduction,The availability of toolkits for this weighted case ( <TARGET_CITATION/> ; van <CITATION/> ) promises to unify much of statistical NLP .,"Such models can be efficiently restricted, manipulated or combined using rational operations as before. An artificial example will appear in §2. The availability of toolkits for this weighted case ( <TARGET_CITATION/> ; van <CITATION/> ) promises to unify much of statistical NLP . The availability of toolkits for this weighted case (<CITATION/>; van <CITATION/>) promises to unify much of statistical NLP. An artificial example will appear in §2. Such models can be efficiently restricted, manipulated or combined using rational operations as before.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,a rational design for a weighted finitestate transducer library,1998,Mehryar Mohri; Fernando C N Pereira; Michael Riley
739,J00-4002,External_32773,[4],,"tionally reconstructed by <TARGET_CITATION/> , the contextindependent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules .","The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasilogical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE). In the CLEQLF approach, as ra tionally reconstructed by <TARGET_CITATION/> , the contextindependent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules . tionally reconstructed by <CITATION/>, the contextindependent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. In the CLEQLF approach, as raThe starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasilogical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE).",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,e87314e4e465ff21001e04a38b8fcdfeadae714e,Monotonic Semantic Interpretation,1992,H. Alshawi; Dick Crouch
740,K15-1002,D08-1031,[2],experiments,"We use a standard split of 268 training documents , 68 development documents , and 106 testing documents <TARGET_CITATION/> .","Datasets The ACE2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents <TARGET_CITATION/> . We use a standard split of 268 training documents, 68 development documents, and 106 testing documents <CITATION/>. Datasets The ACE2004 dataset contains 443 documents.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,454bfea18aebae023e9a716503e3a9956dcea8b4,Understanding the Value of Features for Coreference Resolution,2008,Eric Bengtson; D. Roth
742,J00-2014,External_28137,[0],,But typical OT grammars offer much richer finitestate models of left context <TARGET_CITATION/> than provided by the traditional HMM finitestate topologies .,"For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or partofspeech tagging. Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from E*, based on criteria of wellformedness (transition probabilities) and faithfulness to the input (emission probabilities). But typical OT grammars offer much richer finitestate models of left context <TARGET_CITATION/> than provided by the traditional HMM finitestate topologies . But typical OT grammars offer much richer finitestate models of left context <CITATION/> than provided by the traditional HMM finitestate topologies. Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from E*, based on criteria of wellformedness (transition probabilities) and faithfulness to the input (emission probabilities). For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or partofspeech tagging.",962381e601b37b50cd2a1ae387a1159f1c9209e6,Book Reviews: Optimality Theory,2000,Jason Eisner,fbca315ee6a13d609be222304911ce9ff2505389,Efficient Generation in Primitive Optimality Theory,1997,Jason Eisner
743,W03-0806,External_4179,[0],experiments,"Other tools have been designed around particular techniques , such as finite state machines <TARGET_CITATION/> .","These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques , such as finite state machines <TARGET_CITATION/> . Other tools have been designed around particular techniques, such as finite state machines <CITATION/>. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,a rational design for a weighted finitestate transducer library,1998,Mehryar Mohri; Fernando C N Pereira; Michael Riley
744,W10-4005,W10-3908,[4],experiments,This contrasts with the findings described in <TARGET_CITATION/> where significant improvements could be achieved by increasing the number of source languages .,"3) Little improvement for several source words The right column in Table 1 shows the scores if (using the productofranks algorithm) four source languages are taken into account in parallel. As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. This contrasts with the findings described in <TARGET_CITATION/> where significant improvements could be achieved by increasing the number of source languages . This contrasts with the findings described in <CITATION/> where significant improvements could be achieved by increasing the number of source languages. As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. 3) Little improvement for several source words The right column in Table 1 shows the scores if (using the productofranks algorithm) four source languages are taken into account in parallel.",305ea15f21fb788f20d9b56cfdad590dcc62200c,The Noisier the Better: Identifying Multilingual Word Translations Using a Single Monolingual Corpus,2010,R. Rapp; M. Zock; A. Trotman; Yue Xu,b1b748fd1f1fa3485be04077b1c62a38b99f1cc3,Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation,2010,R. Rapp; M. Zock
745,J06-2002,External_940,[0],introduction,"The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basiclevel Value ( i.e. , the most commonly occurring and psychologically most fundamental level , <TARGET_CITATION/> ) .","The expansion of L and the contraction of C continue until C = S:FindBestValue selects the best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity. The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basiclevel Value ( i.e. , the most commonly occurring and psychologically most fundamental level , <TARGET_CITATION/> ) . The function selects the Value that removes most distractors, but in case of a tie, the least specific contestant is chosen, as long as it is not less specific than the basiclevel Value (i.e., the most commonly occurring and psychologically most fundamental level, Rosch 1978). FindBestValue selects the best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity. The expansion of L and the contraction of C continue until C = S:",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,3f3421a27033ea4e7112024fc60d85efd12192f3,Principles of Categorization,1978,E. Rosch
746,W06-1104,External_861,[4],experiments,<TARGET_CITATION/> pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .,"ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. <TARGET_CITATION/> pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs . <CITATION/> pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. ing could be used.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,evaluating wordnetbased measures of semantic distance,2006,Alexander Budanitsky; Graeme Hirst
747,J00-2001,W94-0319,[0],,"In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) <TARGET_CITATION/> .","Whatever problems result will be handled as best they can, on a casebycase basis. This approach is the one taken (implicitly or explicitly) in the majority of generators. In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) <TARGET_CITATION/> . In fact, Reiter has even argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored (or at least handled on an ad hoc basis) <CITATION/>. This approach is the one taken (implicitly or explicitly) in the majority of generators. Whatever problems result will be handled as best they can, on a casebycase basis.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,48f426fe2018022838bebe3744cc728c0b6053f9,"Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible?",1994,Ehud Reiter
748,D11-1138,P06-1043,[4],related work,"The method is called targeted selftraining as it is similar in vein to selftraining <TARGET_CITATION/> , with the exception that the new parse data is targeted to produce accurate word reorderings .","In that work, a parser is used to first parse a set of manually reordered sentences to produce kbest lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted selftraining as it is similar in vein to selftraining <TARGET_CITATION/> , with the exception that the new parse data is targeted to produce accurate word reorderings . The method is called targeted selftraining as it is similar in vein to selftraining <CITATION/>, with the exception that the new parse data is targeted to produce accurate word reorderings. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. In that work, a parser is used to first parse a set of manually reordered sentences to produce kbest lists.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,91fb3e2b1ac9e588037e37e4d9be485e5fd60b27,Reranking and Self-Training for Parser Adaptation,2006,David McClosky; Eugene Charniak; Mark Johnson
749,D10-1074,W09-3930,[3],related work,"In our previous work <TARGET_CITATION/> , we started an initial investigation on conversation entailment .","Recent studies have also developed approaches to summarize conversations <CITATION/> and to model conversation structures (dialogue acts) from online Twitter conversations <CITATION/>. Here we address a different angle regarding conversation scripts, namely conversation entailment. In our previous work <TARGET_CITATION/> , we started an initial investigation on conversation entailment . In our previous work <CITATION/>, we started an initial investigation on conversation entailment. Here we address a different angle regarding conversation scripts, namely conversation entailment. Recent studies have also developed approaches to summarize conversations <CITATION/> and to model conversation structures (dialogue acts) from online Twitter conversations <CITATION/>.",3d0adc6fca3a0669c108958c5d5204e2695ea4db,Towards Conversation Entailment: An Empirical Investigation,2010,Chen Zhang; J. Chai,ee9fb0cb4487d277233f61286eb40637e82dbb5e,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,Chen Zhang; J. Chai
750,P13-3018,External_22873,[0],related work,<TARGET_CITATION/> tried to construct a semantic analysis based on  prepared '' and  unprepared mind '' .,<CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries. <CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. <TARGET_CITATION/> tried to construct a semantic analysis based on  prepared '' and  unprepared mind '' . <CITATION/> tried to construct a semantic analysis based on prepared'' and unprepared mind''. <CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. <CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries.,97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,causal chains and compound verbsquot,1993,E Bashir
751,A00-1019,External_47988,[0],method,"It is also possible to focus on noncompositional compounds , a key point in bilingual applications ( <TARGET_CITATION/> ; Lin , 99 ) .","One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple nounphrase contraints (<CITATION/>; hua Chen and Chen, 94; <CITATION/>). It is also possible to focus on noncompositional compounds , a key point in bilingual applications ( <TARGET_CITATION/> ; Lin , 99 ) . It is also possible to focus on noncompositional compounds, a key point in bilingual applications (<CITATION/>; Lin, 99). One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple nounphrase contraints (<CITATION/>; hua Chen and Chen, 94; <CITATION/>).",7f61670dcf65a166cb9772b2b53870410159722c,Unit Completion for a Computer-aided Translation Typing System,2000,P. Langlais; George F. Foster; G. Lapalme,,a corpusbased approach to automatic compound extraction,1994,Keh-Yih Su; Ming-Wen Wu; Jing-Shin Chang
752,W04-1610,W98-1009,[0],,"This is mainly due to the fact that Arabic is a nonconcatenative language <TARGET_CITATION/> , and that the stem/infix obtained by suppression of infix and prefix addons is not the same for words derived from the same origin called the root .","Then roots are extracted for words in the document. In Arabic, however, the use of stems will not yield satisfactory categorization. This is mainly due to the fact that Arabic is a nonconcatenative language <TARGET_CITATION/> , and that the stem/infix obtained by suppression of infix and prefix addons is not the same for words derived from the same origin called the root . This is mainly due to the fact that Arabic is a nonconcatenative language <CITATION/>, and that the stem/infix obtained by suppression of infix and prefix addons is not the same for words derived from the same origin called the root. In Arabic, however, the use of stems will not yield satisfactory categorization. Then roots are extracted for words in the document.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,dbabddb6be080c07c5bb9af8dcea6a4390955f92,A Computational Morphology System for Arabic,1998,R. Al-Shalabi; M. Evens
754,P02-1001,External_889,[0],introduction,"For example , the forwardbackward algorithm <CITATION/> trains only Hidden Markov Models , while <TARGET_CITATION/> trains only stochastic edit distance .","Currently, finitestate practitioners derive weights using exogenous training methods, then patch them onto transducer arcs. Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. For example , the forwardbackward algorithm <CITATION/> trains only Hidden Markov Models , while <TARGET_CITATION/> trains only stochastic edit distance . For example, the forwardbackward algorithm <CITATION/> trains only Hidden Markov Models, while <CITATION/> trains only stochastic edit distance. Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. Currently, finitestate practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,21924168415196b125c9cb09578774cdae2d9f5a,Learning String-Edit Distance,1996,E. Ristad; P. Yianilos
755,N01-1010,W00-0104,[3],experiments,"Using the treecut technique described above , our previous work <TARGET_CITATION/> extracted systematic polysemy from WordNet ."," Using the treecut technique described above , our previous work <TARGET_CITATION/> extracted systematic polysemy from WordNet . Using the treecut technique described above, our previous work <CITATION/> extracted systematic polysemy from WordNet.",50f15ec5e2ca5d10b1b77dece8f16f354d90f711,Tree-Cut and a Lexicon Based on Systematic Polysemy,2001,Noriko Tomuro,15744f27f57b5cb227c7a0d25b825e5884e086b5,Automatic Extraction of Systematic Polysemy Using Tree-cut,2000,Noriko Tomuro
756,D12-1037,D11-1004,[4],related work,<CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it .,Several works have proposed discriminative techniques to train loglinear model for SMT. <CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it . <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,b4717ab8f647d28dbd1a8319d838ddc2cdfaf050,Optimal Search for Minimum Error Rate Training,2011,Michel Galley; Chris Quirk
757,J09-4010,External_60688,[2],method,"5 Significant bigrams are obtained using the ngram statistics package NSP <TARGET_CITATION/> , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .","Thus, their low TF.IDF score may have an adverse influence on clustering performance. Nonetheless, in the future, it may be worth investigating a TF.IDFbased representation. 5 Significant bigrams are obtained using the ngram statistics package NSP <TARGET_CITATION/> , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) . 5 Significant bigrams are obtained using the ngram statistics package NSP <CITATION/>, which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram (that it is not a collocation).Nonetheless, in the future, it may be worth investigating a TF.IDFbased representation. Thus, their low TF.IDF score may have an adverse influence on clustering performance.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,the design implementation and use of computational linguistics volume 35 number 4 the ngram statistics package in cicling,2003,S Banerjee; T Pedersen
758,N10-1084,External_56138,[0],related work,<TARGET_CITATION/> et al. ( 2006a ) all belong to the syntactic transformation category .,"<CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <TARGET_CITATION/> all belong to the syntactic transformation category . <CITATION/> all belong to the syntactic transformation category. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,e007ec48e92be4b87a6479144e1f4f69909561e6,The syntax of concealment: reliable methods for plain text information hiding,2007,Brian Murphy; Carl Vogel
759,D09-1056,External_84048,[2],experiments,"2The WePS1 corpus includes data from the Web03 testbed <TARGET_CITATION/> which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .","English stopwords were removed, including Web specific stopwords, as file and domain extensions, etc.. We generated word ngrams of length 2 to 5, 2The WePS1 corpus includes data from the Web03 testbed <TARGET_CITATION/> which follows similar annotation guidelines , although the number of document per ambiguous name is more variable . 2The WePS1 corpus includes data from the Web03 testbed <CITATION/> which follows similar annotation guidelines, although the number of document per ambiguous name is more variable. We generated word ngrams of length 2 to 5,English stopwords were removed, including Web specific stopwords, as file and domain extensions, etc..",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,,multidocument statistical fact extraction and fusion,2006,Gideon S Mann
760,D12-1037,External_91021,[0],introduction,The local training method <TARGET_CITATION/> is widely employed in computer vision <CITATION/> .,"Experiments on NIST ChinesetoEnglish translation tasks show that our local training method significantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method.2 Local Training and Testing The local training method <TARGET_CITATION/> is widely employed in computer vision <CITATION/> . The local training method <CITATION/> is widely employed in computer vision <CITATION/>. 2 Local Training and TestingExperiments on NIST ChinesetoEnglish translation tasks show that our local training method significantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,1ca97e1668e305fb719845f84a05a62dfb946a5d,Local Learning Algorithms,1992,L. Bottou; V. Vapnik
761,W03-0806,P02-1030,[0],introduction,Recent work <TARGET_CITATION/> has suggested that some tasks will benefit from using significantly more data .,"This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data <CITATION/>). Recent work <TARGET_CITATION/> has suggested that some tasks will benefit from using significantly more data . Recent work <CITATION/> has suggested that some tasks will benefit from using significantly more data. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data <CITATION/>). This will require more efficient learning algorithms and implementations.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,e8066c5522ebfa8f0f08589dcbe5f315bfec90c1,Scaling Context Space,2002,J. Curran; M. Moens
765,D13-1115,External_32478,[2],experiments,"We use the same method as <TARGET_CITATION/> for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a wordfeature pair .","In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our nontextual modalities. We use the same method as <TARGET_CITATION/> for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a wordfeature pair . We use the same method as <CITATION/> for generating our multimodal corpora: for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a wordfeature pair. In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our nontextual modalities.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
766,D08-1007,N07-1071,[0],method,MI was also recently used for inferencerule SPs by <TARGET_CITATION/> .,"Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to <CITATION/>'s conditional probability scores for pseudodisambiguation of (v, n, n) triples: Pr(vn) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than cooccurrence frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inferencerule SPs by <TARGET_CITATION/> . MI was also recently used for inferencerule SPs by <CITATION/>.Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to <CITATION/>'s conditional probability scores for pseudodisambiguation of (v, n, n) triples: Pr(vn) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than cooccurrence frequency f(v, n).",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,26e17398019d8342e2ae27712af3a147355a141a,ISP: Learning Inferential Selectional Preferences,2007,Patrick Pantel; Rahul Bhagat; Bonaventura Coppola; Timothy Chklovski; E. Hovy
767,D08-1066,External_28803,[4],conclusion,Based on this advise ( Moore and <TARGET_CITATION/> ) exclude the latent segmentation variables and opt for a heuristic training procedure .,"The most similar efforts to ours, mainly <CITATION/>, conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. Based on this advise ( Moore and <TARGET_CITATION/> ) exclude the latent segmentation variables and opt for a heuristic training procedure . Based on this advise <CITATION/> exclude the latent segmentation variables and opt for a heuristic training procedure. The most similar efforts to ours, mainly <CITATION/>, conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.",3701075b318ea45284c9805f2486f3a3177ec87f,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,2008,M. Mylonakis; K. Sima'an,,an iterativelytrained segmentationfree phrase translation model for statistical machine translation,2007,Quirk
768,W03-0806,External_681,[0],introduction,"For example , 10 million words of the American National Corpus <CITATION/> will have manually corrected POS tags , a tenfold increase over the Penn Treebank <TARGET_CITATION/> , currently used for training POS taggers .","NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus <CITATION/> will have manually corrected POS tags , a tenfold increase over the Penn Treebank <TARGET_CITATION/> , currently used for training POS taggers . For example, 10 million words of the American National Corpus <CITATION/> will have manually corrected POS tags, a tenfold increase over the Penn Treebank <CITATION/>, currently used for training POS taggers. Some of this new data will be manually annotated. NLP is experiencing an explosion in the quantity of electronic text available.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,0b44fcbeea9415d400c5f5789d6b892b6f98daff,Building a Large Annotated Corpus of English: The Penn Treebank,1993,Mitchell P. Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz
769,P02-1001,External_24066,[0],,"The forward and backward probabilities , p0j and pkn , can be computed using singlesource algebraic path for the simpler semiring ( R , + , x ,  )  or equivalently , by solving a sparse linear system of equations over R , a muchstudied problem at O ( n ) space , O ( nm ) time , and faster approximations <TARGET_CITATION/> .","This speedup also works for cyclic graphs and for any V . Write wjk as (pjk, vjk), and let w1jk = (p1jk, v1 jk) denote the weight of the edge from j to k.19 Then it can be shown that w0n = (p0n, Ej,k p0jv1jkpkn). The forward and backward probabilities , p0j and pkn , can be computed using singlesource algebraic path for the simpler semiring ( R , + , x ,  )  or equivalently , by solving a sparse linear system of equations over R , a muchstudied problem at O ( n ) space , O ( nm ) time , and faster approximations <TARGET_CITATION/> . The forward and backward probabilities, p0j and pkn, can be computed using singlesource algebraic path for the simpler semiring (R, +, x, )or equivalently, by solving a sparse linear system of equations over R, a muchstudied problem at O(n) space, O(nm) time, and faster approximations <CITATION/>.Write wjk as (pjk, vjk), and let w1jk = (p1jk, v1 jk) denote the weight of the edge from j to k.19 Then it can be shown that w0n = (p0n, Ej,k p0jv1jkpkn). This speedup also works for cyclic graphs and for any V .",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,271d00e59c669bb10ab838dd6c56f82ff5fb7c84,Iterative methods for solving linear systems,1997,A. Greenbaum
770,P02-1001,External_35216,[2],introduction,"4To prove ( 1 )  ( 3 ) , express f as an FST and apply the wellknown KleeneSch  utzenberger construction <TARGET_CITATION/> , taking care to write each regexp in the construction as a constant times a probabilistic regexp .","A central technique is to define a joint relation as a noisychannel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 <CITATION/>. The general form is illustrated by 3Conceptually, the parameters represent the probabilities of reading another a (A); reading another b (); transducing b to p rather than q (); starting to transduce p to a rather than x (p). 4To prove ( 1 )  ( 3 ) , express f as an FST and apply the wellknown KleeneSch  utzenberger construction <TARGET_CITATION/> , taking care to write each regexp in the construction as a constant times a probabilistic regexp . 4To prove (1)(3), express f as an FST and apply the wellknown KleeneSchutzenberger construction <CITATION/>, taking care to write each regexp in the construction as a constant times a probabilistic regexp. The general form is illustrated by 3Conceptually, the parameters represent the probabilities of reading another a (A); reading another b (); transducing b to p rather than q (); starting to transduce p to a rather than x (p).A central technique is to define a joint relation as a noisychannel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 <CITATION/>.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,d26844d389df6aa01152bbe31832ff4ab73f5b01,Rational series and their languages,1988,J. Berstel; C. Reutenauer
772,W14-1815,D10-1051,[4],related work,"Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <TARGET_CITATION/> or song lyrics <CITATION/> ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .","The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as <CITATION/> which produces weather reports from structured data or <CITATION/> which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <TARGET_CITATION/> or song lyrics <CITATION/> ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <CITATION/> (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as <CITATION/> which produces weather reports from structured data or <CITATION/> which generates descriptions of objects from images.",dcd0e19d450a0d43b0d8b32415bb731f5838e593,Natural Language Generation with Vocabulary Constraints,2014,Benjamin Swanson; Elif Yamangil; Eugene Charniak,a5e4c83b816f2f004ae5dfd600145cea9ea15724,Automatic Analysis of Rhythmic Poetry with Applications to Generation and Translation,2010,Eric Greene; Tugba Bodrumlu; Kevin Knight
773,E03-1005,P97-1021,[0],,"Most DOP models , such as in <TARGET_CITATION/> , Sima'an ( 2000 ) and <CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."," Most DOP models , such as in <TARGET_CITATION/> , Sima'an ( 2000 ) and <CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . Most DOP models, such as in <CITATION/>, Sima'an (2000) and <CITATION/>, use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,d7f285369aa64de131505e2afe667760ea7d0c7b,A DOP Model for Semantic Interpretation,1997,R. Bonnema; R. Bod; R. Scha
774,J02-3002,J97-2002,[4],,The best performance on the WSJ corpus was achieved by a combination of the SATZ system <TARGET_CITATION/> with the Alembic system <CITATION/> : a 0.5 % error rate .,Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. Stateoftheart machine learning and rulebased SBD systems achieve an error rate of 0.81.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system <TARGET_CITATION/> with the Alembic system <CITATION/> : a 0.5 % error rate . The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <CITATION/>: a 0.5% error rate. Stateoftheart machine learning and rulebased SBD systems achieve an error rate of 0.81.5% measured on the Brown corpus and the WSJ corpus. Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.,3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,30154464f549643e825ccf60072a17a3e55291d3,To Appear in Computational Linguistics Adaptive Multilingual Sentence Boundary Disambiguation,2004,D. Palmer; Marti A. Hearst
775,J05-3003,J93-2002,[0],related work,"The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following <TARGET_CITATION/> .","He assumes 19 different subcategorizationframe definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following <TARGET_CITATION/> . The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following <CITATION/>. frame definitions, and the extracted frames include details of specific prepositions. He assumes 19 different subcategorization",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,b0e5ab189f770b7e106db429f2980510065ef125,From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax,1993,M. Brent
776,A00-1019,External_32168,[0],method,"This method allows the efficient retrieval of arbitrary length ngrams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; <TARGET_CITATION/> ) .","Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus. For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus. This method allows the efficient retrieval of arbitrary length ngrams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; <TARGET_CITATION/> ) . This method allows the efficient retrieval of arbitrary length ngrams (Nagao and Mori, 94; Haruno et al., 96; Ikehaxa et al., 96; <CITATION/>). For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus. Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.",7f61670dcf65a166cb9772b2b53870410159722c,Unit Completion for a Computer-aided Translation Typing System,2000,P. Langlais; George F. Foster; G. Lapalme,,retrieving collocations by cooccurrences and word order constraints,1997,Sayori Shimohata; Toshiyuki Sugio; Junji Nagata
777,J86-1002,External_33234,[4],,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/> , assertional statements as in <TARGET_CITATION/> , or semantic nets as in <CITATION/> .","It self activates to bias recognition toward historically observed patterns but is not otherwise observable. The VNLCE processor may be considered to be a learning system of the tradition described, for example, in <CITATION/>. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/> , assertional statements as in <TARGET_CITATION/> , or semantic nets as in <CITATION/> . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>. The VNLCE processor may be considered to be a learning system of the tradition described, for example, in <CITATION/>. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,pattern recognition as ruleguided inductive inference,1980,R Michalski
778,D10-1002,D09-1161,[4],conclusion,Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set <TARGET_CITATION/> .,"Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches <CITATION/> and products of latent variable grammars <CITATION/>, despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set <TARGET_CITATION/> . Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set <CITATION/>. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches <CITATION/> and products of latent variable grammars <CITATION/>, despite being a single generative PCFG. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.",e441126a8dd0cb8363272b7b54207ae92e155bc0,Self-Training with Products of Latent Variable Grammars,2010,Zhongqiang Huang; M. Harper; Slav Petrov,31fd675529208d75d8241329905ec8aa86e7a6ca,K-Best Combination of Syntactic Parsers,2009,Hui Zhang; Min Zhang; C. Tan; Haizhou Li
779,J87-3002,External_24080,[2],,To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATRII system ( <TARGET_CITATION/> and references therein ) .,The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATRII system ( <TARGET_CITATION/> and references therein ) . To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATRII system (<CITATION/> and references therein). The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.,998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,decd25cc661adb7c0769588a2c0bf243caacb49b,The Design of a Computer Language for Linguistic Information,1984,Stuart M. Shieber
780,W01-1510,External_78975,[4],introduction,Other works <TARGET_CITATION/> convert HPSG grammars into LTAG grammars .,"Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works <TARGET_CITATION/> convert HPSG grammars into LTAG grammars . Other works <CITATION/> convert HPSG grammars into LTAG grammars. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Thus the translation was manual and grammar dependent.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,adapting hpsgtotag compilation to widecoverage grammars,2000,Tilman Becker; Patrice Lopez
782,P02-1001,External_11575,[5],,Efficient hardware implementation is also possible via chiplevel parallelism <TARGET_CITATION/> .,19Multiple edges from j to k are summed into a single edge.<CITATION/>. Efficient hardware implementation is also possible via chiplevel parallelism <TARGET_CITATION/> . Efficient hardware implementation is also possible via chiplevel parallelism <CITATION/>. <CITATION/>. 19Multiple edges from j to k are summed into a single edge.,683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,5471d9af6f60d104b68fed60fb8020f63fec4729,A systolic array algorithm for the algebraic path problem (shortest paths; Matrix inversion),1985,G. Rote
783,P97-1063,External_9618,[4],method,"2We could just as easily use other symmetric  association '' measures , such as 02 <CITATION/> or the Dice coefficient <TARGET_CITATION/> .","If uk and vk are indeed mutual translations, then their tendency to The cooccurrence frequency of a word type pair is simply the number of times the pair cooccurs in the corpus. However, n(u) = Ev n(u,v), which is not the same as the frequency of u, because each token of u can cooccur with several differentv's. 2We could just as easily use other symmetric  association '' measures , such as 02 <CITATION/> or the Dice coefficient <TARGET_CITATION/> . 2We could just as easily use other symmetric association'' measures, such as 02 <CITATION/> or the Dice coefficient <CITATION/>.However, n(u) = Ev n(u,v), which is not the same as the frequency of u, because each token of u can cooccur with several differentv's. If uk and vk are indeed mutual translations, then their tendency to The cooccurrence frequency of a word type pair is simply the number of times the pair cooccurs in the corpus.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,how to compile a bilingual collocational lexicon automaticallyquot,1992,F Smadja
784,J06-2002,External_2473,[0],experiments,"<TARGET_CITATION/> have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available .","As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous. 9.4.1 A New Perspective on Salience. <TARGET_CITATION/> have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . <CITATION/> have argued that Dale and Reiter's (1995) dichotomy between salient and nonsalient objects (where the objects in the domain are the salient ones) should be replaced by an account that takes degrees of salience into account: No object can be too unsalient to be referred to, as long as the right properties are available. 9.4.1 A New Perspective on Salience. As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,efficient contextsensitive generation of referring expressions,2002,Emiel Krahmer; Mari¨et Theune
785,J86-1002,External_43441,[4],,The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by <TARGET_CITATION/> where program flowcharts were constructed from traces of their behaviors .,"Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be misrecognized or not recognized as is a noun, adjective, determiner, etc.. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by <TARGET_CITATION/> where program flowcharts were constructed from traces of their behaviors . The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by <CITATION/> where program flowcharts were constructed from traces of their behaviors. A verb is just as likely to be misrecognized or not recognized as is a noun, adjective, determiner, etc..Thus, an error in this work has no pattern but occurs probabilistically.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,construction of programs from example computations,1976,A Biermann; R Krishnaswamy
789,W06-3813,External_15774,[0],related work,"In other methods , lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <TARGET_CITATION/> .","Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>. In other methods , lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <TARGET_CITATION/> . In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,0928bfcd69846877287ba124fac5083a5ddf679b,A representation of complex events and processes for the acquisition of knowledge from texts,1998,F. Gomez
790,J86-1002,External_2536,[0],experiments,"The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph <TARGET_CITATION/> , a deep parse of Si , or some other representation .","The expectation parser will then use this information to improve its ability to recognize the next incoming sentence.We denote the meaning of each sentence Si with the notation M(Si). The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph <TARGET_CITATION/> , a deep parse of Si , or some other representation . The exact form of M(Si) need not be discussed at this point; it could be a conceptual dependence graph <CITATION/>, a deep parse of Si, or some other representation. We denote the meaning of each sentence Si with the notation M(Si). The expectation parser will then use this information to improve its ability to recognize the next incoming sentence.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,scripts plans goals and understanding lawrence erlbaum associates,1977,R Schank; R Abelson
791,D12-1037,N09-2006,[0],introduction,"Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one .","where f and e (e') are source and target sentences, respectively. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one . Some methods are based on likelihood <CITATION/>, error rate <CITATION/>, margin <CITATION/> and ranking <CITATION/>, and among which minimum error rate training (MERT) <CITATION/> is the most popular one. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. where f and e (e') are source and target sentences, respectively.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,5cbb632dc743e1498e5df06d105dff58e67260e9,A Simplex Armijo Downhill Algorithm for Optimizing Statistical Machine Translation Decoding Parameters,2009,B. Zhao; Shengyuan Chen
792,W04-0910,External_51291,[0],introduction,"Similarly , <TARGET_CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , <TARGET_CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . Similarly, <CITATION/> learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,learning to paraphrase an unsupervised approahc using mutliplesequence alignment,2003,R Barzilay; L Lee
793,W06-1104,External_61155,[0],related work,This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in <TARGET_CITATION/> .,"<CITATION/> replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values LCB0,1,2,3,4RCB and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in <TARGET_CITATION/> . This setup is also scalable to a higher number of word pairs (350) as was shown in <CITATION/>. She used an adapted experimental setup where test subjects had to assign discrete values LCB0,1,2,3,4RCB and word pairs were presented in isolation. <CITATION/> replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3b5b95038c6b065f29649c1b11ea3e7855c00a53,Thinking beyond the nouns - computing semantic relatedness across parts of speech,2006,Iryna Gurevych
794,W04-1805,W02-1403,[0],related work,"More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : <CITATION/> uses derivational morphology ; <TARGET_CITATION/> use , as a starting point , a number of identical characters .","This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (<CITATION/>, for example), does not specify the relationship itself. Hence, synonyms, cohyponyms, hyperonyms, etc. are not differentiated. More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : <CITATION/> uses derivational morphology ; <TARGET_CITATION/> use , as a starting point , a number of identical characters . More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms: <CITATION/> uses derivational morphology; <CITATION/> use, as a starting point, a number of identical characters. Hence, synonyms, cohyponyms, hyperonyms, etc. are not differentiated. This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (<CITATION/>, for example), does not specify the relationship itself.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,51dd8780f090e58bc617b2baf549b1434a53907f,Lexically-Based Terminology Structuring: Some Inherent Limits,2002,N. Grabar; Pierre Zweigenbaum
795,P10-2059,External_1047,[0],introduction,"<CITATION/> study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English maptask dialogues <TARGET_CITATION/> and find correlations between the various modalities both within and across speakers .","Work has also been done on prosody and gestures in the specific domain of maptask dialogues, also targeted in this paper. <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. <CITATION/> study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English maptask dialogues <TARGET_CITATION/> and find correlations between the various modalities both within and across speakers . <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. Work has also been done on prosody and gestures in the specific domain of maptask dialogues, also targeted in this paper.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,the hcrc map task corpus language and speech,1991,Anne H Anderson; Miles Bader; Ellen Gurman Bard; Elizabeth Boyle; Gwyneth Doherty; Simon Garrod; Stephen Isard; Jacqueline Kowtko; Jan McAllister; Jim Miller; Catherine Sotillo; Henry S Thompson; Regina Weinert
796,J04-3001,P01-1005,[0],related work,"Some examples include text categorization <CITATION/> , base noun phrase chunking <CITATION/> , partofspeech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation <TARGET_CITATION/> , and word sense disambiguation <CITATION/> .","In addition to PPattachment, as discussed in this article, sample selection has been successfully applied to other classificationapplications. Some examples include text categorization <CITATION/> , base noun phrase chunking <CITATION/> , partofspeech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation <TARGET_CITATION/> , and word sense disambiguation <CITATION/> . Some examples include text categorization <CITATION/>, base noun phrase chunking <CITATION/>, partofspeech tagging (Engelson Dagan 1996), spelling confusion set disambiguation <CITATION/>, and word sense disambiguation <CITATION/>. applications. In addition to PPattachment, as discussed in this article, sample selection has been successfully applied to other classification",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,7628b62d64d2e5c33a13a5a473bc41b2391c1ebc,Scaling to Very Very Large Corpora for Natural Language Disambiguation,2001,Michele Banko; Eric Brill
797,J00-3003,J93-3003,[1],,It is known that certain cue words and phrases <TARGET_CITATION/> can serve as explicit indicators of discourse structure .,"Finally, we present results for a combination of all knowledge sources. DA labeling accuracy results should be compared to a baseline (chance) accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) in our test set.4 5.1 Dialogue Act Classification Using Words DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases <TARGET_CITATION/> can serve as explicit indicators of discourse structure . It is known that certain cue words and phrases <CITATION/> can serve as explicit indicators of discourse structure. DA labeling accuracy results should be compared to a baseline (chance) accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) in our test set.4 5.1 Dialogue Act Classification Using Words DA classification using words is based on the observation that different DAs use distinctive word strings. Finally, we present results for a combination of all knowledge sources.",22d45dadde6b5837eff11dc031045754bc5901c3,Dialogue act modeling for automatic tagging and recognition of conversational speech,2000,A. Stolcke; K. Ries; N. Coccaro; Elizabeth Shriberg; R. Bates; Dan Jurafsky; P. Taylor; Rachel Martin; C. V. Ess-Dykema; M. Meteer,ec179f9b30d3a4e0f8a50c86d951557dfcbb20d1,Empirical Studies on the Disambiguation of Cue Phrases,1993,Julia Hirschberg; D. Litman
799,J87-3002,External_29754,[0],,One approach to this problem is that taken by the ASCOT project <TARGET_CITATION/> .,"Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see <CITATION/>, for further comment). One approach to this problem is that taken by the ASCOT project <TARGET_CITATION/> . One approach to this problem is that taken by the ASCOT project <CITATION/>. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see <CITATION/>, for further comment). Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,designing a computerised lexicon for linguistic purposes,1985,Erik Akkerman; Pieter Masereeuw; Willem Meijs
800,D11-1138,W05-0909,[1],experiments,"Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality <TARGET_CITATION/> and is simpler to measure .","In our experiments we work with a set of EnglishJapanese reordering rules1 and gold reorderings based on human generated correct reordering of an aligned target sentences. We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality <TARGET_CITATION/> and is simpler to measure . Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality <CITATION/> and is simpler to measure.We use a reordering score based on the reordering penalty from the METEOR scoring metric. In our experiments we work with a set of EnglishJapanese reordering rules1 and gold reorderings based on human generated correct reordering of an aligned target sentences.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,7533d30329cfdbf04ee8ee82bfef792d08015ee5,METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments,2005,Satanjeev Banerjee; A. Lavie
803,N13-1036,W11-2602,[4],related work,"In our previous work <TARGET_CITATION/> , we applied our approach to tokenized Arabic and our DAMSA transfer component used feature transfer rules only .","Similarly, we use some character normalization rules, a DA morphological analyzer, and DAMSA dictionaries. In contrast, we use handwritten morphosyntactic transfer rules that focus on translating DA morphemes and lemmas to their MSA equivalents. In our previous work <TARGET_CITATION/> , we applied our approach to tokenized Arabic and our DAMSA transfer component used feature transfer rules only . In our previous work <CITATION/>, we applied our approach to tokenized Arabic and our DAMSA transfer component used feature transfer rules only. In contrast, we use handwritten morphosyntactic transfer rules that focus on translating DA morphemes and lemmas to their MSA equivalents. Similarly, we use some character normalization rules, a DA morphological analyzer, and DAMSA dictionaries.",75c71a379a3e5a9641f879a76860071f0f923b16,Dialectal Arabic to English Machine Translation: Pivoting through Modern Standard Arabic,2013,Wael Salloum; Nizar Habash,aaf1c2c9f5057ae675b1bb4ca56922b8750a580d,Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation,2011,Wael Salloum; Nizar Habash
804,W03-0806,W02-0109,[3],,It has already been used to implement a framework for teaching NLP <TARGET_CITATION/> .,"Python has a number of advantages over other options, such as Java and Perl. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. It has already been used to implement a framework for teaching NLP <TARGET_CITATION/> . It has already been used to implement a framework for teaching NLP <CITATION/>. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. Python has a number of advantages over other options, such as Java and Perl.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,77a02706edbdca6ad63ddf903ebd559dd7d423ac,NLTK: The Natural Language Toolkit,2002,E. Loper; Steven Bird
805,Q13-1020,W06-1606,[0],introduction,"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <CITATION/> , 2006 ; <TARGET_CITATION/> ) .","In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <CITATION/> , 2006 ; <TARGET_CITATION/> ) . Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, treebased translation models have shown promising progress in improving translation quality (<CITATION/>, 2009; <CITATION/>, 2006; <CITATION/>). In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,cb59fada125478c0302c6874aa13a83ab0ac62f1,SPMT: Statistical Machine Translation with Syntactified Target Language Phrases,2006,D. Marcu; Wei Wang; Abdessamad Echihabi; Kevin Knight
806,N10-1084,External_56137,[0],related work,<TARGET_CITATION/> all belong to the syntactic transformation category .,"<CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <TARGET_CITATION/> all belong to the syntactic transformation category . <CITATION/> all belong to the syntactic transformation category. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,,syntactic information hiding in plain text masters thesis trinity college dublin,2001,Brian Murphy
808,W06-2807,External_98156,[0],,"On the other side , wikis started as collective works where each entry is not owned by a single author e.g. <TARGET_CITATION/> .","So blogs are a literary metagenre which started as authored personal diaries or journals. Now they try to collect themselves in socalled blogspheres'. On the other side , wikis started as collective works where each entry is not owned by a single author e.g. <TARGET_CITATION/> . On the other side, wikis started as collective works where each entry is not owned by a single author e.g. <CITATION/>. Now they try to collect themselves in socalled blogspheres'. So blogs are a literary metagenre which started as authored personal diaries or journals.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,wikipedia from wikipedia the free encyclopedia,2005,Wikipedia
810,D08-1034,External_34855,[2],,<TARGET_CITATION/> has built a semantic role classifier exploiting the interdependence of semantic roles ., <TARGET_CITATION/> has built a semantic role classifier exploiting the interdependence of semantic roles . <CITATION/> has built a semantic role classifier exploiting the interdependence of semantic roles.,74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,fc9da1be811f02b000883c99f4fb2cf0ae9a1330,Semantic Argument Classification Exploiting Argument Interdependence,2005,Zheng Ping Jiang; Jia Li; H. Ng
811,W06-1639,External_55734,[0],related work,Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <TARGET_CITATION/> .,"There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. An exception is <CITATION/>, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <TARGET_CITATION/> . Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <CITATION/>. An exception is <CITATION/>, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,7c06832d8f13e4faf149fff4c1368ac4fab1b6d8,Automated classification of congressional legislation,2006,Stephen Purpura; D. Hillard
812,D08-1004,P04-1035,[3],experiments,"8 It is based on the dataset of <TARGET_CITATION/> ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0  F9 ) .","In <CITATION/>, we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.'' 8 It is based on the dataset of <TARGET_CITATION/> ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0  F9 ) . 8 It is based on the dataset of <CITATION/>,9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F0F9). In <CITATION/>, we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.''",14e2aec7e25d8880a851a547cf8d27a9721f8e6c,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,2008,Omar Zaidan; Jason Eisner,167e1359943b96b9e92ee73db1df69a1f65d731d,A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts,2004,B. Pang; Lillian Lee
813,J90-3003,P86-1022,[3],introduction,"In previous work <TARGET_CITATION/> , we described an experimental texttospeech system that determined prosodic phrasing for the Olive  Liberman synthesizer <CITATION/> ."," In previous work <TARGET_CITATION/> , we described an experimental texttospeech system that determined prosodic phrasing for the Olive  Liberman synthesizer <CITATION/> . In previous work <CITATION/>, we described an experimental texttospeech system that determined prosodic phrasing for the OliveLiberman synthesizer <CITATION/>.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,826c7abff3ea69c74ac1dc16acd1c381516b1712,The Contribution of Parsing to Prosodic Phrasing in an Experimental Text-to-Speech System,1986,J. Bachenko; Eileen Fitzpatrick; C. Wright
815,N01-1006,P98-1029,[2],experiments,"The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by <TARGET_CITATION/> .","The goal of this task is to assign to each word in the given sentence a tag corresponding to its part of speech. A multitude of approaches have been proposed to solve this problem, including transformationbased learning, Maximum Entropy models, Hidden Markov models and memorybased approaches. The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by <TARGET_CITATION/> . The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by <CITATION/>. A multitude of approaches have been proposed to solve this problem, including transformationbased learning, Maximum Entropy models, Hidden Markov models and memorybased approaches. The goal of this task is to assign to each word in the given sentence a tag corresponding to its part of speech.",c52f80f056a2de8f503bf912e8025413ec2111ec,Transformation Based Learning in the Fast Lane,2001,G. Ngai; Radu Florian,8e824aaf67f4f4f068455c6dbb7a6ed877794bd6,Classifier Combination for Improved Lexical Disambiguation,1998,Eric Brill; Jun Wu
816,J97-4003,External_98507,[1],introduction,As shown in <TARGET_CITATION/> this is a wellmotivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .,"Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in <TARGET_CITATION/> this is a wellmotivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries . As shown in <CITATION/> this is a wellmotivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries.This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,6fe64d5e31abdf3d3673decbafc7d6ddfaab1e24,"On Implementing an HPSG theory - Aspects of the logical architecture, the formalization, and the implementation of head-driven phrase structure grammars",1994,Walt Detmar Meurers
819,P07-1068,External_5960,[2],introduction,"Following <CITATION/> , we represent use the ACE training data for acquiring our SC clasSCA as a binary value that indicates whether the insifier ; instead , we use the BBN Entity Type Corpus duced SCs of the two NPs involved are the same or <TARGET_CITATION/> , which consists of not .","UnDerive two knowledge sources for coreference like them, (1) we do not perform the full MD task, resolution from the induced SCs. The first as our goal is to investigate the role of SC knowlknowledge source (KS) is semantic class agreement edge in coreference resolution; and (2) we do not (SCA). Following <CITATION/> , we represent use the ACE training data for acquiring our SC clasSCA as a binary value that indicates whether the insifier ; instead , we use the BBN Entity Type Corpus duced SCs of the two NPs involved are the same or <TARGET_CITATION/> , which consists of not . Following <CITATION/>, we represent use the ACE training data for acquiring our SC clasSCA as a binary value that indicates whether the insifier; instead, we use the BBN Entity Type Corpus duced SCs of the two NPs involved are the same or <CITATION/>, which consists of not. The first as our goal is to investigate the role of SC knowlknowledge source (KS) is semantic class agreement edge in coreference resolution; and (2) we do not (SCA). UnDerive two knowledge sources for coreference like them, (1) we do not perform the full MD task, resolution from the induced SCs.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,,bbn pronoun coreference and entity type corpus linguistica data consortium,2005,R Weischedel; A Brunstein
820,W14-1609,W09-1119,[2],introduction,"Following <TARGET_CITATION/> , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .","In our baseline system we use lexicons of months, days, person names, companies, job titles, places, events, organizations, books, films, and some minor others. These lexicons were gathered from US Census data, Wikipedia category pages, and Wikipedia redirects (and will be made publicly available upon publication). Following <TARGET_CITATION/> , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document . Following <CITATION/>, we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document. These lexicons were gathered from US Census data, Wikipedia category pages, and Wikipedia redirects (and will be made publicly available upon publication). In our baseline system we use lexicons of months, days, person names, companies, job titles, places, events, organizations, books, films, and some minor others.",d53d878cf1a3f0bed5d9c68c925994cb72f47304,Lexicon Infused Phrase Embeddings for Named Entity Resolution,2014,Alexandre Passos; Vineet Kumar; A. McCallum,aa9efc8b2737eac0675ba5abb5feab8305482c12,Design Challenges and Misconceptions in Named Entity Recognition,2009,Lev-Arie Ratinov; D. Roth
822,A00-1018,External_30238,[2],,We do this with a firstorder HMM partofspeech tagger ( Merialdo <TARGET_CITATION/> ) .,"Partofspeech tagger. Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates)) These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them. We do this with a firstorder HMM partofspeech tagger ( Merialdo <TARGET_CITATION/> ) . We do this with a firstorder HMM partofspeech tagger (<CITATION/>).Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates)) These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them. Partofspeech tagger.",a0f9181bae5840fa6d1d19a694bd9338b2c96443,An Automatic Reviser: The TransCheck System,2000,Jean-Marc Jutras,4614650c3bb3e835c80612d3bca9586f81db95a3,Tagging English Text with a Probabilistic Model,1994,B. Mérialdo
823,J04-3001,External_9007,[2],,"Using the bottomup , dynamic programming technique ( see the appendix for details ) of computing inside probabilities <TARGET_CITATION/> , we can efficiently compute the probability of the sentence , P ( w  G ) .","Note that P(v  w, G) can be viewed as a density function p(v) (i.e., the probability of assigning v to a random variable V). Mapping it back into the entropy definition from equation (3), we derive the tree entropy of w as follows: Using the bottomup , dynamic programming technique ( see the appendix for details ) of computing inside probabilities <TARGET_CITATION/> , we can efficiently compute the probability of the sentence , P ( w  G ) . Using the bottomup, dynamic programming technique (see the appendix for details) of computing inside probabilities <CITATION/>, we can efficiently compute the probability of the sentence, P(w  G). Mapping it back into the entropy definition from equation (3), we derive the tree entropy of w as follows:Note that P(v  w, G) can be viewed as a density function p(v) (i.e., the probability of assigning v to a random variable V).",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language,1990,Karim A Lari; Steve J Young
827,D14-1083,P13-2142,[3],experiments,"This choice is motivated by an observation we made previously <TARGET_CITATION/> : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) <CITATION/> .","In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously <TARGET_CITATION/> : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) <CITATION/> . This choice is motivated by an observation we made previously <CITATION/>: since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) <CITATION/>. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. In P2, on the other hand, we recast SC as a sequence labeling task.",3543efd388e2c1af4b64ec69d4b8510570c01f8c,Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates,2014,K. Hasan; Vincent Ng,b634a489624ead65075b3b9caae77eab9344174d,Extra-Linguistic Constraints on Stance Recognition in Ideological Debates,2013,K. Hasan; Vincent Ng
828,P97-1063,External_6444,[0],introduction,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <TARGET_CITATION/> ."," Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <TARGET_CITATION/> . Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,a statistical approach to language translationquot,1988,P F Brown; J Cocke; S Della Pietra; V Della Pietra; F Jelinek; R Mercer; P Roossin
829,J00-2005,J98-3004,[0],introduction,"This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems <TARGET_CITATION/> .","This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by <CITATION/>. This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems <TARGET_CITATION/> . This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems <CITATION/>.Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by <CITATION/>. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module.",f10e6b08a31d42bd0c6f51808cfa1058d170fd49,Pipelines and size constraints,2000,Ehud Reiter,f569c8bcddfbfb57ab03b5a74c794fb95fa16f9b,Describing Complex Charts in Natural Language: A Caption Generation System,1998,Vibhu Mittal; Johanna D. Moore; G. Carenini; Steven F. Roth
830,N04-2004,External_5189,[0],introduction,"This approach has its roots in Fillmore 's Case <CITATION/> , and serves as the foundation for two current largescale semantic annotation projects : FrameNet <CITATION/> and PropBank <TARGET_CITATION/> .","A common lexical semantic representation in the computational linguistics literature is a framebased model where syntactic arguments are associated with various semantic roles (essentially frame slots). Verbs are viewed as simple predicates over their arguments. This approach has its roots in Fillmore 's Case <CITATION/> , and serves as the foundation for two current largescale semantic annotation projects : FrameNet <CITATION/> and PropBank <TARGET_CITATION/> . This approach has its roots in Fillmore's Case <CITATION/>, and serves as the foundation for two current largescale semantic annotation projects: FrameNet <CITATION/> and PropBank <CITATION/>. Verbs are viewed as simple predicates over their arguments. A common lexical semantic representation in the computational linguistics literature is a framebased model where syntactic arguments are associated with various semantic roles (essentially frame slots).",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,621566b223a73c0a7d8cf918297ac02c5e58af38,Adding Semantic Annotation to the Penn TreeBank,1998,Andrew J. Fleming; Paul R. Kingsbury; Martha Palmer; Mitchell P. Marcus
831,J87-3002,External_8297,[4],introduction,"Two exceptions to this generalisation are the Linguistic String Project <TARGET_CITATION/> and the IBM CRITIQUE ( formerly EPISTLE ) Project <CITATION/> ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .","Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. <CITATION/>) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project <TARGET_CITATION/> and the IBM CRITIQUE ( formerly EPISTLE ) Project <CITATION/> ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . Two exceptions to this generalisation are the Linguistic String Project <CITATION/> and the IBM CRITIQUE (formerly EPISTLE) Project <CITATION/>; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. <CITATION/>) consult relatively small lexicons, typically generated by hand. Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,184cd16220d0cf381d216d516654c19de22b6309,Natural Language Information Processing Based on the Valid Traditional Syllogisms EIO-4,2019,Feifei Yang; Xiaojun Zhang
832,W00-1017,A00-1014,[4],,They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of <TARGET_CITATION/> .,"In the first two phases, the user holds the initiative, and in the last phase, the system holds the initiative. Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of <TARGET_CITATION/> . They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of <CITATION/>. Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. In the first two phases, the user holds the initiative, and in the last phase, the system holds the initiative.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll
833,N10-1084,External_9487,[0],experiments,"The Google ngram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation <TARGET_CITATION/> , and contains English ngrams and their observed frequency counts , for counts of at least 40 ."," The Google ngram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation <TARGET_CITATION/> , and contains English ngrams and their observed frequency counts , for counts of at least 40 . The Google ngram data was collected by Google Research for statistical language modelling, and has been used for many tasks such as lexical disambiguation <CITATION/>, and contains English ngrams and their observed frequency counts, for counts of at least 40.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,,webscale ngram models for lexical disambiguation,2009,Shane Bergsma; Dekang Lin; Randy Goebel
834,D09-1056,N04-1002,[0],related work,Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <TARGET_CITATION/> .,Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <TARGET_CITATION/> . Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. The most basic is a Bag of Words (BoW) representation of the document text. Many different features have been used to represent documents where an ambiguous name is mentioned.,a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,6322fcf2cb8af5a557cc20fd5757d249c3dcdd3a,Cross-Document Coreference on a Large Scale Corpus,2004,Chung Heong Gooi; James Allan
835,W04-0910,External_51289,[2],,"To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.<TARGET_CITATION/> ) .","That is, we need to fix a concept inventory and to use this inventory in a consistent way in particular, by assigning synonyms the same concept. For non predicative units, we use WordNet synset numbers or when working within a restricted domain with a well defined thesaurus, the descriptors of that thesaurus. To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.<TARGET_CITATION/> ) . To represent the semantics of predicative units, we use FrameNet inventory of frames and frame elements (C.<CITATION/>). For non predicative units, we use WordNet synset numbers or when working within a restricted domain with a well defined thesaurus, the descriptors of that thesaurus. That is, we need to fix a concept inventory and to use this inventory in a consistent way in particular, by assigning synonyms the same concept.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,469449353f4b546716ea31243ff52a628b3b1788,FrameNet: Theory and Practice,2003,Christopher R. Johnson; Miriam R. L. Petruck; Collin F. Baker; Michael Ellsworth; Josef Ruppenhofer; C. Fillmore
836,P13-3018,External_83583,[0],related work,Similar observation for surface word frequency was also observed by <TARGET_CITATION/> where it has been claimed that words having low surface frequency tends to decompose .,"It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations. <CITATION/> with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. Similar observation for surface word frequency was also observed by <TARGET_CITATION/> where it has been claimed that words having low surface frequency tends to decompose . Similar observation for surface word frequency was also observed by <CITATION/> where it has been claimed that words having low surface frequency tends to decompose. <CITATION/> with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency. It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,0c3e2e233a752d09cd29b4fe9bd727bd0c2a1780,Morphological Decomposition and the Reverse Base Frequency Effect,2004,M. Taft
837,P10-2059,External_345,[2],,"These two sets of data were used for automatic dialogue act classification , which was run in the Weka system <TARGET_CITATION/> .","5 Analysis of the dataThe multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification , which was run in the Weka system <TARGET_CITATION/> . These two sets of data were used for automatic dialogue act classification, which was run in the Weka system <CITATION/>. The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.5 Analysis of the data",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,730ca170962a58607e092035beb2afc4b5fa6242,Data Mining Practical Machine Learning Tools and Techniques,2014,อนิรุธ สืบสิงห์
838,W00-1312,External_3897,[2],, The transition probability a is 0.7 using the EM algorithm <TARGET_CITATION/> on the TREC4 adhoc query set .,"According to that model of monolingual retrieval, it can be shown thatwhere W's are query words in Q. Miller et al. estimated probabilities as follows: The transition probability a is 0.7 using the EM algorithm <TARGET_CITATION/> on the TREC4 adhoc query set .  The transition probability a is 0.7 using the EM algorithm <CITATION/> on the TREC4 adhoc query set. where W's are query words in Q. Miller et al. estimated probabilities as follows:According to that model of monolingual retrieval, it can be shown that",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5,A tutorial on hidden Markov models and selected applications in speech recognition,1989,L. Rabiner
839,P10-2059,External_10875,[0],introduction,"<TARGET_CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical , syntactic and prosodic cues , while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues .","Related are also the studies by Rieks op den <CITATION/>: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. Work has also been done on prosody and gestures in the specific domain of maptask dialogues, also targeted in this paper. <TARGET_CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical , syntactic and prosodic cues , while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues . <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. Work has also been done on prosody and gestures in the specific domain of maptask dialogues, also targeted in this paper. Related are also the studies by Rieks op den <CITATION/>: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,a125cd860ec9fedd468e86458c755494f3c14cb5,"Combining lexical, syntactic and prosodic cues for improved online dialog act tagging",2009,V. Sridhar; S. Bangalore; Shrikanth S. Narayanan
840,W04-1610,External_1878,[0],experiments,"<TARGET_CITATION/> has found strong correlations between DF , IG and the X2 statistic for a term .","Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) <CITATION/>, minimum description length principal <CITATION/>, and the X2 statistic. <TARGET_CITATION/> has found strong correlations between DF , IG and the X2 statistic for a term . <CITATION/> has found strong correlations between DF, IG and the X2 statistic for a term. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) <CITATION/>, minimum description length principal <CITATION/>, and the X2 statistic. Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,c3ebcef26c22a373b6f26a67934213eb0582804e,A Comparative Study on Feature Selection in Text Categorization,1997,Yiming Yang; Jan O. Pedersen
841,J09-4010,External_52005,[0],introduction,"Despite this , to date , there has been little work on corpusbased approaches to helpdesk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; <TARGET_CITATION/> ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .","An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that helpdesk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for humangenerated responses if necessary. Despite this , to date , there has been little work on corpusbased approaches to helpdesk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; <TARGET_CITATION/> ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) . Despite this, to date, there has been little work on corpusbased approaches to helpdesk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). This indicates that helpdesk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for humangenerated responses if necessary. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,mercure towards an automatic email followup system,2003,G Lapalme; L Kosseim
842,P00-1012,External_69922,[0],experiments,"To quantify the relative strengths of these transitive inferences , <TARGET_CITATION/> propose to assign a weight to each link .","Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second. The first ordered pairs are more frequent, as are the individual adjectives involved. To quantify the relative strengths of these transitive inferences , <TARGET_CITATION/> propose to assign a weight to each link . To quantify the relative strengths of these transitive inferences, <CITATION/> propose to assign a weight to each link. The first ordered pairs are more frequent, as are the individual adjectives involved. Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.",a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf,4eec0a1ce645151c4acc89b151814b3deb53af43,Ordering Among Premodifiers,1999,James Shaw; V. Hatzivassiloglou
843,P07-1068,External_3699,[0],introduction,While these approaches have been reasonably successful <TARGET_CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .,"In the past decade, knowledgelean approaches have significantly influenced research in noun phrase (NP) coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document. In knowledgelean approaches, coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process (e.g., <CITATION/>). While these approaches have been reasonably successful <TARGET_CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In knowledgelean approaches, coreference resolvers employ only morphosyntactic cues as knowledge sources in the resolution process (e.g., <CITATION/>). In the past decade, knowledgelean approaches have significantly influenced research in noun phrase (NP) coreference resolution  the problem of determining which NPs refer to the same realworld entity in a document.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,e371560794e848ef4225577061b2ed280a57123d,Anaphora Resolution,2020,R. Mitkov
844,J87-3002,External_22623,[0],,"Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , <TARGET_CITATION/> ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .","relational, type. In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage. Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , <TARGET_CITATION/> ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape . Lisp is not particularly well suited for interfacing to complex, structured objects, and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary (of the style described in, eg., Tompa 1986); on the other hand a method of access was clearly required, which was flexible enough to support a range of applications intending to make use of the LDOCE tape. In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage. relational, type.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,database design for a dictionary of the future,1986,Frank Tompa
845,D12-1037,External_762,[2],experiments,We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <CITATION/> with modified KneserNey smoothing <TARGET_CITATION/> .,"The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA++ <CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair. We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <CITATION/> with modified KneserNey smoothing <TARGET_CITATION/> . We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <CITATION/> with modified KneserNey smoothing <CITATION/>. We run GIZA++ <CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,d4e8bed3b50a035e1eabad614fe4218a34b3b178,An Empirical Study of Smoothing Techniques for Language Modeling,1996,Stanley F. Chen; Joshua Goodman
846,J03-3004,External_16447,[0],introduction, Learnability <CITATION/>  Text generation <TARGET_CITATION/>  Speech generation <CITATION/>  Localization ( Sch  aler 1996 ),"Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: Learnability <CITATION/>  Text generation <TARGET_CITATION/>  Speech generation <CITATION/>  Localization ( Sch  aler 1996 )  Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization <CITATION/>More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,d222e0489b7f3a07d37b49e280f0b5d34d6f1d8c,Generating language with a phrasal lexicon,1988,E. Hovy
847,A00-1020,W99-0104,[0],,Details of the top performing heuristics of COCKTAIL were reported in <TARGET_CITATION/> .,Table 1 lists the top performing heuristics of COCKTAIL for pronominal and nominal coreference. Examples of the heuristics operation on the MUC data are presented presented in Table 2. Details of the top performing heuristics of COCKTAIL were reported in <TARGET_CITATION/> . Details of the top performing heuristics of COCKTAIL were reported in <CITATION/>.Examples of the heuristics operation on the MUC data are presented presented in Table 2. Table 1 lists the top performing heuristics of COCKTAIL for pronominal and nominal coreference.,76894392818a9a360feaf2f1a797bbe1eaac82b0,Multilingual Coreference Resolution,2000,S. Harabagiu; Steven J. Maiorano,df9490b518e726a96879f16c92d46e9d4f883c1b,Knowledge-Lean Coreference Resolution and its Relation to Textual Cohesion and Coherence,1999,S. Harabagiu; Steven J. Maiorano
848,J92-1004,H89-1012,[0],introduction,"Representative systems are described in <TARGET_CITATION/> , De <CITATION/> .","In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in <TARGET_CITATION/> , De <CITATION/> . Representative systems are described in <CITATION/>, De <CITATION/>.Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,c730d6ec9ffe803eb4d874565b39c5d967fca219,The BBN Spoken Language System,1989,S. Boisen; Y. Chow; A. Haas; Robert Ingria; S. Roukos; D. Stallard
849,J87-3002,External_43912,[4],,"In addition , <TARGET_CITATION/> note that our Object Raising rule would assign mean to this category incorrectly .","The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require. None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. In addition , <TARGET_CITATION/> note that our Object Raising rule would assign mean to this category incorrectly . In addition, <CITATION/> note that our Object Raising rule would assign mean to this category incorrectly. None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,computer exploitation of ldoces grammatical codes paper presented at a conference on survey of english language,1985,A Moulin; J Jansen; A Michiels
850,N13-1036,External_3667,[2],,The parallel corpus is wordaligned using GIZA + + <TARGET_CITATION/> .,We use the opensource Moses toolkit <CITATION/> to build a phrasebased SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data. Our system uses a standard phrasebased architecture. The parallel corpus is wordaligned using GIZA + + <TARGET_CITATION/> . The parallel corpus is wordaligned using GIZA++ <CITATION/>. Our system uses a standard phrasebased architecture. We use the opensource Moses toolkit <CITATION/> to build a phrasebased SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.,75c71a379a3e5a9641f879a76860071f0f923b16,Dialectal Arabic to English Machine Translation: Pivoting through Modern Standard Arabic,2013,Wael Salloum; Nizar Habash,de2df29b0a0312de7270c3f5a0af6af5645cf91a,A Systematic Comparison of Various Statistical Alignment Models,2003,F. Och; H. Ney
851,P07-1068,External_380,[2],introduction,"tions for the remaining 20 % of the instances ; and ( 3 ) train an SVM classifier ( using the LIBSVM package <TARGET_CITATION/> ) on these 20 % of the instances , where each instance , i , is represented by a set of 31 binary features .","Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.'s method to independently make predic7See http://www.cs.ualberta.ca/lindek/downloads.htm 8In our implementation of Soon's method, we label an instance as OTHERS if no NE or WN CLASS feature is generated; otherwise its label is the value of the NE feature or the ACE SC that has the WN CLASS features as its keywords (see Table 1). tions for the remaining 20 % of the instances ; and ( 3 ) train an SVM classifier ( using the LIBSVM package <TARGET_CITATION/> ) on these 20 % of the instances , where each instance , i , is represented by a set of 31 binary features . tions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package <CITATION/>) on these 20% of the instances, where each instance, i, is represented by a set of 31 binary features. 7See http://www.cs.ualberta.ca/lindek/downloads.htm 8In our implementation of Soon's method, we label an instance as OTHERS if no NE or WN CLASS feature is generated; otherwise its label is the value of the NE feature or the ACE SC that has the WN CLASS features as its keywords (see Table 1).Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.'s method to independently make predic",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,,libsvm a library for support vector machines software available at httpwwwcsientuedutw cjlinlibsvm,2001,C-C Chang; C-J Lin
852,J04-3001,P92-1017,[4],,Our algorithm is similar to the approach taken by <TARGET_CITATION/> for inducing PCFG parsers .,"For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as ((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)'' Our algorithm is similar to the approach taken by <TARGET_CITATION/> for inducing PCFG parsers . Our algorithm is similar to the approach taken by <CITATION/> for inducing PCFG parsers. would be labeled as ((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)'' For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,15e4843e2c55843b5c5b429f89dad3d99e801f02,Inside-Outside Reestimation From Partially Bracketed Corpora,1992,Fernando C Pereira; Yves Schabes
853,W06-1639,External_5221,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,cbb786e7e42d02de0080299dc2e114357e816002,Conditional Models of Identity Uncertainty with Application to Noun Coreference,2004,A. McCallum; Ben Wellner
854,W06-1104,External_3502,[0],related work,This experiment was again replicated by <TARGET_CITATION/> with 10 subjects .,Test subjects were instructed to order the cards according to the similarity of meaning'' and then assign a continuous similarity value (0.0 4.0) to each card. <CITATION/> replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. This experiment was again replicated by <TARGET_CITATION/> with 10 subjects . This experiment was again replicated by <CITATION/> with 10 subjects. <CITATION/> replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs. Test subjects were instructed to order the cards according to the similarity of meaning'' and then assign a continuous similarity value (0.0 4.0) to each card.,e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,265be00bf112c6cb2fa3e8176bff8394a114dbde,Using Information Content to Evaluate Semantic Similarity in a Taxonomy,1995,P. Resnik
855,J06-2002,External_40450,[0],,Children use vague adjectives among their first dozens of words <CITATION/> and understand some of their intricacies as early as their 24th month <TARGET_CITATION/> .,"Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words <CITATION/> and understand some of their intricacies as early as their 24th month <TARGET_CITATION/> . Children use vague adjectives among their first dozens of words <CITATION/> and understand some of their intricacies as early as their 24th month <CITATION/>. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Gradability is especially widespread in adjectives.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,08cd4b60cb85fc7f89de5c6324f76b7622891a83,"Children's use of context in interpreting ""big"" and ""little"".",1994,K. Ebeling; S. Gelman
857,D11-1138,N09-1028,[0],introduction,"This includes work on question answering <CITATION/> , sentiment analysis <CITATION/> , MT reordering <TARGET_CITATION/> , and many other tasks .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <CITATION/> , sentiment analysis <CITATION/> , MT reordering <TARGET_CITATION/> , and many other tasks . This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,7d7a1f1412a0b078b29278b94cf4c5ac75f36d53,Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages,2009,P. Xu; Jaeho Kang; Michael Ringgaard; F. Och
858,P07-1068,External_2201,[2],introduction,"( 4 ) NE : We use BBN 's IdentiFinder <TARGET_CITATION/> , a MUCstyle NE recognizer to determine the NE type of NPZ .","(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NPZ participates in a verbobject relation. Again, this represents our attempt to coarsely model subcategorization. ( 4 ) NE : We use BBN 's IdentiFinder <TARGET_CITATION/> , a MUCstyle NE recognizer to determine the NE type of NPZ . (4) NE: We use BBN's IdentiFinder <CITATION/>, a MUCstyle NE recognizer to determine the NE type of NPZ. Again, this represents our attempt to coarsely model subcategorization. (3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NPZ participates in a verbobject relation.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,927abef52678ed23edf6c508ef7a26569440a329,An Algorithm that Learns What's in a Name,1999,D. Bikel; R. Schwartz; R. Weischedel
859,D08-1034,N07-1069,[4],conclusion,<TARGET_CITATION/> has made the first attempt working on the single semantic role level to make further improvement .,"Then we integrated the idea of exploiting argument interdependence to further improve the performance of our system and explained linguistically why the results of our system were different from the ones in previous research. Although we make discriminations of arguments and adjuncts, the analysis is still coarsegrained. <TARGET_CITATION/> has made the first attempt working on the single semantic role level to make further improvement . <CITATION/> has made the first attempt working on the single semantic role level to make further improvement. Although we make discriminations of arguments and adjuncts, the analysis is still coarsegrained. Then we integrated the idea of exploiting argument interdependence to further improve the performance of our system and explained linguistically why the results of our system were different from the ones in previous research.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,3d1f6b2321f4e94441e4d962c01042a1d919f6a1,Can Semantic Roles Generalize Across Genres?,2007,Szu-ting Yi; E. Loper; Martha Palmer
860,W06-1639,P02-1053,[0],introduction,"In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> .","Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> . In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,9e7c7853a16a378cc24a082153b282257a9675b7,Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews,2002,Peter D. Turney
861,W14-3902,W13-2249,[3],experiments,"raw length value as a feature , we follow our previous work <TARGET_CITATION/> and create multiple features for length using a decision tree ( J48 ) .","3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work <TARGET_CITATION/> and create multiple features for length using a decision tree ( J48 ) . raw length value as a feature, we follow our previous work <CITATION/> and create multiple features for length using a decision tree (J48). Length of words (L): Instead of using the3.",004c857f76779260664a9907645938c96cb1896a,Code Mixing: A Challenge for Language Identification in the Language of Social Media,2014,Utsab Barman; Amitava Das; Joachim Wagner; Jennifer Foster,09bd7a01202263a130bc2562452feb31b92d8389,DCU-Symantec at the WMT 2013 Quality Estimation Shared Task,2013,Raphaël Rubino; Joachim Wagner; Jennifer Foster; Johann Roturier; R. Kaljahi; Fred Hollowood
863,W06-3309,N04-1015,[2],method,"Following <TARGET_CITATION/> , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts ."," Following <TARGET_CITATION/> , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts . Following <CITATION/>, we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,984efc8932edb635d09ec1a5fd8fc1d1ceccad45,"Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",2004,R. Barzilay; Lillian Lee
864,D13-1115,External_9239,[0],related work,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) ."," The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) . The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,23e568fcf0192e4ff5e6bed7507ee5b9e6c43598,Relative attributes,2011,Devi Parikh; K. Grauman
865,Q13-1020,P08-1066,[0],introduction,"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <CITATION/> , 2006 ; <TARGET_CITATION/> ) .","In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <CITATION/> , 2006 ; <TARGET_CITATION/> ) . Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, treebased translation models have shown promising progress in improving translation quality (<CITATION/>, 2009; <CITATION/>, 2006; <CITATION/>). In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,248d32911670e551db4835a5a5279d2d9673ee37,A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model,2008,Libin Shen; Jinxi Xu; R. Weischedel
866,A00-1014,External_23295,[0],,Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction <TARGET_CITATION/> .,"user queries (steps 18)5 (van <CITATION/>), and 2) providing answers to wellformed queries (steps 911).3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction <TARGET_CITATION/> . Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction <CITATION/>. 3.2.3 Strategy Selectionuser queries (steps 18)5 (van <CITATION/>), and 2) providing answers to wellformed queries (steps 911).",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,4e6795d8406bde57551d6acff131d4f47435bcb6,Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation,1990,M. Walker; S. Whittaker
867,W03-0806,External_23777,[0],experiments,Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) <CITATION/> and the Alembic Workbench <TARGET_CITATION/> ) as well as NLP tools and resources that can be manipulated from the GUI .,There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) <CITATION/> and the Alembic Workbench <TARGET_CITATION/> ) as well as NLP tools and resources that can be manipulated from the GUI . Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) <CITATION/> and the Alembic Workbench <CITATION/>) as well as NLP tools and resources that can be manipulated from the GUI. There are a number of generalised NLP systems in the literature.,7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,mixedinitiative development of language processing systems,1997,David Day; John Aberdeen; Lynette Hirschman; Robyn Kozierok; Patricia Robinson; Marc Vilain
868,J86-1002,External_33235,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , <TARGET_CITATION/> , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."," A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , <TARGET_CITATION/> , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,the sperry univac system for continuous speech recognition in,1980,M Medress
869,W02-1601,External_69141,[0],,It allows the construction of a nonTAL <TARGET_CITATION/> .,"STAG is a variant of Tree Adjoining Grammar (TAG) introduced by <CITATION/> to characterize correspondences between tree adjoining languages. Considering the original definition of STAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a nonTAL <TARGET_CITATION/> . It allows the construction of a nonTAL <CITATION/>. Considering the original definition of STAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. STAG is a variant of Tree Adjoining Grammar (TAG) introduced by <CITATION/> to characterize correspondences between tree adjoining languages.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,,nonisomorphic synchronous tags,2000,K Harbusch; P Poller
870,W06-1705,External_25122,[0],introduction,"In addition , the advantages of using linguistically annotated data over raw data are well documented <TARGET_CITATION/> .","Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem <CITATION/>. This topic generated intense interest at workshops held at the University of Heidelberg <CITATION/>, University of Bologna <CITATION/>, University of Birmingham <CITATION/> and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented <TARGET_CITATION/> . In addition, the advantages of using linguistically annotated data over raw data are well documented <CITATION/>. This topic generated intense interest at workshops held at the University of Heidelberg <CITATION/>, University of Bologna <CITATION/>, University of Birmingham <CITATION/> and now in Trento in April 2006. Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem <CITATION/>.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,2a1373db2d4c179d935d2830697eaee464996cf7,Automatic Profiling of Learner Texts,1998,S. Granger; Paul Rayson
871,W06-1104,External_76806,[1],experiments,"In particular , the  Semantic Information Retrieval '' project ( SIR <TARGET_CITATION/> ) systematically investigates the use of lexicalsemantic relations between words or concepts for improving the performance of information retrieval systems .","We extracted word pairs from three different domainspecific corpora (see Table 2). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular , the  Semantic Information Retrieval '' project ( SIR <TARGET_CITATION/> ) systematically investigates the use of lexicalsemantic relations between words or concepts for improving the performance of information retrieval systems . In particular, the Semantic Information Retrieval'' project (SIR <CITATION/>) systematically investigates the use of lexicalsemantic relations between words or concepts for improving the performance of information retrieval systems. This is motivated by the aim to enable research in information retrieval incorporating SR measures. We extracted word pairs from three different domainspecific corpora (see Table 2).",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,project ‘semantic information retrieval’,2006,SIR Project
873,J02-3002,M95-1012,[0],,"For instance , the Alembic workbench <TARGET_CITATION/> contains a sentencesplitting module that employs over 100 regularexpression rules written in Flex .","The rulebased systems use manually built rules that are usually encoded in terms of regularexpression grammars supplemented with lists of abbreviations, common words, proper names, etc.. To put together a few rules is fast and easy, but to develop a rulebased system with good performance is quite a laborconsuming enterprise. For instance , the Alembic workbench <TARGET_CITATION/> contains a sentencesplitting module that employs over 100 regularexpression rules written in Flex . For instance, the Alembic workbench <CITATION/> contains a sentencesplitting module that employs over 100 regularexpression rules written in Flex. To put together a few rules is fast and easy, but to develop a rulebased system with good performance is quite a laborconsuming enterprise. The rulebased systems use manually built rules that are usually encoded in terms of regularexpression grammars supplemented with lists of abbreviations, common words, proper names, etc..",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,14fee4e396303e9eb00beff50ee018ca9f7aff1a,MITRE: Description of the Alembic System Used for MUC-6,1995,J. Aberdeen; J. Burger; David S. Day; L. Hirschman; Patricia Robinson; M. Vilain
874,N01-1001,External_13196,[0],,"In contrast , a single statistical model allows one to maintain a single table <TARGET_CITATION/> .","If we want to use a Viterbistyle algorithm which defines equivalence classes for chart edges and only keeps the current best one for each class in a table, we need to maintain a separate table for each instance. This is because we cannot combine scores from different instances. In contrast , a single statistical model allows one to maintain a single table <TARGET_CITATION/> . In contrast, a single statistical model allows one to maintain a single table <CITATION/>. This is because we cannot combine scores from different instances. If we want to use a Viterbistyle algorithm which defines equivalence classes for chart edges and only keeps the current best one for each class in a table, we need to maintain a separate table for each instance.",3a01b4ffbdf23d73ec4fe54826d9462c8ded048e,Instance-Based Natural Language Generation,2001,S. Varges; C. Mellish,,forestbased statistical sentence generation,2000,Irene Langkilde
875,D13-1115,External_32478,[2],method,Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <TARGET_CITATION/> ., Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <TARGET_CITATION/> . Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <CITATION/>.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
877,Q13-1020,D09-1037,[4],related work,<TARGET_CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .,<CITATION/> focused on joint parsing and alignment. They utilized the bilingual Treebank to train a joint model for both parsing and word alignment. <TARGET_CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . <CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. They utilized the bilingual Treebank to train a joint model for both parsing and word alignment. <CITATION/> focused on joint parsing and alignment.,aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,dc605e05765f4948328525d1c60aeca58970afe1,A Bayesian Model of Syntax-Directed Tree to String Grammar Induction,2009,Trevor Cohn; Phil Blunsom
878,W06-1705,External_29778,[0],introduction,"In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( <TARGET_CITATION/> : 56 ) unless the web is used as a corpus <CITATION/> .","The motivation for increasingly large data sets remains the same. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( <TARGET_CITATION/> : 56 ) unless the web is used as a corpus <CITATION/> . In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (<CITATION/>: 56) unless the web is used as a corpus <CITATION/>. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. The motivation for increasingly large data sets remains the same.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,f96f2806e8af9d8ee75d6cd983e57ff0150f6bc3,An Introduction to Corpus Linguistics,1998,Graeme D. Kennedy
879,J87-3002,External_29752,[0],introduction,"In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see <TARGET_CITATION/> for details ) .","<CITATION/>) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project <CITATION/> and the IBM CRITIQUE (formerly EPISTLE) Project <CITATION/>; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see <TARGET_CITATION/> for details ) . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see <CITATION/> for details). Two exceptions to this generalisation are the Linguistic String Project <CITATION/> and the IBM CRITIQUE (formerly EPISTLE) Project <CITATION/>; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. <CITATION/>) consult relatively small lexicons, typically generated by hand.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,and forthcoming machine readable dictionaries and research in computational linguistics,1986,Branimir Boguraev
880,E03-1004,A00-2018,[2],experiments,"We carried out two parallel experiments with two parsers available for Czech , parser I <CITATION/> and parser II <TARGET_CITATION/> .","The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors. We carried out two parallel experiments with two parsers available for Czech , parser I <CITATION/> and parser II <TARGET_CITATION/> . We carried out two parallel experiments with two parsers available for Czech, parser I <CITATION/> and parser II <CITATION/>. The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.",55559a2ee9693969d30237534ac290f4b0077a3a,Czech-English Dependency Tree-based Machine Translation,2003,Martin Cmejrek; J. Curín; Jirí Havelka,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
881,A00-1012,External_3896,[0],,Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger <TARGET_CITATION/> ) and parsers generally aim to produce a tree spanning each sentence .,"ation which is not available in ASR output is sentence boundary information. However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger <TARGET_CITATION/> ) and parsers generally aim to produce a tree spanning each sentence . Part of speech taggers typically require input in the format of a single sentence per line (for example Brill's tagger <CITATION/>) and parsers generally aim to produce a tree spanning each sentence. However, knowledge of sentence boundaries is required by many NLP technologies. ation which is not available in ASR output is sentence boundary information.",40242a0afdbdde785371df13027c1f7055eb2425,Experiments on Sentence Boundary Detection,2000,Mark Stevenson; R. Gaizauskas,,a simple rulebased part of speech tagger,1992,E Brill
882,J97-4003,External_42180,[2],introduction,"The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques <TARGET_CITATION/> .29 The unfolding transformation is also referred to as partial execution , for example , by <CITATION/> .","For each interaction definition we can therefore check which of the frame clauses are applicable and discard the nonapplicable ones. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques <TARGET_CITATION/> .29 The unfolding transformation is also referred to as partial execution , for example , by <CITATION/> . The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques <CITATION/>.29 The unfolding transformation is also referred to as partial execution, for example, by <CITATION/>. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. For each interaction definition we can therefore check which of the frame clauses are applicable and discard the nonapplicable ones.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,unfoldfold transformation of logic programs,1984,Hisao Tamaki; Taisuke Sato
883,J08-1003,External_1004,[1],introduction,"Problems such as these have motivated research on more abstract , dependencybased parser evaluation <TARGET_CITATION/> .","3. Because a treebased gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as handcrafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract , dependencybased parser evaluation <TARGET_CITATION/> . Problems such as these have motivated research on more abstract, dependencybased parser evaluation <CITATION/>. Because a treebased gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as handcrafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser.3.",9c8e756fda6c46d9f78430ee4f7bbce66b168921,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,2008,A. Cahill; Michael Burke; Ruth O'Donovan; S. Riezler; Josef van Genabith; Andy Way,47a4a47c3fba8b2e1d649d1353355011f5de62c3,The PARC 700 Dependency Bank,2003,Tracy Holloway King; Dick Crouch; S. Riezler; M. Dalrymple; R. Kaplan
884,D13-1115,P12-1015,[4],experiments,This seems to provide additional evidence of <TARGET_CITATION/> ) 's suggestion that something like a distributional hypothesis of images is plausible .,"The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the textonly model. Since the SURF and GIST image features tend to capture objectlikeness and scenelikeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of <TARGET_CITATION/> ) 's suggestion that something like a distributional hypothesis of images is plausible . This seems to provide additional evidence of <CITATION/>'s suggestion that something like a distributional hypothesis of images is plausible. Since the SURF and GIST image features tend to capture objectlikeness and scenelikeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the textonly model.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,917fbd64a435cb33e0e5b4cd73fe830db7b166db,Distributional Semantics in Technicolor,2012,Elia Bruni; Gemma Boleda; Marco Baroni; N. Tran
885,W03-0806,External_20440,[0],experiments,"For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors <TARGET_CITATION/> .","There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) <CITATION/> and the Alembic Workbench <CITATION/>) as well as NLP tools and resources that can be manipulated from the GUI. For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors <TARGET_CITATION/> . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors <CITATION/>. Many provide graphical user interfaces (GUI) for manual annotation (e.g. General Architecture for Text Engineering (GATE) <CITATION/> and the Alembic Workbench <CITATION/>) as well as NLP tools and resources that can be manipulated from the GUI. There are a number of generalised NLP systems in the literature.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,f85c60067ec8ae9a9887cd3afdb04c5924b69aeb,Developing Language Processing Components with GATE,2001,H. Cunningham; D. Maynard; V. Tablan; Cristian Ursu; Kalina Bontcheva
886,D08-1113,D07-1094,[0],conclusion,Latent variables we wish to consider are an increased number of word classes ; more flexible regions  see <TARGET_CITATION/> on learning a state transition diagram for acoustic regions in phone recognition  and phonological features and syllable boundaries .,"In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and stringtransduction tasks. We would like to use features that look at wide context on the input side, which is inexpensive <CITATION/>. Latent variables we wish to consider are an increased number of word classes ; more flexible regions  see <TARGET_CITATION/> on learning a state transition diagram for acoustic regions in phone recognition  and phonological features and syllable boundaries . Latent variables we wish to consider are an increased number of word classes; more flexible regionssee <CITATION/> on learning a state transition diagram for acoustic regions in phone recognitionand phonological features and syllable boundaries. We would like to use features that look at wide context on the input side, which is inexpensive <CITATION/>. In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and stringtransduction tasks.",62ac088d966d4a9122959f13301759c6bbda6c36,Latent-Variable Modeling of String Transductions with Finite-State Methods,2008,Markus Dreyer; Jason R. Smith; Jason Eisner,20f304959b928bb0b08a3e1c56f770632300b978,Learning Structured Models for Phone Recognition,2007,Slav Petrov; Adam Pauls; D. Klein
888,D10-1074,W09-3930,[3],method,"Using the implicit modeling of argument consistency , we follow the same approach as in our previous work <TARGET_CITATION/> and trained a logistic regression model to predict verb alignment based on the features in Table 1 .","x2's alignments in the conversation segment, among which x9 (You) is the closest to x11 (see). In Figure 2(a), we find the distance between x11 and x9 is 3. Using the implicit modeling of argument consistency , we follow the same approach as in our previous work <TARGET_CITATION/> and trained a logistic regression model to predict verb alignment based on the features in Table 1 . Using the implicit modeling of argument consistency, we follow the same approach as in our previous work <CITATION/> and trained a logistic regression model to predict verb alignment based on the features in Table 1.In Figure 2(a), we find the distance between x11 and x9 is 3. x2's alignments in the conversation segment, among which x9 (You) is the closest to x11 (see).",3d0adc6fca3a0669c108958c5d5204e2695ea4db,Towards Conversation Entailment: An Empirical Investigation,2010,Chen Zhang; J. Chai,ee9fb0cb4487d277233f61286eb40637e82dbb5e,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,Chen Zhang; J. Chai
889,J87-3002,External_198,[0],introduction,"Recent developments in linguistics , and especially on grammatical theory  for example , Generalised Phrase Structure Grammar ( GPSG ) <CITATION/> , Lexical Functional Grammar ( LFG ) <TARGET_CITATION/>  and on natural language parsing frameworks  for example , Functional Unification Grammar ( FUG ) <CITATION/> , PATRII <CITATION/>  make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .","The grammar coding system employed by the Longman Dictionary of Contemporary English (henceforth LDOCE) is the most comprehensive description of grammatical properties of words to be found in any published dictionary available in machine readable form. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. Recent developments in linguistics , and especially on grammatical theory  for example , Generalised Phrase Structure Grammar ( GPSG ) <CITATION/> , Lexical Functional Grammar ( LFG ) <TARGET_CITATION/>  and on natural language parsing frameworks  for example , Functional Unification Grammar ( FUG ) <CITATION/> , PATRII <CITATION/>  make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . Recent developments in linguistics, and especially on grammatical theory  for example, Generalised Phrase Structure Grammar (GPSG) <CITATION/>, Lexical Functional Grammar (LFG) <CITATION/>  and on natural language parsing frameworks  for example, Functional Unification Grammar (FUG) <CITATION/>, PATRII <CITATION/>  make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. The grammar coding system employed by the Longman Dictionary of Contemporary English (henceforth LDOCE) is the most comprehensive description of grammatical properties of words to be found in any published dictionary available in machine readable form.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,e17117dbee804d7d177d8eb9fadf0bda1ebc4d22,Lexical Functional Grammar A Formal System for Grammatical Representation,2004,Ronald M. Kaplan
890,D12-1037,External_361,[2],experiments,We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <TARGET_CITATION/> with modified KneserNey smoothing <CITATION/> .,"The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA++ <CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair. We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <TARGET_CITATION/> with modified KneserNey smoothing <CITATION/> . We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <CITATION/> with modified KneserNey smoothing <CITATION/>. We run GIZA++ <CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,399da68d3b97218b6c80262df7963baa89dcc71b,SRILM - an extensible language modeling toolkit,2002,A. Stolcke
891,D09-1056,External_37184,[0],related work,It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <TARGET_CITATION/> .,The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>. Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <TARGET_CITATION/> . It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <CITATION/>. Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>.,a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,db8fc6e3192fa0d9703dc9ad4500d07e91a6a68b,A testbed for people searching strategies in the WWW,2005,J. Artiles; Julio Gonzalo; M. Verdejo
892,J87-3002,External_40338,[4],,This deficiency is rectified in the verb classification system employed by <TARGET_CITATION/> in the Brandeis verb catalogue .,I They detest all that shooting and killing.This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as would''. This deficiency is rectified in the verb classification system employed by <TARGET_CITATION/> in the Brandeis verb catalogue . This deficiency is rectified in the verb classification system employed by <CITATION/> in the Brandeis verb catalogue. This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as would''. I They detest all that shooting and killing.,998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,a key to the brandeis verb catalog unpublished mimeo under nsf grant ist8420073 quotinformation structure of a natural language lexiconquot,1985,Ray Jackendoff; Jane Grimshaw
893,A00-1014,External_12080,[0],,Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction <TARGET_CITATION/> .,"user queries (steps 18)5 (van <CITATION/>), and 2) providing answers to wellformed queries (steps 911).3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction <TARGET_CITATION/> . Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction <CITATION/>. 3.2.3 Strategy Selectionuser queries (steps 18)5 (van <CITATION/>), and 2) providing answers to wellformed queries (steps 911).",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,,cues and control in expertclient dialogues,1988,Steve Whittaker; Phil Stenton
894,W04-1805,External_74223,[4],related work,"1990 ) , on linguisitic acquisition ( by the use of PartofSpeech filters handcrafted by a linguist ) <TARGET_CITATION/> or , more frequently , on a combination of the two ( <CITATION/> , for example ) .","On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, However, our interpretation of LFs in this work is much looser, since we admitted verbs that would not be considered to be members of true collocations as Melcuk et al. (1984 1999) define them, i.e. groups of lexical units that share a restricted cooccurrence relationship. 1990 ) , on linguisitic acquisition ( by the use of PartofSpeech filters handcrafted by a linguist ) <TARGET_CITATION/> or , more frequently , on a combination of the two ( <CITATION/> , for example ) . 1990), on linguisitic acquisition (by the use of PartofSpeech filters handcrafted by a linguist) <CITATION/> or, more frequently, on a combination of the two (<CITATION/>, for example). Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, However, our interpretation of LFs in this work is much looser, since we admitted verbs that would not be considered to be members of true collocations as Melcuk et al. (1984 1999) define them, i.e. groups of lexical units that share a restricted cooccurrence relationship. On the other hand, other work has been carried out in order to acquire collocations.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,aide a lacquisition de connaissances a partir de corpus,1999,Rochdi Oueslati
895,D13-1115,External_64538,[1],experiments,"It is frequently used in tasks like scene identification , and <TARGET_CITATION/> shows that distance in GIST space correlates well with semantic distance in WordNet .","Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall gist'' of the whole image. It is frequently used in tasks like scene identification , and <TARGET_CITATION/> shows that distance in GIST space correlates well with semantic distance in WordNet . It is frequently used in tasks like scene identification, and <CITATION/> shows that distance in GIST space correlates well with semantic distance in WordNet. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall gist'' of the whole image. Unlike SURF descriptors, GIST produces a single vector representation for an image.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,6ce77f485677387a791f41c0f2130e84ce8a8a1d,Visual and semantic similarity in ImageNet,2011,Thomas Deselaers; V. Ferrari
896,Q13-1020,D09-1037,[2],method,"Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow <TARGET_CITATION/> and decompose the prior probability P0 ( r  N ) into two factors as follows :","The base distribution P0(r  N) is designed to assign prior probabilities to the STSG production rules. Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow <TARGET_CITATION/> and decompose the prior probability P0 ( r  N ) into two factors as follows : Because each rule r consists of a target tree fragment frag and a source string str in the model, we follow <CITATION/> and decompose the prior probability P0(r  N) into two factors as follows:The base distribution P0(r  N) is designed to assign prior probabilities to the STSG production rules.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,dc605e05765f4948328525d1c60aeca58970afe1,A Bayesian Model of Syntax-Directed Tree to String Grammar Induction,2009,Trevor Cohn; Phil Blunsom
898,W11-1402,P11-2088,[3],introduction,"In our prior work <TARGET_CITATION/> , we examined whether techniques used for predicting the helpfulness of product reviews <CITATION/> could be tailored to our peerreview domain , where the definition of helpfulness is largely influenced by the educational context of peer review .","ically predict peerreview helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques. Such an intelligent component could enable peerreview systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews. In our prior work <TARGET_CITATION/> , we examined whether techniques used for predicting the helpfulness of product reviews <CITATION/> could be tailored to our peerreview domain , where the definition of helpfulness is largely influenced by the educational context of peer review . In our prior work <CITATION/>, we examined whether techniques used for predicting the helpfulness of product reviews <CITATION/> could be tailored to our peerreview domain, where the definition of helpfulness is largely influenced by the educational context of peer review. Such an intelligent component could enable peerreview systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews. ically predict peerreview helpfulness based on features mined from textual reviews using Natural Language Processing (NLP) techniques.",ef2ff38f2b4b1bacc252d42056ddcef3014f3fed,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,2011,Wenting Xiong; D. Litman,006ac84e4432ce8dab7a6ff0c6260d13c4a77e1f,Automatically Predicting Peer-Review Helpfulness,2011,Wenting Xiong; D. Litman
899,W06-1639,External_57193,[0],related work,Previous sentimentanalysis work in different domains has considered interdocument similarity <TARGET_CITATION/> or explicit,"We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentimentanalysis work in different domains has considered interdocument similarity <TARGET_CITATION/> or explicit Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitRelationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,3497dcdac3db7db37aabc1db94b516780e89ea8e,Sentiment Analysis : A New Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be Classified .,2005,Alekh Agarwal
900,D13-1115,P12-1015,[0],related work,<TARGET_CITATION/> ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .,They use a Bag of Visual Words (BoVW) model <CITATION/> to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. <TARGET_CITATION/> ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation . <CITATION/> show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. They use a Bag of Visual Words (BoVW) model <CITATION/> to create a bimodal vocabulary describing documents.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,917fbd64a435cb33e0e5b4cd73fe830db7b166db,Distributional Semantics in Technicolor,2012,Elia Bruni; Gemma Boleda; Marco Baroni; N. Tran
902,J06-2002,External_3887,[0],introduction,"The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed <TARGET_CITATION/> : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .","(3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm)Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cfXXX, Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed <TARGET_CITATION/> : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large . The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed <CITATION/>: (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cfXXX, Kennedy 1999). (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm)",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,fitting words vague language in context linguistics and philosophy,2000,Alice Kyburg; Michael Morreau
903,P10-2059,External_41,[2],introduction,Agreement between two annotation sets is calculated here in terms of Cohen 's kappa <TARGET_CITATION/> 1 and corrected kappa <CITATION/> 2 .,"However, one dialogue was coded independently and in parallel by two expert annotators to measure intercoder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa <TARGET_CITATION/> 1 and corrected kappa <CITATION/> 2 . Agreement between two annotation sets is calculated here in terms of Cohen's kappa <CITATION/>1 and corrected kappa <CITATION/>2. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. However, one dialogue was coded independently and in parallel by two expert annotators to measure intercoder agreement.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,9e463eefadbcd336c69270a299666e4104d50159,A Coefficient of Agreement for Nominal Scales,1960,Jacob Cohen
904,J02-3002,J97-2002,[4],conclusion,On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in <TARGET_CITATION/> ( 0.44 % vs. 0.5 % error rate ) .,"Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in <TARGET_CITATION/> ( 0.44 % vs. 0.5 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in <CITATION/> (0.44% vs. 0.5% error rate). The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,30154464f549643e825ccf60072a17a3e55291d3,To Appear in Computational Linguistics Adaptive Multilingual Sentence Boundary Disambiguation,2004,D. Palmer; Marti A. Hearst
905,W00-1312,External_7479,[0],related work,<TARGET_CITATION/> studied the issue of disambiguation for monolingual M.,"While word sense disambiguation has been a central topic in previous studies for crosslingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <CITATION/>. <TARGET_CITATION/> studied the issue of disambiguation for monolingual M. <CITATION/> studied the issue of disambiguation for monolingual M.Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <CITATION/>. While word sense disambiguation has been a central topic in previous studies for crosslingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,word sense disambiguation and information retrievalquot,1994,M Sanderson
906,W06-1639,External_220,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,f4ba954b0412773d047dc41231c733de0c1f4926,Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,2001,J. Lafferty; A. McCallum; Fernando Pereira
909,J97-4003,External_98539,[4],related work,Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; <TARGET_CITATION/> ; Sanfilippo 1995 ) .,"Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification <TARGET_CITATION/> . Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,statische programmtransformationen zur effizienten verarbeitung constraintbasierter grammatiken diplomarbeit,1995,Annette Opalka
910,J09-4010,External_2296,[4],,Berger et al. 2000 ; Jijkoun and de Rijke 2005 ; <TARGET_CITATION/> ) .,"With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpusbased approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that, like helpdesk, deal with questionanswer pairs are: summarization of email threads <CITATION/>, and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000 ; Jijkoun and de Rijke 2005 ; <TARGET_CITATION/> ) . Berger et al. 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006). Two applications that, like helpdesk, deal with questionanswer pairs are: summarization of email threads <CITATION/>, and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000;With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpusbased approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,1c78809a7fd22c95f4d60cd707c32280a019940f,Automatic question answering using the web: Beyond the Factoid,2006,Radu Soricut; Eric Brill
911,J06-2002,External_280,[0],introduction,We will examine the worstcase complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects <TARGET_CITATION/> ., We will examine the worstcase complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects <TARGET_CITATION/> . We will examine the worstcase complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects <CITATION/>.,0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,object reference in a shared domain of conversation pragmatics and cognition,1998,Robbert-Jan Beun; Anita Cremers
912,J01-4001,C96-1021,[0],,"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; <TARGET_CITATION/> ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; <TARGET_CITATION/> ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) . A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,5bbf53a9a8ee5a52e8b3e4c4e04cfebc8cc1b1c9,Anaphora for Everyone: Pronominal Anaphora Resolution without a Parser,1996,Christopher Kennedy; B. Boguraev
913,W02-1601,External_69140,[0],,"From the MeaningText Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts <TARGET_CITATION/> ."," From the MeaningText Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts <TARGET_CITATION/> . From the MeaningText Theory (MTT)1 point of view, Natural Language (NL) is considered as a correspondence between meanings and texts <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,34636478e9756c4535aa9ed55d869e83dc9529e0,What Is a Natural Language and How to Describe It? Meaning-Text Approaches in Contrast with Generative Approaches (Invited Talk),2001,Sylvain Kahane
915,P13-3018,External_90447,[0],related work,The fulllisting model claims that polymorphic words are represented as a whole in the human mental lexicon <TARGET_CITATION/> .,Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the fulllisting and the morphemic model. The fulllisting model claims that polymorphic words are represented as a whole in the human mental lexicon <TARGET_CITATION/> . The fulllisting model claims that polymorphic words are represented as a whole in the human mental lexicon <CITATION/>. Most of the studies are designed to support one of the two mutually exclusive paradigms: the fulllisting and the morphemic model. Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.,97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,lexical representation of derivational relation,1980,D Bradley
917,P07-1068,J05-3004,[0],introduction,"However , learningbased resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , <TARGET_CITATION/> ) .","As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However , learningbased resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC <TARGET_CITATION/> . However, learningbased resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., <CITATION/>). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>).",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,6f74a07467545a686346f8e30eefed35b338edc8,Comparing Knowledge Sources for Nominal Anaphora Resolution,2005,K. Markert; M. Nissim
919,W06-3309,W00-1302,[0],introduction,"The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <TARGET_CITATION/> , information retrieval <CITATION/> , information extraction <CITATION/> , and question answering .","As an example, scientific abstracts across many differentfields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <TARGET_CITATION/> , information retrieval <CITATION/> , information extraction <CITATION/> , and question answering . The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/>, information retrieval <CITATION/>, information extraction <CITATION/>, and question answering. fields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. As an example, scientific abstracts across many different",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,9a66167f0ec32ecec86eb1021be12ea611aa3d46,What’s Yours and What’s Mine: Determining Intellectual Attribution in Scientific Text,2000,Simone Teufel; M. Moens
920,E03-1002,P93-1005,[0],introduction,"Many statistical parsers <CITATION/> are based on a historybased probability model <TARGET_CITATION/> , where the probability of each decision in a parse is conditioned on the previous decisions in the parse ."," Many statistical parsers <CITATION/> are based on a historybased probability model <TARGET_CITATION/> , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . Many statistical parsers <CITATION/> are based on a historybased probability model <CITATION/>, where the probability of each decision in a parse is conditioned on the previous decisions in the parse.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1993,Ezra Black; F. Jelinek; J. Lafferty; David M. Magerman; R. Mercer; S. Roukos
921,W02-1601,External_55336,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,,towards memorybased translation,1990,S Sato; M Nagao
922,W04-0910,External_2298,[0],introduction,"For instance , <TARGET_CITATION/> acquire twoargument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .","More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , <TARGET_CITATION/> acquire twoargument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . For instance, <CITATION/> acquire twoargument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,discovery of inference rules for question answering natural language engineering,2001,Dekang Lin; Patrick Pantel
923,P02-1001,External_8687,[4],,"Perstate joint normalization ( <TARGET_CITATION/> , § 8.2 ) is similar but drops the dependence on a .","Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a. Then the predicted vector given  is Ej,a dj,a (expected feature counts on a randomly chosen arc in Dj,a). Perstate joint normalization ( <TARGET_CITATION/> , § 8.2 ) is similar but drops the dependence on a . Perstate joint normalization (<CITATION/>, §8.2) is similar but drops the dependence on a. Then the predicted vector given  is Ej,a dj,a (expected feature counts on a randomly chosen arc in Dj,a). Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,expectation semirings flexible em for finitestate transducers,2001,Jason Eisner
924,W00-1312,External_16322,[0],related work,Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <TARGET_CITATION/> .,"(<CITATION/>; Hull and (3refenstette, 1996). While word sense disambiguation has been a central topic in previous studies for crosslingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <TARGET_CITATION/> . Other studies on the value of disambiguation for crosslingual IR include Hiemstra and de <CITATION/>. While word sense disambiguation has been a central topic in previous studies for crosslingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. (<CITATION/>; Hull and (3refenstette, 1996).",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,using structured queries for disambiguation in crosslanguage information retrievalquot,1997,D A Hull
925,J92-1004,External_77778,[2],,"We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort Nbest outputs , giving a significant improvement in performance <TARGET_CITATION/> .","successfully parses, or until a quitting criterion is reached, such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions <CITATION/>, the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort Nbest outputs , giving a significant improvement in performance <TARGET_CITATION/> . We have not yet made use of TINA'S probabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort Nbest outputs, giving a significant improvement in performance <CITATION/>. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. successfully parses, or until a quitting criterion is reached, such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions <CITATION/>, the tightly coupled system allows the parser to discard partial theories that have no way of continuing.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,5b4252e01ca3bb6baa8ebc059cafc33fb14fa5db,Full integration of speech and language understanding in the MIT spoken language system,1991,D. Goodine; S. Seneff; L. Hirschman; M. Phillips
926,P10-2002,P08-1114,[1],method,These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work <TARGET_CITATION/> : 1 .,ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative contextbased features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work <TARGET_CITATION/> : 1 . These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work <CITATION/>: 1. We incorporate into the ME based model the following informative contextbased features to train CBSM and CBTM. ME approach has the merit of easily combining different features to predict the probability of each class.,4899e043c69f8ab61c760b1f56961be9f1a5be9a,A Joint Rule Selection Model for Hierarchical Phrase-Based Translation,2010,Lei Cui; Dongdong Zhang; Mu Li; M. Zhou; T. Zhao,067b67a955302dab7e450bd51c246fd6bfbec545,Soft Syntactic Constraints for Hierarchical Phrased-Based Translation,2008,Yuval Marton; P. Resnik
927,D09-1056,W07-2024,[0],related work,"Nevertheless , the full document text is present in most systems , sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <TARGET_CITATION/>  .","The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. Nevertheless , the full document text is present in most systems , sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <TARGET_CITATION/>  . Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. The most basic is a Bag of Words (BoW) representation of the document text.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,90f8b83210aba36bf43792f370670191a62b020d,CU-COMSEM: Exploring Rich Features for Unsupervised Web Personal Name Disambiguation,2007,Ying Chen; James H. Martin
928,W04-1610,External_883,[0],related work,"More recently , <TARGET_CITATION/> has performed a good survey of document categorization ; recent works can also be found in <CITATION/> .","For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces. A good study comparing document categorization algorithms can be found in <CITATION/>. More recently , <TARGET_CITATION/> has performed a good survey of document categorization ; recent works can also be found in <CITATION/> . More recently, <CITATION/> has performed a good survey of document categorization; recent works can also be found in <CITATION/>. A good study comparing document categorization algorithms can be found in <CITATION/>. For example, <CITATION/> discusses the evaluation of two different text categorization strategies with several variations of their feature spaces.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,6b20af22b0734757d9ead382b201a65f9dd637cc,Machine learning in automated text categorization,2001,F. Sebastiani
929,J00-2001,W94-0312,[0],,"McDonald has even argued for extending the model to a large number of components <CITATION/> , and several systems have indeed added an additional component between the planner and the linguistic component <TARGET_CITATION/> .","Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES <CITATION/>, EPICURE <CITATION/>, SPOKESMAN <CITATION/>, Sibun's work on local organization of text <CITATION/>, and COMET <CITATION/> all are organized this way. McDonald has even argued for extending the model to a large number of components <CITATION/> , and several systems have indeed added an additional component between the planner and the linguistic component <TARGET_CITATION/> . McDonald has even argued for extending the model to a large number of components <CITATION/>, and several systems have indeed added an additional component between the planner and the linguistic component <CITATION/>. For example, DIOGENES <CITATION/>, EPICURE <CITATION/>, SPOKESMAN <CITATION/>, Sibun's work on local organization of text <CITATION/>, and COMET <CITATION/> all are organized this way. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,3e470ab8e78ef5b1c766ac2844361aa4fab1b508,Generating Event Descriptions with Sage: A Simulation and Generation Environment,1994,M. Meteer
931,J86-1002,External_15073,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , <TARGET_CITATION/> , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."," A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , <TARGET_CITATION/> , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,0983f5caa7319673df4ebd70b3d560a89a81982d,The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty,1980,L. Erman; F. Hayes-Roth; V. Lesser; R. Reddy
933,D13-1115,P10-1126,[0],introduction,"Some efforts have tackled tasks such as automatic image caption generation <TARGET_CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <CITATION/> .","Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <TARGET_CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <CITATION/> . Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1,How Many Words Is a Picture Worth? Automatic Caption Generation for News Images,2010,Yansong Feng; Mirella Lapata
934,J00-1003,P98-1101,[0],,"This idea was proposed by Krauwer and des <CITATION/> , and was rediscovered by <CITATION/> and recently by <TARGET_CITATION/> .","By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the contextfree language, and therefore a subset approximation results. This idea was proposed by Krauwer and des <CITATION/> , and was rediscovered by <CITATION/> and recently by <TARGET_CITATION/> . This idea was proposed by Krauwer and des <CITATION/>, and was rediscovered by <CITATION/> and recently by <CITATION/>. By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the contextfree language, and therefore a subset approximation results.",b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof,0abfb66d1b634709cf03dd15de02059c7672c33a,Finite-state Approximation of Constraint-based Grammars using Left-corner Grammar Transforms,1998,Mark Johnson
935,J00-3003,P93-1035,[4],,A nonprobabilistic approach for DA labeling proposed by <CITATION/> is transformationbased learning <TARGET_CITATION/> .,"As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by <CITATION/> is transformationbased learning <TARGET_CITATION/> . A nonprobabilistic approach for DA labeling proposed by <CITATION/> is transformationbased learning <CITATION/>. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way.",22d45dadde6b5837eff11dc031045754bc5901c3,Dialogue act modeling for automatic tagging and recognition of conversational speech,2000,A. Stolcke; K. Ries; N. Coccaro; Elizabeth Shriberg; R. Bates; Dan Jurafsky; P. Taylor; Rachel Martin; C. V. Ess-Dykema; M. Meteer,ad120a6635aeb1f0fbf798e5a1b97b65e25b716e,Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach,1993,Eric Brill
937,D12-1037,D11-1004,[0],introduction,"Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one .","where f and e (e') are source and target sentences, respectively. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one . Some methods are based on likelihood <CITATION/>, error rate <CITATION/>, margin <CITATION/> and ranking <CITATION/>, and among which minimum error rate training (MERT) <CITATION/> is the most popular one. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. where f and e (e') are source and target sentences, respectively.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,b4717ab8f647d28dbd1a8319d838ddc2cdfaf050,Optimal Search for Minimum Error Rate Training,2011,Michel Galley; Chris Quirk
938,J06-2003,External_5983,[1],,The algorithm we implemented is inspired by the work of <TARGET_CITATION/> on word sense disambiguation .,"We use a decisionlist algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDESTYLE DISTINCTIONS. These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of <TARGET_CITATION/> on word sense disambiguation . The algorithm we implemented is inspired by the work of <CITATION/> on word sense disambiguation. These are split further for each leaf class, as explained in Section 2.3. We use a decisionlist algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDESTYLE DISTINCTIONS.",4274016dfe32971fcb5bb2043b35555f6fcf91c9,Building and Using a Lexical Knowledge Base of Near-Synonym Differences,2006,Diana Inkpen; Graeme Hirst,944cba683d10d8c1a902e05cd68e32a9f47b372e,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,1995,David Yarowsky
939,W06-1104,J05-4002,[0],,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <CITATION/> , informationbased <CITATION/> or distributional <TARGET_CITATION/> ."," Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <CITATION/> , informationbased <CITATION/> or distributional <TARGET_CITATION/> . Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,429952086b36f1bf61816efb5fbfd449143469be,Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity,2005,Julie Weeds; David J. Weir
941,N01-1002,W00-1425,[0],introduction,It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of <CITATION/> ) <TARGET_CITATION/> .,Work on aggregation is satisfied with devising a few rules to allow some degrees of embedding to generate subordinate NP components. There is no indepth discussion of the problem of generating nonreferring modifiers in general. It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of <CITATION/> ) <TARGET_CITATION/> . It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence (in the sense of <CITATION/>) <CITATION/>. There is no indepth discussion of the problem of generating nonreferring modifiers in general. Work on aggregation is satisfied with devising a few rules to allow some degrees of embedding to generate subordinate NP components.,a463350fc2e38decfe736e0801d465874cef0891,Corpus-based NP Modifier Generation,2001,Hua Cheng; Massimo Poesio; R. Henschel; C. Mellish,6ff81cb8db2b5f5ea70432ad267e6462e57efd87,Capturing the Interaction between Aggregation and Text Planning in Two Generation Systems,2000,Hua Cheng; C. Mellish
942,J09-4010,External_52005,[4],method, Only qualitative observations of the responses were reported ( no formal evaluation was performed ) <TARGET_CITATION/> .,In <CITATION/> we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. Only qualitative observations of the responses were reported ( no formal evaluation was performed ) <TARGET_CITATION/> .  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. These systems addressed the evaluation issue as follows.In <CITATION/> we identified several systems that resemble ours in that they provide answers to queries.,a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,mercure towards an automatic email followup system,2003,G Lapalme; L Kosseim
943,D10-1074,W09-3930,[4],experiments,"A stops B from doing something ; A disagreees with B on something , 8 % and 12 % ) Note that in our original work <TARGET_CITATION/> , only development data were used to show some initial observations .","1The data is available for download at http: //links.cse.msu.edu:8000/lair/projects/ conversationentailment_data.html.were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants' beliefs and opinions (34% and 35%); (3) desire: participants' desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g.. A stops B from doing something ; A disagreees with B on something , 8 % and 12 % ) Note that in our original work <TARGET_CITATION/> , only development data were used to show some initial observations . A stops B from doing something; A disagreees with B on something, 8% and 12%) Note that in our original work <CITATION/>, only development data were used to show some initial observations. were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants' beliefs and opinions (34% and 35%); (3) desire: participants' desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g.. 1The data is available for download at http: //links.cse.msu.edu:8000/lair/projects/ conversationentailment_data.html.",3d0adc6fca3a0669c108958c5d5204e2695ea4db,Towards Conversation Entailment: An Empirical Investigation,2010,Chen Zhang; J. Chai,ee9fb0cb4487d277233f61286eb40637e82dbb5e,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,Chen Zhang; J. Chai
944,K15-1003,External_681,[2],experiments,"We evaluated on the English CCGBank <CITATION/> , which is a transformation of the Penn Treebank <TARGET_CITATION/> ; the CTBCCG <CITATION/> transformation of the Penn Chinese Treebank <CITATION/> ; and the CCGTUT corpus <CITATION/> , built from the TUT corpus of Italian text <CITATION/> .","In our evaluation we compared our supertagcontext approach to (our reimplementation of) the bestperforming model of our previous work <CITATION/>, which SCM extends. We evaluated on the English CCGBank <CITATION/> , which is a transformation of the Penn Treebank <TARGET_CITATION/> ; the CTBCCG <CITATION/> transformation of the Penn Chinese Treebank <CITATION/> ; and the CCGTUT corpus <CITATION/> , built from the TUT corpus of Italian text <CITATION/> . We evaluated on the English CCGBank <CITATION/>, which is a transformation of the Penn Treebank <CITATION/>; the CTBCCG <CITATION/> transformation of the Penn Chinese Treebank <CITATION/>; and the CCGTUT corpus <CITATION/>, built from the TUT corpus of Italian text <CITATION/>. In our evaluation we compared our supertagcontext approach to (our reimplementation of) the bestperforming model of our previous work <CITATION/>, which SCM extends.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,0b44fcbeea9415d400c5f5789d6b892b6f98daff,Building a Large Annotated Corpus of English: The Penn Treebank,1993,Mitchell P. Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz
945,P02-1001,External_1429,[2],,The EM algorithm <TARGET_CITATION/> can maximize these functions .,"Maximumposterior estimation tries to maximize P(0)Hi f(xi, yi) where P(0) is a prior probability. In a loglinear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <CITATION/>. The EM algorithm <TARGET_CITATION/> can maximize these functions . The EM algorithm <CITATION/> can maximize these functions. In a loglinear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <CITATION/>. Maximumposterior estimation tries to maximize P(0)Hi f(xi, yi) where P(0) is a prior probability.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,d36efb9ad91e00faa334b549ce989bfae7e2907a,Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper,1977,A. Dempster; N. Laird; D. Rubin
946,P10-2059,External_88746,[0],introduction,"Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <TARGET_CITATION/> .","<CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues. <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <TARGET_CITATION/> . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication <CITATION/>. <CITATION/> study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English maptask dialogues <CITATION/> and find correlations between the various modalities both within and across speakers. <CITATION/> obtain promising results in dialogue act tagging of the SwitchboardDAMSL corpus using lexical, syntactic and prosodic cues, while <CITATION/> examine the relation between particular acoustic and prosodic turnyielding cues and turn taking in a large corpus of taskoriented dialogues.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,a5a96cf06bf053feefde23332e2637b51c8238c8,Head gestures for perceptual interfaces: The role of context in improving recognition,2007,Louis-Philippe Morency; C. Sidner; Christopher Lee; Trevor Darrell
947,W04-0910,External_4260,[5],,"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <TARGET_CITATION/> .","Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <CITATION/>. Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <TARGET_CITATION/> . For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <CITATION/>. Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <CITATION/>.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
950,D09-1056,External_97098,[0],related work,Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <TARGET_CITATION/> .,"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <TARGET_CITATION/> . Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <CITATION/>. Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,,disambiguation algorithm for people search on the web in,2007,Dmitri V Kalashnikov; Stella Chen; Rabia Nuray; Sharad Mehrotra; Naveen Ashish
951,J87-3002,C86-1066,[0],introduction,"The research described below is taking place in the context of three collaborative projects <TARGET_CITATION/> to develop a generalpurpose , wide coverage morphological and syntactic analyser for English .","These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Realtime parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects <TARGET_CITATION/> to develop a generalpurpose , wide coverage morphological and syntactic analyser for English . The research described below is taking place in the context of three collaborative projects <CITATION/> to develop a generalpurpose, wide coverage morphological and syntactic analyser for English. Realtime parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,2bb9a366ddf710d678b06c6c84fc97781c886624,A Dictionary and Morphological Analyser for English,1986,G. Russell; S. Pulman; G. Ritchie; A. Black
952,P07-1068,W99-0613,[2],introduction,"We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in <TARGET_CITATION/> , motivated by its success in the related tasks of word sense disambiguation <CITATION/> and NE classification <CITATION/> .","To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependencybased thesaurus, which is constructed using a distributional approach combined with an informationtheoretic definition of similarity. Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in <TARGET_CITATION/> , motivated by its success in the related tasks of word sense disambiguation <CITATION/> and NE classification <CITATION/> . We experiment with four learners commonly employed in language learning: Decision List (DL): We use the DL learner as described in <CITATION/>, motivated by its success in the related tasks of word sense disambiguation <CITATION/> and NE classification <CITATION/>. Learning algorithms. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependencybased thesaurus, which is constructed using a distributional approach combined with an informationtheoretic definition of similarity.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,1c0ece611643cfb8f3a23e4802c754ea583ebe37,Unsupervised Models for Named Entity Classification,1999,M. Collins; Y. Singer
953,D11-1138,D08-1059,[2],experiments,transitionbased dependency parsing framework <CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <TARGET_CITATION/> with a beam size of 8 .,For our experiments we focus on two dependency parsers. Transitionbased: An implementation of the transitionbased dependency parsing framework <CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <TARGET_CITATION/> with a beam size of 8 . transitionbased dependency parsing framework <CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8.  Transitionbased: An implementation of theFor our experiments we focus on two dependency parsers.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,ad1181d188f730b7917a95ae452efb48f830c90a,A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing,2008,Yue Zhang; S. Clark
954,P11-1134,N10-1146,[4],introduction,"Using the basic solution proposed by <TARGET_CITATION/> as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions : ( 1 ) What is the potential of the existing multilingual lexical resources to approach CLTE ?","Along this direction, westart from the acquisition and use of lexical knowledge, which represents the basic building block of any TE system. Using the basic solution proposed by <TARGET_CITATION/> as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions : ( 1 ) What is the potential of the existing multilingual lexical resources to approach CLTE ? Using the basic solution proposed by <CITATION/> as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions: (1) What is the potential of the existing multilingual lexical resources to approach CLTE? start from the acquisition and use of lexical knowledge, which represents the basic building block of any TE system. Along this direction, we",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,4876b9d79886960e034a5d52adcdad640b363c76,Syntactic/Semantic Structures for Textual Entailment Recognition,2010,Yashar Mehdad; Alessandro Moschitti; Fabio Massimo Zanzotto
955,W00-1017,External_46368,[2],,Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan <TARGET_CITATION/> .,"word hypotheses. As the recognition engine, either VoiceRex, developed by NTT <CITATION/>, or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan <TARGET_CITATION/> . Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan <CITATION/>. As the recognition engine, either VoiceRex, developed by NTT <CITATION/>, or HTK from Entropic Research can be used. word hypotheses.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,d36b3c6b455ab857f698b19141711af250be7296,ASJ continuous speech corpus for research,1992,Tetsunori Kobayashi
956,W04-1610,External_8808,[0],introduction,"Automatic text categorization has been used in search engines , digital library systems , and document management systems <TARGET_CITATION/> .","It consists of assigning and labeling documents using a set of predefined categories based on document contents. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into prespecified subject themes. Automatic text categorization has been used in search engines , digital library systems , and document management systems <TARGET_CITATION/> . Automatic text categorization has been used in search engines, digital library systems, and document management systems <CITATION/>. As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into prespecified subject themes. It consists of assigning and labeling documents using a set of predefined categories based on document contents.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,890c16ca29a781a7b793c603822ffd57aee9f57f,An Evaluation of Statistical Approaches to Text Categorization,1999,Yiming Yang
957,D13-1115,External_4554,[2],experiments,"For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group <TARGET_CITATION/> containing approximately 1.7 B word tokens ."," For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group <TARGET_CITATION/> containing approximately 1.7 B word tokens . For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group <CITATION/> containing approximately 1.7B word tokens.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,the wacky wide web a collection of very large linguistically processed webcrawled corpora language resources and evaluation,2009,Marco Baroni; Silvia Bernardini; Adriano Ferraresi; Eros Zanchetta
958,W02-0309,External_45876,[4],experiments,Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in <TARGET_CITATION/> ) one can not really recommend this method .,"Trigram indexing (TG) yields the poorest results of all methodologies being tested. It is comparable to WS at low recall levels ( ), but at high ones its precision decreases almost dramatically. Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in <TARGET_CITATION/> ) one can not really recommend this method . Unless very high rates of misspellings are to be expected (this explains the favorable results for trigram indexing in <CITATION/>) one cannot really recommend this method. It is comparable to WS at low recall levels ( ), but at high ones its precision decreases almost dramatically. Trigram indexing (TG) yields the poorest results of all methodologies being tested.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,64d808827e092001df1a298433afbb4dfed173fc,Automated coding of diagnoses-three methods compared,2000,P. Franz; A. Zaiß; S. Schulz; U. Hahn; R. Klar
959,W04-0910,External_15756,[4],,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena <TARGET_CITATION/> ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information <CITATION/> .","While corpus driven efforts along the PARSEVAL lines <CITATION/> are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena <TARGET_CITATION/> ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information <CITATION/> . For english, there is for instance the 15 year old HewlettPackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena <CITATION/>; and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information <CITATION/>. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. While corpus driven efforts along the PARSEVAL lines <CITATION/> are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,towards evaluation of nlp systems,1987,D Flickinger; J Nerbonne; I Sag; T Wasow
960,D08-1034,N04-1032,[0],introduction,"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <TARGET_CITATION/> .","With the efforts of many researchers (Carreras and M rquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <TARGET_CITATION/> . Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as <CITATION/>. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. With the efforts of many researchers (Carreras and M rquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,df87a18357f3c462fc21df0ce9f60160714fd1e3,Shallow Semantic Parsing of Chinese,2004,H. Sun; Dan Jurafsky
961,J97-4003,External_9078,[4],related work,A similar method is included in PATRII <TARGET_CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <CITATION/> .,"Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATRII <TARGET_CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <CITATION/> . A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <CITATION/>. This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). Another common approach to lexical rules is to encode them as unary phrase structure rules.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,8a42578d95f2126cbaab4f20cc7d88fdf96df056,The Formalism and Implementation of PATR-II,1983,Stuart M. Shieber; H. Uszkoreit; F. Pereira; Jane J. Robinson; M. Tyson
962,D08-1007,J03-3005,[4],method,"Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models <TARGET_CITATION/> .","This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs. Similaritysmoothed models can make use of the regularities across similar verbs, but not the finergrained stringand tokenbased features. Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models <TARGET_CITATION/> . Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models <CITATION/>. Similaritysmoothed models can make use of the regularities across similar verbs, but not the finergrained stringand tokenbased features. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,5dfed29550d75cca99019aa52d40038dcb23b3cb,Using the Web to Obtain Frequencies for Unseen Bigrams,2003,Frank Keller; Mirella Lapata
965,J92-1004,External_84960,[2],,The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database <TARGET_CITATION/> .,"Portability and trainability concern the ease with which an existing grammar can be ported to a new task, as well as the amount of training data necessary before the grammar is able to generalize well to unseen data.To date, four distinct domainspecific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database <TARGET_CITATION/> . The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database <CITATION/>. To date, four distinct domainspecific versions of TINA have been implemented. Portability and trainability concern the ease with which an existing grammar can be ported to a new task, as well as the amount of training data necessary before the grammar is able to generalize well to unseen data.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,speech database development design and analysis of the acousticphonetic corpusquot,1986,L Lamel; R H Kassel; S Seneff
966,J00-4002,External_32773,[4],,A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by <TARGET_CITATION/> .,"This makes the task of expressing the output of some application system in a contextdependent way quite difficult: rather than being related to an RQLF, this output has to be related to a QLF that is sufficiently instantiated for a contextually unambiguous sentence to be generated from it. The resolution mechanism is not intended to be reversible, although by redefining resolution rules, reversibility is achievable to some extent within the limitations just discussed <CITATION/>. A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by <TARGET_CITATION/> . A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt: it is that taken by <CITATION/>. The resolution mechanism is not intended to be reversible, although by redefining resolution rules, reversibility is achievable to some extent within the limitations just discussed <CITATION/>. This makes the task of expressing the output of some application system in a contextdependent way quite difficult: rather than being related to an RQLF, this output has to be related to a QLF that is sufficiently instantiated for a contextually unambiguous sentence to be generated from it.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,e87314e4e465ff21001e04a38b8fcdfeadae714e,Monotonic Semantic Interpretation,1992,H. Alshawi; Dick Crouch
967,N04-2004,External_40239,[3],,"In <TARGET_CITATION/> , I present evidence from Mandarin Chinese that this analysis is on the right track .","Note that in the causative form, vDO is unmodified by a verbal rootthe manner of activity is left unspecified, i.e., John did something that caused the window to undergo the change of state break.'' Given this framework, deadjectival verbs such as flatten can be directly derived in the syntax: (15) The tire flattened. In <TARGET_CITATION/> , I present evidence from Mandarin Chinese that this analysis is on the right track . In <CITATION/>, I present evidence from Mandarin Chinese that this analysis is on the right track. Given this framework, deadjectival verbs such as flatten can be directly derived in the syntax: (15) The tire flattened.Note that in the causative form, vDO is unmodified by a verbal rootthe manner of activity is left unspecified, i.e., John did something that caused the window to undergo the change of state break.''",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,20823e7c5defc16555124d045d6c4b92ff10254b,Event structure and the encoding of arguments: the syntax of the Mandarin and English verb phrase,2004,Jimmy J. Lin
968,P10-4003,External_5662,[2],experiments,Interaction between components is coordinated by the dialogue manager which uses the informationstate approach <TARGET_CITATION/> ., Interaction between components is coordinated by the dialogue manager which uses the informationstate approach <TARGET_CITATION/> . Interaction between components is coordinated by the dialogue manager which uses the informationstate approach <CITATION/>.,1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,0059e902765146e58542a6716b69e2993793be55,Information state and dialogue management in the TRINDI dialogue move engine toolkit,2000,Staffan Larsson; D. Traum
970,J87-3002,External_11433,[0],,"There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see <TARGET_CITATION/> , for further details ) .","Give is coded as [D1(to)] which allows us to recover the information that this verb permits dative movement and requires a prepositional phrase headed by to'': (Takes NP NP ToPP) and (Takes NP NP NP). On the other hand, donate is coded [T1 (to)], which tells us that it does not undergo dative movement but does require a prepositional phrase headed by to'': (Takes NP NP ToPP). There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see <TARGET_CITATION/> , for further details ) . There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers (see <CITATION/>, for further details). On the other hand, donate is coded [T1 (to)], which tells us that it does not undergo dative movement but does require a prepositional phrase headed by to'': (Takes NP NP ToPP). Give is coded as [D1(to)] which allows us to recover the information that this verb permits dative movement and requires a prepositional phrase headed by to'': (Takes NP NP ToPP) and (Takes NP NP NP).",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,exploiting a large dictionary data base,1982,Archibal Michiels
971,P97-1063,External_19461,[0],introduction,The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other <TARGET_CITATION/> .,"A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word cooccurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other <TARGET_CITATION/> . The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other <CITATION/>. Word cooccurrence can be defined in various ways. A bitext comprises a pair of texts in two languages, where each text is a translation of the other.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,a geometric approach to mapping bitext correspondencequot,1996,I D Melamed
972,J87-3002,External_25753,[5],conclusion,<TARGET_CITATION/> ) .,"This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. <TARGET_CITATION/> ) . <CITATION/>). In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,882282a0d9b359618aaabed1638309988b1e10a8,The Automatic Grammatical Tagging of the LOB Corpus,1983,G. Leech; R. Garside; E. Atwell
973,J90-3003,External_5587,[0],introduction,"<TARGET_CITATION/> proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .","a performance factor, related to the difficulty of producing right branching structures such as <CITATION/>'' (p. 372). Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. <TARGET_CITATION/> proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model . <CITATION/> proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model. Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper. a performance factor, related to the difficulty of producing right branching structures such as <CITATION/>'' (p. 372).",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,9795acdcf9a4a57d921c5862c51aed575ff65f4c,Finite-State Parsing of Phrase-Structure Languages and the Status of Readjustment Rules in Grammar,2010,D. Langendoen
974,E03-1004,External_10615,[2],,"financial news , we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 14 , see <TARGET_CITATION/> ) on the training part of the EnglishCzech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary .","Also 12 translations were handled as 11  two words in one trlemma attribute. Compare an example of a Czech tectogrammatical tree after the lexical transfer step (Figure 3), with the original English sentence in Figure 2. financial news , we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 14 , see <TARGET_CITATION/> ) on the training part of the EnglishCzech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary . financial news, we created a probabilistic CzechEnglish dictionary by running GIZA++ training (translation models 14, see <CITATION/>) on the training part of the EnglishCzech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary. Compare an example of a Czech tectogrammatical tree after the lexical transfer step (Figure 3), with the original English sentence in Figure 2.Also 12 translations were handled as 11  two words in one trlemma attribute.",55559a2ee9693969d30237534ac290f4b0077a3a,Czech-English Dependency Tree-based Machine Translation,2003,Martin Cmejrek; J. Curín; Jirí Havelka,c9214ebe91454e6369720136ab7dd990d52a07d4,Improved Statistical Alignment Models,2000,F. Och; H. Ney
975,D12-1037,External_91022,[0],,"In the field of machine learning research , incremental training has been employed in the work <TARGET_CITATION/> , but there is little work for tuning parameters of statistical machine translation .","4 Incremental Training Based on Ultraconservative UpdateCompared with retraining mode, incremental training can improve the training efficiency. In the field of machine learning research , incremental training has been employed in the work <TARGET_CITATION/> , but there is little work for tuning parameters of statistical machine translation . In the field of machine learning research, incremental training has been employed in the work <CITATION/>, but there is little work for tuning parameters of statistical machine translation. Compared with retraining mode, incremental training can improve the training efficiency. 4 Incremental Training Based on Ultraconservative Update",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,e3948c28d605e0d90e88e160556cfc14fbba57c8,Incremental and Decremental Support Vector Machine Learning,2000,G. Cauwenberghs; T. Poggio
976,J97-4003,External_62674,[4],related work,A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <TARGET_CITATION/> .,"Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <TARGET_CITATION/> . A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <CITATION/>. This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). Another common approach to lexical rules is to encode them as unary phrase structure rules.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,cfe8d0f7ee87ff1cc90dc6b5286a58b80f11f62a,"Tfs the Typed Feature Structure Representation Formalism Delis Stands for \descriptive Lexical Speciications and Tools for Corpus-based Lexicon Building"". Delis",1994,Martin C. Emele
977,P02-1001,External_3025,[2],,"In a loglinear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <TARGET_CITATION/> .","Maximumlikelihood estimation guesses 0 to be the 0 maximizing Hi f(xi, yi). Maximumposterior estimation tries to maximize P(0)Hi f(xi, yi) where P(0) is a prior probability. In a loglinear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <TARGET_CITATION/> . In a loglinear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <CITATION/>. Maximumposterior estimation tries to maximize P(0)Hi f(xi, yi) where P(0) is a prior probability. Maximumlikelihood estimation guesses 0 to be the 0 maximizing Hi f(xi, yi).",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a,A Gaussian Prior for Smoothing Maximum Entropy Models,1999,Stanley F. Chen; R. Rosenfeld
978,P07-1068,External_392,[2],,"We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonlyused MUC scorer <TARGET_CITATION/> , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .","As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonlyused MUC scorer <TARGET_CITATION/> , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved . We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonlyused MUC scorer <CITATION/>, and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,,a modeltheoretic coreference scoring scheme,1995,M Vilain; J Burger; J Aberdeen; D Connolly; L Hirschman
979,D09-1056,External_60027,[0],introduction,A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 1117 % of the queries were composed of a person name with additional terms and 4 % were identified as person names <TARGET_CITATION/> .,"This ambiguity has recently become an active research topic and, simultaneously, in a relevant application domain for web search services: Zoominfo.com, Spock.com, 123people.com are examples of sites which perform web people search, although with limited disambiguation capabilities. A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 1117 % of the queries were composed of a person name with additional terms and 4 % were identified as person names <TARGET_CITATION/> . A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 1117% of the queries were composed of a person name with additional terms and 4% were identified as person names <CITATION/>. com are examples of sites which perform web people search, although with limited disambiguation capabilities. This ambiguity has recently become an active research topic and, simultaneously, in a relevant application domain for web search services: Zoominfo.com, Spock.com, 123people.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,1ff49e0f29e79fedde0966b0b4320aa7c3d76103,Searching for people on Web search engines,2004,A. Spink; B. Jansen; Jan O. Pedersen
980,J02-3002,External_62865,[4],,"This was done because purely unsupervised techniques ( e.g. , BaumWelch [ Baum and Petrie 1966 ] or Brill 's <TARGET_CITATION/> ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .","First, we used 20,000 tagged words to bootstrap'' the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , BaumWelch [ Baum and Petrie 1966 ] or Brill 's <TARGET_CITATION/> ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . This was done because purely unsupervised techniques (e.g., BaumWelch [Baum and Petrie 1966] or Brill's [Brill 1995b]) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. First, we used 20,000 tagged words to bootstrap'' the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging,1995,Eric Brill
981,W10-3814,P08-1024,[5],conclusion,"Future research should apply the work of <TARGET_CITATION/> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multinonterminal grammars .","Hierarchical phrasebased MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntaxaugmented MT with its thousands of nonterminals, and made even worse by its joint sourceandtarget extension. Future research should apply the work of <TARGET_CITATION/> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multinonterminal grammars . Future research should apply the work of <CITATION/>, who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multinonterminal grammars. This problem is exacerbated by syntaxaugmented MT with its thousands of nonterminals, and made even worse by its joint sourceandtarget extension. Hierarchical phrasebased MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",1d14215e704bc262ce7ab600aa49530b6f0599fc,New Parameterizations and Features for PSCFG-Based Machine Translation,2010,Andreas Zollmann; S. Vogel,02bcc68113cff36226eb9d977f7367f14e2157e5,A Discriminative Latent Variable Model for Statistical Machine Translation,2008,Phil Blunsom; Trevor Cohn; M. Osborne
982,J00-2004,W95-0115,[0],,"In informal experiments described elsewhere <TARGET_CITATION/> , I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02 .","The statistical interdependence between two word types can be estimated more robustly by considering the whole table. For example, Gale and Church (1991, 154) suggest that 02, a x2like statistic, seems to be a particularly good choice because it makes good use of the offdiagonal cells'' in the contingency table. In informal experiments described elsewhere <TARGET_CITATION/> , I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02 . In informal experiments described elsewhere <CITATION/>, I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02. For example, Gale and Church (1991, 154) suggest that 02, a x2like statistic, seems to be a particularly good choice because it makes good use of the offdiagonal cells'' in the contingency table.The statistical interdependence between two word types can be estimated more robustly by considering the whole table.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,42fd4d469c53e4eedd7eb76e7859e3270367f795,Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best Translation Lexicons,1995,I. D. Melamed
983,P07-1068,J01-4004,[1],,"porating these two KSs into our resolver : they can Following <TARGET_CITATION/> , we select as the aneach be represented as a constraint or as a feature , tecedent of each NP , NPS , the closest preceding NP and they can be applied to the resolver in isolation that is classified as coreferent with NPS .","Incorporate the two KSs into the baseline reAfter training, the decision tree classifier is used solver. Recall that there are eight ways of incorto select an antecedent for each NP in a test text. porating these two KSs into our resolver : they can Following <TARGET_CITATION/> , we select as the aneach be represented as a constraint or as a feature , tecedent of each NP , NPS , the closest preceding NP and they can be applied to the resolver in isolation that is classified as coreferent with NPS . porating these two KSs into our resolver: they can Following <CITATION/>, we select as the aneach be represented as a constraint or as a feature, tecedent of each NP, NPS, the closest preceding NP and they can be applied to the resolver in isolation that is classified as coreferent with NPS. Recall that there are eight ways of incorto select an antecedent for each NP in a test text. Incorporate the two KSs into the baseline reAfter training, the decision tree classifier is used solver.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,a20bfec3c95aad003dcb45a21a220c19cca8bb66,A Machine Learning Approach to Coreference Resolution of Noun Phrases,2001,Wee Meng Soon; H. Ng; Chung Yong Lim
984,W06-3309,External_14127,[2],method,"Using the section labels , the HMM was trained using the HTK toolkit <TARGET_CITATION/> , which efficiently performs the forwardbackward algorithm and BaumWelch estimation .","The transition probability matrix of the HMM was initialized with uniform probabilities over a fully connected graph. The output probabilities were modeled as fourdimensional Gaussians mixtures with diagonal covariance matrices. Using the section labels , the HMM was trained using the HTK toolkit <TARGET_CITATION/> , which efficiently performs the forwardbackward algorithm and BaumWelch estimation . Using the section labels, the HMM was trained using the HTK toolkit <CITATION/>, which efficiently performs the forwardbackward algorithm and BaumWelch estimation. The output probabilities were modeled as fourdimensional Gaussians mixtures with diagonal covariance matrices. The transition probability matrix of the HMM was initialized with uniform probabilities over a fully connected graph.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,a98e481ce418a437cdfae107d85f009a5da6a790,The HTK book,1995,S. Young; Jack Jansen; J. Odell; Dg Ollason; P. Woodland
985,W04-1610,External_69209,[4],experiments,"In this paper , we use TFIDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in <TARGET_CITATION/> .","<CITATION/> has found strong correlations between DF, IG and the X2 statistic for a term. On the other hand, <CITATION/> reports the X2 to produce best performance. In this paper , we use TFIDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in <TARGET_CITATION/> . In this paper, we use TFIDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in <CITATION/>. On the other hand, <CITATION/> reports the X2 to produce best performance. <CITATION/> has found strong correlations between DF, IG and the X2 statistic for a term.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,toward an arabic web page classifierquot master project,2001,M Yahyaoui
986,P00-1007,External_5861,[0],introduction,"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences ( e.g. , <TARGET_CITATION/> ) ."," A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences <TARGET_CITATION/> . A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences (e.g., <CITATION/>).",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,54c846ee00c6132d70429cc279e8577f63ed05e4,A Linear Observed Time Statistical Parser Based on Maximum Entropy Models,1997,A. Ratnaparkhi
987,P13-3018,External_90451,[0],introduction,"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages <TARGET_CITATION/> .","Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages <TARGET_CITATION/> . There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (<CITATION/>; Grainger, et al., 1991; <CITATION/>). Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,d7996fce2deb35b71bcc7ddc63209678e26458d1,Masked morphological priming in visual word recognition.,1991,J. Grainger; P. Colé; J. Segui
988,J02-3002,A00-2035,[4],,"In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in <TARGET_CITATION/> .","The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <CITATION/>: a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by <CITATION/>, who trained a decision tree classifier on a 25millionword corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in <TARGET_CITATION/> . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in <CITATION/>. The best performance on the Brown corpus, a 0.2% error rate, was reported by <CITATION/>, who trained a decision tree classifier on a 25millionword corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <CITATION/>: a 0.5% error rate.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,ec2f036f1e6f56ad6400e42cf1eb14f4ebe122c6,Tagging Sentence Boundaries,2000,Andrei Mikheev
990,Q13-1020,P11-1001,[4],related work,<TARGET_CITATION/> further labeled the SCFG rules with POS tags and unsupervised word classes .,"Compared to their work, we do not rely on any Treebank resources and focus on generating effective unsupervised tree structures for treebased translation models. <CITATION/> substituted the nonterminal X in hierarchical phrasebased model by extended syntactic categories. <TARGET_CITATION/> further labeled the SCFG rules with POS tags and unsupervised word classes . <CITATION/> further labeled the SCFG rules with POS tags and unsupervised word classes. <CITATION/> substituted the nonterminal X in hierarchical phrasebased model by extended syntactic categories. Compared to their work, we do not rely on any Treebank resources and focus on generating effective unsupervised tree structures for treebased translation models.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,feb8011d6bcd06bfa2ae2e50517a8d5b283b7d71,A Word-Class Approach to Labeling PSCFG Rules for Machine Translation,2011,Andreas Zollmann; S. Vogel
991,N10-1084,External_64567,[0],introduction,"Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer <TARGET_CITATION/> ."," Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer <TARGET_CITATION/> . Steganography is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer <CITATION/>.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,b6749876c0b6eedbd4a410ffc214e04bc415fa43,"Steganography in Digital Media: Principles, Algorithms, and Applications",2009,J. Fridrich
992,P02-1001,P96-1029,[0],introduction,"We offer a theorem that highlights the broad applicability of these modeling techniques .4 If f ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ; ( 2 ) f can be computed by a Markovian FST that halts with probability 1 ; ( 3 ) f can be expressed as a probabilistic regexp , i.e. , a regexp built up from atomic expressions a : b ( for a E E U LCB E RCB , b E A U LCB E RCB ) using concatenation , probabilistic union + p , and probabilistic closure * p. For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules <CITATION/> , ( 3 ) by compilation of decision trees <TARGET_CITATION/> , ( 4 ) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation ( Gerdemann and van <CITATION/> ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","These 4 parameters have global effects on Fig. 1a, thanks to complex parameter tying: arcs  b:p ) @,  b:q )  in Fig. 1b get respective probabilities (1  A) and (1  ), which covary with  and vary oppositely with . Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a. We offer a theorem that highlights the broad applicability of these modeling techniques .4 If f ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ; ( 2 ) f can be computed by a Markovian FST that halts with probability 1 ; ( 3 ) f can be expressed as a probabilistic regexp , i.e. , a regexp built up from atomic expressions a : b ( for a E E U LCB E RCB , b E A U LCB E RCB ) using concatenation , probabilistic union + p , and probabilistic closure * p. For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules <CITATION/> , ( 3 ) by compilation of decision trees <TARGET_CITATION/> , ( 4 ) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation ( Gerdemann and van <CITATION/> ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below . We offer a theorem that highlights the broad applicability of these modeling techniques.4 If f(input, output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a E E U LCBERCB, b E A U LCBERCB) using concatenation, probabilistic union +p, and probabilistic closure *p. For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules <CITATION/>, (3) by compilation of decision trees <CITATION/>, (4) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation (Gerdemann and van <CITATION/>),5 (5) by conditionalization of a joint relation as discussed below. Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a. These 4 parameters have global effects on Fig. 1a, thanks to complex parameter tying: arcs  b:p ) @,  b:q )  in Fig. 1b get respective probabilities (1  A) and (1  ), which covary with  and vary oppositely with .",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,110fd6d66c7380556b377ada84cfb5cd0b6bbaa0,Compilation of Weighted Finite-State Transducers from Decision Trees,1996,R. Sproat; M. Riley
993,W06-1104,External_14759,[2],experiments,"Word pairs containing polysemous words are expanded to concept pairs using GermaNet <TARGET_CITATION/> , the German equivalent to WordNet , as a sense inventory for each word .","7http://berufenet.arbeitsagentur.de 8l=logarithmic term frequency, t=logarithmic inverse document frequency, c=cosine normalization.were nounnoun pairs, 15% nounverb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet <TARGET_CITATION/> , the German equivalent to WordNet , as a sense inventory for each word . Word pairs containing polysemous words are expanded to concept pairs using GermaNet <CITATION/>, the German equivalent to WordNet, as a sense inventory for each word. were nounnoun pairs, 15% nounverb pairs and so on. 7http://berufenet.arbeitsagentur.de 8l=logarithmic term frequency, t=logarithmic inverse document frequency, c=cosine normalization.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,lexikalischsemantische wortnetze chapter computerlinguistik und sprachtechnologie,2004,Claudia Kunze
994,Q13-1020,N04-1035,[2],method,"Using the initial target Utrees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes <TARGET_CITATION/> .","To get the initial Utrees, we recursively and randomly segment a sentence into two parts and simultaneously create a tree node to dominate each part. The created tree nodes are labeled by the nonterminals described in section 3. Using the initial target Utrees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes <TARGET_CITATION/> . Using the initial target Utrees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes <CITATION/>. The created tree nodes are labeled by the nonterminals described in section 3. To get the initial Utrees, we recursively and randomly segment a sentence into two parts and simultaneously create a tree node to dominate each part.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,a7e925a65860e90b2b4eb427a8bc497f76b2fe6e,What’s in a translation rule?,2004,Michel Galley; Mark Hopkins; Kevin Knight; D. Marcu
995,J04-3001,P92-1017,[0],introduction,"For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data <TARGET_CITATION/> .","Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data <TARGET_CITATION/> . For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data <CITATION/>. Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,15e4843e2c55843b5c5b429f89dad3d99e801f02,Inside-Outside Reestimation From Partially Bracketed Corpora,1992,Fernando C Pereira; Yves Schabes
996,P00-1007,External_555,[4],,"Our results are lower than those of full parsers , e.g. , <TARGET_CITATION/> as might be expected since much less structural data , and no lexical data are being used .","There are currently no other partial parsers on these tasks to compare the combined VP and NP results to. Tjong Kim <CITATION/> presented result for composite NP, obtained by repeated cascading, similar to our results with seperate base and composite NPs and no internal structure. Our results are lower than those of full parsers , e.g. , <TARGET_CITATION/> as might be expected since much less structural data , and no lexical data are being used . Our results are lower than those of full parsers, e.g., <CITATION/> as might be expected since much less structural data, and no lexical data are being used.Tjong Kim <CITATION/> presented result for composite NP, obtained by repeated cascading, similar to our results with seperate base and composite NPs and no internal structure. There are currently no other partial parsers on these tasks to compare the combined VP and NP results to.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,0ffa423a5283396c88ff3d4033d541796bd039cc,"Three Generative, Lexicalised Models for Statistical Parsing",1997,M. Collins
997,W14-1815,External_73192,[4],related work,"Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <CITATION/> ( Ramakrishnan <TARGET_CITATION/> ) , where specified meter or rhyme schemes are enforced .","The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as <CITATION/> which produces weather reports from structured data or <CITATION/> which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <CITATION/> ( Ramakrishnan <TARGET_CITATION/> ) , where specified meter or rhyme schemes are enforced . Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry <CITATION/> or song lyrics <CITATION/> (Ramakrishnan A et al., 2009), where specified meter or rhyme schemes are enforced. The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as <CITATION/> which produces weather reports from structured data or <CITATION/> which generates descriptions of objects from images.",dcd0e19d450a0d43b0d8b32415bb731f5838e593,Natural Language Generation with Vocabulary Constraints,2014,Benjamin Swanson; Elif Yamangil; Eugene Charniak,8fc7dc19ccf01fa10847f9b8e77ce4ec6c80844f,Automatic Generation of Tamil Lyrics for Melodies,2009,A. Ramakrishnan A.; S. Kuppan; S. Lalitha Devi
998,J06-2002,External_2854,[0],introduction,"Hermann and Deutsch ( 1976 ; also reported in <TARGET_CITATION/> ) show that greater differences are most likely to be chosen , presumably because they are more striking .","Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Hermann and Deutsch ( 1976 ; also reported in <TARGET_CITATION/> ) show that greater differences are most likely to be chosen , presumably because they are more striking . Hermann and Deutsch (1976; also reported in Levelt 1989) show that greater differences are most likely to be chosen, presumably because they are more striking. Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x. Which of these should come first? Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,2dd7698124dae613def704272629fe03404ce62b,Speaking: From Intention to Articulation,1989,W. Levelt
1000,W06-1705,External_36200,[0],method,"This system has been successfully tested with the development of plugins supporting instant messaging , distributed video encoding <TARGET_CITATION/> , distributed virtual worlds <CITATION/> and digital library management <CITATION/> .","We have designed this environment so that specific application functionalitycan be captured within plugins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plugins supporting instant messaging , distributed video encoding <TARGET_CITATION/> , distributed virtual worlds <CITATION/> and digital library management <CITATION/> . This system has been successfully tested with the development of plugins supporting instant messaging, distributed video encoding <CITATION/>, distributed virtual worlds <CITATION/> and digital library management <CITATION/>. can be captured within plugins that can then integrate with the environment and utilise its functionality. We have designed this environment so that specific application functionality",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,distributed video encoding over a peertopeer network,2005,D Hughes; J Walkerdine
1001,J05-3003,External_198,[2],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ <TARGET_CITATION/> ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ <TARGET_CITATION/> ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,e17117dbee804d7d177d8eb9fadf0bda1ebc4d22,Lexical Functional Grammar A Formal System for Grammatical Representation,2004,Ronald M. Kaplan
1002,K15-1002,External_62,[2],,We then use Illinois Chunker <CITATION/> 6 to extract more noun phrases from the text and employ Collins head rules <TARGET_CITATION/> to identify their heads .,"Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker <CITATION/> 6 to extract more noun phrases from the text and employ Collins head rules <TARGET_CITATION/> to identify their heads . We then use Illinois Chunker <CITATION/>6 to extract more noun phrases from the text and employ Collins head rules <CITATION/> to identify their heads. 3), we train on a set of candidates with precision larger than 50%. Specifically, after mention head candidate generation (described in Sec.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,,headdriven statistical models for natural language parsing,1999,M Collins
1003,P13-3018,External_90446,[0],related work,<TARGET_CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .,"A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. <CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries. <TARGET_CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . <CITATION/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. <CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries. A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,conscious choice and some light verbs in urduquot,1993,M Butt
1004,P10-2059,External_2679,[0],introduction,"feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Intercoder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see <TARGET_CITATION/> , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent <CITATION/> .","We used slices of 0.04 seconds. The intercoder agreement figures obtained for the three types of annotation are given in Table 2. feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Intercoder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see <TARGET_CITATION/> , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent <CITATION/> . feature Cohen's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2: Intercoder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see <CITATION/>, it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent <CITATION/>. The intercoder agreement figures obtained for the three types of annotation are given in Table 2. We used slices of 0.04 seconds.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,intercoder agreement for computational linguistics,2008,Ron Artstein; Massimo Poesio
1005,E03-1005,A00-2018,[4],experiments,"Collins 1996 , Charniak 1997 , Collins 1999 and <TARGET_CITATION/> ) .","as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. Collins 1996 , Charniak 1997 , Collins 1999 and <TARGET_CITATION/> ) . Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000).Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. as Bod01 and Bon99.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
1006,J97-4003,External_84379,[0],introduction,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the metalevel lexical rules ( MLRs ; Calcagno 1995 ; <TARGET_CITATION/> ) and the .","While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the metalevel lexical rules ( MLRs ; Calcagno 1995 ; <TARGET_CITATION/> ) and the . Two formalizations of lexical rules as used by HPSG linguists have been proposed, the metalevel lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the .While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,1e7199f4a7ce6e4fd9c1a25e6d95b66cec8452dd,Lexical Rules in HPSG: What are they?,2001,M. Calcagno
1007,P11-1134,External_3667,[2],,"We run TreeTagger <CITATION/> for tokenization , and used the Giza + + <TARGET_CITATION/> to align the tokenized corpora at the word level .","The one adopted in this work consists in learning phrase alignments from a wordaligned bilingual corpus. In order to build EnglishSpanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations SpanishEnglish parallel corpora released for the WMT101. We run TreeTagger <CITATION/> for tokenization , and used the Giza + + <TARGET_CITATION/> to align the tokenized corpora at the word level . We run TreeTagger <CITATION/> for tokenization, and used the Giza++ <CITATION/> to align the tokenized corpora at the word level. In order to build EnglishSpanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations SpanishEnglish parallel corpora released for the WMT101. The one adopted in this work consists in learning phrase alignments from a wordaligned bilingual corpus.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,de2df29b0a0312de7270c3f5a0af6af5645cf91a,A Systematic Comparison of Various Statistical Alignment Models,2003,F. Och; H. Ney
1008,J01-4001,P96-1036,[0],,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <TARGET_CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <TARGET_CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Charniak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form <CITATION/>; and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a, 2001b). Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,a46bc99d90bd064e9ccf4def70ac89b27537283b,Functional Centering,1996,M. Strube; U. Hahn
1009,J00-4002,External_32773,[4],introduction,"We then go on to compare the current approach with that of some other theories with similar aims : the  standard '' version of quasilogical form implemented in the Core Language Engine , as rationally reconstructed by <TARGET_CITATION/> ; underspecified Discourse Representation Theory <CITATION/> ; and the  glue language '' approach of <CITATION/> .","There is far more to say about each of these phenomena, of course, and the analyses here are by no means claimed to be definitive. The aim is merely to show that we can, to a first approximation, provide a reasonably fully worked out description of these phenomena in a truly bidirectional way. We then go on to compare the current approach with that of some other theories with similar aims : the  standard '' version of quasilogical form implemented in the Core Language Engine , as rationally reconstructed by <TARGET_CITATION/> ; underspecified Discourse Representation Theory <CITATION/> ; and the  glue language '' approach of <CITATION/> . We then go on to compare the current approach with that of some other theories with similar aims: the standard'' version of quasilogical form implemented in the Core Language Engine, as rationally reconstructed by <CITATION/>; underspecified Discourse Representation Theory <CITATION/>; and the glue language'' approach of <CITATION/>. The aim is merely to show that we can, to a first approximation, provide a reasonably fully worked out description of these phenomena in a truly bidirectional way. There is far more to say about each of these phenomena, of course, and the analyses here are by no means claimed to be definitive.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,e87314e4e465ff21001e04a38b8fcdfeadae714e,Monotonic Semantic Interpretation,1992,H. Alshawi; Dick Crouch
1010,J03-3004,External_29257,[0],introduction,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX <TARGET_CITATION/> ; Veale and Way 1997 ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed .","When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX <TARGET_CITATION/> ; Veale and Way 1997 ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed . Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cfXXX Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,6429a59bfe5b435961463ff628c0a554951b0993,Toward Memorybased Translation,1990,S. Sato
1011,N01-1002,W00-1425,[0],,"Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in <TARGET_CITATION/> .","For an adjective, we use WordNet to derive its category rather than directly assigning a predefined value. In WordNet, if an adjective ascribes a value to a noun concept, e.g. round gives a value of shape, WordNet will contain a pointer between the adjective and the noun by which the appropriate attribute is lexicalised, e.g. between round and shape. Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in <TARGET_CITATION/> . Using WordNet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in <CITATION/>. In WordNet, if an adjective ascribes a value to a noun concept, e.g. round gives a value of shape, WordNet will contain a pointer between the adjective and the noun by which the appropriate attribute is lexicalised, e.g. between round and shape. For an adjective, we use WordNet to derive its category rather than directly assigning a predefined value.",a463350fc2e38decfe736e0801d465874cef0891,Corpus-based NP Modifier Generation,2001,Hua Cheng; Massimo Poesio; R. Henschel; C. Mellish,6ff81cb8db2b5f5ea70432ad267e6462e57efd87,Capturing the Interaction between Aggregation and Text Planning in Two Generation Systems,2000,Hua Cheng; C. Mellish
1012,W00-1312,External_24242,[4],related work,One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries <TARGET_CITATION/> .,Our work has focused on crosslingual retrieval. Many approaches to crosslingual IR have been published. One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries <TARGET_CITATION/> . One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents to the language of the queries <CITATION/>. Many approaches to crosslingual IR have been published. Our work has focused on crosslingual retrieval.,e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,705b4c2770a1b658dcd2fc9ca99dcdeb778161ac,A comparative study of query and document translation for cross-language information retrieval,1998,Douglas W. Oard
1014,W03-0806,External_83364,[0],introduction,Software engineering research on Generative Programming <TARGET_CITATION/> attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .,"Finally, NLP is growing in terms of the number of tasks, methods and languages being researched. Although many problems share algorithms and data structures there is a tendency to reinvent the wheel. Software engineering research on Generative Programming <TARGET_CITATION/> attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems . Software engineering research on Generative Programming <CITATION/> attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems. Although many problems share algorithms and data structures there is a tendency to reinvent the wheel. Finally, NLP is growing in terms of the number of tasks, methods and languages being researched.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,268a30e57724f7e3b92b16f19b82f003ba725778,"Generative Programming: Methods, Tools, and Applications",2000,K. Czarnecki; U. Eisenecker
1015,J05-3003,External_61277,[2],method,Both use the evaluation software and triple encoding presented in <TARGET_CITATION/> .,"In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the fstructure annotations. The results of two different evaluations of the automatically generated fstructures are presented in Table 2. Both use the evaluation software and triple encoding presented in <TARGET_CITATION/> . Both use the evaluation software and triple encoding presented in <CITATION/>. The results of two different evaluations of the automatically generated fstructures are presented in Table 2. In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the fstructure annotations.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,b361b5941d79d0568efef14145417274240017ad,A Comparison of Evaluation Metrics for a Broad-Coverage Stochastic Parser,2003,Dick Crouch; R. Kaplan; Tracy Holloway King; S. Riezler
1016,W02-1601,External_4496,[0],,"In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. <TARGET_CITATION/> cases ) .","As mentioned earlier, there are some nonstandard phenomena exist between different languages, that cause challenges for synchronized formalisms. In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. <TARGET_CITATION/> cases ) . In this Section, we will describe some example cases, which are drawn from the problem of using synchronous formalisms to define translations between languages (e.g. <CITATION/> cases). As mentioned earlier, there are some nonstandard phenomena exist between different languages, that cause challenges for synchronized formalisms.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,e6a125ae8b2f635c050e708ff8e053f26cc3281c,RESTRICTING THE WEAK‐GENERATIVE CAPACITY OF SYNCHRONOUS TREE‐ADJOINING GRAMMARS,1994,Stuart M. Shieber
1017,J04-3001,External_90833,[0],related work,"Some examples include text categorization <CITATION/> , base noun phrase chunking <TARGET_CITATION/> , partofspeech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation <CITATION/> , and word sense disambiguation <CITATION/> .","In addition to PPattachment, as discussed in this article, sample selection has been successfully applied to other classificationapplications. Some examples include text categorization <CITATION/> , base noun phrase chunking <TARGET_CITATION/> , partofspeech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation <CITATION/> , and word sense disambiguation <CITATION/> . Some examples include text categorization <CITATION/>, base noun phrase chunking <CITATION/>, partofspeech tagging (Engelson Dagan 1996), spelling confusion set disambiguation <CITATION/>, and word sense disambiguation <CITATION/>. applications. In addition to PPattachment, as discussed in this article, sample selection has been successfully applied to other classification",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,52e16b234c8d15d20de0e892e190bb6af04576e3,Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun Phrase Chunking,2000,G. Ngai; David Yarowsky
1018,K15-1002,D13-1057,[0],introduction,Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( <TARGET_CITATION/> ; Bj  orkelund and <CITATION/> ) .,Mention detection is rarely studied as a standalone research problem (<CITATION/> is one key exception). Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( <TARGET_CITATION/> ; Bj  orkelund and <CITATION/> ) . Most coreference resolution work simply mentions it in passing as a module in the pipelined system <CITATION/>. Mention detection is rarely studied as a standalone research problem (<CITATION/> is one key exception).,f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,be2f82cfd32c41d6493dc3ffb414de27b4f9e15b,A Constrained Latent Variable Model for Coreference Resolution,2013,Kai-Wei Chang; Rajhans Samdani; D. Roth
1019,J09-4010,External_3555,[2],method,"Because the judges do not evaluate the same cases , we could not employ standard interannotator agreement measures <TARGET_CITATION/> .","Each judge was given 20 of these cases, and was asked to assess the generated responses on the four criteria listed previously.14 We maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable. Because the judges do not evaluate the same cases , we could not employ standard interannotator agreement measures <TARGET_CITATION/> . Because the judges do not evaluate the same cases, we could not employ standard interannotator agreement measures <CITATION/>. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable. Each judge was given 20 of these cases, and was asked to assess the generated responses on the four criteria listed previously.14 We maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,613b6c9a85ae338cd3b405dc019c8edb1c15717c,Assessing Agreement on Classification Tasks: The Kappa Statistic,1996,J. Carletta
1020,P10-2059,External_81761,[0],introduction,Related are also the studies by Rieks op den <TARGET_CITATION/> : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .,"Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, <CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements, while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den <TARGET_CITATION/> : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus . Related are also the studies by Rieks op den <CITATION/>: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. For example, <CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements, while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels. Others have looked at the application of machine learning algorithms to annotated multimodal corpora.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,detecting action meetings in meetings,2008,Gabriel Murray; Steve Renals
1021,N04-2004,P01-1017,[0],introduction,"Due to advances in statistical syntactic parsing techniques <TARGET_CITATION/> , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .","The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content. Due to advances in statistical syntactic parsing techniques <TARGET_CITATION/> , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences . Due to advances in statistical syntactic parsing techniques <CITATION/>, attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences. The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,436772d9a916f0382800cf18581cfdfd4f83c457,Immediate-Head Parsing for Language Models,2001,Eugene Charniak
1024,E03-1005,External_681,[2],experiments,"For our experiments we used the standard division of the WSJ <TARGET_CITATION/> , with sections 2 through 21 for training ( approx ."," For our experiments we used the standard division of the WSJ <TARGET_CITATION/> , with sections 2 through 21 for training ( approx . For our experiments we used the standard division of the WSJ <CITATION/>, with sections 2 through 21 for training (approx.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,0b44fcbeea9415d400c5f5789d6b892b6f98daff,Building a Large Annotated Corpus of English: The Penn Treebank,1993,Mitchell P. Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz
1025,P13-3018,External_63932,[2],introduction,"With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( <TARGET_CITATION/> ; Bentin , S. and <CITATION/> ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .","As linguistic arguments have so far not led to a consensus, we here use cognitiveexperiments to probe the brain signatures of verbverb combinations and propose cognitive as well as computational models regarding the possible organization and processing of Bangla CVs in the mental lexicon (ML). With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( <TARGET_CITATION/> ; Bentin , S. and <CITATION/> ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla . With respect to this, we apply the different priming and other lexical decision experiments, described in literature (<CITATION/>; Bentin, S. and <CITATION/>) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla. experiments to probe the brain signatures of verbverb combinations and propose cognitive as well as computational models regarding the possible organization and processing of Bangla CVs in the mental lexicon (ML). As linguistic arguments have so far not led to a consensus, we here use cognitive",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,df408a7acf643fead01848986fec63f26da09bbb,Morphology and meaning in the English mental lexicon.,1994,W. Marslen-Wilson; L. Tyler; Rachelle Waksler; Lianne Older
1026,J03-3004,External_41371,[0],introduction,"From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a searchandreplace engine , albeit a rather sophisticated one <TARGET_CITATION/> .","TM systems store a set of (source, target) translation pairs in their databases. If a new input string cannot be found exactly in the translation database, a search is conducted for close (or fuzzy'') matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation. From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a searchandreplace engine , albeit a rather sophisticated one <TARGET_CITATION/> . From this description, it should be clear that TM systems do not translate: Indeed, some researchers consider them to be little more than a searchandreplace engine, albeit a rather sophisticated one <CITATION/>. If a new input string cannot be found exactly in the translation database, a search is conducted for close (or fuzzy'') matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation. TM systems store a set of (source, target) translation pairs in their databases.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,what’s been forgotten in translation memory in envisioning machine translation in the information future,2000,Elliott Macklovitch; Graham Russell
1027,J01-4001,External_62494,[0],,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; <TARGET_CITATION/> ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; <TARGET_CITATION/> ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Charniak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form <CITATION/>; and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a, 2001b). Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,f0a1dfcef6506b12a35cd1e1cd1ddcee42e9e8a9,Probabilistic Coreference in Information Extraction,1997,A. Kehler
1028,W01-1510,C00-1060,[0],introduction,"Our group has developed a widecoverage HPSG grammar for Japanese <CITATION/> , which is used in a highaccuracy Japanese dependency analyzer <TARGET_CITATION/> .","Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>. In practical context, German, English, and Japanese HPSGbased grammars are developed and used in the Verbmobil project <CITATION/>. Our group has developed a widecoverage HPSG grammar for Japanese <CITATION/> , which is used in a highaccuracy Japanese dependency analyzer <TARGET_CITATION/> . Our group has developed a widecoverage HPSG grammar for Japanese <CITATION/>, which is used in a highaccuracy Japanese dependency analyzer <CITATION/>.In practical context, German, English, and Japanese HPSGbased grammars are developed and used in the Verbmobil project <CITATION/>. Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,97fce4f41691b8707b12421320601f42b8cf134f,A Hybrid Japanese Parser with Hand-crafted Grammar and Statistics,2000,H. Kanayama; Kentaro Torisawa; Yutaka Mitsuishi; Junichi Tsujii
1029,W00-1312,External_826,[4],method,There are several variations of such a method <TARGET_CITATION/> .,"A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method <TARGET_CITATION/> . There are several variations of such a method <CITATION/>. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. A second method is to structure the translated query, separating the translations for one term from translations for other terms.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,the effects of query structure and dictionary setups in dictionarybased crosslanguage information retrievalquot,1998,An Pirkola
1031,D09-1056,I08-1020,[0],related work,<TARGET_CITATION/> compared the performace of NEs versus BoW features .,"In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents see for instance <CITATION/>. For instance, <CITATION/> uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. <TARGET_CITATION/> compared the performace of NEs versus BoW features . <CITATION/> compared the performace of NEs versus BoW features. For instance, <CITATION/> uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents see for instance <CITATION/>.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,d3b89f9bf287714a4e4b0246ed143dec68064a16,Experiments on Semantic-based Clustering for Cross-document Coreference,2008,Horacio Saggion
1032,K15-1003,W14-1615,[4],method,The rightside context of a nonterminal category  the probability of generating a category to the right of the current constituent 's category  corresponds directly to the category transitions used for the HMM supertagger of <TARGET_CITATION/> .,"In order to encourage our model to choose trees in which the constituent labels fit'' into their supertag contexts, we want to bias our context parameters toward context categories that are combinable with the constituent label. The rightside context of a nonterminal category  the probability of generating a category to the right of the current constituent 's category  corresponds directly to the category transitions used for the HMM supertagger of <TARGET_CITATION/> . The rightside context of a nonterminal category  the probability of generating a category to the right of the current constituent's category  corresponds directly to the category transitions used for the HMM supertagger of <CITATION/>. In order to encourage our model to choose trees in which the constituent labels fit'' into their supertag contexts, we want to bias our context parameters toward context categories that are combinable with the constituent label.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,05bbf7436bfcb1e49e1132c16f25706b61de6c93,Weakly-Supervised Bayesian Learning of a CCG Supertagger,2014,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith
1033,J00-4002,External_37743,[4],,"The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasilogical form as described in <TARGET_CITATION/> ) , and implemented in SRI 's Core Language Engine ( CLE ) .","Comparison with Alternative Approaches5.1 Core Language Engine QuasiLogical Form The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasilogical form as described in <TARGET_CITATION/> ) , and implemented in SRI 's Core Language Engine ( CLE ) . The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasilogical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE). 5.1 Core Language Engine QuasiLogical FormComparison with Alternative Approaches",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,334f124cbd015e0a37c9562094d9d8c195c41edc,Resolving Quasi Logical Forms,1990,H. Alshawi
1034,J92-1004,H91-1014,[5],,"However , the method we are currently using in the ATIS domain <TARGET_CITATION/> represents our most promising approach to this problem .","There are no separate semantic rules off to the side; rather, the semantic information is encoded directly as names attached to nodes in the tree. Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain <TARGET_CITATION/> represents our most promising approach to this problem . However, the method we are currently using in the ATIS domain <CITATION/> represents our most promising approach to this problem. Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. There are no separate semantic rules off to the side; rather, the semantic information is encoded directly as names attached to nodes in the tree.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,4911a09ce25987e260c4a8c7773a703335977c99,Development and Preliminary Evaluation of the MIT ATIS System,1991,S. Seneff; James R. Glass; D. Goddeau; D. Goodine; L. Hirschman; H. Leung; M. Phillips; J. Polifroni; V. Zue
1036,E03-1005,External_6640,[4],experiments,"<TARGET_CITATION/> , Charniak 1997 , Collins 1999 and Charniak 2000 ) .","as Bod01 and Bon99. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. <TARGET_CITATION/> , Charniak 1997 , Collins 1999 and Charniak 2000 ) . Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000).Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. as Bod01 and Bon99.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,3764baa7465201f054083d02b58fa75f883c4461,A New Statistical Parser Based on Bigram Lexical Dependencies,1996,M. Collins
1037,D13-1115,External_9243,[0],related work,"To name a few examples , <CITATION/> show how semantic information from text can be used to improve zeroshot classification ( i.e. , classifying neverbeforeseen objects ) , and <TARGET_CITATION/> show that verb clusters can be used to improve activity recognition in videos .","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>. The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , <CITATION/> show how semantic information from text can be used to improve zeroshot classification ( i.e. , classifying neverbeforeseen objects ) , and <TARGET_CITATION/> show that verb clusters can be used to improve activity recognition in videos . To name a few examples, <CITATION/> show how semantic information from text can be used to improve zeroshot classification (i.e., classifying neverbeforeseen objects), and <CITATION/> show that verb clusters can be used to improve activity recognition in videos.The Computer Vision community has also benefited greatly from efforts to unify the two modalities. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,26f99ebfe29078c13cb7972661d0f43dec7933b8,Improving Video Activity Recognition using Object Recognition and Text Mining,2012,Tanvi S. Motwani; R. Mooney
1038,J97-4003,External_51366,[2],,The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and <TARGET_CITATION/> ; Gotz and Meurers 1997a ) ., The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and <TARGET_CITATION/> ; Gotz and Meurers 1997a ) . The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system <CITATION/>.,d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,an expanded logical formalism for headdriven phrase structure grammar,1994,Paul King
1041,D08-1034,J08-2004,[4],experiments,"To prove that our method is effective , we also make a comparison between the performances of our system and <TARGET_CITATION/> .","This made the interdependence of core arguments can be directly explored from the extraction of semantic context features. So the ARGX sub task is improved. To prove that our method is effective , we also make a comparison between the performances of our system and <TARGET_CITATION/> . To prove that our method is effective, we also make a comparison between the performances of our system and <CITATION/>. So the ARGX sub task is improved. This made the interdependence of core arguments can be directly explored from the extraction of semantic context features.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,03541f4c7b737964289b3cb2cde4b6ac08a1c29d,Labeling Chinese Predicates with Semantic Roles,2008,Nianwen Xue
1042,E03-1005,A00-2018,[0],introduction,"But while Bod 's estimator obtains stateoftheart results on the WSJ , comparable to <TARGET_CITATION/> , Bonnema et al. 's estimator performs worse and is comparable to <CITATION/> .","This paper presents the first published results with Goodman's PCFGreductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFGreductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod 's estimator obtains stateoftheart results on the WSJ , comparable to <TARGET_CITATION/> , Bonnema et al. 's estimator performs worse and is comparable to <CITATION/> . But while Bod's estimator obtains stateoftheart results on the WSJ, comparable to <CITATION/>, Bonnema et al.'s estimator performs worse and is comparable to <CITATION/>. We show that these PCFGreductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). This paper presents the first published results with Goodman's PCFGreductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
1044,J02-3002,External_32929,[4],,"As <TARGET_CITATION/> rightly pointed out , however ,  Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .","12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As <TARGET_CITATION/> rightly pointed out , however ,  Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . As <CITATION/> rightly pointed out, however, Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not. Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. 12.2.2 Disambiguation of Capitalized Words.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10,A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,1988,Kenneth Ward Church
1045,W00-1017,A00-2028,[0],introduction,The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> ., The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> . The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <CITATION/>.,143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,628176f850f7ba7bb0db10c9e458d80b1cd46766,Learning to Predict Problematic Situations in a Spoken Dialogue System: Experiments with How May I Help You?,2000,M. Walker; Irene Langkilde-Geary; Jeremy H. Wright; A. Gorin; D. Litman
1046,P07-1068,P06-1060,[4],introduction,"For instance , part of the ACE Phase 2 also adopted a corpusbased approach to SC deterevaluation involves classifying an NP as PERSON , mination that is investigated as part of the mention ORGANIZATION , GPE ( a geographicalpolitical redetection ( MD ) task <TARGET_CITATION/> .","In ACE, we are primarily concerned with classify2 Related Work ing an NP as belonging to one of the ACE semanMention detection. Many ACE participants have tic classes. For instance , part of the ACE Phase 2 also adopted a corpusbased approach to SC deterevaluation involves classifying an NP as PERSON , mination that is investigated as part of the mention ORGANIZATION , GPE ( a geographicalpolitical redetection ( MD ) task <TARGET_CITATION/> . For instance, part of the ACE Phase 2 also adopted a corpusbased approach to SC deterevaluation involves classifying an NP as PERSON, mination that is investigated as part of the mention ORGANIZATION, GPE (a geographicalpolitical redetection (MD) task (e.g., <CITATION/>). Many ACE participants have tic classes. In ACE, we are primarily concerned with classify2 Related Work ing an NP as belonging to one of the ACE semanMention detection.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,3221a2c32488439c61bcfad832c50917e1ef3bdf,Factorizing Complex Models: A Case Study in Mention Detection,2006,Radu Florian; Hongyan Jing; N. Kambhatla; I. Zitouni
1047,D11-1138,W06-1615,[0],introduction,"In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> . In most cases, the accuracy of parsers degrades when run on outofdomain data <CITATION/>. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,9fa8d73e572c3ca824a04a5f551b602a17831bc5,Domain Adaptation with Structural Correspondence Learning,2006,John Blitzer; Ryan T. McDonald; Fernando C Pereira
1048,K15-1002,W09-1119,[1],,"Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOUrepresentation as it has advantages over traditional BIOrepresentation , as shown , e.g. in <TARGET_CITATION/> .","The sequence labeling component builds on the following assumption: Assumption Different mentions have different heads, and heads do not overlap with each other. That is, for each mi,j, we have a corresponding head ha,b where i < a < b < j. Moreover, for another head ha',b', we have the satisfying condition a  b' > 0 or b  a' < 0 bha,b,ha',b'. Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOUrepresentation as it has advantages over traditional BIOrepresentation , as shown , e.g. in <TARGET_CITATION/> . Based on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the BILOUrepresentation as it has advantages over traditional BIOrepresentation, as shown, e.g. in <CITATION/>. That is, for each mi,j, we have a corresponding head ha,b where i < a < b < j. Moreover, for another head ha',b', we have the satisfying condition a  b' > 0 or b  a' < 0 bha,b,ha',b'. The sequence labeling component builds on the following assumption: Assumption Different mentions have different heads, and heads do not overlap with each other.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,aa9efc8b2737eac0675ba5abb5feab8305482c12,Design Challenges and Misconceptions in Named Entity Recognition,2009,Lev-Arie Ratinov; D. Roth
1049,W04-1610,External_52082,[1],experiments,"TF is given by TFD , t , and it denotes frequency of term t in document D. IDF is given by IDFt = log ( N/dft ) , where N is the number of documents in the collection , and dft is the number of documents containing the term t. <TARGET_CITATION/> proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .","While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents. The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. TF is given by TFD , t , and it denotes frequency of term t in document D. IDF is given by IDFt = log ( N/dft ) , where N is the number of documents in the collection , and dft is the number of documents containing the term t. <TARGET_CITATION/> proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance . TF is given by TFD,t, and it denotes frequency of term t in document D. IDF is given by IDFt = log(N/dft), where N is the number of documents in the collection, and dft is the number of documents containing the term t. <CITATION/> proposed the combination of TF and IDF as weighting schemes, and it has been shown that their product gave better performance. The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,on the specification of term values in automatic indexingquot,1973,G Salton; C S Yang
1050,W01-1510,External_7877,[0],introduction,"An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures <TARGET_CITATION/> ."," An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures <TARGET_CITATION/> . An HPSG grammar consists of lexical entries and ID grammar rules, each of which is described with typed feature structures <CITATION/>.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,48d6038cbf55280042a736fb9349a0b95ff66f28,The logic of typed feature structures,1992,Bob Carpenter
1051,J05-3003,External_1004,[0],method,"More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank <TARGET_CITATION/> , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .","For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%. There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size. More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank <TARGET_CITATION/> , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators . More recently, Burke, Cahill, et al. (2004a) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank <CITATION/>, a set of 700 randomly selected sentences from Section 23 which have been parsed, converted to dependency format, and manually corrected and extended by human validators. There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size. For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,47a4a47c3fba8b2e1d649d1353355011f5de62c3,The PARC 700 Dependency Bank,2003,Tracy Holloway King; Dick Crouch; S. Riezler; M. Dalrymple; R. Kaplan
1052,J09-4010,W00-0405,[0],method,"In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization <TARGET_CITATION/> .","This task can be cast as extractive multidocument summarization. Unlike a document reuse approach, sentencelevel approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization <TARGET_CITATION/> . In our work, we gather sets of sentences, and assume (but do not employ) existing approaches for their organization <CITATION/>. Unlike a document reuse approach, sentencelevel approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. This task can be cast as extractive multidocument summarization.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,a79c4801090e7b6c1f31b83559f2d2b4af4b9c68,Multi-Document Summarization By Sentence Extraction,2000,J. Goldstein; Vibhu Mittal; J. Carbonell; M. Kantrowitz
1053,W02-1601,P93-1004,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,0d8436f85424a450b15015610caf4d5fd4e4321b,Structural Matching of Parallel Texts,1993,Yuji Matsumoto; Hiroyuki Ishimoto; T. Utsuro
1054,J02-3002,External_62875,[4],,"This was done because purely unsupervised techniques ( e.g. , BaumWelch <TARGET_CITATION/> or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .","First, we used 20,000 tagged words to bootstrap'' the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , BaumWelch <TARGET_CITATION/> or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . This was done because purely unsupervised techniques (e.g., BaumWelch [Baum and Petrie 1966] or Brill's [Brill 1995b]) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. First, we used 20,000 tagged words to bootstrap'' the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,603bdbb17ba1f909280405a076455ac4f878fbf3,Statistical Inference for Probabilistic Functions of Finite State Markov Chains,1966,L. Baum; T. Petrie
1055,J97-4003,External_84380,[0],introduction,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the metalevel lexical rules ( MLRs ; <TARGET_CITATION/> ; Calcagno and Pollard 1995 ) and the .","While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the metalevel lexical rules ( MLRs ; <TARGET_CITATION/> ; Calcagno and Pollard 1995 ) and the . Two formalizations of lexical rules as used by HPSG linguists have been proposed, the metalevel lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the .While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,interpreting lexical rules,1995,Mike Calcagno
1056,D11-1138,External_681,[2],experiments,"In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) <TARGET_CITATION/> .","For some experiments we alsomeasure the standard intrinsic parser metrics unlabeled attachment score (UAS) and labeled attachment score (LAS) <CITATION/>. In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) <TARGET_CITATION/> . In terms of treebank data, the primary training corpus is the Penn Wall Street Journal Treebank (PTB) <CITATION/>. measure the standard intrinsic parser metrics unlabeled attachment score (UAS) and labeled attachment score (LAS) <CITATION/>. For some experiments we also",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,0b44fcbeea9415d400c5f5789d6b892b6f98daff,Building a Large Annotated Corpus of English: The Penn Treebank,1993,Mitchell P. Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz
1058,W04-1610,External_4161,[0],experiments,"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) <CITATION/> , minimum description length principal <TARGET_CITATION/> , and the X2 statistic .","Crossvalidation, using feature selectionFeature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) <CITATION/> , minimum description length principal <TARGET_CITATION/> , and the X2 statistic . Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) <CITATION/>, minimum description length principal <CITATION/>, and the X2 statistic. Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Crossvalidation, using feature selection",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,newsweeder learning to filter netnewsquot,1995,K Lang
1059,D08-1007,External_19410,[0],related work,"Usually , the classes are from WordNet <CITATION/> , although they can also be inferred from clustering <TARGET_CITATION/> .","Most approaches to SPs generalize from observed predicateargument pairs to semantically similar ones by modeling the semantic class of the argument, following <CITATION/>. For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually , the classes are from WordNet <CITATION/> , although they can also be inferred from clustering <TARGET_CITATION/> . Usually, the classes are from WordNet <CITATION/>, although they can also be inferred from clustering <CITATION/>. For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Most approaches to SPs generalize from observed predicateargument pairs to semantically similar ones by modeling the semantic class of the argument, following <CITATION/>.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,,inducing a semantically annotated lexicon via embased clustering,1999,M Rooth; S Riezler; D Prescher; G Carroll; F Beil
1060,J06-2002,External_40449,[0],experiments,"The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area <TARGET_CITATION/> .","8. Incrementality: Help or Hindrance? The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area <TARGET_CITATION/> . The account sketched in Section 4 was superimposed on an incremental GRE algorithm, partly because incrementality is well established in this area <CITATION/>. Incrementality: Help or Hindrance?8.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,a32c486987fb5df4d8dc9133180d51cee899478a,Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions,1995,R. Dale; Ehud Reiter
1061,D11-1138,P06-1043,[0],introduction,"In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> . In most cases, the accuracy of parsers degrades when run on outofdomain data <CITATION/>. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,91fb3e2b1ac9e588037e37e4d9be485e5fd60b27,Reranking and Self-Training for Parser Adaptation,2006,David McClosky; Eugene Charniak; Mark Johnson
1062,P02-1001,External_24069,[0],,"Now for some important remarks on efficiency :  Computing ti is an instance of the wellknown algebraic path problem ( <TARGET_CITATION/> ; Tar an , 1981a ) ."," Now for some important remarks on efficiency :  Computing ti is an instance of the wellknown algebraic path problem ( <TARGET_CITATION/> ; Tar an , 1981a ) . Now for some important remarks on efficiency:  Computing ti is an instance of the wellknown algebraic path problem (<CITATION/>; Tar an, 1981a).",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,2d88b3e79fab46395dcff9ba640a64953fe98c18,Algebraic Structures for Transitive Closure,1976,D. Lehmann
1063,D13-1115,External_23881,[0],related work,<TARGET_CITATION/> take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .,"<CITATION/> furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, <CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task. <TARGET_CITATION/> take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity . <CITATION/> take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. In a similar vein, <CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task. <CITATION/> furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,43f8e62d8e0f7599f6b928fea28cc1315234c8ad,Perceptual Inference Through Global Lexical Similarity,2012,Brendan T. Johns; Michael N. Jones
1064,P10-4003,External_33727,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <TARGET_CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <TARGET_CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,5757680951cda896fa7a03b3b7e6ef83bfd7403d,"Content-Learning Correlations in Spoken Tutoring Dialogs at Word, Turn, and Discourse Levels",2008,A. Purandare; D. Litman
1066,J00-4002,External_37743,[0],,"We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any <TARGET_CITATION/> , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity ' 6 Separate equivalences might also make it easier to encode determinerspecific preferences , such as that of each for wide scope .","(19) Every manager uses a computer. ex istsl ( e. pos( pres( u se(e,eve ry( m a na ger), a c.,(com put er)))) We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any <TARGET_CITATION/> , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity ' 6 Separate equivalences might also make it easier to encode determinerspecific preferences , such as that of each for wide scope . We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any <CITATION/>, which will resolve in different ways depending on its linguistic context, but here we avoid this complexity' 6 Separate equivalences might also make it easier to encode determinerspecific preferences, such as that of each for wide scope. ex istsl ( e. pos( pres( u se(e,eve ry( m a na ger), a c.,(com put er))))(19) Every manager uses a computer.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,334f124cbd015e0a37c9562094d9d8c195c41edc,Resolving Quasi Logical Forms,1990,H. Alshawi
1067,J06-2002,External_20228,[2],,"Following <TARGET_CITATION/> , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .","Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives.1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following <TARGET_CITATION/> , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole . Following <CITATION/>, such expressions will be called vague descriptions even though, as we shall see, the vagueness of the adjective does not extend to the description as a whole. This article focuses on gradable adjectives, also called degree adjectives.1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,a32f35b6464e7a68b8546d8cdf68dffb95793712,How to Refer with Vague Descriptions,1979,Manfred Pinkal
1068,P02-1001,External_220,[0],introduction,Such approaches have been tried recently in restricted cases <TARGET_CITATION/> .,"This allows meaningful parameter tying: if certain arcs such asu:i *, *, and a:ae o:e * share a contextual vowelfronting'' feature, then their weights rise and fall together with the strength of that feature. The resulting machine must be normalized, either perstate or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases <TARGET_CITATION/> . Such approaches have been tried recently in restricted cases <CITATION/>. The resulting machine must be normalized, either perstate or globally, to obtain a joint or a conditional distribution as desired. This allows meaningful parameter tying: if certain arcs such asu:i *, *, and a:ae o:e * share a contextual vowelfronting'' feature, then their weights rise and fall together with the strength of that feature.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,f4ba954b0412773d047dc41231c733de0c1f4926,Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,2001,J. Lafferty; A. McCallum; Fernando Pereira
1069,E03-1002,External_62,[4],,The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history <TARGET_CITATION/> .,"The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history <TARGET_CITATION/> . The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history <CITATION/>. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The main difficulty with this estimation is that the history d1,..., di_1 is of unbounded length.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,,headdriven statistical models for natural language parsing,1999,M Collins
1070,D13-1115,External_32478,[2],experiments,"Following <TARGET_CITATION/> , we measure association norm prediction as an average of percentile ranks .","Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following <TARGET_CITATION/> , we measure association norm prediction as an average of percentile ranks . Following <CITATION/>, we measure association norm prediction as an average of percentile ranks. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Additionally, we also evaluate using the Association Norms data set described in Section 3.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
1071,N04-2004,External_1172,[0],,There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <TARGET_CITATION/> ; Rappaport <CITATION/> ) .,"The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <TARGET_CITATION/> ; Rappaport <CITATION/> ) . There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structurerepresentations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity (<CITATION/>; Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>).",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,259d0304adcb49e40436137684b78a80c9ef097b,The Generative Lexicon,1991,J. Pustejovsky
1073,Q13-1020,D12-1078,[4],related work,<TARGET_CITATION/> retrained the linguistic parsers bilingually based on word alignment .,They utilized the bilingual Treebank to train a joint model for both parsing and word alignment. <CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. <TARGET_CITATION/> retrained the linguistic parsers bilingually based on word alignment . <CITATION/> retrained the linguistic parsers bilingually based on word alignment. <CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. They utilized the bilingual Treebank to train a joint model for both parsing and word alignment.,aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,ba80590f4cc0325e9b6e75de49c74414a15720ce,Re-training Monolingual Parser Bilingually for Syntactic SMT,2012,Shujie Liu; Chi-Ho Li; Mu Li; M. Zhou
1074,J97-4003,External_38715,[0],introduction,"de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <TARGET_CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements .","email: LCBdm,minnenRCB@sfs.nphil.unituebingen. de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <TARGET_CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements . de URL: http://www.sfs.nphil.unituebingen.de/sfb /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements. nphil.unituebingen.email: LCBdm,minnenRCB@sfs.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,flipped out aux in german,1989,Erhard Hinrichs; Tsuneko Nakazawa
1075,W02-1601,External_103,[0],,"Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures <TARGET_CITATION/> , such as the relation between syntax and semantic ."," Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures <TARGET_CITATION/> , such as the relation between syntax and semantic . Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences (translations) between layers of representation structures <CITATION/>, such as the relation between syntax and semantic.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,ac568f84563f4832e0bc50c37d1f203a178c8729,Synchronous Models of Language,1996,Owen Rambow; G. Satta
1076,W02-0309,External_56807,[0],introduction,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> .","When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> . From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexicosemantic aspects of dederivation and decomposition <CITATION/>. This is particularly true for the medical domain. When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,adb95aa594222953deb6a18097e21b23fd634339,Morphologic Analysis of Compound Words,1985,F. Wingert
1078,J06-2002,External_18164,[4],experiments,"A similar problem is discussed in the psycholinguistics of interpretation <TARGET_CITATION/> : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .","This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eyetracking experiments suggesting that speakers start speaking while still scanning distractors <CITATION/>. A similar problem is discussed in the psycholinguistics of interpretation <TARGET_CITATION/> : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . A similar problem is discussed in the psycholinguistics of interpretation <CITATION/>: Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known. This means that the linguistic realization cannot start until CD is concluded, contradicting eyetracking experiments suggesting that speakers start speaking while still scanning distractors <CITATION/>. This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,74f6152e2c68a5eac9c4d1d6bf3dd08dc73368ca,Achieving incremental semantic interpretation through contextual representation,1999,Julie C. Sedivy; M. Tanenhaus; C. Chambers; G. Carlson
1079,D13-1115,External_32478,[2],conclusion,"In this paper , we evaluated the role of lowlevel image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of <TARGET_CITATION/> ."," In this paper , we evaluated the role of lowlevel image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of <TARGET_CITATION/> . In this paper, we evaluated the role of lowlevel image features, SURF and GIST, for their compatibility with the multimodal Latent Dirichlet Allocation model of <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
1080,A00-1009,A97-1039,[3],,"Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent <CITATION/> , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework <TARGET_CITATION/> .","The DSyntSs and SSyntSs correspond closely to the equivalent structures of the MeaningText Theory (MTT; <CITATION/>): both structures are unordered syntactic representations, but a DSyntS only includes full meaningbearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions. In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguisticbased MT <CITATION/>. Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent <CITATION/> , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework <TARGET_CITATION/> . Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent <CITATION/>, represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework <CITATION/>. In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguisticbased MT <CITATION/>. The DSyntSs and SSyntSs correspond closely to the equivalent structures of the MeaningText Theory (MTT; <CITATION/>): both structures are unordered syntactic representations, but a DSyntS only includes full meaningbearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions.",6602edbc2f35e085dc4ee0361da214c4f14c5a07,A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing,2000,Benoit Lavoie; R. Kittredge; Tanya Korelsky; Owen Rambow,6e4fbe7b485045b61d468a2ebc4df81c0d5ea52d,A Fast and Portable Realizer for Text Generation Systems,1997,Benoit Lavoie; O. Rainbow
1081,E03-1005,External_63,[0],introduction,"And <TARGET_CITATION/> argues for  keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in <CITATION/> who use exactly the same set of ( all ) tree fragments as proposed in <CITATION/> .","While the models of <CITATION/> restricted the fragments to the locality of headwords, later models showed the importance of including context from higher nodes in the tree <CITATION/>. The importance of including nonheadwords has become uncontroversial <CITATION/>. And <TARGET_CITATION/> argues for  keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in <CITATION/> who use exactly the same set of ( all ) tree fragments as proposed in <CITATION/> . And <CITATION/> argues for keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in <CITATION/> who use exactly the same set of (all) tree fragments as proposed in <CITATION/>. The importance of including nonheadwords has become uncontroversial <CITATION/>. While the models of <CITATION/> restricted the fragments to the locality of headwords, later models showed the importance of including context from higher nodes in the tree <CITATION/>.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,844db702be4bc149b06b822b47247e15f5894cc3,Discriminative Reranking for Natural Language Parsing,2000,M. Collins; Terry Koo
1082,J03-3004,External_37705,[0],introduction,<TARGET_CITATION/> use a tagged parallel corpus to extract translationally equivalent EnglishGreek clauses on the basis of word occurrence and cooccurrence probabilities .,"<CITATION/> attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. <CITATION/> replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. <TARGET_CITATION/> use a tagged parallel corpus to extract translationally equivalent EnglishGreek clauses on the basis of word occurrence and cooccurrence probabilities . <CITATION/> use a tagged parallel corpus to extract translationally equivalent EnglishGreek clauses on the basis of word occurrence and cooccurrence probabilities. <CITATION/> replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. <CITATION/> attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,aligning clauses in parallel texts,1998,Sotiris Boutsis; Stelios Piperidis
1084,J97-4003,External_24317,[4],related work,"A common computational treatment of lexical rules adopted , for example , in the ALE system <TARGET_CITATION/> consists of computing the transitive closure of the base lexical entries under lexical rule application at compiletime ."," A common computational treatment of lexical rules adopted , for example , in the ALE system <TARGET_CITATION/> consists of computing the transitive closure of the base lexical entries under lexical rule application at compiletime . A common computational treatment of lexical rules adopted, for example, in the ALE system <CITATION/> consists of computing the transitive closure of the base lexical entries under lexical rule application at compiletime.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,a3179ab83aaf84c8147fc24a3357c595d10687fb,"ALE : the attribute logic engine user's guide, version 2.0.1",1992,Bob Carpenter; Gerald Penn
1085,J05-3003,External_13798,[2],related work,Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on <TARGET_CITATION/> ;,"Unlike our approach, those of <CITATION/> include a substantial initial correction and cleanup of the PennII trees. <CITATION/> describe a methodology for acquiring an English HPSG from the PennII Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on <TARGET_CITATION/> ; Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on <CITATION/> describe a methodology for acquiring an English HPSG from the PennII Treebank. Unlike our approach, those of <CITATION/> include a substantial initial correction and cleanup of the PennII trees.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce,Natural Language Parsing as Statistical Pattern Recognition,1994,David M. Magerman
1086,W06-2807,External_91607,[0],,Authors may choose this right with the NoDeriv option of the Creative Commons licences <TARGET_CITATION/> .,"In the second one, the last user, who has edited the lexia, may claim the attribution for himself. The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). Authors may choose this right with the NoDeriv option of the Creative Commons licences <TARGET_CITATION/> . Authors may choose this right with the NoDeriv option of the Creative Commons licences <CITATION/>.The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2). In the second one, the last user, who has edited the lexia, may claim the attribution for himself.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,cc944b0a929d4ca6ce8a47fc84cadae38cfe8c27,Free Culture: How Big Media Uses Technology and the Law to Lock Down Culture and Control Creativity,2004,Lawrence Lessig
1087,W06-1104,External_928,[0],,"Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric <TARGET_CITATION/> .4 However , mathematical analysis can not tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application .","The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to <CITATION/>, there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric <TARGET_CITATION/> .4 However , mathematical analysis can not tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application . Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric <CITATION/>.4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. According to <CITATION/>, there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,an informationtheoretic definition of similarity in,1998,Dekang Lin
1088,W11-2020,W09-3918,[3],,"The same annotation scheme as in our previous work on anger detection has been applied , see e.g. <TARGET_CITATION/> .","To create an upper baseline of our model we further introduce the negative emotional state of the user that is manually annotated by a human rater who chooses one of the labels garbage, nonangry, slightly angry, very angry for each single user turn: Emo EMOTIONALSTATE: emotional state of the caller in the current exchange. One of garbage, nonangry, slightly angry, very angry. The same annotation scheme as in our previous work on anger detection has been applied , see e.g. <TARGET_CITATION/> . The same annotation scheme as in our previous work on anger detection has been applied, see e.g. <CITATION/>. One of garbage, nonangry, slightly angry, very angry. To create an upper baseline of our model we further introduce the negative emotional state of the user that is manually annotated by a human rater who chooses one of the labels garbage, nonangry, slightly angry, very angry for each single user turn: Emo EMOTIONALSTATE: emotional state of the caller in the current exchange.",2fcbc316d58a725f8dac2b58e426d1f2279568da,Modeling and Predicting Quality in Spoken Human-Computer Interaction,2011,Alexander Schmitt; Benjamin Schatz; W. Minker,dfd957b24ba5526d7376d735d699d56089c01540,"On NoMatchs, NoInputs and BargeIns: Do Non-Acoustic Features Support Anger Detection?",2009,Alexander Schmitt; Tobias Heinroth; J. Liscombe
1089,D13-1115,External_9241,[2],experiments,We also compute GIST vectors <TARGET_CITATION/> for every image using LearGIST <CITATION/> .,"Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities.3http://simplecv.org We also compute GIST vectors <TARGET_CITATION/> for every image using LearGIST <CITATION/> . We also compute GIST vectors <CITATION/> for every image using LearGIST <CITATION/>. 3http://simplecv.orgIdeally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,869171b2f56cfeaa9b81b2626cb4956fea590a57,Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope,2001,A. Oliva; A. Torralba
1090,J05-3003,External_234,[0],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] <TARGET_CITATION/> ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] <TARGET_CITATION/> ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,046f24b1ba2ee394cdcb8f85d7d98a4740c2adf4,On the order of words,1982,A. Ades; Mark Steedman
1091,P10-4003,P10-2009,[5],conclusion,"The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed <TARGET_CITATION/> .","Such annotation can provide useful input for statistical learning algorithms to detect and recover from misunderstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed <TARGET_CITATION/> . The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed <CITATION/>. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. Such annotation can provide useful input for statistical learning algorithms to detect and recover from misunderstandings.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,6af61cc4a435ff0f24761e68149323c31dc2c2c3,The Impact of Interpretation Problems on Tutorial Dialogue,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell
1092,J97-4003,External_25992,[0],introduction,A logic that provides the formal architecture required by <CITATION/> was defined by <TARGET_CITATION/> ) .,"The signature consists of the type hierarchy defining which types of objects exist and the appropriateness conditions specifying which objects have which features defined on them to represent their properties.' A signature is interpreted as follows: Every object is assigned exactly one most specific type, and in case a feature is appropriate for some object of a certain type, then it is appropriate for all objects of this type.' A logic that provides the formal architecture required by <CITATION/> was defined by <TARGET_CITATION/> ) . A logic that provides the formal architecture required by <CITATION/> was defined by King (1989, 1994). A signature is interpreted as follows: Every object is assigned exactly one most specific type, and in case a feature is appropriate for some object of a certain type, then it is appropriate for all objects of this type.' The signature consists of the type hierarchy defining which types of objects exist and the appropriateness conditions specifying which objects have which features defined on them to represent their properties.'",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,a logical formalism for headdriven phrase structure grammar,1989,Paul King
1093,W06-3309,External_11028,[0],introduction,"fields generally follow the pattern of  introduction '' ,  methods '' ,  results '' , and  conclusions '' <TARGET_CITATION/> .","Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications. As an example, scientific abstracts across many different fields generally follow the pattern of  introduction '' ,  methods '' ,  results '' , and  conclusions '' <TARGET_CITATION/> . fields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. As an example, scientific abstracts across many differentCertain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,,genre analysis english inacademic and research settings,1990,John M Swales
1095,J97-4003,External_5529,[0],introduction,"The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques <CITATION/> .29 The unfolding transformation is also referred to as partial execution , for example , by <TARGET_CITATION/> .","For each interaction definition we can therefore check which of the frame clauses are applicable and discard the nonapplicable ones. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques <CITATION/> .29 The unfolding transformation is also referred to as partial execution , for example , by <TARGET_CITATION/> . The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques <CITATION/>.29 The unfolding transformation is also referred to as partial execution, for example, by <CITATION/>. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. For each interaction definition we can therefore check which of the frame clauses are applicable and discard the nonapplicable ones.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,777570c140f6f995385f4d5a3b6bd902dfab3f5d,Review of Prolog and natural-language analysis: CSLI lecture notes 10 by Fernando C. N. Pereira and Stuart M. Shieber. Center for the Study of Language and Information 1987.,1988,P. Saint-Dizier
1096,P10-4003,External_11756,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,learning to assess lowlevel conceptual understanding,2008,Rodney D Nielsen; Wayne Ward; James H Martin
1098,P10-2019,External_69697,[1],,"Inspired by <TARGET_CITATION/> , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent .","The column CHUNK 1 illustrates this kind of chunk type definition. The second is more complicated. Inspired by <TARGET_CITATION/> , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent . Inspired by <CITATION/>, we split one phrase type into several subsymbols, which contain category information of current constituent's parent. The second is more complicated. The column CHUNK 1 illustrates this kind of chunk type definition.",5e6c93540af4d9e203e59858aeb343445f0fafd1,Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling,2010,Weiwei SUN,a600850ac0120cb09a0b7de7da80bb6a7a76de06,Accurate Unlexicalized Parsing,2003,D. Klein; Christopher D. Manning
1099,W00-1017,External_21175,[0],introduction,The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> ., The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> . The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <CITATION/>.,143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,,jupiter a telephonebased conversational interface for weather information,2000,Victor Zue; Stephanie Seneff; James Glass; Joseph Polifroni; Christine Pao; Timothy J Hazen; Lee Hetherington
1100,J04-3001,P98-1091,[2],,"Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a contextfree variant of treeadjoining grammars <CITATION/> , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism <TARGET_CITATION/> , and Collins 's Model 2 parser ( 1997 ) .","Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a contextfree variant of treeadjoining grammars <CITATION/> , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism <TARGET_CITATION/> , and Collins 's Model 2 parser ( 1997 ) . Moreover, in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain, we have performed the study on two parsing models: one based on a contextfree variant of treeadjoining grammars <CITATION/>, the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism <CITATION/>, and Collins's Model 2 parser (1997). In this section, we investigate whether these observations hold true for training statistical parsing models as well. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,4535a8309c1a84ad885865200de13eaae3ebe754,An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion Grammars,1998,R. Hwa
1101,J05-3003,External_61279,[0],method,"Most of the early work on automatic fstructure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; <TARGET_CITATION/> ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .","The first step in the application of our methodology is the production of a treebank annotated with LFG fstructure information. Fstructures are attributevalue structures which represent abstract syntactic information, approximating to basic predicateargumentmodifier structures. Most of the early work on automatic fstructure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; <TARGET_CITATION/> ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept . Most of the early work on automatic fstructure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept. Fstructures are attributevalue structures which represent abstract syntactic information, approximating to basic predicateargumentmodifier structures. The first step in the application of our methodology is the production of a treebank annotated with LFG fstructure information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,automatic fstructure annotation of treebank trees,2000,Anette Frank
1103,J05-3003,External_12481,[0],related work,Recent work by <TARGET_CITATION/> on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate backoff estimates for hypothesis selection .,The frames incorporate control information and details of specific prepositions. <CITATION/> refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by <TARGET_CITATION/> on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate backoff estimates for hypothesis selection . Recent work by <CITATION/> on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate backoff estimates for hypothesis selection. <CITATION/> refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. The frames incorporate control information and details of specific prepositions.,ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,899c4c76cc96e16dfb173d0b9af8c9219fe995ac,Subcategorization Acquisition as an Evaluation Method for WSD,2002,Judita Preiss; A. Korhonen; Ted Briscoe
1104,D08-1007,External_19410,[4],experiments,Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times <TARGET_CITATION/> .,"Passive subjects (the car was bought) were converted to objects (bought car). We set the MIthreshold, T, to be 0, and the negativetopositive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times <TARGET_CITATION/> . Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times <CITATION/>. We set the MIthreshold, T, to be 0, and the negativetopositive ratio, K, to be 2. Passive subjects (the car was bought) were converted to objects (bought car).",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,,inducing a semantically annotated lexicon via embased clustering,1999,M Rooth; S Riezler; D Prescher; G Carroll; F Beil
1105,D13-1115,D12-1137,[0],introduction,"Some efforts have tackled tasks such as automatic image caption generation <CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <TARGET_CITATION/> .","Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/> , text illustration <CITATION/> , or automatic location identification of Twitter users <TARGET_CITATION/> . Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,6390ae400a61ff367e4f4778a1cc7bea3e24561a,Supervised Text-based Geolocation Using Language Models on an Adaptive Grid,2012,Stephen Roller; Michael Speriosu; Sarat Rallapalli; Benjamin Wing; Jason Baldridge
1106,W06-1639,External_12284,[5],method,"Default parameters were used , although experimentation with different parameter settings is an important direction for future work <TARGET_CITATION/> .","In our experiments, we employed the wellknown classifier SVMght to obtain individualdocument classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis <CITATION/>, the input to SVMght consisted of normalized presenceoffeature (rather than frequencyoffeature) vectors. The ind value 5SVMlight is available at svmlight.joachims.org. Default parameters were used , although experimentation with different parameter settings is an important direction for future work <TARGET_CITATION/> . Default parameters were used, although experimentation with different parameter settings is an important direction for future work <CITATION/>. The ind value 5SVMlight is available at svmlight.joachims.org. In our experiments, we employed the wellknown classifier SVMght to obtain individualdocument classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis <CITATION/>, the input to SVMght consisted of normalized presenceoffeature (rather than frequencyoffeature) vectors.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,fdaf595804efb3b33e05611b1255c4b1b136f6fa,Evaluation of Machine Learning Methods for Natural Language Processing Tasks,2002,Walter Daelemans; Veronique Hoste
1107,W04-0910,External_6133,[4],,The language chosen for semantic representation is a flat semantics along the line of <TARGET_CITATION/> .,"The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The language chosen for semantic representation is a flat semantics along the line of <TARGET_CITATION/> . The language chosen for semantic representation is a flat semantics along the line of <CITATION/>. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,e867a965033a074e4074875e0916ce1ca42f3bf6,Minimal Recursion Semantics: An Introduction,2005,Ann A. Copestake; D. Flickinger; C. Pollard; I. Sag
1108,P10-2059,External_88753,[0],introduction,"For example , <TARGET_CITATION/> et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels .","Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see <CITATION/> for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example , <TARGET_CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements , while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels . For example, <CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements, while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels. Others have looked at the application of machine learning algorithms to annotated multimodal corpora. Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see <CITATION/> for an overview).",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,clustering experiments on the communicative prop erties of gaze and gestures,2007,Kristiina Jokinen; Anton Ragni
1110,P00-1007,External_29000,[5],conclusion,"In a similar vain to <TARGET_CITATION/> , the method extends an existing flat shallowparsing method to handle composite structures .","In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information. An additional possibility is to use wordtypes, such as a special tag for beverbs, or for prepositions like of' which attaches mainly to nouns <CITATION/>. In a similar vain to <TARGET_CITATION/> , the method extends an existing flat shallowparsing method to handle composite structures . In a similar vain to <CITATION/>, the method extends an existing flat shallowparsing method to handle composite structures. An additional possibility is to use wordtypes, such as a special tag for beverbs, or for prepositions like of' which attaches mainly to nouns <CITATION/>. In a future work, we plan to use the system for providing instance candidates, and disambiguate them using an algorithm more suitable for handling lexical information.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,5db4b1405a1e77311a4a78414fe7eedc7712c4ed,Cascaded Grammatical Relation Assignment,1999,S. Buchholz; Jorn Veenstra; Walter Daelemans
1111,W06-3309,N04-1015,[4],conclusion,"An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in <TARGET_CITATION/> ."," An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in <TARGET_CITATION/> . An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in <CITATION/>.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,984efc8932edb635d09ec1a5fd8fc1d1ceccad45,"Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",2004,R. Barzilay; Lillian Lee
1112,J03-3004,External_28667,[0],introduction," language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; <TARGET_CITATION/> ; Gough , Way , and Hearne 2002 )","The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different languagerelated tasks, including language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; <TARGET_CITATION/> ; Gough , Way , and Hearne 2002 )  language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002)The marker hypothesis has been used for a number of different languagerelated tasks, includingThe marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,gaijin a bootstrapping templatedriven approach to examplebased machine translation,1997,Tony Veale; Andy Way
1113,J05-3003,External_1147,[0],,"In <TARGET_CITATION/> , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .","Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%). The rate of accession may also be represented graphically. In <TARGET_CITATION/> , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . In <CITATION/>, it was observed that treebank grammars (CFGs extracted from treebanks) are very large and grow with the size of the treebank. The rate of accession may also be represented graphically. Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,938ca87821c6ec6fcb263179aaa71824aefa9b5a,Compacting the Penn Treebank Grammar,1998,Alexander Krotov; Mark Hepple; R. Gaizauskas; Y. Wilks
1114,W01-1510,External_60,[2],introduction,"We apply our system to the latest version of the XTAG English grammar ( The XTAG Research <TARGET_CITATION/> ) , which is a largescale FBLTAG grammar .","Investigating the relation will be apparently valuable for both communities. In this paper, we show that the strongly equivalent grammars enable the sharing of parsing techniques'', which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar ( The XTAG Research <TARGET_CITATION/> ) , which is a largescale FBLTAG grammar . We apply our system to the latest version of the XTAG English grammar (The XTAG Research <CITATION/>), which is a largescale FBLTAG grammar. In this paper, we show that the strongly equivalent grammars enable the sharing of parsing techniques'', which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. Investigating the relation will be apparently valuable for both communities.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,6fff864e0622d6dd84122a02c6bf990f0a390f8c,A Lexicalized Tree Adjoining Grammar for English,1990,Abeillé; Kathleen Bishop; Sharon Cote; Yves Schabes
1116,W06-2933,External_20154,[0],experiments,"Typical examples are Bulgarian <CITATION/> , Chinese <CITATION/> , Danish <CITATION/> , and Swedish <TARGET_CITATION/> .","before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. Typical examples are Bulgarian <CITATION/> , Chinese <CITATION/> , Danish <CITATION/> , and Swedish <TARGET_CITATION/> . Typical examples are Bulgarian <CITATION/>, Chinese <CITATION/>, Danish <CITATION/>, and Swedish <CITATION/>. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,a565b0790a32c1f1e7d9fa7527ec4faa94e608f3,MAMBA Meets TIGER: Reconstructing a Swedish Treebank from Antiquity,2005,Jens Nilsson; Johan Hall; Joakim Nivre
1117,P10-4003,External_22829,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <TARGET_CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <TARGET_CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,generalizing tutorial dialogue results,2009,Diane Litman; Johanna Moore; Myroslava Dzikovska; Elaine Farrow
1118,J06-2002,External_24084,[0],introduction,"Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX <TARGET_CITATION/> Chapter 8 ) .","If computing the intersection of two sets takes constant time then this makes the complexity of interpreting nonvague descriptions linear: O(nd), where nd is the number of properties used. In a vague description, the property last added to the description is context dependent. Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX <TARGET_CITATION/> Chapter 8 ) . Worst case, calculating the set corresponding with such a property, of the form size(x) = maxm, for example, involves sorting the distractors as to their size, which may amount to O(n2d) or O(nd log nd) calculations (depending on the sorting algorithm: cfXXX [Aho et al. 1983] Chapter 8). In a vague description, the property last added to the description is context dependent. If computing the intersection of two sets takes constant time then this makes the complexity of interpreting nonvague descriptions linear: O(nd), where nd is the number of properties used.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,eeb202523ad89569d4d36414a52d4f5052fa3d4b,DATA STRUCTURES AND ALGORITHMS LABORATORY,2023,M. P. Kumar
1119,J03-3004,External_59887,[0],introduction," language learning <TARGET_CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different languagerelated tasks, including language learning <TARGET_CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )  language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002)The marker hypothesis has been used for a number of different languagerelated tasks, includingThe marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,1824dd0d62577c0dff87cdc4b69eb07c22249885,The necessity of syntax markers: Two experiments with artificial languages,1979,T. Green
1121,W04-0910,H91-1060,[0],,"While corpus driven efforts along the PARSEVAL lines <TARGET_CITATION/> are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .","Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?) While corpus driven efforts along the PARSEVAL lines <TARGET_CITATION/> are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . While corpus driven efforts along the PARSEVAL lines <CITATION/> are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. and its degree of overgeneration (does it generate only the sentences of the described language?) Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,7689778171dc100bb636fc0e4e2ce4063967d3c9,A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars,1991,Ezra Black; Steven P. Abney; D. Flickenger; Claudia Gdaniec; R. Grishman; P. Harrison; Donald Hindle; Robert Ingria; F. Jelinek; Judith L. Klavans; M. Liberman; Mitchell P. Marcus; S. Roukos; Beatrice Santorini; T. Strzalkowski
1122,J02-3002,External_3272,[2],experiments,"Before using the DCA method , we applied a Russian morphological processor <TARGET_CITATION/> to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .","We automatically created the supporting resources from 364,000 documents from the Russian corpus of the European Corpus Initiative, using the method described in section 5. Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue. Before using the DCA method , we applied a Russian morphological processor <TARGET_CITATION/> to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. . Before using the DCA method, we applied a Russian morphological processor <CITATION/> to convert each word in the text to its main form: nominative case singular for nouns and adjectives, infinitive for verbs, etc.. Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue. We automatically created the supporting resources from 364,000 documents from the Russian corpus of the European Corpus Initiative, using the method described in section 5.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,79278f599adf7d9ca2c63562ee2ee7411a44badf,Russian morphology: An engineering approach,1995,Andrei Mikheev; Liubov Liubushkina
1124,W06-1639,External_14152,[0],related work,"Also relevant is work on the general problems of dialogact tagging <CITATION/> , citation analysis <TARGET_CITATION/> , and computational rhetorical analysis <CITATION/> .","Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Also relevant is work on the general problems of dialogact tagging <CITATION/> , citation analysis <TARGET_CITATION/> , and computational rhetorical analysis <CITATION/> . Also relevant is work on the general problems of dialogact tagging <CITATION/>, citation analysis <CITATION/>, and computational rhetorical analysis <CITATION/>. More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,analyzing research papers using citation sentences,1990,W Lehnert; C Cardie; E Riloff
1125,W04-0910,External_4805,[4],,The language chosen for semantic representation is a flat semantics along the line of <TARGET_CITATION/> .,"The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The language chosen for semantic representation is a flat semantics along the line of <TARGET_CITATION/> . The language chosen for semantic representation is a flat semantics along the line of <CITATION/>. In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place. The top feature structure encodes information that needs to be percolated up the tree should an adjunction take place.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,03b35a338823efff6a1a5211ac5ac1119dfe28b0,Predicate logic unplugged,1996,Johan Bos
1126,W06-1705,External_46140,[0],related work,"In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet <TARGET_CITATION/> .","In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for dataintensive NLP and textmining for eScience <CITATION/>. While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet <TARGET_CITATION/> . In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet <CITATION/>. While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach. In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for dataintensive NLP and textmining for eScience <CITATION/>.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,listening to napster in peertopeer harnessing the power of disruptive technologies,2001,C Shirky
1127,J09-4010,External_60687,[4],,A common way to combine different models consists of selecting the model that is most confident regarding its decision <TARGET_CITATION/> .,"However, as shown by our results, these thresholds were sometimes suboptimal. In this section, we describe a metalevel process which can automatically select a responsegeneration method to address a new request without using such thresholds. A common way to combine different models consists of selecting the model that is most confident regarding its decision <TARGET_CITATION/> . A common way to combine different models consists of selecting the model that is most confident regarding its decision <CITATION/>. In this section, we describe a metalevel process which can automatically select a responsegeneration method to address a new request without using such thresholds. However, as shown by our results, these thresholds were sometimes suboptimal.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,hybrid recommender systems user modeling and useradapted interaction,2002,R Burke
1128,J06-2002,External_1368,[0],experiments,Common sense ( as well as the Gricean maxims ; <TARGET_CITATION/> ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication ., Common sense ( as well as the Gricean maxims ; <TARGET_CITATION/> ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication . Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.,0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,b25e5bca74d74abb1687315fa3c637bb9911554d,Logic and Conversation,2005,Siobhan Chapman
1129,J97-4003,C90-3052,[4],related work,A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <TARGET_CITATION/> .,"Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <TARGET_CITATION/> . A similar method is included in PATRII <CITATION/> and can be used to encode lexical rules as binary relations in the CUF system <CITATION/> or the TFS system <CITATION/>. This approach is taken, for example, in LKB <CITATION/> where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). Another common approach to lexical rules is to encode them as unary phrase structure rules.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,cd41174a2d3ee305f536c471bed6910b041b6ee0,Typed Unification Grammars,1990,Martin C. Emele; Rémi Zajac
1130,W10-4215,P01-1008,[5],conclusion,For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in <TARGET_CITATION/> .,"In future work, we will focus on mapping text (in monologue form) to dialogue. For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form. For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in <TARGET_CITATION/> . For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in <CITATION/>. For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form. In future work, we will focus on mapping text (in monologue form) to dialogue.",4fc6a6aa1748469f9e76e674acbe6926349e45a6,Harvesting Re-usable High-level Rules for Expository Dialogue Generation,2010,Svetlana Stoyanchev; P. Piwek,8d7a692d8763a38283db54e19f1dcc694a34e706,Extracting Paraphrases from a Parallel Corpus,2001,R. Barzilay; K. McKeown
1131,J97-4003,External_25992,[0],introduction,This description can then be given the standard settheoretical interpretation of <TARGET_CITATION/> ) . ',"One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory. The formalization of DLRs provided by <CITATION/> defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. This description can then be given the standard settheoretical interpretation of <TARGET_CITATION/> ) . ' This description can then be given the standard settheoretical interpretation of King (1989, 1994).'The formalization of DLRs provided by <CITATION/> defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1. One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,a logical formalism for headdriven phrase structure grammar,1989,Paul King
1132,D13-1115,External_6800,[0],related work,"<TARGET_CITATION/> helped pave the path for cognitivelinguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms .","1http://stephenroller.com/research/ emnlp13cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). <TARGET_CITATION/> helped pave the path for cognitivelinguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms . <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). 1http://stephenroller.com/research/ emnlp13",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,0f3ab6835042ea45d2aab8e4a70151c11ca9a1d6,Topics in Semantic Representation,2007,T. Griffiths; M. Steyvers; J. Tenenbaum; L. Griffiths
1133,J06-2002,External_9768,[0],,"A more flexible approach is used by <TARGET_CITATION/> , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .","The FOG weatherforecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by <TARGET_CITATION/> , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A more flexible approach is used by <CITATION/>, where users can specify boundary values for attributes like rainfall, specifying, for example, rain counts as moderate above 7 mm/h, as heavy above 20 mm/h, and so on. FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. The FOG weatherforecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,6e862065b597f791b66a8861bc625bce7a8240d9,Should Corpora Texts Be Gold Standards for NLG?,2002,Ehud Reiter; S. Sripada
1134,E03-1002,External_63,[0],,"<CITATION/> define a kernel over parse trees and apply it to reranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on reranking using a finite set of features <TARGET_CITATION/> .","We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.feature sets, but then efficiency becomes a problem. <CITATION/> define a kernel over parse trees and apply it to reranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on reranking using a finite set of features <TARGET_CITATION/> . <CITATION/> define a kernel over parse trees and apply it to reranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on reranking using a finite set of features <CITATION/>. feature sets, but then efficiency becomes a problem. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,844db702be4bc149b06b822b47247e15f5894cc3,Discriminative Reranking for Natural Language Parsing,2000,M. Collins; Terry Koo
1136,K15-1001,W02-1001,[4],,"In practice , perceptrontype algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set <TARGET_CITATION/> .","The generalization bound tells us how far the expected average regret E[REGT] (or average risk, in terms of <CITATION/>) is from the average regret that we actually observe in a specific instantiation of the algorithm. Generalization for OnlinetoBatch Conversion. In practice , perceptrontype algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set <TARGET_CITATION/> . In practice, perceptrontype algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set <CITATION/>. Generalization for OnlinetoBatch Conversion. The generalization bound tells us how far the expected average regret E[REGT] (or average risk, in terms of <CITATION/>) is from the average regret that we actually observe in a specific instantiation of the algorithm.",571e0df1b33cf9e232f9c96428618b530fdd6be1,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,2015,Artem Sokolov; S. Riezler; Shay B. Cohen,5a7958b418bceb48a315384568091ab1898b1640,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,2002,M. Collins
1138,D08-1006,P07-1010,[1],,"Unfortunately , as shown in <TARGET_CITATION/> , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a nonlinear largemargin classifier ( see section 3 for details ) .","AEn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in <TARGET_CITATION/> , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a nonlinear largemargin classifier ( see section 3 for details ) . Unfortunately, as shown in <CITATION/>, with the represetation of sentences that we use, linear classifiers cannot discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a nonlinear largemargin classifier (see section 3 for details). As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. AEn both cases essentially linear classifiers were used as features.",00dc508fdf5dcbf78bee0ea779aad408830c20e2,Refining Generative Language Models using Discriminative Learning,2008,Ben Sandbank,060d5bb21f318efd785e86aa50cb97aed090a0fb,A discriminative language model with pseudo-negative samples,2007,Daisuke Okanohara; Junichi Tsujii
1140,J04-3001,W01-0501,[0],related work,"<TARGET_CITATION/> have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for largescale training .","Another technique for making better use of unlabeled data is cotraining <CITATION/>, in which two sufficiently different learners help each other learn by labeling training data for one another. The work of <CITATION/> and Steedman, Osborne, et al. (2003) suggests that cotraining can be helpful for statistical parsing. <TARGET_CITATION/> have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for largescale training . <CITATION/> have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for largescale training. The work of <CITATION/> and Steedman, Osborne, et al. (2003) suggests that cotraining can be helpful for statistical parsing. Another technique for making better use of unlabeled data is cotraining <CITATION/>, in which two sufficiently different learners help each other learn by labeling training data for one another.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,df3520f52fcf42b4f10ed4b35b3b3f9cd050f290,Limitations of Co-Training for Natural Language Learning from Large Datasets,2001,D. Pierce; Claire Cardie
1141,N01-1006,P00-1036,[4],experiments," The regular TBL , as described in section 2 ;  An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ;  The FastTBL algorithm ;  The ICA algorithm <TARGET_CITATION/> .","A more detailed description of each task, data and the system parameters are presented in the following subsections. Four algorithms are compared during the following experiments: The regular TBL , as described in section 2 ;  An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ;  The FastTBL algorithm ;  The ICA algorithm <TARGET_CITATION/> .  The regular TBL, as described in section 2;  An improved version of TBL, which makes extensive use of indexes to speed up the rules' update;  The FastTBL algorithm;  The ICA algorithm <CITATION/>.Four algorithms are compared during the following experiments:A more detailed description of each task, data and the system parameters are presented in the following subsections.",c52f80f056a2de8f503bf912e8025413ec2111ec,Transformation Based Learning in the Fast Lane,2001,G. Ngai; Radu Florian,4a27822c8718bcfdd01fd5cc9e75dd1df9f9add5,Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers,2000,Mark Hepple
1142,P10-2059,External_88751,[0],introduction,"For example , <TARGET_CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements , while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels .","Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see <CITATION/> for an overview). Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example , <TARGET_CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements , while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels . For example, <CITATION/> find that machine learning algorithms can be trained to recognise some of the functions of head movements, while <CITATION/> show that there is a dependence between focus of attention and assignment of dialogue act labels. Others have looked at the application of machine learning algorithms to annotated multimodal corpora. Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see <CITATION/> for an overview).",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,c276dbceed4c261ce3ff1b7865fd72d106ad1b38,Distinguishing the Communicative Functions of Gestures,2008,Kristiina Jokinen; Costanza Navarretta; Patrizia Paggio
1143,J92-1004,J86-3002,[4],,This approach resembles the work by <TARGET_CITATION/> et al. ( 1975 ) on selectional restrictions .,"In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by <TARGET_CITATION/> on selectional restrictions . This approach resembles the work by <CITATION/> on selectional restrictions. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,819c80bc13dc40da5d0dd25f496c0cfa7dfcf832,Discovery Procedures for Sublanguage Selectional Patterns: Initial Experiments,1986,R. Grishman; L. Hirschman; N. Nhan
1144,J92-1004,External_1193,[0],,"Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an Ngram backoff model <TARGET_CITATION/> .","This is a problem to be aware of in building grammars from example sentences. In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out. Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an Ngram backoff model <TARGET_CITATION/> . Unless it is desired to intentionally filter these out as being outside of the new domain, one can insert some arbitrarily small probability for these arcs, using, for example, an Ngram backoff model <CITATION/>.In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out. This is a problem to be aware of in building grammars from example sentences.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,b0130277677e5b915d5cd86b3afafd77fd08eb2e,Estimation of probabilities from sparse data for the language model component of a speech recognizer,1987,Slava M. Katz
1145,P10-2059,External_1195,[0],introduction,"feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Intercoder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see <CITATION/> , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent <TARGET_CITATION/> .","We used slices of 0.04 seconds. The intercoder agreement figures obtained for the three types of annotation are given in Table 2. feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Intercoder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see <CITATION/> , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent <TARGET_CITATION/> . feature Cohen's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2: Intercoder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see <CITATION/>, it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent <CITATION/>. The intercoder agreement figures obtained for the three types of annotation are given in Table 2. We used slices of 0.04 seconds.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,cfb4592221080deb127de94e8063fb403b13a298,Measuring nominal scale agreement among many raters.,1971,J. Fleiss
1146,D13-1115,External_7269,[0],related work,"To name a few examples , <TARGET_CITATION/> show how semantic information from text can be used to improve zeroshot classification ( i.e. , classifying neverbeforeseen objects ) , and <CITATION/> show that verb clusters can be used to improve activity recognition in videos .","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>. The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , <TARGET_CITATION/> show how semantic information from text can be used to improve zeroshot classification ( i.e. , classifying neverbeforeseen objects ) , and <CITATION/> show that verb clusters can be used to improve activity recognition in videos . To name a few examples, <CITATION/> show how semantic information from text can be used to improve zeroshot classification (i.e., classifying neverbeforeseen objects), and <CITATION/> show that verb clusters can be used to improve activity recognition in videos.The Computer Vision community has also benefited greatly from efforts to unify the two modalities. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,zeroshot learning through crossmodal transfer,2013,Richard Socher; Milind Ganjoo; Hamsa Sridhar; Osbert Bastani; Christopher D Manning; Andrew Y Ng
1147,W02-0309,External_44559,[0],conclusion,Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories <TARGET_CITATION/> .,"There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>. The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories <TARGET_CITATION/> . Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories <CITATION/>. The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,acddd8959a22b55050de23b0810428b80cf100c5,Viewing stemming as recall enhancement,1996,Wessel Kraaij; Renée Pohlmann
1148,W01-1510,External_329,[0],introduction,"The XTAG group <TARGET_CITATION/> at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .","A derivation tree is a structural description in LTAG and represents the history of combinations of elementary trees. There are several grammars developed in the FBLTAG formalism, including the XTAG English grammar, a largescale grammar for English (The XTAG Research <CITATION/>). The XTAG group <TARGET_CITATION/> at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . The XTAG group <CITATION/> at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. There are several grammars developed in the FBLTAG formalism, including the XTAG English grammar, a largescale grammar for English (The XTAG Research <CITATION/>). A derivation tree is a structural description in LTAG and represents the history of combinations of elementary trees.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,0aedc08cce73ec1f05e229338db47a2866f8bd59,Evolution of the XTAG System,2000,Christine Doran; Beth Ann Hockey; Anoop Sarkar; B. Srinivas; Fei Xia
1149,D08-1009,W07-1431,[4],introduction,"HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by <TARGET_CITATION/> ."," HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by <TARGET_CITATION/> . HOLMES is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by <CITATION/>.",cf3ba53a5030b8dd6ec65101b6f5a9b8e4d06f80,Scaling Textual Inference to the Web,2008,Stefan Schoenmackers; Oren Etzioni; Daniel S. Weld,4639cb3718e4eb698a0285548ed2bf23ad9908a9,Natural Logic for Textual Inference,2007,Bill MacCartney; Christopher D. Manning
1150,J00-2009,External_34919,[0],,"The lexicon is used to mediate and map between a languageindependent domain model and a languagedependent ontology widely used in NLG , the Upper Model <TARGET_CITATION/> .","However, at a second, morecareful, reading, everything falls into place. The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993). The lexicon is used to mediate and map between a languageindependent domain model and a languagedependent ontology widely used in NLG , the Upper Model <TARGET_CITATION/> . The lexicon is used to mediate and map between a languageindependent domain model and a languagedependent ontology widely used in NLG, the Upper Model <CITATION/>. The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993). However, at a second, morecareful, reading, everything falls into place.",df826f3d817334f64cf7edd2e843c333b9b86dc5,Book Reviews: Lexical Semantics and Knowledge Representation in Multilingual Text Generation,1999,Manfred Stede,ed521609ebfc7494410d1b367b3ba73c0d7da6f4,Upper Modeling: organizing knowledge for natural language processing,1990,J. Bateman
1151,D13-1115,External_4068,[2],experiments,"ImageNet is a largescale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets <TARGET_CITATION/> .","BilderNetle (little ImageNet'' in Swabian German) is our new data set of German nountoImageNet synset mappings. ImageNet is a largescale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets <TARGET_CITATION/> . ImageNet is a largescale and widely used image database, built on top of WordNet, which maps words into groups of images, called synsets <CITATION/>. BilderNetle (little ImageNet'' in Swabian German) is our new data set of German nountoImageNet synset mappings.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,imagenet a largescale hierarchical image database,2009,Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei
1152,D11-1138,P05-1012,[2],experiments,based parsing algorithms with an arcfactored parameterization <TARGET_CITATION/> .,All feature conjunctions are included. Graphbased: An implementation of graph based parsing algorithms with an arcfactored parameterization <TARGET_CITATION/> . based parsing algorithms with an arcfactored parameterization <CITATION/>.  Graphbased: An implementation of graphAll feature conjunctions are included.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,d3b27746f7a53f2dc5d9b8c2f3d343313622ec36,Online Large-Margin Training of Dependency Parsers,2005,Ryan T. McDonald; K. Crammer; Fernando C Pereira
1153,J09-4010,P00-1038,[4],,"Two applications that , like helpdesk , deal with question  answer pairs are : summarization of email threads <CITATION/> , and answer extraction in FAQs ( Frequently Asked Questions ) ( <TARGET_CITATION/> ;","However, the personalization component of the casebased reasoning approach was rulebased (e.g., rules were applied to substitute names of individuals and companies in texts). With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpusbased approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). Two applications that , like helpdesk , deal with question  answer pairs are : summarization of email threads <CITATION/> , and answer extraction in FAQs ( Frequently Asked Questions ) ( <TARGET_CITATION/> ; Two applications that, like helpdesk, deal with questionanswer pairs are: summarization of email threads <CITATION/>, and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000;With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpusbased approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence). However, the personalization component of the casebased reasoning approach was rulebased (e.g., rules were applied to substitute names of individuals and companies in texts).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,324b4b6a2031821b115c1594f52f3a252b049994,Query-Relevant Summarization using FAQs,2000,A. Berger; Vibhu Mittal
1154,W02-0309,External_2004,[2],experiments,"The retrieval process relies on the vector space model <TARGET_CITATION/> , with the cosine measure expressing the similarity between a query and a document .","For unbiased evaluation of our approach, we used a homegrown search engine (implemented in the PYTHON script language). It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tfidf metric. The retrieval process relies on the vector space model <TARGET_CITATION/> , with the cosine measure expressing the similarity between a query and a document . The retrieval process relies on the vector space model <CITATION/>, with the cosine measure expressing the similarity between a query and a document. It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tfidf metric. For unbiased evaluation of our approach, we used a homegrown search engine (implemented in the PYTHON script language).",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f,"Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer",1989,G. Salton
1155,E03-1002,External_13378,[0],,"For rightbranching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial <TARGET_CITATION/> , as has conditioning on the leftcorner child <CITATION/> .","For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the leftcorner ancestor of top, (which is below top, on the stack), top 's leftcorner child (its leftmost child, if any), and top 's most recent child (which was top,_1, if any). For rightbranching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial <TARGET_CITATION/> , as has conditioning on the leftcorner child <CITATION/> . For rightbranching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial <CITATION/>, as has conditioning on the leftcorner child <CITATION/>. These nodes are the leftcorner ancestor of top, (which is below top, on the stack), top 's leftcorner child (its leftmost child, if any), and top 's most recent child (which was top,_1, if any). For this reason, D(top) includes nodes which are structurally local to top,.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,6c9f553e723a40a6713453b734b552c1928bf52b,PCFG Models of Linguistic Tree Representations,1998,Mark Johnson
1156,W06-1639,P05-1015,[0],introduction,A few others incorporate various measures of interdocument similarity between the texts to be labeled <TARGET_CITATION/> .,"In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Most sentimentpolarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of interdocument similarity between the texts to be labeled <TARGET_CITATION/> . A few others incorporate various measures of interdocument similarity between the texts to be labeled <CITATION/>. Most sentimentpolarity classifiers proposed in the recent literature categorize each document independently. In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,6af58c061f2e4f130c3b795c21ff0c7e3903278f,Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales,2005,B. Pang; Lillian Lee
1157,W06-1639,External_57193,[0],introduction,A few others incorporate various measures of interdocument similarity between the texts to be labeled <TARGET_CITATION/> .,"In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Most sentimentpolarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of interdocument similarity between the texts to be labeled <TARGET_CITATION/> . A few others incorporate various measures of interdocument similarity between the texts to be labeled <CITATION/>. Most sentimentpolarity classifiers proposed in the recent literature categorize each document independently. In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,3497dcdac3db7db37aabc1db94b516780e89ea8e,Sentiment Analysis : A New Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be Classified .,2005,Alekh Agarwal
1158,J02-3002,External_13267,[4],,This is similar to  one sense per collocation '' idea of <TARGET_CITATION/> .,"Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations. In capitalizedword disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (ngrams). This is similar to  one sense per collocation '' idea of <TARGET_CITATION/> . This is similar to one sense per collocation'' idea of <CITATION/>. In capitalizedword disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (ngrams). Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,785737feb1e55d46b021d261ad5ecc705a79103e,One Sense per Collocation,1993,David Yarowsky
1159,J01-4001,External_41833,[0],,"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> .","The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> . Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,db51c9525f49b4a2c9711b45f31f2f6b6522c1f2,BART: A Multilingual Anaphora Resolution System,2010,Samuel Broscheit; Massimo Poesio; Simone Paolo Ponzetto; K. Rodríguez; Lorenza Romano; Olga Uryupina; Yannick Versley; Roberto Zanoli
1160,W06-1705,External_46138,[0],method,"This system has been successfully tested with the development of plugins supporting instant messaging , distributed video encoding <CITATION/> , distributed virtual worlds <CITATION/> and digital library management <TARGET_CITATION/> .","We have designed this environment so that specific application functionalitycan be captured within plugins that can then integrate with the environment and utilise its functionality. This system has been successfully tested with the development of plugins supporting instant messaging , distributed video encoding <CITATION/> , distributed virtual worlds <CITATION/> and digital library management <TARGET_CITATION/> . This system has been successfully tested with the development of plugins supporting instant messaging, distributed video encoding <CITATION/>, distributed virtual worlds <CITATION/> and digital library management <CITATION/>. can be captured within plugins that can then integrate with the environment and utilise its functionality. We have designed this environment so that specific application functionality",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,p2p4dl digital library over peertopeer,2004,J Walkerdine; P Rayson
1161,D08-1034,W05-0630,[4],,"be found in figure 2 , which is similar with that in <TARGET_CITATION/> .","However, in this paper, we did SRC in two steps. The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the highlevel description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2 , which is similar with that in <TARGET_CITATION/> . be found in figure 2, which is similar with that in <CITATION/>.The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the highlevel description of selfdescriptive features, e.g. phrase type, are limited. However, in this paper, we did SRC in two steps.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,41dca48ae5074f9d51c61a2c7c5e0f3f4b1c8109,Hierarchical Semantic Role Labeling,2005,Alessandro Moschitti; Ana-Maria Giuglea; Bonaventura Coppola; Roberto Basili
1162,W14-1609,W09-1119,[1],introduction,It is inspired by the system described in <TARGET_CITATION/> .,In this section we describe in detail the baseline NER system we use. It is inspired by the system described in <TARGET_CITATION/> . It is inspired by the system described in <CITATION/>. In this section we describe in detail the baseline NER system we use.,d53d878cf1a3f0bed5d9c68c925994cb72f47304,Lexicon Infused Phrase Embeddings for Named Entity Resolution,2014,Alexandre Passos; Vineet Kumar; A. McCallum,aa9efc8b2737eac0675ba5abb5feab8305482c12,Design Challenges and Misconceptions in Named Entity Recognition,2009,Lev-Arie Ratinov; D. Roth
1163,J09-4010,W06-1303,[4],method," A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours <TARGET_CITATION/> ."," Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>.  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>. A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours <TARGET_CITATION/> .  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours <CITATION/>.  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>.  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,edfa9baa3ca75550b5c40901324a61358742d25f,Building Effective Question Answering Characters,2006,Anton Leuski; Ronakkumar Patel; D. Traum; Brandon Kennedy
1164,A00-1025,E99-1011,[4],,"Although a number of methods for querydependent text summarization are beginning to be developed and evaluated in a variety of realistic settings <TARGET_CITATION/> , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task <CITATION/> :","We next hypothesize that querydependent text summarization algorithms will improve the perfor mance of the QA system by focusing the system on the most relevant portions of the retrieved documents. The goal for querydependent summarization algorithms is to provide a short summary of a document with respect to a specific query. Although a number of methods for querydependent text summarization are beginning to be developed and evaluated in a variety of realistic settings <TARGET_CITATION/> , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task <CITATION/> : Although a number of methods for querydependent text summarization are beginning to be developed and evaluated in a variety of realistic settings <CITATION/>, we again propose the use of vector space methods from IR, which can be easily extended to the summarization task <CITATION/>:The goal for querydependent summarization algorithms is to provide a short summary of a document with respect to a specific query. We next hypothesize that querydependent text summarization algorithms will improve the perfor mance of the QA system by focusing the system on the most relevant portions of the retrieved documents.",7acb7b3e7ad16adbc68626cddd4bcba515b86e23,Examining the Role of Statistical and Linguistic Knowledge Sources in a General-Knowledge Question-Answering System,2000,Claire Cardie; Vincent Ng; D. Pierce; C. Buckley,3b856c6567fe6f7770306f8cc20cd480c44f513c,The TIPSTER SUMMAC Text Summarization Evaluation,1999,I. Mani; D. House; Gary Klein; L. Hirschman; Therese Firmin; B. Sundheim
1165,J97-4003,C94-2131,[4],related work,Riehemann 1993 ; <TARGET_CITATION/> ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .,"Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification <TARGET_CITATION/> . Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,83ae4bda3c1f714ed14f2fbb71c81bd96c75f1cc,HPSG Lexicon Without Lexical Rules,1994,K. Oliva
1166,D09-1160,W08-0804,[5],conclusion,"When we run our classifiers on resourcetight environments such as cellphones , we can use a random feature mixing technique <TARGET_CITATION/> or a memoryefficient trie implementation based on a succinct data structure <CITATION/> to reduce required memory usage .","We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a realtime application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resourcetight environments such as cellphones , we can use a random feature mixing technique <TARGET_CITATION/> or a memoryefficient trie implementation based on a succinct data structure <CITATION/> to reduce required memory usage . When we run our classifiers on resourcetight environments such as cellphones, we can use a random feature mixing technique <CITATION/> or a memoryefficient trie implementation based on a succinct data structure <CITATION/> to reduce required memory usage. To speed up classifiers used in a realtime application, we can build fstries incrementally by using feature vectors generated from user inputs. We plan to apply our method to wider range of classifiers used in various NLP tasks.",c1732c8cf3cd5b421366ba3fda7e7f2407087b59,Polynomial to Linear: Efficient Classification with Conjunctive Features,2009,Naoki Yoshinaga; M. Kitsuregawa,3fb1c64b763d27fba2a18c923637c5ea0e048e3b,Small Statistical Models by Random Feature Mixing,2008,Kuzman Ganchev; Mark Dredze
1167,J03-3004,External_84344,[4],conclusion,"These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an ngram target language model ) , in effect forms a multiengine MT system as described by <TARGET_CITATION/> .","We have presented an EBMT system based on the marker hypothesis that uses post hoc validation and correction via the Web.11 Over 218,000 NPs and VPs were extracted automatically from the PennII Treebank using just 59 of its 29,000 rule types. These phrases were then translated automatically by three online MT systems. These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an ngram target language model ) , in effect forms a multiengine MT system as described by <TARGET_CITATION/> . These translations gave rise to a number of automatically constructed linguistic resources: (1) the original (source,target) phrasal translation pairs, (2) the marker lexicon, (3) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system, seeded with input from multiple translation systems, with a postvalidation process via the Web (amounting to an ngram target language model), in effect forms a multiengine MT system as described by <CITATION/>.These phrases were then translated automatically by three online MT systems. We have presented an EBMT system based on the marker hypothesis that uses post hoc validation and correction via the Web.11 Over 218,000 NPs and VPs were extracted automatically from the PennII Treebank using just 59 of its 29,000 rule types.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,5d944003ff22ecde679fa2a78a4106a2bba4ab04,Integrating Translations from Multiple Sources within the PANGLOSS Mark III Machine Translation System,1994,R. Frederking; S. Nirenburg; D. Farwell; Stephen Helmreich; E. Hovy; Kevin Knight; S. Beale; Constantine Domashnev; Donalee H. Attardo; D. Grannes; Robert G. Brown
1168,P00-1007,External_9757,[0],introduction,"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences ( e.g. , <TARGET_CITATION/> ) ."," A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences <TARGET_CITATION/> . A variety of statistical methods were proposed over the recent years for learning to produce a full parse of freetext sentences (e.g., <CITATION/>).",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,,statistical decisiontree models for parsing,1995,David M Magerman
1169,W06-3309,External_1850,[0],introduction,"Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , <TARGET_CITATION/> .","Building on the work of <CITATION/> in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cfXXX <CITATION/>. Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , <TARGET_CITATION/> . Discriminative approaches (especially SVMs) have been shown to be very effective for many supervised classification tasks; see, for example, <CITATION/>. Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Building on the work of <CITATION/> in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cfXXX <CITATION/>.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,a351ffd1634b0930bd51ff225a8836e540269947,Text Categorization with Support Vector Machines: Learning with Many Relevant Features,1999,T. Joachims
1171,W06-1639,W06-3808,[0],introduction,A few others incorporate various measures of interdocument similarity between the texts to be labeled <TARGET_CITATION/> .,"In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Most sentimentpolarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of interdocument similarity between the texts to be labeled <TARGET_CITATION/> . A few others incorporate various measures of interdocument similarity between the texts to be labeled <CITATION/>. Most sentimentpolarity classifiers proposed in the recent literature categorize each document independently. In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,6fec21a78eb9279c87cc89ef7efa0acf22ff4abd,Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization,2006,A. Goldberg; Xiaojin Zhu
1172,J97-4003,External_69616,[0],introduction,11 <TARGET_CITATION/> proposes to unify these two steps by including an update operator in the,"For space reasons, the SYNSEM feature is abbreviated by its first letter. The traditional (First I Rest) list notation is used, and the operator ED stands for the append relation in the usual way. 11 <TARGET_CITATION/> proposes to unify these two steps by including an update operator in the 11 <CITATION/> proposes to unify these two steps by including an update operator in theThe traditional (First I Rest) list notation is used, and the operator ED stands for the append relation in the usual way. For space reasons, the SYNSEM feature is abbreviated by its first letter.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,the update operation in feature logic,1995,Suresh Manandhar
1173,W02-1601,C90-3101,[0],,"For instance , when building translation units in EBMT approaches <TARGET_CITATION/> , etc. , where SSSTC can be used to represent the entries of the BKB or when SSSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ' extraction from parallel parsed corpus <CITATION/> , ( Watanabe et al. ,","  Dominance: Given two subtrees S and T, there is aNote that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the SSSTC (i.e. between the two languages). For instance , when building translation units in EBMT approaches <TARGET_CITATION/> , etc. , where SSSTC can be used to represent the entries of the BKB or when SSSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ' extraction from parallel parsed corpus <CITATION/> , ( Watanabe et al. , For instance, when building translation units in EBMT approaches <CITATION/>, etc., where SSSTC can be used to represent the entries of the BKB or when SSSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus <CITATION/>, (Watanabe et al.,Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the SSSTC (i.e. between the two languages).   Dominance: Given two subtrees S and T, there is a",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,fbdebca0f1cd0b3e0ffcca5d40ad474b1b7ee622,Pilot Implementation of a Bilingual Knowledge Bank,1990,Victor Sadler; Ronald Vendelmans
1174,D13-1115,D12-1130,[0],related work,"<TARGET_CITATION/> introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCAbased model and others on association norm prediction , held out feature prediction , and word similarity .","In a similar vein, <CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task. <CITATION/> take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. <TARGET_CITATION/> introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCAbased model and others on association norm prediction , held out feature prediction , and word similarity . <CITATION/> introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCAbased model and others on association norm prediction, held out feature prediction, and word similarity. <CITATION/> take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. In a similar vein, <CITATION/> showed that a different featuretopic model improved predictions on a fillintheblank task.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,d4149dbef949644dad4833012e2def98529c0241,Grounded Models of Semantic Representation,2012,Carina Silberer; Mirella Lapata
1175,A00-1009,J94-4004,[0],,More details on how the structural divergences described in <TARGET_CITATION/> can be accounted for using our formalism can be found in <CITATION/> .,More general lexicostructural rules for transfer can also be implemented using our grammar rule formalism. Figure 8 gives an EnglishFrench transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST: More details on how the structural divergences described in <TARGET_CITATION/> can be accounted for using our formalism can be found in <CITATION/> . More details on how the structural divergences described in <CITATION/> can be accounted for using our formalism can be found in <CITATION/>.Figure 8 gives an EnglishFrench transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST:More general lexicostructural rules for transfer can also be implemented using our grammar rule formalism.,6602edbc2f35e085dc4ee0361da214c4f14c5a07,A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing,2000,Benoit Lavoie; R. Kittredge; Tanya Korelsky; Owen Rambow,21e69bb8d3a36235a19da1408279290055b373bc,Machine Translation Divergences: A Formal Description and Proposed Solution,1994,B. Dorr
1176,P10-4003,W09-3906,[2],experiments,"At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy <TARGET_CITATION/> .","For highspecificity, it attempts to hint at a twoplace relation, for example, Here's a hint: the battery is connected to something.'' The tutorial policy makes a highlevel decision as to which strategy to use (for example, acknowledge the correct part and give a high specificity hint'') based on the answer analysis and dialogue context. At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy <TARGET_CITATION/> . At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy <CITATION/>. The tutorial policy makes a highlevel decision as to which strategy to use (for example, acknowledge the correct part and give a high specificity hint'') based on the answer analysis and dialogue context. For highspecificity, it attempts to hint at a twoplace relation, for example, Here's a hint: the battery is connected to something.''",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,69ea2a94de722b5be238b0496d3b6fe512fdbb33,Dealing with Interpretation Errors in Tutorial Dialogue,2009,M. Dzikovska; Charles B. Callaway; Elaine Farrow; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell
1177,D13-1115,P13-1056,[0],related work,"More recently , <TARGET_CITATION/> show that visual attribute classifiers , which have been immensely successful in object recognition <CITATION/> , act as excellent substitutes for feature","<CITATION/> show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. <CITATION/> show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently , <TARGET_CITATION/> show that visual attribute classifiers , which have been immensely successful in object recognition <CITATION/> , act as excellent substitutes for feature More recently, <CITATION/> show that visual attribute classifiers, which have been immensely successful in object recognition <CITATION/>, act as excellent substitutes for feature<CITATION/> show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. <CITATION/> show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4adca62f888226d3a16654ca499bf2a7d3d11b71,Models of Semantic Representation with Visual Attributes,2013,Carina Silberer; V. Ferrari; Mirella Lapata
1178,D12-1084,W11-1208,[3],experiments,"We found the same number using our previous approach <TARGET_CITATION/> , which is roughly equivalent to our core module .","We cannot directly compare this performance to other systems, as all other approaches have different data sources. However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One stateoftheart system introduced by <CITATION/> extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67. We found the same number using our previous approach <TARGET_CITATION/> , which is roughly equivalent to our core module . We found the same number using our previous approach <CITATION/>, which is roughly equivalent to our core module. However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One stateoftheart system introduced by <CITATION/> extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67. We cannot directly compare this performance to other systems, as all other approaches have different data sources.",a59a0185e02bf46b9f03274da718e87a24e7b8a8,Using Discourse Information for Paraphrase Extraction,2012,Michaela Regneri; Rui Wang,2a2a7b8a93ca9e7dad6c2223a6ebac7f33616869,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,2011,Rui Wang; Chris Callison-Burch
1179,J06-2002,External_3884,[0],experiments,Similar things hold for multifaceted properties like intelligence <TARGET_CITATION/> .,"If there exists a formula for mapping three dimensions into one (e.g., length x width x height) then the result is one dimension (overallsize), and the algorithm of Section 4 can be applied verbatim. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. Similar things hold for multifaceted properties like intelligence <TARGET_CITATION/> . Similar things hold for multifaceted properties like intelligence <CITATION/>. But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation. If there exists a formula for mapping three dimensions into one (e.g., length x width x height) then the result is one dimension (overallsize), and the algorithm of Section 4 can be applied verbatim.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,d7689bea78120d1eb2efbc09506d9ae98452d59e,Two theories about adjectives,2013,H. Kamp
1181,D12-1037,N09-2006,[4],related work,<CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it .,Several works have proposed discriminative techniques to train loglinear model for SMT. <CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it . <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,5cbb632dc743e1498e5df06d105dff58e67260e9,A Simplex Armijo Downhill Algorithm for Optimizing Statistical Machine Translation Decoding Parameters,2009,B. Zhao; Shengyuan Chen
1182,W02-0309,External_3337,[0],conclusion,"There has been some controversy , at least for simple stemmers <TARGET_CITATION/> , about the effectiveness of morphological analysis for document retrieval <CITATION/> ."," There has been some controversy , at least for simple stemmers <TARGET_CITATION/> , about the effectiveness of morphological analysis for document retrieval <CITATION/> . There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,6b3853f08c482fe1bfbe39d656d50a8c73976f3c,Development of a stemming algorithm,1968,J. B. Lovins
1183,W06-1104,External_1728,[4],experiments,<TARGET_CITATION/> reported an intrasubject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .,"Therefore, intersubject correlation is lower than the results obtained by <CITATION/>. In our experiment, intrasubject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intrasubject correlation of r=.647. <TARGET_CITATION/> reported an intrasubject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . <CITATION/> reported an intrasubject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. In our experiment, intrasubject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intrasubject correlation of r=.647. Therefore, intersubject correlation is lower than the results obtained by <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,7ef3ac14cdb484aaa2b039850093febd5cf73a21,Contextual correlates of synonymy,1965,H. Rubenstein; J. Goodenough
1184,D09-1056,External_2273,[0],related work,Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <TARGET_CITATION/> .,Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <TARGET_CITATION/> . Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. The most basic is a Bag of Words (BoW) representation of the document text. Many different features have been used to represent documents where an ambiguous name is mentioned.,a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,,entitybased crossdocument coreferencing using the vector space model,1998,Amit Bagga; Breck Baldwin
1185,J03-3004,External_56438,[0],introduction,"More recently , <TARGET_CITATION/> have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch  aler ( 2002 ) and Sch  aler , Way , and Carl ( 2003 , pages 108  109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .","More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization <CITATION/> More recently , <TARGET_CITATION/> have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch  aler ( 2002 ) and Sch  aler , Way , and Carl ( 2003 , pages 108  109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment . More recently, <CITATION/> have proposed the exploitation of TMs at a subsentential level, while <CITATION/> and Schaler, Way, and Carl (2003, pages 108109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment.  Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization <CITATION/>More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,subsentential exploitation of translation memories,2001,Michel Simard; Philippe Langlais
1186,N04-2004,External_29663,[0],,"With a minimal set of features and a small number of lexical entries , <CITATION/> has successfully modeled many of the argument alternations described by <CITATION/> using a <TARGET_CITATION/> style analysis .","The currently implemented system is still at the toy parser'' stage. Although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena. With a minimal set of features and a small number of lexical entries , <CITATION/> has successfully modeled many of the argument alternations described by <CITATION/> using a <TARGET_CITATION/> style analysis . With a minimal set of features and a small number of lexical entries, <CITATION/> has successfully modeled many of the argument alternations described by <CITATION/> using a <CITATION/> style analysis. Although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena. The currently implemented system is still at the toy parser'' stage.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,244b8a2de2ff31039585ae8dbcf4b1ce29ed9460,On Argument Structure and the Lexical Expression of Syntactic Relations,1993,K. Hale; S. J. Keyser
1187,J06-2002,External_40447,[0],introduction,"For some adjectives , including the ones that <TARGET_CITATION/> called evaluative ( as opposed to dimensional ) , this is clearly inadequate .","What we said above has also disregarded elements of the global'' (i.e., not immediately available) context. For some adjectives , including the ones that <TARGET_CITATION/> called evaluative ( as opposed to dimensional ) , this is clearly inadequate . For some adjectives, including the ones that <CITATION/> called evaluative (as opposed to dimensional), this is clearly inadequate. What we said above has also disregarded elements of the global'' (i.e., not immediately available) context.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,045c7943233099492b0a4bbdb487e06434385092,The Semantics of Gradation,1989,M. Bierwisch
1188,W00-1017,P99-1026,[3],experiments,"WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system <TARGET_CITATION/> , a videorecording programming system , a schedule management system <CITATION/> , and a weather infomiation system <CITATION/> .","This makes it easy to find errors in the domain specifications.$ Implementation WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system <TARGET_CITATION/> , a videorecording programming system , a schedule management system <CITATION/> , and a weather infomiation system <CITATION/> . WIT has been implemented in Common Lisp and C on UNIX, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system <CITATION/>, a videorecording programming system, a schedule management system <CITATION/>, and a weather infomiation system <CITATION/>. $ ImplementationThis makes it easy to find errors in the domain specifications.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,b9317d1aa658d94f18fe7cbad6c8ab3ac64b73f9,Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems,1999,Mikio Nakano; Noboru Miyazaki; Jun-ichi Hirasawa; Kohji Dohsaka; T. Kawabata
1189,J00-4002,J94-4005,[5],,"The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a  packed '' structure <TARGET_CITATION/> , with the resolution process as described here .","There are several stategies that might be pursued. One is to adopt Pinkal's radical underspecification'' approach <CITATION/> and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a  packed '' structure <TARGET_CITATION/> , with the resolution process as described here . The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs, either individually or in a packed'' structure <CITATION/>, with the resolution process as described here. One is to adopt Pinkal's radical underspecification'' approach <CITATION/> and use underspecified representations for all types of ambiguity, even syntactic ambiguity. There are several stategies that might be pursued.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,3376bc52798561e74f82ae92d2ea55bdf8b2bcce,Training and Scaling Preference Functions for Disambiguation,1994,H. Alshawi; D. Carter
1190,P11-1134,External_12195,[1],,"They proved to be useful in a number of NLP applications such as natural language generation <CITATION/> , multidocument summarization <TARGET_CITATION/> , automatic evaluation of MT <CITATION/> , and TE <CITATION/> .","1http://www.statmt.org/wmt10/Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation <CITATION/> , multidocument summarization <TARGET_CITATION/> , automatic evaluation of MT <CITATION/> , and TE <CITATION/> . They proved to be useful in a number of NLP applications such as natural language generation <CITATION/>, multidocument summarization <CITATION/>, automatic evaluation of MT <CITATION/>, and TE <CITATION/>. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. 1http://www.statmt.org/wmt10/",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,6902ff693206b75ecf974b8a86713f0f1aa9e87c,Tracking and summarizing news on a daily basis with Columbia's Newsblaster,2002,K. McKeown; R. Barzilay; D. Evans; V. Hatzivassiloglou; Judith L. Klavans; A. Nenkova; C. Sable; Barry Schiffman; Sergey Sigelman
1191,D08-1009,W07-1431,[4],related work,"While many approaches have addressed this problem , our work is most closely related to that of <TARGET_CITATION/> , which convert the inputs into logical forms and then attempt to  prove ' H from T plus a set of axioms .","Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T <CITATION/>. While many approaches have addressed this problem , our work is most closely related to that of <TARGET_CITATION/> , which convert the inputs into logical forms and then attempt to  prove ' H from T plus a set of axioms . While many approaches have addressed this problem, our work is most closely related to that of <CITATION/>, which convert the inputs into logical forms and then attempt to prove' H from T plus a set of axioms. Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T <CITATION/>.",cf3ba53a5030b8dd6ec65101b6f5a9b8e4d06f80,Scaling Textual Inference to the Web,2008,Stefan Schoenmackers; Oren Etzioni; Daniel S. Weld,4639cb3718e4eb698a0285548ed2bf23ad9908a9,Natural Logic for Textual Inference,2007,Bill MacCartney; Christopher D. Manning
1192,W02-0309,External_34413,[0],introduction,"While this is simply irrelevant for generalpurpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical freetexts in an IR setting <TARGET_CITATION/> .","Hence, enumerating morphological variants in a semiautomatically generated lexicon, such as proposed for French <CITATION/>, turns out to be infeasible, at least for German and related languages.Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neoclassical compounding <CITATION/>. While this is simply irrelevant for generalpurpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical freetexts in an IR setting <TARGET_CITATION/> . While this is simply irrelevant for generalpurpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical freetexts in an IR setting <CITATION/>. Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neoclassical compounding <CITATION/>. Hence, enumerating morphological variants in a semiautomatically generated lexicon, such as proposed for French <CITATION/>, turns out to be infeasible, at least for German and related languages.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,f338eab7bf084f4a6b74a45fb0c076cc524dd960,The Use of Morphosemantic Regularities in the Medical Vocabulary for Automatic Lexical Coding,1984,S. Wolff
1194,J06-2002,External_1368,[0],,"In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles <TARGET_CITATION/> .","Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles <TARGET_CITATION/> . In English, where the base form is morphologically simpler than the other two, this rule could be argued to follow from Gricean principles <CITATION/>. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. Minimality.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,b25e5bca74d74abb1687315fa3c637bb9911554d,Logic and Conversation,2005,Siobhan Chapman
1195,P00-1012,P93-1024,[5],conclusion,"More generally , distributional clustering techniques ( Sch  utze , 1992 ; <TARGET_CITATION/> ) could be applied to extract semantic classes from the corpus itself .","Furthermore, any realistic dialog system would make use of some limited vocabularyfor which semantic information would be available. More generally , distributional clustering techniques ( Sch  utze , 1992 ; <TARGET_CITATION/> ) could be applied to extract semantic classes from the corpus itself . More generally, distributional clustering techniques <CITATION/> could be applied to extract semantic classes from the corpus itself. for which semantic information would be available. Furthermore, any realistic dialog system would make use of some limited vocabulary",a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf,a69b04113c8f890a09e07ef86d0ccfe6982b8289,40 80 11 v 1 2 2 A ug 1 99 4 DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS,2011,Fernando C Pereira
1196,W06-1705,External_33310,[0],related work,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( <TARGET_CITATION/> , 2004b ) and received a special issue of the journal Computational Linguistics <CITATION/> .","Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times <TARGET_CITATION/> and received a special issue of the journal Computational Linguistics <CITATION/> . The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,the biggest corpus of allquot,2000,M Rundell
1197,J03-3004,External_55797,[0],introduction,<TARGET_CITATION/> replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .,"There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 <CITATION/> attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the (source, target) words have a similar distribution. <CITATION/> attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. <TARGET_CITATION/> replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance . <CITATION/> replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. <CITATION/> attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 <CITATION/> attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the (source, target) words have a similar distribution.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,2b99c1609596e308cd44c04e92722d9560fc56ff,Further Experiments in Bilingual Text Alignment,1998,H. Somers
1198,J09-4010,P00-1038,[4],method," Only an automatic evaluation was performed , which relied on having model responses <TARGET_CITATION/> .","These systems addressed the evaluation issue as follows. Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. Only an automatic evaluation was performed , which relied on having model responses <TARGET_CITATION/> .  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>.  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. These systems addressed the evaluation issue as follows.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,324b4b6a2031821b115c1594f52f3a252b049994,Query-Relevant Summarization using FAQs,2000,A. Berger; Vibhu Mittal
1200,J09-4010,External_42504,[0],method,In <TARGET_CITATION/> ) we identified several systems that resemble ours in that they provide answers to queries .," The relationship between the results obtained by the automatic evaluation of the responses generated by our system and people's assessments of these responses is unclear, in particular for partial responses.These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response. In <TARGET_CITATION/> ) we identified several systems that resemble ours in that they provide answers to queries . In <CITATION/> we identified several systems that resemble ours in that they provide answers to queries. These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response.  The relationship between the results obtained by the automatic evaluation of the responses generated by our system and people's assessments of these responses is unclear, in particular for partial responses.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,evaluation of a largescale email response system,2007,Y Marom; I Zukerman
1201,J86-1002,J83-3002,[4],,The problem of handling illformed input has been studied by <TARGET_CITATION/> .,"While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. The problem of handling illformed input has been studied by <TARGET_CITATION/> . The problem of handling illformed input has been studied by <CITATION/>. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,9de9fd1835d7cac1b46685b3ffa93e3da49a3be4,Parse Fitting and Prose Fixing: Getting a Hold on III-Formedness,1983,Karen Jensen; George E. Heidorn; L. A. Miller; Yael Ravin
1202,J02-3002,External_10625,[0],,A variety of such lists for many languages are already available <TARGET_CITATION/> .,"The first list on which our method relies is a list of common words. This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list. A variety of such lists for many languages are already available <TARGET_CITATION/> . A variety of such lists for many languages are already available <CITATION/>. This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list. The first list on which our method relies is a list of common words.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,celex a guide for users centre for lexical information,1990,Gavin Burnage
1203,J06-2002,External_20230,[1],experiments,"In principle , this might be done by providing the generator with vague input  in which case no special algorithms are needed  but suitably contextualized vague input is often not available <TARGET_CITATION/> .","ConclusionIf the usefulness of NLG resides in its ability to present data in humanaccessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input  in which case no special algorithms are needed  but suitably contextualized vague input is often not available <TARGET_CITATION/> . In principle, this might be done by providing the generator with vague inputin which case no special algorithms are neededbut suitably contextualized vague input is often not available <CITATION/>. If the usefulness of NLG resides in its ability to present data in humanaccessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. Conclusion",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,understanding shortcuts in nlg systems,2000,Chris Mellish
1204,N10-1084,External_56141,[0],related work,<TARGET_CITATION/> ) all belong to the syntactic transformation category .,"<CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <TARGET_CITATION/> ) all belong to the syntactic transformation category . <CITATION/> all belong to the syntactic transformation category. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,0d3ba30dd4c6a29349539e6c94ab0ffd95c22b3e,Words are not enough: sentence level natural language watermarking,2006,Mercan Topkara; Umut Topkara; M. Atallah
1205,P97-1063,External_15612,[2],,"We induced a twoclass wordtoword model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in <TARGET_CITATION/> .","It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available. For each kind of evaluation, we have found one case where we can come close. We induced a twoclass wordtoword model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in <TARGET_CITATION/> . We induced a twoclass wordtoword model of translational equivalence from 13 million words of the Canadian Hansards, aligned using the method in <CITATION/>. For each kind of evaluation, we have found one case where we can come close. It is impossible to replicate the experiments used to evaluate other translation models in the literature, because neither the models nor the programs that induce them are generally available.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,4fe2a45babab10c1bfae05d2464363f4e52bbaf9,A Program for Aligning Sentences in Bilingual Corpora,1993,W. Gale; Kenneth Ward Church
1206,N04-2004,External_14119,[0],,"<TARGET_CITATION/> observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .","Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote ongoing dynamic situations. Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time. <TARGET_CITATION/> observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity . <CITATION/> observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity. Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time. Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote ongoing dynamic situations.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,54aa7bcb3ad8dda689762ec1c40348c300dfa017,Grammaticalizing aspect and affectedness,1987,C. Tenny
1207,J86-1002,External_6911,[4],experiments,"[ The current system should be distinguished from an earlier voice system ( VNLC , <TARGET_CITATION/> ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]","The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [ The current system should be distinguished from an earlier voice system ( VNLC , <TARGET_CITATION/> ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ] [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.] The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,natural language with discrete speech as a mode for humantomachine communication,1985,A Biermann; R Rodman; D Rubin; J Heidlage
1209,W06-3813,External_34278,[0],introduction,He was a grammarian who analysed Sanskrit <TARGET_CITATION/> .,This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 51h century B.C. and the work of Paninil. He was a grammarian who analysed Sanskrit <TARGET_CITATION/> . He was a grammarian who analysed Sanskrit <CITATION/>. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 51h century B.C. and the work of Paninil. This is an old idea.,f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,0976466d36cee4c1c4c84f2e3e52413ebe955124,The Descriptive Technique of Pāṇini: An Introduction,1966,V. N. Misra
1212,K15-1002,W12-4501,[2],experiments,"The OntoNotes5 .0 dataset , which is released for the CoNLL2012 Shared Task <TARGET_CITATION/> , contains 3,145 annotated documents .","Datasets The ACE2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents <CITATION/>. The OntoNotes5 .0 dataset , which is released for the CoNLL2012 Shared Task <TARGET_CITATION/> , contains 3,145 annotated documents . The OntoNotes5.0 dataset, which is released for the CoNLL2012 Shared Task <CITATION/>, contains 3,145 annotated documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents <CITATION/>. Datasets The ACE2004 dataset contains 443 documents.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,f8cdf754fb7c08caf6e2f82b176819230910be5b,CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes,2012,Sameer Pradhan; Alessandro Moschitti; Nianwen Xue; Olga Uryupina; Yuchen Zhang
1213,P07-1068,External_5795,[4],,"and <TARGET_CITATION/> , as described below .","Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by highwe can see, the baseline achieves an Fmeasure of performing resolvers such as <CITATION/> 57.0 and a resolution accuracy of 48.4. and <TARGET_CITATION/> , as described below . and <CITATION/>, as described below. positional features that have been employed by highwe can see, the baseline achieves an Fmeasure of performing resolvers such as <CITATION/> 57.0 and a resolution accuracy of 48.4. Each instance is represented by 33 lexical, grammatical, semantic, and",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,,coreference resolution using competitive learning approach,2003,X Yang; G Zhou; J Su; C L Tan
1214,D08-1004,P04-1035,[2],,We collect substring rationales for a sentiment classification task <TARGET_CITATION/> and use them to obtain significant accuracy improvements for each annotator .,"We present a generative model of how a given annotator, knowing the true 0, stochastically chooses rationales. Thus, observing the rationales helps us infer the true 0. We collect substring rationales for a sentiment classification task <TARGET_CITATION/> and use them to obtain significant accuracy improvements for each annotator . We collect substring rationales for a sentiment classification task <CITATION/> and use them to obtain significant accuracy improvements for each annotator. Thus, observing the rationales helps us infer the true 0. We present a generative model of how a given annotator, knowing the true 0, stochastically chooses rationales.",14e2aec7e25d8880a851a547cf8d27a9721f8e6c,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,2008,Omar Zaidan; Jason Eisner,167e1359943b96b9e92ee73db1df69a1f65d731d,A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts,2004,B. Pang; Lillian Lee
1215,P13-3018,External_90450,[2],method,"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in <TARGET_CITATION/> for Bangla morphologically complex words ."," We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in <TARGET_CITATION/> for Bangla morphologically complex words . We apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in <CITATION/> for Bangla morphologically complex words.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,3fa07bd057446a98384db3886a6fa16b28d0d04b,REPETITION PRIMING AND FREQUENCY ATTENUATION IN LEXICAL ACCESS,1984,K. Forster; Chris Davis
1216,W06-1639,External_2169,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,702c2fde33ccb4328be06405c11e208a4b3ee347,Learning associative Markov networks,2004,B. Taskar; Vassil Chatalbashev; D. Koller
1217,W06-1639,P04-1085,[0],related work,"More sophisticated approaches have been proposed <CITATION/> , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments <TARGET_CITATION/> .","Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. More sophisticated approaches have been proposed <CITATION/> , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments <TARGET_CITATION/> . More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,18d079a6d72e3f0b0c9214f597b6b178265b05ee,Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies,2004,Michel Galley; K. McKeown; Julia Hirschberg; Elizabeth Shriberg
1218,J86-1002,External_3690,[2],experiments,The expectation parser uses an ATNlike representation for its grammar <TARGET_CITATION/> .,"These ratings are computed this way because they remain integral and still fairly accurately represent the correct values. Also, they can simply be added and subtracted rather than multiplied and divided in the hundreds of calculations required for a single sentence parse. The expectation parser uses an ATNlike representation for its grammar <TARGET_CITATION/> . The expectation parser uses an ATNlike representation for its grammar <CITATION/>. Also, they can simply be added and subtracted rather than multiplied and divided in the hundreds of calculations required for a single sentence parse. These ratings are computed this way because they remain integral and still fairly accurately represent the correct values.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,transition network grammars for natural language analysisquot,1970,W A Woods
1219,J09-4010,External_12994,[4],method," Only an automatic evaluation was performed , which relied on having model responses <TARGET_CITATION/> .","These systems addressed the evaluation issue as follows. Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. Only an automatic evaluation was performed , which relied on having model responses <TARGET_CITATION/> .  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>.  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>. These systems addressed the evaluation issue as follows.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,bridging the lexical chasm statistical approaches to answerfinding,2000,A Berger; R Caruana; D Cohn; D Freitag; V Mittal
1220,J02-3002,External_24096,[4],conclusion,The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( <TARGET_CITATION/> : 0.28 % vs. 0.20 % error rate ) .,"Training on the 300,000word NYT text collection took about two minutes. Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( <TARGET_CITATION/> : 0.28 % vs. 0.20 % error rate ) . The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. Training on the 300,000word NYT text collection took about two minutes.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,some applications of treebased modeling to speech and language indexing”,1989,Michael D Riley
1221,D14-1222,D13-1077,[3],introduction,We follow our previous work <TARGET_CITATION/> and restrict bridging to noncoreferential cases .,Bridging or associative anaphora has been widely discussed in the linguistic literature <CITATION/>. <CITATION/> include cases where antecedent and anaphor are coreferent but do not share the same head noun (differenthead coreference). We follow our previous work <TARGET_CITATION/> and restrict bridging to noncoreferential cases . We follow our previous work <CITATION/> and restrict bridging to noncoreferential cases. <CITATION/> include cases where antecedent and anaphor are coreferent but do not share the same head noun (differenthead coreference). Bridging or associative anaphora has been widely discussed in the linguistic literature <CITATION/>.,e1284218dd71d220c02121ff9278e80de4cd6acd,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,2014,Yufang Hou; K. Markert; M. Strube,8de4164931eea9c4ebea42091e31e334e9acef11,Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set,2013,Yufang Hou; K. Markert; M. Strube
1222,J09-4010,External_20399,[2],method,"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion <TARGET_CITATION/> .","Hence, we keep their representation at a low level of abstraction (bagoflemmas). The idea behind the DocPred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion <TARGET_CITATION/> . In our case, the clustering is performed by the program Snob, which implements mixture modeling combined with model selection based on the Minimum Message Length (MML) criterion <CITATION/>. The idea behind the DocPred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. Hence, we keep their representation at a low level of abstraction (bagoflemmas).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,a1d76ee9b6fb4d441e31abcd4dadc4e44c576017,An Information Measure for Classification,1968,C. S. Wallace; D. Boulton
1223,D14-1157,W14-2710,[4],method,This is in line with our previous findings from <TARGET_CITATION/> that candidates with higher power attempt to shift topics less often than others when responding to moderators .,"In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from <TARGET_CITATION/> that candidates with higher power attempt to shift topics less often than others when responding to moderators . This is in line with our previous findings from <CITATION/> that candidates with higher power attempt to shift topics less often than others when responding to moderators. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings.",469a4152eeeda5ed2c4a9ef64c9d94ed881d57e8,Staying on Topic: An Indicator of Power in Political Debates,2014,Vinodkumar Prabhakaran; Ashima Arora; Owen Rambow,fc86c2bbc899b698e3403778d7f2964f1ca1f3fa,Power of Confidence: How Poll Scores Impact Topic Dynamics in Political Debates,2014,Vinodkumar Prabhakaran; Ashima Arora; Owen Rambow
1224,W06-1639,P04-1035,[4],method,"As has been previously observed and exploited in the NLP literature <TARGET_CITATION/> , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","where c(s) is the opposite'' class from c(s). A minimumcost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individualdocument classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature <TARGET_CITATION/> , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs . As has been previously observed and exploited in the NLP literature <CITATION/>, the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. A minimumcost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individualdocument classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. where c(s) is the opposite'' class from c(s).",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,167e1359943b96b9e92ee73db1df69a1f65d731d,A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts,2004,B. Pang; Lillian Lee
1225,P97-1063,External_7175,[0],method,It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <TARGET_CITATION/> .,"Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u, v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <TARGET_CITATION/> . It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <CITATION/>. This step significantly reduces the computational burden of the algorithm. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u, v) < 1.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,robust word alignment for machine aided translationquot,1993,I Dagan; K Church; SZ W Gale
1226,D09-1056,L08-1126,[2],experiments,It provides a fine grained NE recognition covering 100 different NE types <TARGET_CITATION/> .,"Three types of entities were extracted: person, location and organisation. OAK7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc). It provides a fine grained NE recognition covering 100 different NE types <TARGET_CITATION/> . It provides a fine grained NE recognition covering 100 different NE types <CITATION/>. OAK7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc). Three types of entities were extracted: person, location and organisation.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,24424f4050700dfa940851385d2e1ab7ba5d0cdc,Extended Named Entity Ontology with Attribute Information,2008,S. Sekine
1227,A00-1004,External_32171,[0],introduction,"Only a few such corpora exist , including the Hansard EnglishFrench corpus and the HKUST EnglishChinese corpus <TARGET_CITATION/> .","Therefore, CUR is a suitable application for such a translation model. However, a major obstacle to this approach is the lack of parallel corpora for model training. Only a few such corpora exist , including the Hansard EnglishFrench corpus and the HKUST EnglishChinese corpus <TARGET_CITATION/> . Only a few such corpora exist, including the Hansard EnglishFrench corpus and the HKUST EnglishChinese corpus <CITATION/>. However, a major obstacle to this approach is the lack of parallel corpora for model training. Therefore, CUR is a suitable application for such a translation model.",14ffbd58082d1197ea454ec9162b5cfd36cac9f9,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,2000,Jiang Chen; Jian-Yun Nie,,aligning a parallel englishchinese corpus statistically with lexical criteria,1994,Dekai Wu
1228,W06-1639,External_2168,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,9cc36397e1fef5c922d64e88211a7e08ecc64759,Discriminative Probabilistic Models for Relational Data,2002,B. Taskar; P. Abbeel; D. Koller
1229,J09-4010,P00-1038,[0],,"In FAQs , <TARGET_CITATION/> employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .","This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document. Unfortunately, we could not pursue this avenue of research owing to time limitations. In FAQs , <TARGET_CITATION/> employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document . In FA<CITATION/> employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. Unfortunately, we could not pursue this avenue of research owing to time limitations. This suggests that there are situations where one can generalize a response that is longer than a sentence but shorter than a whole document.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,324b4b6a2031821b115c1594f52f3a252b049994,Query-Relevant Summarization using FAQs,2000,A. Berger; Vibhu Mittal
1230,Q13-1020,W04-3250,[2],experiments,The statistical significance test is performed by the resampling approach <TARGET_CITATION/> .,"Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by caseinsensitive BLEU4 with the shortest length penalty. The statistical significance test is performed by the resampling approach <TARGET_CITATION/> . The statistical significance test is performed by the resampling approach <CITATION/>. The translation quality is evaluated by caseinsensitive BLEU4 with the shortest length penalty. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,cb826a3899752b796f14df1c50378c64954a6b0a,Statistical Significance Tests for Machine Translation Evaluation,2004,Philipp Koehn
1231,N04-2004,External_164,[0],,"With a minimal set of features and a small number of lexical entries , <CITATION/> has successfully modeled many of the argument alternations described by <TARGET_CITATION/> using a <CITATION/> style analysis .","The currently implemented system is still at the toy parser'' stage. Although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena. With a minimal set of features and a small number of lexical entries , <CITATION/> has successfully modeled many of the argument alternations described by <TARGET_CITATION/> using a <CITATION/> style analysis . With a minimal set of features and a small number of lexical entries, <CITATION/> has successfully modeled many of the argument alternations described by <CITATION/> using a <CITATION/> style analysis. Although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena. The currently implemented system is still at the toy parser'' stage.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,6cbc1eb25f4ab29a613418b3b0740e74141a0f17,English Verb Classes and Alternations: A Preliminary Investigation,1993,B. Levin
1232,D13-1038,W09-0629,[4],experiments,"Although evaluated on different data sets , this result is consistent with results from previous work <TARGET_CITATION/> .","In theory, it can represent any type of group based on different similarity criteria. Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets , this result is consistent with results from previous work <TARGET_CITATION/> . Although evaluated on different data sets, this result is consistent with results from previous work <CITATION/>. Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. In theory, it can represent any type of group based on different similarity criteria.",e065f4c930f5dad1f6f82a14c180815d418ff765,Towards Situated Dialogue: Revisiting Referring Expression Generation,2013,Rui Fang; Changsong Liu; Lanbo She; J. Chai,899e806a782c0377e12719d051f98969729c65ee,The TUNA-REG Challenge 2009: Overview and Evaluation Results,2009,Albert Gatt; A. Belz; Eric Kow
1233,W06-1104,I05-1067,[2],related work,"We used the revised experimental setup <TARGET_CITATION/> , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .","To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. In our experiment, we annotated a high number of pairs similar in size to the test sets by <CITATION/>. We used the revised experimental setup <TARGET_CITATION/> , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs . We used the revised experimental setup <CITATION/>, based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. In our experiment, we annotated a high number of pairs similar in size to the test sets by <CITATION/>. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,2ebe88fab46e53e980b5aef28c414c04d133fc6f,Using the Structure of a Conceptual Network in Computing Semantic Relatedness,2005,Iryna Gurevych
1234,J87-3002,External_21701,[4],introduction,"<TARGET_CITATION/> ) consult relatively small lexicons , typically generated by hand .","on developing dictionary servers for office automation systems <CITATION/>. Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. <TARGET_CITATION/> ) consult relatively small lexicons , typically generated by hand . <CITATION/>) consult relatively small lexicons, typically generated by hand. Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. on developing dictionary servers for office automation systems <CITATION/>.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,7b773624063af98bf0bfb75c705e489f736aa7f8,DIAGRAM: a grammar for dialogues,1986,Jane J. Robinson
1236,W06-1639,External_62774,[0],introduction,"In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the  electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may  [ alter ] the citizengovernment relationship '' <TARGET_CITATION/> .","1It is worth pointing out that the United States' Library of Congress was an extremely early adopter of Web technology: the THOMAS database (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially societychanging effect. In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the  electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may  [ alter ] the citizengovernment relationship '' <TARGET_CITATION/> . In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may [alter] the citizengovernment relationship'' <CITATION/>. online accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially societychanging effect. 1It is worth pointing out that the United States' Library of Congress was an extremely early adopter of Web technology: the THOMAS database (http://thomas.loc.gov) of congres",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,electronic rulemaking new frontiers in public participation prepared for the annual meeting of the american political science association,2002,S Shulman; D Schlosberg
1237,W02-0309,External_44554,[5],experiments,"This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , <TARGET_CITATION/> ) are incorporated into our system .","We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas wordbased methods failed in 29.6% of the layman queries and 8% of the expert queries, cfXXX Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , <TARGET_CITATION/> ) are incorporated into our system . This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas wordbased methods failed in 29.6% of the layman queries and 8% of the expert queries, cfXXX Figure 5).",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,3418cf24575313d9942af12d1a6f572ac0d34579,Medical Subject Headings (MeSH).,2000,C. Lipscomb
1238,D12-1037,C08-1074,[4],related work,<CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it .,Several works have proposed discriminative techniques to train loglinear model for SMT. <CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it . <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,02dcd2387e5cf7cf4f74dec4825256266b672bbb,Random Restarts in Minimum Error Rate Training for Statistical Machine Translation,2008,Robert C. Moore; Chris Quirk
1239,W14-3902,S14-2036,[3],experiments,"raw length value as a feature , we follow our previous work <TARGET_CITATION/> and create multiple features for length using a decision tree ( J48 ) .","3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work <TARGET_CITATION/> and create multiple features for length using a decision tree ( J48 ) . raw length value as a feature, we follow our previous work <CITATION/> and create multiple features for length using a decision tree (J48). Length of words (L): Instead of using the3.",004c857f76779260664a9907645938c96cb1896a,Code Mixing: A Challenge for Language Identification in the Language of Social Media,2014,Utsab Barman; Amitava Das; Joachim Wagner; Jennifer Foster,e88500eceea7b08bf80afbd23e8342c37d52c964,DCU: Aspect-based Polarity Classification for SemEval Task 4,2014,Joachim Wagner; Piyush Arora; Santiago Cortés; Utsab Barman; D. Bogdanova; Jennifer Foster; L. Tounsi
1240,E03-1005,P02-1034,[0],,"Most DOP models , such as in <CITATION/> , Sima'an ( 2000 ) and <TARGET_CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."," Most DOP models , such as in <CITATION/> , Sima'an ( 2000 ) and <TARGET_CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . Most DOP models, such as in <CITATION/>, Sima'an (2000) and <CITATION/>, use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,fe638b5610475d4524684fb2c2b7b08c119c8700,"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron",2002,M. Collins; Nigel P. Duffy
1241,J02-3002,External_4380,[0],,In some systems such dependencies are learned from labeled examples <TARGET_CITATION/> .,"In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name. Namedentity recognition systems usually use sets of complex handcrafted rules that employ a gazetteer and a local context <CITATION/>. In some systems such dependencies are learned from labeled examples <TARGET_CITATION/> . In some systems such dependencies are learned from labeled examples <CITATION/>. Namedentity recognition systems usually use sets of complex handcrafted rules that employ a gazetteer and a local context <CITATION/>. In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,nymble a highperformance learning namefinder,1997,D M Bikel; S Miller; R Schwartz; R Weischedel
1242,J06-2002,External_2473,[0],experiments,"CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( <TARGET_CITATION/> , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .","Some generalizations of our method are fairly straightforward. For example, consider a relational description (cfXXX, Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( <TARGET_CITATION/> , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed . CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. For example, consider a relational description (cfXXX, Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. Some generalizations of our method are fairly straightforward.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,efficient contextsensitive generation of referring expressions,2002,Emiel Krahmer; Mari¨et Theune
1243,J87-3002,External_58821,[0],,"Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see <TARGET_CITATION/> , for further discussion ) .","(1) John believes the Earth to be roundit will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see <TARGET_CITATION/> , for further discussion ) . Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see <CITATION/>, for further discussion). it will function as the logical subject of the predicate complement. (1) John believes the Earth to be round",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,predication linguistic inquiry,1980,E S Williams
1244,W06-2807,External_36047,[0],related work,"While wikis have spread from a detailed design <TARGET_CITATION/> , unfortunately blogs have not been designed under a model .","The main source of Novelle are wikis and blogs. While wikis have spread from a detailed design <TARGET_CITATION/> , unfortunately blogs have not been designed under a model . While wikis have spread from a detailed design <CITATION/>, unfortunately blogs have not been designed under a model. The main source of Novelle are wikis and blogs.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,1f4081abc0c973ff5d7830655ac3341ad28ecdc2,The Wiki Way: Quick Collaboration on the Web,2001,Bo Leuf; Ward Cunningham
1245,W06-1104,External_61155,[4],related work,"In our experiment , we annotated a high number of pairs similar in size to the test sets by <TARGET_CITATION/> .","Concept annotated datasets can be used to test the ability of a measure to differentiate between senses when determining the relatedness of polysemous words. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. In our experiment , we annotated a high number of pairs similar in size to the test sets by <TARGET_CITATION/> . In our experiment, we annotated a high number of pairs similar in size to the test sets by <CITATION/>. To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset. Concept annotated datasets can be used to test the ability of a measure to differentiate between senses when determining the relatedness of polysemous words.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3b5b95038c6b065f29649c1b11ea3e7855c00a53,Thinking beyond the nouns - computing semantic relatedness across parts of speech,2006,Iryna Gurevych
1247,W00-1017,P96-1009,[0],introduction,The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> ., The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <TARGET_CITATION/> . The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems <CITATION/>.,143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,9f59b4a6052cb6441532f86979b8db21d69749ab,A Robust System for Natural Spoken Dialogue,1996,James F. Allen; B. Miller; Eric K. Ringger; Teresa Sikorski
1248,W06-3813,External_5559,[0],related work,"Most approaches rely on VerbNet <TARGET_CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/> .","The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. Most approaches rely on VerbNet <TARGET_CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/> . Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/>. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,,classbased construction of a verb lexicon in,2000,Karin Kipper; Hoa Trang Dang; Martha Palmer
1249,N10-1084,External_56146,[0],related work,"<TARGET_CITATION/> aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .","It requires some sophisticated tools and knowledge to model natural language semantics. <CITATION/> used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources. <TARGET_CITATION/> aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence . <CITATION/> aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence.<CITATION/> used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources. It requires some sophisticated tools and knowledge to model natural language semantics.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,edae85917596d02abc0f6da52148f5931cc6190d,A method of text watermarking using presuppositions,2007,O. Vybornova; B. Macq
1252,E03-1005,External_16080,[0],,"Most DOP models , such as in <TARGET_CITATION/> , Sima'an ( 2000 ) and <CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."," Most DOP models , such as in <TARGET_CITATION/> , Sima'an ( 2000 ) and <CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . Most DOP models, such as in <CITATION/>, Sima'an (2000) and <CITATION/>, use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa,Efficient Algorithms for Parsing the DOP Model,1996,J. Goodman
1253,P11-1134,P05-1074,[0],introduction,"Some previous works <TARGET_CITATION/> indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in <CITATION/>, even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works <TARGET_CITATION/> indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words . Some previous works <CITATION/> indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. As emerges from the ablation tests reported in <CITATION/>, even the most common resources proved to have a positive impact on some systems and a negative impact on others. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,36e8b8b35e51e5b39fcafdb5c2bc763796d0672e,Paraphrasing with Bilingual Parallel Corpora,2005,Colin Bannard; Chris Callison-Burch
1255,J05-3003,External_555,[2],related work,Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on <TARGET_CITATION/> ;,"Unlike our approach, those of <CITATION/> include a substantial initial correction and cleanup of the PennII trees. <CITATION/> describe a methodology for acquiring an English HPSG from the PennII Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on <TARGET_CITATION/> ; Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on <CITATION/> describe a methodology for acquiring an English HPSG from the PennII Treebank. Unlike our approach, those of <CITATION/> include a substantial initial correction and cleanup of the PennII trees.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,0ffa423a5283396c88ff3d4033d541796bd039cc,"Three Generative, Lexicalised Models for Statistical Parsing",1997,M. Collins
1256,D09-1143,W04-2403,[5],conclusion,"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <CITATION/> or shallow semantic trees , <TARGET_CITATION/> .","For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <CITATION/> or shallow semantic trees , <TARGET_CITATION/> . Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNetbased features <CITATION/> or shallow semantic trees, <CITATION/>. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task.",a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi,43bfc58ed49a4d89d82ff6a51ecc9b58b75a9aeb,A Semantic Kernel for Predicate Argument Classification,2004,Alessandro Moschitti; C. Bejan
1259,P02-1001,External_3025,[2],,"In a loglinear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <TARGET_CITATION/> .","Maximumlikelihood estimation guesses 0 to be the 0 maximizing Hi f(xi, yi). Maximumposterior estimation tries to maximize P(0)Hi f(xi, yi) where P(0) is a prior probability. In a loglinear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <TARGET_CITATION/> . In a loglinear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting <CITATION/>. Maximumposterior estimation tries to maximize P(0)Hi f(xi, yi) where P(0) is a prior probability. Maximumlikelihood estimation guesses 0 to be the 0 maximizing Hi f(xi, yi).",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a,A Gaussian Prior for Smoothing Maximum Entropy Models,1999,Stanley F. Chen; R. Rosenfeld
1260,A00-1018,P91-1023,[2],,This alignment is done on the basis of both length ( Gale and Church <TARGET_CITATION/> ) and a notion of cognateness ( <CITATION/> ) .,An aligner. After identification of word and sentence boundaries the text is processed into a bitext by an alignment program. This alignment is done on the basis of both length ( Gale and Church <TARGET_CITATION/> ) and a notion of cognateness ( <CITATION/> ) . This alignment is done on the basis of both length (<CITATION/>) and a notion of cognateness (<CITATION/>). After identification of word and sentence boundaries the text is processed into a bitext by an alignment program. An aligner.,a0f9181bae5840fa6d1d19a694bd9338b2c96443,An Automatic Reviser: The TransCheck System,2000,Jean-Marc Jutras,6287c8f49c8fc5fcf7821004e3f787a856307055,A Program for Aligning Sentences in Bilingual Corpora,1991,W. Gale; Kenneth Ward Church
1263,D13-1115,External_14809,[2],experiments,"The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from <TARGET_CITATION/> .","In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents. We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from <TARGET_CITATION/> . The high Dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from <CITATION/>. We do not optimize these hyperparameters or vary them over time. In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,2d8cbd7370b4ce666edd864e66f83ebf20963516,Online Learning for Latent Dirichlet Allocation,2010,M. Hoffman; D. Blei; F. Bach
1264,J86-1002,External_33228,[0],experiments,"The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , <TARGET_CITATION/> ) .","An offtheshelf speech recognition device, a Nippon Electric Corporation DP200, was added to an existing natural language processing system, the Natural Language Computer (NLC) <CITATION/>. The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , <TARGET_CITATION/> ) . The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. An offtheshelf speech recognition device, a Nippon Electric Corporation DP200, was added to an existing natural language processing system, the Natural Language Computer (NLC) <CITATION/>.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,cede7e5150ecfbd5024a8cdc441010ad61299a4e,"The acquisition and use of dialogue expectation in speech recognition (natural language processing, artificial intelligence)",1983,P. Fink
1265,J15-3005,J11-1005,[4],introduction,"In our previous papers <TARGET_CITATION/> , we applied a set of beams to this structure , which makes it similar to the data structure used for phrasebased MT decoding <CITATION/> .","First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling. Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. In our previous papers <TARGET_CITATION/> , we applied a set of beams to this structure , which makes it similar to the data structure used for phrasebased MT decoding <CITATION/> . In our previous papers <CITATION/>, we applied a set of beams to this structure, which makes it similar to the data structure used for phrasebased MT decoding <CITATION/>. Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.",f67ec8d10f04442c55ada6821031cf39e06aaa8e,Discriminative Syntax-Based Word Ordering for Text Generation,2015,Yue Zhang; S. Clark,f24fb33e739b0181fc171b515f957190e3bb6430,Syntactic Processing Using the Generalized Perceptron and Beam Search,2011,Yue Zhang; S. Clark
1266,J86-1002,External_96136,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( <TARGET_CITATION/> , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."," A number of speech understanding systems have been developed during the past fifteen years ( <TARGET_CITATION/> , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,the sdc speech understanding system in lea,1980,J Barnett; M Berstein; R Gillman; I Kameny
1267,D13-1115,D12-1130,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,d4149dbef949644dad4833012e2def98529c0241,Grounded Models of Semantic Representation,2012,Carina Silberer; Mirella Lapata
1268,W06-1639,External_19099,[0],introduction,"In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> .","Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> . In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,yahoo for amazon extracting market sentiment from stock message boards,2001,S Das; M Chen
1269,A00-1004,External_9617,[0],method,"A number of alignment techniques have been proposed , varying from statistical methods <CITATION/> to lexical methods <TARGET_CITATION/> .","Some are highly parallel and easy to align while others can be very noisy. Aligning EnglishChinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages. A number of alignment techniques have been proposed , varying from statistical methods <CITATION/> to lexical methods <TARGET_CITATION/> . A number of alignment techniques have been proposed, varying from statistical methods <CITATION/> to lexical methods <CITATION/>. Aligning EnglishChinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages. Some are highly parallel and easy to align while others can be very noisy.",14ffbd58082d1197ea454ec9162b5cfd36cac9f9,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,2000,Jiang Chen; Jian-Yun Nie,1f55d2bca810edbf9870934a41d956d06ae2d9cf,Aligning Sentences in Bilingual Corpora Using Lexical Information,1993,Stanley F. Chen
1270,J87-3002,External_11433,[0],introduction,( <TARGET_CITATION/> contains further description and discussion of LDOCE . ),"We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system. Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled core' vocabulary in defining the words throughout the dictionary. ( <TARGET_CITATION/> contains further description and discussion of LDOCE . ) (<CITATION/> contains further description and discussion of LDOCE.) Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled core' vocabulary in defining the words throughout the dictionary. We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,exploiting a large dictionary data base,1982,Archibal Michiels
1271,J03-3004,External_61553,[4],introduction,<TARGET_CITATION/> ) assumes that words ending in  ed are verbs .,"<QUANT> all : tous <PREP> of : d' <LEX> uses : usages <LEX> asbestos : asbesteThat is, using the marker hypothesis method of segmentation, smaller aligned segments can be extracted from the phrasal lexicon without recourse to any detailed parsing techniques or complex coocurrence measures. <TARGET_CITATION/> ) assumes that words ending in  ed are verbs . Juola (1994, 1997) assumes that words ending in ed are verbs. That is, using the marker hypothesis method of segmentation, smaller aligned segments can be extracted from the phrasal lexicon without recourse to any detailed parsing techniques or complex coocurrence measures.<QUANT> all : tous <PREP> of : d' <LEX> uses : usages <LEX> asbestos : asbeste",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,a psycholinguistic approach to corpusbased machine translation,1994,Patrick Juola
1272,J02-3002,External_12989,[2],,"For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC7 ) <TARGET_CITATION/> ."," frequent proper name  abbreviation (as opposed to regular word)These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC7 ) <TARGET_CITATION/> . For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times (NYT) corpus that was supplied as training data for the 7th Message Understanding Conference (MUC7) <CITATION/>. These four lists can be acquired completely automatically from raw (unlabeled) texts.  frequent proper name  abbreviation (as opposed to regular word)",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,overview of muc7”,1998,Nancy Chinchor
1273,W06-2807,External_91607,[2],,"We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences <TARGET_CITATION/> .","With our typology of links, we aim to solve the framing problem as defined in Section 1.2. We want to model views as dynamic objects the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences <TARGET_CITATION/> . We would also provide a user facility for choosing the right licence for every lexia, following the model of Creative Commons licences <CITATION/>.We want to model views as dynamic objects the creation of context will be still arbitrary, but changes are very easily. With our typology of links, we aim to solve the framing problem as defined in Section 1.2.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,cc944b0a929d4ca6ce8a47fc84cadae38cfe8c27,Free Culture: How Big Media Uses Technology and the Law to Lock Down Culture and Control Creativity,2004,Lawrence Lessig
1274,D13-1115,External_9732,[0],related work,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) ."," The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <TARGET_CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <CITATION/> ) . The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,distinctive image features from scaleinvariant keypoints,2004,David G Lowe
1275,W02-0309,External_30988,[0],conclusion,"There has been some controversy , at least for simple stemmers <CITATION/> , about the effectiveness of morphological analysis for document retrieval <TARGET_CITATION/> ."," There has been some controversy , at least for simple stemmers <CITATION/> , about the effectiveness of morphological analysis for document retrieval <TARGET_CITATION/> . There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,ebf313a1dfa16d62e2b6cb804102151d33e6a7f8,Viewing morphology as an inference process,1993,Robert Krovetz
1276,J03-3004,External_4623,[5],introduction,"We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm <TARGET_CITATION/> .","We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the wordlevel lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm <TARGET_CITATION/> . We have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm <CITATION/>.That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the wordlevel lexicon into generalized marker chunks.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,22cd8ee12f34e75781dd34be2f9cd9f5a1ffe2fc,The weighted majority algorithm,1989,N. Littlestone; Manfred K. Warmuth
1277,W06-2807,External_98154,[0],related work,"Generally speaking , we find that the personal public diary metaphor behind blogs <TARGET_CITATION/> may bring to an unsatisfactory representation of the context .","While wikis have spread from a detailed design <CITATION/>, unfortunately blogs have not been designed under a model. So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal. Generally speaking , we find that the personal public diary metaphor behind blogs <TARGET_CITATION/> may bring to an unsatisfactory representation of the context . Generally speaking, we find that the personal public diary metaphor behind blogs <CITATION/> may bring to an unsatisfactory representation of the context. So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal. While wikis have spread from a detailed design <CITATION/>, unfortunately blogs have not been designed under a model.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,f9f9f50b962e505d52895724ca4491f5bd83dcb8,Genre Under Construction: The Diary on the Internet,2005,L. Mcneill
1278,J00-2004,External_42502,[0],related work,Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by <TARGET_CITATION/> ) ., Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by <TARGET_CITATION/> ) . Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by <CITATION/>.,38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,fc593d91a7974bb1d3fac1ffe47b787ce1853a88,But Dictionaries Are Data Too,1993,P. Brown; S. D. Pietra; V. D. Pietra; Meredith J. Goldsmith; Jan Hajic; R. Mercer; Surya Mohanty
1279,J01-4001,External_41829,[0],,"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; <TARGET_CITATION/> ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; <TARGET_CITATION/> ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) . A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,robust method of pronoun resolution using fulltext information,1994,Tetsuya Nasukawa
1280,D12-1037,W04-3250,[2],experiments,The significance testing is performed by paired bootstrap resampling <TARGET_CITATION/> .,We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <CITATION/> with modified KneserNey smoothing <CITATION/>. In our experiments the translation performances are measured by caseinsensitive BLEU4 metric <CITATION/> and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap resampling <TARGET_CITATION/> . The significance testing is performed by paired bootstrap resampling <CITATION/>. In our experiments the translation performances are measured by caseinsensitive BLEU4 metric <CITATION/> and we use mtevalv13a.pl as the evaluation tool. We train a 4gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits <CITATION/> with modified KneserNey smoothing <CITATION/>.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,cb826a3899752b796f14df1c50378c64954a6b0a,Statistical Significance Tests for Machine Translation Evaluation,2004,Philipp Koehn
1281,Q13-1020,P06-1121,[2],experiments,The system is implemented based on <TARGET_CITATION/> .,"To create the baseline system, we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased (HPB) system, and a syntaxaugmented MT (SAMT) 11 system <CITATION/> respectively. The translation system used for testing the effectiveness of our Utrees is our inhouse stringtotree system (abbreviated as s2t). The system is implemented based on <TARGET_CITATION/> . The system is implemented based on <CITATION/>. The translation system used for testing the effectiveness of our Utrees is our inhouse stringtotree system (abbreviated as s2t). To create the baseline system, we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased (HPB) system, and a syntaxaugmented MT (SAMT) 11 system <CITATION/> respectively.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,d01737b617acc555153f4660417908bf3971b1a5,Scalable Inference and Training of Context-Rich Syntactic Translation Models,2006,Michel Galley; Jonathan Graehl; Kevin Knight; D. Marcu; Steve DeNeefe; Wei Wang; I. Thayer
1282,N01-1001,External_3731,[0],experiments,IGEN uses standard chart generation techniques <TARGET_CITATION/> in its base generator to efficiently produce generation candidates .,The ranker scores these candidates according to a similarity metric which measures their distance to the elements in the instance base. The ranks are determined by the similarity to the closest instances and the highest ranked sentence is chosen as the final generation output. IGEN uses standard chart generation techniques <TARGET_CITATION/> in its base generator to efficiently produce generation candidates . IGEN uses standard chart generation techniques <CITATION/> in its base generator to efficiently produce generation candidates. The ranks are determined by the similarity to the closest instances and the highest ranked sentence is chosen as the final generation output. The ranker scores these candidates according to a similarity metric which measures their distance to the elements in the instance base.,3a01b4ffbdf23d73ec4fe54826d9462c8ded048e,Instance-Based Natural Language Generation,2001,S. Varges; C. Mellish,d29da6f2d904a47bc3569fbf708f03f094de3512,Contextual Chart Generation for Cyber Deception,2024,David D. Nguyen; David Liebowitz; Surya Nepal; S. Kanhere; Sharif Abuadbba
1283,D08-1034,J08-2004,[2],,"Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from <TARGET_CITATION/> .","The candidate feature templates include: Voice from <CITATION/>. Head word POS, Head Word of Prepositional Phrases, Constituent tree distance, from Pradhan etc. (2004). Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from <TARGET_CITATION/> . Position, subcat frame, phrase type, first word, last word, subcat frame+, predicate, path, head word and its POS, predicate + head word, predicate + phrase type, path to BA and BEI, verb class 3 , verb class + head word, verb class + phrase type, from <CITATION/>. Head word POS, Head Word of Prepositional Phrases, Constituent tree distance, from Pradhan etc. (2004). The candidate feature templates include: Voice from <CITATION/>.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,03541f4c7b737964289b3cb2cde4b6ac08a1c29d,Labeling Chinese Predicates with Semantic Roles,2008,Nianwen Xue
1284,J90-3003,External_24319,[0],introduction,"The psycholinguistic studies of <TARGET_CITATION/> , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency ."," The psycholinguistic studies of <TARGET_CITATION/> , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . The psycholinguistic studies of <CITATION/>, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,3185f75000efa42a0ce91fecf9e03de3d5c66bcf,Performance structures: A psycholinguistic and linguistic appraisal,1983,J. Gee; F. Grosjean
1286,P97-1063,External_79712,[0],,<TARGET_CITATION/> reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .,"translation lexicons with 94% precision and 30% recall, when trained on French/English software manuals totaling about 400,000 words. The model was also used to induce a translation lexicon from a 6200word corpus of French/English weather reports. <TARGET_CITATION/> reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . <CITATION/> reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. The model was also used to induce a translation lexicon from a 6200word corpus of French/English weather reports. translation lexicons with 94% precision and 30% recall, when trained on French/English software manuals totaling about 400,000 words.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,semiautomatic acquisition of domainspecific translation lexiconsquot,1997,personal communication Nasr
1287,J05-3003,External_13800,[0],related work,<TARGET_CITATION/> describe a simple tool which uses finegrained rules to identify the arguments of verb occurrences in the PennII Treebank .,"The extensive evaluation carried out by Schulte im Walde will be discussed in greater detail in Section 6. Approaches using treebankbased data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. <TARGET_CITATION/> describe a simple tool which uses finegrained rules to identify the arguments of verb occurrences in the PennII Treebank . <CITATION/> describe a simple tool which uses finegrained rules to identify the arguments of verb occurrences in the PennII Treebank. Approaches using treebankbased data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. The extensive evaluation carried out by Schulte im Walde will be discussed in greater detail in Section 6.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,77cb4ecff20216ae8154765186233f1176a78f90,Identifying Verb Arguments and their Syntactic Function in the Penn Treebank,2002,A. Kinyon; C. Prolo
1288,P10-4003,P02-1011,[4],experiments,"The contextual interpreter then uses a reference resolution approach similar to <TARGET_CITATION/> , and an ontology mapping mechanism <CITATION/> to produce a domainspecific semantic representation of the student 's output .","We use the TRIPS dialogue parser <CITATION/> to parse the utterances. The parser provides a domainindependent semantic representation including highlevel word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to <TARGET_CITATION/> , and an ontology mapping mechanism <CITATION/> to produce a domainspecific semantic representation of the student 's output . The contextual interpreter then uses a reference resolution approach similar to <CITATION/>, and an ontology mapping mechanism <CITATION/> to produce a domainspecific semantic representation of the student's output. The parser provides a domainindependent semantic representation including highlevel word senses and semantic role labels. We use the TRIPS dialogue parser <CITATION/> to parse the utterances.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,6226b2787e03d8c51f94ffaa250f08a74bcc5292,Resolving Pronominal Reference to Abstract Entities,2002,D. Byron
1289,W06-1639,External_641,[0],related work,<TARGET_CITATION/> maintains a survey of this area .,interdocument references in the form of hyperlinks <CITATION/>. Notable early papers on graphbased semisupervised learning include <CITATION/>. <TARGET_CITATION/> maintains a survey of this area . <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>. interdocument references in the form of hyperlinks <CITATION/>.,dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,semisupervised learning literature survey computer sciences,2005,J Zhu
1292,D10-1052,D09-1024,[4],related work,"With respect to the focus on function words , our reordering model is closely related to the UALIGN system <TARGET_CITATION/> .","Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words , our reordering model is closely related to the UALIGN system <TARGET_CITATION/> . With respect to the focus on function words, our reordering model is closely related to the UALIGN system <CITATION/>. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items.",c97c609c34db8787505d83dfa03077d4813c7f19,Discriminative Word Alignment with a Function Word Reordering Model,2010,Hendra Setiawan; Christopher Dyer; P. Resnik,9e4acda231ca052623c8b9044bb907469f1b33c4,Improved Word Alignment with Statistics and Linguistic Heuristics,2009,U. Hermjakob
1294,D08-1007,N07-1071,[0],related work,Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules <TARGET_CITATION/> .,"<CITATION/> compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and <CITATION/>'s informationtheoretic metric work best. Similaritysmoothed models are simple to compute, potentially adaptable to new domains, and require no manuallycompiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules <TARGET_CITATION/> . Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules <CITATION/>. Similaritysmoothed models are simple to compute, potentially adaptable to new domains, and require no manuallycompiled resources such as WordNet. <CITATION/> compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and <CITATION/>'s informationtheoretic metric work best.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,26e17398019d8342e2ae27712af3a147355a141a,ISP: Learning Inferential Selectional Preferences,2007,Patrick Pantel; Rahul Bhagat; Bonaventura Coppola; Timothy Chklovski; E. Hovy
1295,D13-1115,D12-1130,[0],method,Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together <TARGET_CITATION/> .,Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <CITATION/>. Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together <TARGET_CITATION/> . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together <CITATION/>. Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <CITATION/>.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,d4149dbef949644dad4833012e2def98529c0241,Grounded Models of Semantic Representation,2012,Carina Silberer; Mirella Lapata
1296,J00-2001,External_80545,[0],,Something like this approach is in fact used in some systems <TARGET_CITATION/> .,"If that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to test various features of the information being expressed. The planner could supply whatever information is needed to drive the network.' Something like this approach is in fact used in some systems <TARGET_CITATION/> . Something like this approach is in fact used in some systems <CITATION/>. The planner could supply whatever information is needed to drive the network.' If that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to test various features of the information being expressed.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,,generating natural language linder pragmatic constraints lawrence erlbaum,1988,Eduard H Hovy
1297,W06-2807,External_98154,[0],,"The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre <TARGET_CITATION/> .","It resembles the English word novel' and the French word nuovelle'. We believe that this name is clearly understable to every people educated in a Europeanbased culture, and this is why we have chosen it. The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre <TARGET_CITATION/> . The emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre <CITATION/>. We believe that this name is clearly understable to every people educated in a Europeanbased culture, and this is why we have chosen it. It resembles the English word novel' and the French word nuovelle'.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,f9f9f50b962e505d52895724ca4491f5bd83dcb8,Genre Under Construction: The Diary on the Internet,2005,L. Mcneill
1298,A00-1004,External_32171,[5],method,"For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in <TARGET_CITATION/> .","Despite their large length difference, the two 0002 sentences are still aligned as a 11 pair, because the sentences in the following 4 alignments (0003 0003; 0004 0004, 0005; 0005 0006; 0006 0007) have rather similar HTML markups and are taken by the program to be the most likely alignments. Beside HTML markups, other criteria may also be incorporated. For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in <TARGET_CITATION/> . For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in <CITATION/>. Beside HTML markups, other criteria may also be incorporated. Despite their large length difference, the two 0002 sentences are still aligned as a 11 pair, because the sentences in the following 4 alignments (0003 0003; 0004 0004, 0005; 0005 0006; 0006 0007) have rather similar HTML markups and are taken by the program to be the most likely alignments.",14ffbd58082d1197ea454ec9162b5cfd36cac9f9,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,2000,Jiang Chen; Jian-Yun Nie,,aligning a parallel englishchinese corpus statistically with lexical criteria,1994,Dekai Wu
1300,D12-1084,P10-1100,[3],related work,We take some core ideas from our previous work on mining script information <TARGET_CITATION/> .,"Our approach for sentential paraphrase extraction is related to the one introduced by <CITATION/>, who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information <TARGET_CITATION/> . We take some core ideas from our previous work on mining script information <CITATION/>. However, they use MSA at the sentence level rather than at the discourse level. Our approach for sentential paraphrase extraction is related to the one introduced by <CITATION/>, who also employ multiple sequence alignment (MSA).",a59a0185e02bf46b9f03274da718e87a24e7b8a8,Using Discourse Information for Paraphrase Extraction,2012,Michaela Regneri; Rui Wang,13cab51c98d2dcb94038d6dcd73cff2858dfe7c4,Learning Script Knowledge with Web Experiments,2010,Michaela Regneri; Alexander Koller; Manfred Pinkal
1301,J06-2002,External_942,[0],experiments,"While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."," While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . While IA is generally thought to be consistent with findings on human language production <CITATION/>, the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,24f33d218ce41311ca9f81030323bb445c188be1,Incremental speech production and referential overspecification,1989,T. Pechmann
1302,W06-2933,External_3360,[0],experiments,"Typical examples are Bulgarian <TARGET_CITATION/> , Chinese <CITATION/> , Danish <CITATION/> , and Swedish <CITATION/> .","before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. Typical examples are Bulgarian <TARGET_CITATION/> , Chinese <CITATION/> , Danish <CITATION/> , and Swedish <CITATION/> . Typical examples are Bulgarian <CITATION/>, Chinese <CITATION/>, Danish <CITATION/>, and Swedish <CITATION/>. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,,design and implementation of the bulgarian hpsgbased treebank,2005,K Simov; P Osenova; A Simov; M Kouylekov
1303,J09-4010,External_23257,[4],method," A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours <TARGET_CITATION/> ."," Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>.  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>. A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours <TARGET_CITATION/> .  A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours <CITATION/>.  Only an automatic evaluation was performed, which relied on having model responses <CITATION/>.  Only qualitative observations of the responses were reported (no formal evaluation was performed) <CITATION/>.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,an intelligent discussionbot for answering student queries in threaded discussions,2006,D Feng; E Shaw; J Kim; E Hovy
1304,J90-3003,External_2221,[4],introduction,"Our work on the prosodic phrase status of clause final prepositional phrases , which we discuss below , suggests the existence of a discourseneutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase .3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in <TARGET_CITATION/> , which  are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of nullhypothesis patterns that emerge when Computational Linguistics Volume 16 , Number 3 , September 1990 157 J. Bachenko and E. Fitzpatrick DiscourseNeutral Prosodic Phrasing in English there is no good reason to take some other option '' ( p. 251 ) .","Thus both of these efforts leave open the question as to whether discourse features completely determine prosodic phrasing or are a complement to some more basic set of determinants, syntactic and/or phonological. In other words, when prosodic features that reflect facts of the discourse are removed, is there a residual, neutral phrasing? Our work on the prosodic phrase status of clause final prepositional phrases , which we discuss below , suggests the existence of a discourseneutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase .3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in <TARGET_CITATION/> , which  are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of nullhypothesis patterns that emerge when Computational Linguistics Volume 16 , Number 3 , September 1990 157 J. Bachenko and E. Fitzpatrick DiscourseNeutral Prosodic Phrasing in English there is no good reason to take some other option '' ( p. 251 ) . Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourseneutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in <CITATION/>, which are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of nullhypothesis patterns that emerge when Computational Linguistics Volume 16, Number 3, September 1990 157 J. Bachenko and E. Fitzpatrick DiscourseNeutral Prosodic Phrasing in English there is no good reason to take some other option'' (p. 251). In other words, when prosodic features that reflect facts of the discourse are removed, is there a residual, neutral phrasing? Thus both of these efforts leave open the question as to whether discourse features completely determine prosodic phrasing or are a complement to some more basic set of determinants, syntactic and/or phonological.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,b8ec853894551c0e7a822df50dc04eccd613d46f,On stress and linguistic rhythm,1977,M. Liberman; A. Prince
1305,D13-1115,External_8378,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,42f3a746319ca6e6a4448b84ad31459eeaa5bce1,Combining Feature Norms and Text Data with Topic Models,None,M. Steyvers
1306,J06-2002,External_74967,[0],experiments,"When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways <TARGET_CITATION/> .","9.3.1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways <TARGET_CITATION/> . When objects are compared in terms of several dimensions, these dimensions can be weighed in different ways <CITATION/>. 9.3.1 Combinations of Adjectives.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,b19be26f94641d9576eda6bc78a6adccb606bfff,Games and Information: An Introduction to Game Theory,2006,E. Rasmusen
1307,E03-1005,P00-1008,[0],,"Most DOP models , such as in <TARGET_CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."," Most DOP models , such as in <TARGET_CITATION/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . Most DOP models, such as in <CITATION/>, Sima'an (2000) and <CITATION/>, use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,d64be13007f11bc9650b399530f5577ed8679a96,Tree-gram Parsing: Lexical Dependencies and Structural Relations,2000,K. Sima'an
1308,J06-2002,External_2854,[0],experiments,"While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."," While IA is generally thought to be consistent with findings on human language production <TARGET_CITATION/> , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates . While IA is generally thought to be consistent with findings on human language production <CITATION/>, the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,2dd7698124dae613def704272629fe03404ce62b,Speaking: From Intention to Articulation,1989,W. Levelt
1310,W01-1510,External_15846,[0],introduction,The RenTAL system automatically converts an FBLTAG grammar into a strongly equivalent HPSGstyle grammar <TARGET_CITATION/> .,This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar (FBLTAG1) <CITATION/> and HeadDriven Phrase Structure Grammar (HPSG) <CITATION/> by a method of grammar conversion. The RenTAL system automatically converts an FBLTAG grammar into a strongly equivalent HPSGstyle grammar <TARGET_CITATION/> . The RenTAL system automatically converts an FBLTAG grammar into a strongly equivalent HPSGstyle grammar <CITATION/>. This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar (FBLTAG1) <CITATION/> and HeadDriven Phrase Structure Grammar (HPSG) <CITATION/> by a method of grammar conversion.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,grammar conversion from fbltag to hpsg,2001,Naoki Yoshinaga; Yusuke Miyao
1311,D08-1010,P06-1077,[3],experiments,The features can be easily obtained by modifying the TAT extraction algorithm described in <TARGET_CITATION/> .,"Furthermore, we also build MERS models for lexicalized and unlexicalized source trees. Note that for lexicalized tree, features do not include the information of subtrees since there is no nonterminals. The features can be easily obtained by modifying the TAT extraction algorithm described in <TARGET_CITATION/> . The features can be easily obtained by modifying the TAT extraction algorithm described in <CITATION/>. Note that for lexicalized tree, features do not include the information of subtrees since there is no nonterminals. Furthermore, we also build MERS models for lexicalized and unlexicalized source trees.",5a11dcdb5076fb927d627e6ef0e90faed33dfb27,Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation,2008,Qun Liu; Zhongjun He; Yang Liu; Shouxun Lin,7e982f360b44094552264010781a476d85ac78a7,Tree-to-String Alignment Template for Statistical Machine Translation,2006,Yang Liu; Qun Liu; Shouxun Lin
1312,J05-3003,External_13797,[0],related work,<TARGET_CITATION/> present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank <CITATION/> .,"This is achieved using three different hypothesis tests: BHT, loglikelihood ratio, and tscore. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). <TARGET_CITATION/> present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank <CITATION/> . <CITATION/> present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank <CITATION/>. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). This is achieved using three different hypothesis tests: BHT, loglikelihood ratio, and tscore.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,974c95c5ced94fd18c3d2bd7afb2605f8a09f574,Automatic Extraction of Subcategorization Frames from the Bulgarian Tree Bank,2004,Svetoslav Marinov; C. Hemming
1313,W06-1705,J03-3001,[0],related work,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( <CITATION/> , 2004b ) and received a special issue of the journal Computational Linguistics <TARGET_CITATION/> .","Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times ( <CITATION/> , 2004b ) and received a special issue of the journal Computational Linguistics <TARGET_CITATION/> . The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,e965483087d63b9ca957fa04c6e639d945b3a49b,Introduction to the Special Issue on the Web as Corpus,2003,A. Kilgarriff; G. Grefenstette
1314,W04-1805,External_52784,[0],method,"ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) <TARGET_CITATION/> , which infers general morphosyntactic patterns from a set of examples ( this set is noted E + hereafter ) and counterexamples ( E  ) of the elements one","ASARES is presented in detail in <CITATION/>. We simply give a short account of its basic principles herein. ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) <TARGET_CITATION/> , which infers general morphosyntactic patterns from a set of examples ( this set is noted E + hereafter ) and counterexamples ( E  ) of the elements one ASARES is based on a Machine Learning technique, Inductive Logic Programming (ILP) <CITATION/>, which infers general morphosyntactic patterns from a set of examples (this set is noted E+ hereafter) and counterexamples (E) of the elements oneWe simply give a short account of its basic principles herein. ASARES is presented in detail in <CITATION/>.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,f47f99a16f60d649f7d3c1c6a26c6eff68e2e3da,Inductive Logic Programming: Theory and Methods,1994,S. Muggleton; L. D. Raedt
1315,W06-1705,External_3869,[0],related work,<TARGET_CITATION/> built a corpus by iteratively searching Google for a small set of seed terms .,Studies have used several different methods to mine web data. <CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler. <TARGET_CITATION/> built a corpus by iteratively searching Google for a small set of seed terms . <CITATION/> built a corpus by iteratively searching Google for a small set of seed terms. <CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler. Studies have used several different methods to mine web data.,a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,f185b09d262700f5b03911e90e94883103ba504f,BootCaT: Bootstrapping Corpora and Terms from the Web,2004,Marco Baroni; Silvia Bernardini
1316,J87-3002,External_11433,[3],,"Expanding on a suggestion of <TARGET_CITATION/> , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .","In any subcategorisation frame which involves a predicate complement there will be a nontransparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of <TARGET_CITATION/> , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . Expanding on a suggestion of <CITATION/>, we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it. In these situations the parser can use the semantic type of the verb to compute this relationship. In any subcategorisation frame which involves a predicate complement there will be a nontransparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,exploiting a large dictionary data base,1982,Archibal Michiels
1319,P97-1063,External_15612,[0],introduction,"With the exception of <CITATION/> , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts <TARGET_CITATION/> .","The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy.2 Cooccurrence With the exception of <CITATION/> , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts <TARGET_CITATION/> . With the exception of <CITATION/>, previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts <CITATION/>. 2 CooccurrenceThe hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,4fe2a45babab10c1bfae05d2464363f4e52bbaf9,A Program for Aligning Sentences in Bilingual Corpora,1993,W. Gale; Kenneth Ward Church
1320,D08-1022,P08-1023,[3],introduction,"Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a stateoftheart treetostring system <TARGET_CITATION/> , which is also 0.5 points better than ( and twice as fast as ) extracting on 30best parses .","also inefficient to extract rules separately from each of these very similar trees (or from the crossproduct of k2 similar treepairs in treetotree models). We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a stateoftheart treetostring system <TARGET_CITATION/> , which is also 0.5 points better than ( and twice as fast as ) extracting on 30best parses . Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a stateoftheart treetostring system <CITATION/>, which is also 0.5 points better than (and twice as fast as) extracting on 30best parses. We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. also inefficient to extract rules separately from each of these very similar trees (or from the crossproduct of k2 similar treepairs in treetotree models).",90840ced4df35ddbf5d85282a717f949ea9b3dcf,Forest-based Translation Rule Extraction,2008,Haitao Mi; Liang Huang,c0a785bc32a724e70840a71e1f60ac5901775ae2,Forest-Based Translation,2008,Haitao Mi; Liang Huang; Qun Liu
1321,E14-1023,D10-1100,[3],related work,"Our approach to extract and classify social events builds on our previous work <TARGET_CITATION/> , which in turn builds on work from the relation extraction community <CITATION/> .","(6) Take it,'' said Emma, smiling, and pushing the paper towards Harriet it is for you. Take your own.'' Our approach to extract and classify social events builds on our previous work <TARGET_CITATION/> , which in turn builds on work from the relation extraction community <CITATION/> . Our approach to extract and classify social events builds on our previous work <CITATION/>, which in turn builds on work from the relation extraction community <CITATION/>. Take your own.'' (6) Take it,'' said Emma, smiling, and pushing the paper towards Harriet it is for you.",a2cc50193235214c556cfa9d954292624ee734d7,Frame Semantic Tree Kernels for Social Network Extraction from Text,2014,Apoorv Agarwal; Sriramkumar Balasubramanian; Anup Kotalwar; Jiehan Zheng; Owen Rambow,330d82cedd1567515d42163b766197944adf6647,Automatic Detection and Classification of Social Events,2010,Apoorv Agarwal; Owen Rambow
1322,W06-1639,P05-1015,[0],related work,Previous sentimentanalysis work in different domains has considered interdocument similarity <TARGET_CITATION/> or explicit,"We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. Previous sentimentanalysis work in different domains has considered interdocument similarity <TARGET_CITATION/> or explicit Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitRelationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,6af58c061f2e4f130c3b795c21ff0c7e3903278f,Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales,2005,B. Pang; Lillian Lee
1323,W00-1017,C94-2164,[5],conclusion,"For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar <TARGET_CITATION/> .","Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar <TARGET_CITATION/> . For example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar <CITATION/>. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. Recording some dialogue history is also possible.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,8bfd69676f93f6aac3681d803001f3842a4629cb,A Grammar and a Parser for Spontaneous Speech,1994,Mikio Nakano; Akira Shimazu; Kiyoshi Kogure
1324,Q13-1020,D12-1021,[4],related work,<TARGET_CITATION/> employed a Bayesian method to learn discontinuous SCFG rules .,"Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrasebased and hierarchical phrasebased system <CITATION/>. <TARGET_CITATION/> employed a Bayesian method to learn discontinuous SCFG rules . <CITATION/> employed a Bayesian method to learn discontinuous SCFG rules. The obtained SCFG is further used in a phrasebased and hierarchical phrasebased system <CITATION/>. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,32cf5f31034c650680ae559568ca4b0858aa8863,A Bayesian Model for Learning SCFGs with Discontiguous Rules,2012,A. Levenberg; Chris Dyer; Phil Blunsom
1326,K15-1003,W14-1615,[2],experiments,We use the same splits as <TARGET_CITATION/> .,"We evaluated on the English CCGBank <CITATION/>, which is a transformation of the Penn Treebank <CITATION/>; the CTBCCG <CITATION/> transformation of the Penn Chinese Treebank <CITATION/>; and the CCGTUT corpus <CITATION/>, built from the TUT corpus of Italian text <CITATION/>. Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as <TARGET_CITATION/> . We use the same splits as <CITATION/>. Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We evaluated on the English CCGBank <CITATION/>, which is a transformation of the Penn Treebank <CITATION/>; the CTBCCG <CITATION/> transformation of the Penn Chinese Treebank <CITATION/>; and the CCGTUT corpus <CITATION/>, built from the TUT corpus of Italian text <CITATION/>.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,05bbf7436bfcb1e49e1132c16f25706b61de6c93,Weakly-Supervised Bayesian Learning of a CCG Supertagger,2014,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith
1327,P10-2059,External_88751,[4],,"These results are slightly worse than those obtained in previous studies using the same annotation scheme <TARGET_CITATION/> , but are still sat ","The figures obtained are given in Table 3. feature Cohen's k corrected k face segment 69.89 91.37 face annotate 71.53 94.25 head mov segment 71.21 91.75 head mov annotate 71.65 95.14 These results are slightly worse than those obtained in previous studies using the same annotation scheme <TARGET_CITATION/> , but are still sat These results are slightly worse than those obtained in previous studies using the same annotation scheme <CITATION/>, but are still satfeature Cohen's k corrected k face segment 69.89 91.37 face annotate 71.53 94.25 head mov segment 71.21 91.75 head mov annotate 71.65 95.14The figures obtained are given in Table 3.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,c276dbceed4c261ce3ff1b7865fd72d106ad1b38,Distinguishing the Communicative Functions of Gestures,2008,Kristiina Jokinen; Costanza Navarretta; Patrizia Paggio
1328,J90-3003,External_2548,[0],introduction,"In <TARGET_CITATION/> , this flattening process is not part of the grammar .","To account for such mismatches, readjustment rules'' that change constituent structure by adjoining each embedded sentence to the node dominating it have been posited. The result is a flattened structure that more accurately reflects the prosodic phrasing. In <TARGET_CITATION/> , this flattening process is not part of the grammar . In <CITATION/>, this flattening process is not part of the grammar. The result is a flattened structure that more accurately reflects the prosodic phrasing. To account for such mismatches, readjustment rules'' that change constituent structure by adjoining each embedded sentence to the node dominating it have been posited.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,2ff60c8f2df9a57b5dfba38e9be6d6401d88a1e9,The Sound Pattern of English,1968,Noam Chomsky; M. Halle
1330,P13-3018,External_90452,[0],introduction,"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( <TARGET_CITATION/> ; Grainger , et al. , 1991 ; <CITATION/> ) .","Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( <TARGET_CITATION/> ; Grainger , et al. , 1991 ; <CITATION/> ) . There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (<CITATION/>; Grainger, et al., 1991; <CITATION/>). Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,2f96ec3c3c9094c411d78fa5a12ad926a4fdd994,What can we learn from the morphology of Hebrew? A masked-priming investigation of morphological representation.,1997,Ram Frost; Kenneth I. Forster; Avital Deutsch
1332,A00-1020,W99-0104,[2],,"For this research , we used a coreference resolution system ( <TARGET_CITATION/> ) that implements different sets of heuristics corresponding to various forms of coreference .","Nevertheless, recent results show that knowledgepoor methods perform with amazing accuracy (cfXXX <CITATION/>). For example, CogNIAC <CITATION/>, a system based on seven ordered heuristics, generates highprecision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system <TARGET_CITATION/> that implements different sets of heuristics corresponding to various forms of coreference . For this research, we used a coreference resolution system (<CITATION/>) that implements different sets of heuristics corresponding to various forms of coreference. For example, CogNIAC <CITATION/>, a system based on seven ordered heuristics, generates highprecision resolution (over 90%) for some cases of pronominal reference. Nevertheless, recent results show that knowledgepoor methods perform with amazing accuracy (cfXXX <CITATION/>).",76894392818a9a360feaf2f1a797bbe1eaac82b0,Multilingual Coreference Resolution,2000,S. Harabagiu; Steven J. Maiorano,df9490b518e726a96879f16c92d46e9d4f883c1b,Knowledge-Lean Coreference Resolution and its Relation to Textual Cohesion and Coherence,1999,S. Harabagiu; Steven J. Maiorano
1333,D11-1138,P07-1036,[4],related work,"The work that is most similar to ours is that of <TARGET_CITATION/> , who introduced the Constraint Driven Learning algorithm ( CODL ) .","In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives. The work that is most similar to ours is that of <TARGET_CITATION/> , who introduced the Constraint Driven Learning algorithm ( CODL ) . The work that is most similar to ours is that of <CITATION/>, who introduced the Constraint Driven Learning algorithm (CODL). Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,dc9f999632bf6d82882cc54e2d2cc2d32eaed932,Guiding Semi-Supervision with Constraint-Driven Learning,2007,Ming-Wei Chang; Lev-Arie Ratinov; D. Roth
1334,P02-1001,External_11573,[0],,"For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX <TARGET_CITATION/> , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11 What to optimize ?","These are assumed to be independent random samples from a joint distribution of the form fe(x, y); the goal is to recover the true 0. Samples need not be fully observed (partly supervised training): thus xi C E*, yi C A* may be given as regular sets in which input and output were observed to fall. For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX <TARGET_CITATION/> , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11 What to optimize ? For example, in ordinary HMM training, xi = E* and represents a completely hidden state sequence (cfXXX <CITATION/>, who allows any regular set), while yi is a single string representing a completely observed emission sequence.11 What to optimize? Samples need not be fully observed (partly supervised training): thus xi C E*, yi C A* may be given as regular sets in which input and output were observed to fall. These are assumed to be independent random samples from a joint distribution of the form fe(x, y); the goal is to recover the true 0.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,hidden markov models with finite state supervision in,1998,E Ristad
1336,D08-1006,P07-1010,[1],experiments,"As shown in <TARGET_CITATION/> , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .","The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in <TARGET_CITATION/> , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . As shown in <CITATION/>, using this representation, a linear classifier cannot distinguish sentences sampled from a trigram and real sentences. This code was extensively optimized to take advantage of the very sparse sentence representation described above. The code for the classifier was generously provided by Daisuke Okanohara.",00dc508fdf5dcbf78bee0ea779aad408830c20e2,Refining Generative Language Models using Discriminative Learning,2008,Ben Sandbank,060d5bb21f318efd785e86aa50cb97aed090a0fb,A discriminative language model with pseudo-negative samples,2007,Daisuke Okanohara; Junichi Tsujii
1337,W06-1104,External_32017,[0],,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <TARGET_CITATION/> , informationbased <CITATION/> or distributional <CITATION/> ."," Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionarybased <CITATION/> , ontologybased <TARGET_CITATION/> , informationbased <CITATION/> or distributional <CITATION/> . Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionarybased <CITATION/>, ontologybased <CITATION/>, informationbased <CITATION/> or distributional <CITATION/>.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,wordnet an electronic lexical database chapter combining local context and wordnet similarity for word sense identification,1998,Claudia Leacock; Martin Chodorow
1338,N04-2004,External_1172,[0],,"This framework , where the  semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by <TARGET_CITATION/> ) , among many others .","(7) JflatK = [state flat]In this case, the complete event structure of a word can be compositionally derived from its component morphemes. This framework , where the  semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by <TARGET_CITATION/> ) , among many others . This framework, where the semantic load'' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by <CITATION/>, among many others. In this case, the complete event structure of a word can be compositionally derived from its component morphemes. (7) JflatK = [state flat]",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,259d0304adcb49e40436137684b78a80c9ef097b,The Generative Lexicon,1991,J. Pustejovsky
1339,J05-3003,External_13650,[0],related work,<TARGET_CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank .,"This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. <TARGET_CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank . <CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,d4943720cc393626ed1ff87e6bad9622e69cb284,Extracting Tree Adjoining Grammars from Bracketed Corpora,2009,Fei Xia; Chung-hye Han; A. Joshi; Martha Palmer; C. Prolo; Anoop Sarkar
1340,J92-1004,External_4900,[0],,"To a first approximation , a CURRENTFOCUS reaches only nodes that are ccommanded <TARGET_CITATION/> by its generator .","The CURRENTFOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree. That is to say, the CURRENTFOCUS is a feature, like verbmode, that is blocked when an [end] node is encountered. To a first approximation , a CURRENTFOCUS reaches only nodes that are ccommanded <TARGET_CITATION/> by its generator . To a first approximation, a CURRENTFOCUS reaches only nodes that are ccommanded <CITATION/> by its generator. That is to say, the CURRENTFOCUS is a feature, like verbmode, that is blocked when an [end] node is encountered. The CURRENTFOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,on whmovementquot in formal syntax edited by,1977,Noam Chomsky
1341,J05-4005,J96-3004,[4],related work,"A previous work along this line is <TARGET_CITATION/> , which is based on weighted finitestate transducers ( FSTs ) .","We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is <TARGET_CITATION/> , which is based on weighted finitestate transducers ( FSTs ) . A previous work along this line is <CITATION/>, which is based on weighted finitestate transducers (FSTs). We propose a unified approach that solves both problems simultaneously. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.",5c4d9251b7e27ff058e8469e492b39c79e6f9fbe,Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach,2005,Jianfeng Gao; Mu Li; C. Huang; Andi Wu,54e985fb5b0af022ed4ef6fd19f920ca3d79b51e,A Stochastic Finite-State Word-Segmentation Algorithm for Chinese,1996,R. Sproat; Chilin Shih; W. Gale; Nancy Chang
1342,W06-3813,External_48311,[0],related work,"Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <TARGET_CITATION/> .","The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <TARGET_CITATION/> . Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/>. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,1ae5c1646ea445a670fe6cc8bf72b589dd9f6e5c,Semantic Role Labeling Using Different Syntactic Views,2005,Sameer Pradhan; Wayne H. Ward; K. Hacioglu; James H. Martin; Dan Jurafsky
1343,P00-1006,External_3692,[0],,<TARGET_CITATION/> describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .,"Since MEMD models are trained by finding the set of feature weights which maximizes the likelihood of the training corpus, it is natural to rate features according to how much they contribute to this likelihood. A powerful strategy for using gains is to build a model iteratively by adding at each step the feature which gives the highest gain with respect to those already added. <TARGET_CITATION/> describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t . <CITATION/> describe an efficient algorithm for accomplishing this in which approximations to Pst (TIS) are computed in parallel for all (new) features ft by holding all weights in the existing model fixed and optimizing only over a8t. A powerful strategy for using gains is to build a model iteratively by adding at each step the feature which gives the highest gain with respect to those already added. Since MEMD models are trained by finding the set of feature weights which maximizes the likelihood of the training corpus, it is natural to rate features according to how much they contribute to this likelihood.",d60cf1a4c7a35f859e2e203d27f3ec71994e2e3e,A Maximum Entropy/Minimum Divergence Translation Model,2000,George F. Foster,fb486e03369a64de2d5b0df86ec0a7b55d3907db,A Maximum Entropy Approach to Natural Language Processing,1996,Adam L. Berger; S. D. Pietra; V. D. Pietra
1344,W10-4005,W10-3908,[1],introduction,"As suggested in <TARGET_CITATION/> this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .","So far, we always computed translations to single source words. However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the productofranks algorithm. As suggested in <TARGET_CITATION/> this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks . As suggested in <CITATION/> this can be done by looking up the ranks of each of the four given words (i.e. the words occurring in a particular word equation) within the association vector of a translation candidate, and by multiplying these ranks. However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the productofranks algorithm. So far, we always computed translations to single source words.",305ea15f21fb788f20d9b56cfdad590dcc62200c,The Noisier the Better: Identifying Multilingual Word Translations Using a Single Monolingual Corpus,2010,R. Rapp; M. Zock; A. Trotman; Yue Xu,b1b748fd1f1fa3485be04077b1c62a38b99f1cc3,Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation,2010,R. Rapp; M. Zock
1345,J97-4003,External_12787,[0],introduction,32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints <TARGET_CITATION/> can be used to circumvent constraint propagation .,Applying constraint propagation to the extended lexical entry of Figure 17 yields the result shown in Figure 23. The information common to all solutions to the interaction call is lifted up into the lexical entry and becomes available upon lexical lookup. 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints <TARGET_CITATION/> can be used to circumvent constraint propagation . 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints <CITATION/> can be used to circumvent constraint propagation. The information common to all solutions to the interaction call is lifted up into the lexical entry and becomes available upon lexical lookup.Applying constraint propagation to the extended lexical entry of Figure 17 yields the result shown in Figure 23.,d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,f929ea7f4b5c68e8d86675be7d966c4e2ed040eb,An Overview of Disjunctive Constraint Satisfaction,1989,John T. Maxwell; R. Kaplan
1347,J00-2001,P83-1012,[0],,"These include devices such as interleaving the components <CITATION/> , backtracking on failure <CITATION/> , allowing the linguistic component to interrogate the planner <TARGET_CITATION/> , and Hovy 's notion of restrictive ( i.e. , bottomup ) planning ( Hovy 1988a , 1988c ) .","Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity. There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components <CITATION/> , backtracking on failure <CITATION/> , allowing the linguistic component to interrogate the planner <TARGET_CITATION/> , and Hovy 's notion of restrictive ( i.e. , bottomup ) planning ( Hovy 1988a , 1988c ) . These include devices such as interleaving the components <CITATION/>, backtracking on failure <CITATION/>, allowing the linguistic component to interrogate the planner <CITATION/>, and Hovy's notion of restrictive (i.e., bottomup) planning (Hovy 1988a, 1988c). There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,98c8a6d5fd98e37acaf5a564b905f54ad7a646dc,An Overview of the Nigel Text Generation Grammar,1983,W. Mann
1349,W01-1510,External_331,[4],experiments,"In Table 2 , lem refers to the LTAG parser <TARGET_CITATION/> , ANSI C implementation of the twophase parsing algorithm that performs the head corner parsing ( van <CITATION/> ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .","This result empirically attested the strong equivalence of our algorithm. Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2 , lem refers to the LTAG parser <TARGET_CITATION/> , ANSI C implementation of the twophase parsing algorithm that performs the head corner parsing ( van <CITATION/> ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) . In Table 2, lem refers to the LTAG parser <CITATION/>, ANSI C implementation of the twophase parsing algorithm that performs the head corner parsing (van <CITATION/>) without features (phase 1), and then executes feature unification (phase 2). Table 2 shows the average parsing time with the LTAG and HPSG parsers. This result empirically attested the strong equivalence of our algorithm.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,1ce9b606f14ee1d394f6f7faa9175e6067ab2bba,Some Experiments on Indicators of Parsing Complexity for Lexicalized Grammars,2000,Anoop Sarkar; Fei Xia; A. Joshi
1350,Q13-1020,D08-1092,[4],related work,<TARGET_CITATION/> et al. ( 2010 ) focused on joint parsing and alignment .,"This study differs from their work because we concentrate on constructing tree structures for treebased translation models. Our Utrees are learned based on STSG, which is more appropriate for treebased translation models than SCFG. <TARGET_CITATION/> focused on joint parsing and alignment . <CITATION/> focused on joint parsing and alignment. Our Utrees are learned based on STSG, which is more appropriate for treebased translation models than SCFG. This study differs from their work because we concentrate on constructing tree structures for treebased translation models.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,b74e96dc254d8e1bc79a136bfdf01668c0acc281,Two Languages are Better than One (for Syntactic Parsing),2008,David Burkett; D. Klein
1351,J05-3003,External_198,[0],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ <TARGET_CITATION/> ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ <TARGET_CITATION/> ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,e17117dbee804d7d177d8eb9fadf0bda1ebc4d22,Lexical Functional Grammar A Formal System for Grammatical Representation,2004,Ronald M. Kaplan
1352,W06-2807,External_91603,[0],introduction,"For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation  i.e. literary criticism  unlike in the previous times <TARGET_CITATION/> .","Terms as chapter', page' or footnote' simply become meaningless in the new texts, or they highly change their meaning. When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation  i.e. literary criticism  unlike in the previous times <TARGET_CITATION/> . For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation  i.e. literary criticism  unlike in the previous times <CITATION/>. When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. Terms as chapter', page' or footnote' simply become meaningless in the new texts, or they highly change their meaning.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,a3ef81c4b1340cccd7001ca6c73cbff23235e77a,The Printing Revolution in Early Modern Europe,1984,E. Eisenstein
1353,W06-3309,External_2736,[0],introduction,"Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) <CITATION/> , and the availability of software that leverages this knowledge  MetaMap <TARGET_CITATION/> for concept identification and SemRep <CITATION/> for relation extraction  provide a foundation for studying the role of semantics in various tasks .","Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine(NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) <CITATION/> , and the availability of software that leverages this knowledge  MetaMap <TARGET_CITATION/> for concept identification and SemRep <CITATION/> for relation extraction  provide a foundation for studying the role of semantics in various tasks . Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. (NLM), which also serves as a readily available corpus of abstracts for our experiments. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,1406f6b5ed4034b72ed2dccc3fcfa4c5c0810924,Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program,2001,A. Aronson
1354,W00-1017,P99-1026,[0],,"Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking <TARGET_CITATION/> .","If the system holds the initiative, the module executes the initial function of the phase. In typical questionanswer systems, the user has the initiative when asking questions and the system has it when answering. Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking <TARGET_CITATION/> . Since the language generation module works in parallel with the language understanding module, utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking <CITATION/>. In typical questionanswer systems, the user has the initiative when asking questions and the system has it when answering. If the system holds the initiative, the module executes the initial function of the phase.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,b9317d1aa658d94f18fe7cbad6c8ab3ac64b73f9,Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems,1999,Mikio Nakano; Noboru Miyazaki; Jun-ichi Hirasawa; Kohji Dohsaka; T. Kawabata
1355,W06-3813,External_25876,[0],introduction,This idea was expanded to include nouns and their modifiers through verb nominalizations <TARGET_CITATION/> .,"Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations <CITATION/>. <CITATION/>, who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants  for example, agent or instrument  to such grammatical elements as subject, direct object, indirect object. This idea was expanded to include nouns and their modifiers through verb nominalizations <TARGET_CITATION/> . This idea was expanded to include nouns and their modifiers through verb nominalizations <CITATION/>. <CITATION/>, who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants  for example, agent or instrument  to such grammatical elements as subject, direct object, indirect object. Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,a341279eab673f136a1f3347eed49c3d3ef07820,Remarks on Nominalization,2020,Noam Chomsky
1356,W02-1601,External_68467,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <TARGET_CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,f984dc9935315bf172dbabde08b897267a7400d7,A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora,2001,Arul Menezes; Stephen D. Richardson
1357,E03-1002,P02-1034,[0],,"<TARGET_CITATION/> define a kernel over parse trees and apply it to reranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on reranking using a finite set of features <CITATION/> .","We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.feature sets, but then efficiency becomes a problem. <TARGET_CITATION/> define a kernel over parse trees and apply it to reranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on reranking using a finite set of features <CITATION/> . <CITATION/> define a kernel over parse trees and apply it to reranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on reranking using a finite set of features <CITATION/>. feature sets, but then efficiency becomes a problem. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,fe638b5610475d4524684fb2c2b7b08c119c8700,"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron",2002,M. Collins; Nigel P. Duffy
1358,Q13-1020,N04-1035,[2],method,9 We only use the minimal GHKM rules <TARGET_CITATION/> here to reduce the complexity of the sampler .,"Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules <TARGET_CITATION/> here to reduce the complexity of the sampler . 9 We only use the minimal GHKM rules <CITATION/> here to reduce the complexity of the sampler.Our final experiments verify this point and we will conduct a much detailed analysis in future.Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,a7e925a65860e90b2b4eb427a8bc497f76b2fe6e,What’s in a translation rule?,2004,Michel Galley; Mark Hopkins; Kevin Knight; D. Marcu
1359,W01-1510,External_1855,[0],introduction,FBLTAG <TARGET_CITATION/> is an extension of the LTAG formalism .,Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled x onto an internal node of another tree with the same symbol x (Figure 4). FBLTAG <TARGET_CITATION/> is an extension of the LTAG formalism . FBLTAG <CITATION/> is an extension of the LTAG formalism. Adjunction grafts an auxiliary tree with the root node and foot node labeled x onto an internal node of another tree with the same symbol x (Figure 4). Substitution replaces a substitution node with another initial tree (Figure 3).,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,84c8d41dfdd41b1bd735182148857dfd0dd28f58,A study of tree adjoining grammars,1987,K. Vijayashanker; A. Joshi
1360,J87-3002,External_33765,[2],,"In addition to headwords , dictionary search through the pronunciation field is available ; <TARGET_CITATION/> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <CITATION/> .","While no application currently makes use of this facility, the motivation for such an approach to dictionary access comes from envisaging a parser which will operate on the basis of the online LDOCE; and any serious parser must be able to recognise compounds before it segments its input into separate words. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; <TARGET_CITATION/> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <CITATION/> . In addition to headwords, dictionary search through the pronunciation field is available; <CITATION/> has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <CITATION/>. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. While no application currently makes use of this facility, the motivation for such an approach to dictionary access comes from envisaging a parser which will operate on the basis of the online LDOCE; and any serious parser must be able to recognise compounds before it segments its input into separate words.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,an information theoretic analysis of phonetic dictionary access computer speech and language,1987,David Carter
1362,J09-4010,External_9392,[0],introduction,"Despite this , to date , there has been little work on corpusbased approaches to helpdesk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; <TARGET_CITATION/> ; Malik , Subramaniam , and Kaushik 2007 ) .","An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that helpdesk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for humangenerated responses if necessary. Despite this , to date , there has been little work on corpusbased approaches to helpdesk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; <TARGET_CITATION/> ; Malik , Subramaniam , and Kaushik 2007 ) . Despite this, to date, there has been little work on corpusbased approaches to helpdesk response automation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007). This indicates that helpdesk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for humangenerated responses if necessary. An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,e1d2a76d5934b016f90447cb1940e2cc209f2b8f,Learning from Message Pairs for Automatic Email Answering,2004,S. Bickel; T. Scheffer
1364,P00-1006,External_3692,[2],introduction,A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling <TARGET_CITATION/> .,"But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model. Much recent research in SMT, eg <CITATION/> deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling <TARGET_CITATION/> . A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence (MEMD) modeling <CITATION/>. Much recent research in SMT, eg <CITATION/> deals with the decoding problem, either directly or indirectly because of constraints imposed on the form of the translation model. But this comes at the cost of increased decoding complexity, because the chain rule can no longer be applied as in (1) due to the reversed direction of the translation model.",d60cf1a4c7a35f859e2e203d27f3ec71994e2e3e,A Maximum Entropy/Minimum Divergence Translation Model,2000,George F. Foster,fb486e03369a64de2d5b0df86ec0a7b55d3907db,A Maximum Entropy Approach to Natural Language Processing,1996,Adam L. Berger; S. D. Pietra; V. D. Pietra
1365,P07-1068,External_502,[2],,Our baseline coreference system uses the C4 .5 decision tree learner <TARGET_CITATION/> to acquire a classifier on the training texts for determining whether two NPs are coreferent ., Our baseline coreference system uses the C4 .5 decision tree learner <TARGET_CITATION/> to acquire a classifier on the training texts for determining whether two NPs are coreferent . Our baseline coreference system uses the C4.5 decision tree learner <CITATION/> to acquire a classifier on the training texts for determining whether two NPs are coreferent.,476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,,c45 programs for machine learning,1993,J R Quinlan
1366,D10-1074,W09-3930,[2],method,This alignment is obtained by following the same set of rules learned from the development dataset as in <TARGET_CITATION/> .,"Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in <TARGET_CITATION/> . This alignment is obtained by following the same set of rules learned from the development dataset as in <CITATION/>.Note that in this figure the alignment between x5 = suggests from the hypothesis and u4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2.",3d0adc6fca3a0669c108958c5d5204e2695ea4db,Towards Conversation Entailment: An Empirical Investigation,2010,Chen Zhang; J. Chai,ee9fb0cb4487d277233f61286eb40637e82dbb5e,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,Chen Zhang; J. Chai
1367,K15-1002,D13-1057,[2],experiments,"Developed Systems Our developed system is built on the work by <TARGET_CITATION/> , using Constrained Latent LeftLinking Model ( CL3M ) as our mentionpair coreference model in the joint framework10 .","3.1.1 can be verified empirically on both ACE2004 and OntoNotes5.0 datasets. Baseline Systems We choose three publicly available stateoftheart endtoend coreference systems as our baselines: Stanford system <CITATION/>, Berkeley system <CITATION/> and HOTCoref system <CITATION/>. Developed Systems Our developed system is built on the work by <TARGET_CITATION/> , using Constrained Latent LeftLinking Model ( CL3M ) as our mentionpair coreference model in the joint framework10 . Developed Systems Our developed system is built on the work by <CITATION/>, using Constrained Latent LeftLinking Model (CL3M) as our mentionpair coreference model in the joint framework10. Baseline Systems We choose three publicly available stateoftheart endtoend coreference systems as our baselines: Stanford system <CITATION/>, Berkeley system <CITATION/> and HOTCoref system <CITATION/>. 3.1.1 can be verified empirically on both ACE2004 and OntoNotes5.0 datasets.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,be2f82cfd32c41d6493dc3ffb414de27b4f9e15b,A Constrained Latent Variable Model for Coreference Resolution,2013,Kai-Wei Chang; Rajhans Samdani; D. Roth
1368,J02-3002,External_24096,[4],,"The best performance on the Brown corpus , a 0.2 % error rate , was reported by <TARGET_CITATION/> , who trained a decision tree classifier on a 25millionword corpus .","Stateoftheart machine learning and rulebased SBD systems achieve an error rate of 0.81.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <CITATION/>: a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by <TARGET_CITATION/> , who trained a decision tree classifier on a 25millionword corpus . The best performance on the Brown corpus, a 0.2% error rate, was reported by <CITATION/>, who trained a decision tree classifier on a 25millionword corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <CITATION/>: a 0.5% error rate. Stateoftheart machine learning and rulebased SBD systems achieve an error rate of 0.81.5% measured on the Brown corpus and the WSJ corpus.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,,some applications of treebased modeling to speech and language indexing”,1989,Michael D Riley
1369,W06-3813,External_6941,[0],related work,"Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <TARGET_CITATION/> .","The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <TARGET_CITATION/> . Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/>. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,95c2195e9fbebc8b5dbb4bb9797d1dd2851b0463,"Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing",2005,Lei Shi; Rada Mihalcea
1370,J00-2001,P83-1011,[0],,"These include devices such as interleaving the components <TARGET_CITATION/> , backtracking on failure <CITATION/> , allowing the linguistic component to interrogate the planner <CITATION/> , and Hovy 's notion of restrictive ( i.e. , bottomup ) planning ( Hovy 1988a , 1988c ) .","Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity. There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components <TARGET_CITATION/> , backtracking on failure <CITATION/> , allowing the linguistic component to interrogate the planner <CITATION/> , and Hovy 's notion of restrictive ( i.e. , bottomup ) planning ( Hovy 1988a , 1988c ) . These include devices such as interleaving the components <CITATION/>, backtracking on failure <CITATION/>, allowing the linguistic component to interrogate the planner <CITATION/>, and Hovy's notion of restrictive (i.e., bottomup) planning (Hovy 1988a, 1988c). There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,259918795730b7d7b041582ca038e241b9a26ffe,TELEGRAM: A Grammar Formalism for Language Planning,1983,D. Appelt
1371,P00-1007,External_5861,[4],,"The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <TARGET_CITATION/> , and became a common testbed ."," The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <TARGET_CITATION/> , and became a common testbed . The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 (Table 1), same as used by <CITATION/>, and became a common testbed.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,54c846ee00c6132d70429cc279e8577f63ed05e4,A Linear Observed Time Statistical Parser Based on Maximum Entropy Models,1997,A. Ratnaparkhi
1372,W06-2807,External_76563,[0],introduction,<TARGET_CITATION/> was the first scholar who stressed the impact of the digital revolution to the medium of writing ., <TARGET_CITATION/> was the first scholar who stressed the impact of the digital revolution to the medium of writing . <CITATION/> was the first scholar who stressed the impact of the digital revolution to the medium of writing.,48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,6adec314a809225a2da206232c084b4a53064f3a,"Writing Space: The Computer, Hypertext, and the History of Writing, Jay David Bolter (Ed.). Lawrence Erlbaum Associates (1991), 258",1991,
1373,J06-2002,External_18164,[0],experiments,"A similar problem is discussed in the psycholinguistics of interpretation <TARGET_CITATION/> : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .","This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eyetracking experiments suggesting that speakers start speaking while still scanning distractors <CITATION/>. A similar problem is discussed in the psycholinguistics of interpretation <TARGET_CITATION/> : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . A similar problem is discussed in the psycholinguistics of interpretation <CITATION/>: Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known. This means that the linguistic realization cannot start until CD is concluded, contradicting eyetracking experiments suggesting that speakers start speaking while still scanning distractors <CITATION/>. This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,74f6152e2c68a5eac9c4d1d6bf3dd08dc73368ca,Achieving incremental semantic interpretation through contextual representation,1999,Julie C. Sedivy; M. Tanenhaus; C. Chambers; G. Carlson
1374,W02-1601,External_103,[0],introduction,Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures <TARGET_CITATION/> .,There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures <TARGET_CITATION/> . Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences (translations) between layers of representation structures <CITATION/>. There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.,582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,ac568f84563f4832e0bc50c37d1f203a178c8729,Synchronous Models of Language,1996,Owen Rambow; G. Satta
1375,N01-1010,W00-0104,[3],introduction,"In our previous work <TARGET_CITATION/> , we applied this method to a small subset of WordNet nouns and showed potential applicability .","In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called treecut <CITATION/>. In our previous work <TARGET_CITATION/> , we applied this method to a small subset of WordNet nouns and showed potential applicability . In our previous work <CITATION/>, we applied this method to a small subset of WordNet nouns and showed potential applicability. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called treecut <CITATION/>. In this paper, we describes a lexicon organized around systematic polysemy.",50f15ec5e2ca5d10b1b77dece8f16f354d90f711,Tree-Cut and a Lexicon Based on Systematic Polysemy,2001,Noriko Tomuro,15744f27f57b5cb227c7a0d25b825e5884e086b5,Automatic Extraction of Systematic Polysemy Using Tree-cut,2000,Noriko Tomuro
1376,J01-4001,External_13822,[0],,"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <TARGET_CITATION/> , which was difficult both to represent and to process , and which required considerable human input .","The drive toward corpusbased robust NLP solutions further stimulated interest in alternative and/or dataenriched approaches. Last, but not least, applicationdriven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <TARGET_CITATION/> , which was difficult both to represent and to process , and which required considerable human input . Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. Last, but not least, applicationdriven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. The drive toward corpusbased robust NLP solutions further stimulated interest in alternative and/or dataenriched approaches.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,9f975473b7b0261db043ea150e52923d1a97eb40,An Architecture for Anaphora Resolution,1988,E. Rich; S. Luperfoy
1378,W02-1601,C88-1013,[0],,"For more details on the proprieties of SSTC , see <TARGET_CITATION/> .","The case depicted in Figure 2, describes how the SSTC structure treats some nonstandard linguistic phenomena. The particle up'' is featurised into the verb pick'' and in discontinuous manner (e.g. up'' (45) in pickup'' (12+45)) in the sentence He picks the box up''. For more details on the proprieties of SSTC , see <TARGET_CITATION/> . For more details on the proprieties of SSTC, see <CITATION/>.The particle up'' is featurised into the verb pick'' and in discontinuous manner (e.g. up'' (45) in pickup'' (12+45)) in the sentence He picks the box up''. The case depicted in Figure 2, describes how the SSTC structure treats some nonstandard linguistic phenomena.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,545e6410b23db2b0c6d3178430f61cb472a41e5e,Representation Trees and String-Tree Correspondences,1988,C. Boitet; Yusoff Zaharin
1379,W02-0309,External_34417,[0],introduction,"This has been reported for other languages , too , dependent on the generality of the chosen approach ( J  appinen and Niemist  o , 1988 ; <TARGET_CITATION/> ; Ekmekc  ioglu et al. , 1995 ; <CITATION/> ) .","For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator. mers <CITATION/> demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J  appinen and Niemist  o , 1988 ; <TARGET_CITATION/> ; Ekmekc  ioglu et al. , 1995 ; <CITATION/> ) . This has been reported for other languages, too, dependent on the generality of the chosen approach <CITATION/>. mers <CITATION/> demonstrably improve retrieval performance. For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,b71f7e55305e237698f4e2aebe70cadb310895e8,The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data,1992,M. Popovic; P. Willett
1380,J09-4010,External_2268,[2],method,"Specifically , we used Decision Graphs <CITATION/> for DocPred , and SVMs <TARGET_CITATION/> for SentPred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .","The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically , we used Decision Graphs <CITATION/> for DocPred , and SVMs <TARGET_CITATION/> for SentPred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) . Specifically, we used Decision Graphs <CITATION/> for DocPred, and SVMs <CITATION/> for SentPred.11 Additionally, we used unigrams for clustering documents and sentences, and unigrams and bigrams for predicting document clusters and sentence clusters (Sections 3.1.2 and 3.2.2). Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,4c75b748911ddcd888c5122f7672f69caa5d661f,Statistical Learning Theory,2021,Yuhai Wu
1381,K15-1001,External_4216,[2],experiments,"To prepare SMT outputs for postediting , the creators of the corpus used their own WMT10 system <CITATION/> , based on the Moses phrasebased decoder <TARGET_CITATION/> with dense features .","We used the LIG corpus3 which consists of 10,881 tuples of FrenchEnglish postedits <CITATION/>. The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for postediting , the creators of the corpus used their own WMT10 system <CITATION/> , based on the Moses phrasebased decoder <TARGET_CITATION/> with dense features . To prepare SMT outputs for postediting, the creators of the corpus used their own WMT10 system <CITATION/>, based on the Moses phrasebased decoder <CITATION/> with dense features. The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. We used the LIG corpus3 which consists of 10,881 tuples of FrenchEnglish postedits <CITATION/>.",571e0df1b33cf9e232f9c96428618b530fdd6be1,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,2015,Artem Sokolov; S. Riezler; Shay B. Cohen,4ee2eab4c298c1824a9fb8799ad8eed21be38d21,Moses: Open Source Toolkit for Statistical Machine Translation,2007,Philipp Koehn; Hieu T. Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; N. Bertoldi; Brooke Cowan; Wade Shen; C. Moran; Richard Zens; Chris Dyer; Ondrej Bojar; Alexandra Constantin; Evan Herbst
1382,D10-1052,P07-1090,[2],,"To model o ( Li , S  T ) , o ( Ri , S  T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by <TARGET_CITATION/> ."," To model o ( Li , S  T ) , o ( Ri , S  T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by <TARGET_CITATION/> . To model o(Li,ST), o(Ri,ST), i.e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by <CITATION/>.",c97c609c34db8787505d83dfa03077d4813c7f19,Discriminative Word Alignment with a Function Word Reordering Model,2010,Hendra Setiawan; Christopher Dyer; P. Resnik,4db12fdbc657443e50c969e73e9a616d1eaa9fb6,Ordering Phrases with Function Words,2007,Hendra Setiawan; Min-Yen Kan; Haizhou Li
1383,W06-3813,External_42081,[2],experiments,"The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT <TARGET_CITATION/> .","There are 4686 word tokens and 969 types. The difference between the number of types (2850) and tokens (573) in the extracted pairs (which contain only openclass words) shows that the same concepts recur, as expected in a didactic text. The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT <TARGET_CITATION/> . The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, DIPETT <CITATION/>. The difference between the number of types (2850) and tokens (573) in the extracted pairs (which contain only openclass words) shows that the same concepts recur, as expected in a didactic text. There are 4686 word tokens and 969 types.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,8547d182f7a3d960aac87d1e36db13cb664ad554,Realistic Parsing: Practical Solutions of Difficult Problems,2009,S. Delisle
1384,W04-0910,External_89014,[5],,"In particular , <TARGET_CITATION/> lists the converses of some 3 500 predicative nouns .","For shuffling paraphrases, french alternations are partially described in <CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables <CITATION/> can furthermore be resorted to, which list detailed syntacticosemantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular , <TARGET_CITATION/> lists the converses of some 3 500 predicative nouns . In particular, <CITATION/> lists the converses of some 3 500 predicative nouns.For complementing this database and for converse constructions, the LADL tables <CITATION/> can furthermore be resorted to, which list detailed syntacticosemantic descriptions for 5 000 verbs and 25 000 verbal expressions. For shuffling paraphrases, french alternations are partially described in <CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,fb161a95fd58d98f9f1d0e45b37d6fb7dcfd25a4,Les constructions converses du français,1989,G. Gross
1385,K15-1003,D07-1071,[4],experiments,"This is similar to the  deletion '' strategy employed by <TARGET_CITATION/> , but we do it directly in the grammar .","If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the  deletion '' strategy employed by <TARGET_CITATION/> , but we do it directly in the grammar . This is similar to the deletion'' strategy employed by <CITATION/>, but we do it directly in the grammar. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). If that fails, then it searches for a parse with any root.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,774113732db34ce0b797fc3dcceded811fb6edbc,Online Learning of Relaxed CCG Grammars for Parsing to Logical Form,2007,Luke Zettlemoyer; M. Collins
1386,W06-3309,P02-1047,[4],related work,"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <TARGET_CITATION/> .","Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <TARGET_CITATION/> . Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <CITATION/>. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,d208d98c010f3c1b5e2bc40180b9e2e69b88c289,An Unsupervised Approach to Recognizing Discourse Relations,2002,D. Marcu; Abdessamad Echihabi
1387,W06-1104,External_861,[0],introduction,Many NLP applications require knowledge about semantic relatedness rather than just similarity <TARGET_CITATION/> .,"2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word.tween two words <CITATION/>.3 Dissimilar words can be semantically related, e.g. via functional relationships (night  dark) or when they are antonyms (high  low). Many NLP applications require knowledge about semantic relatedness rather than just similarity <TARGET_CITATION/> . Many NLP applications require knowledge about semantic relatedness rather than just similarity <CITATION/>. tween two words <CITATION/>.3 Dissimilar words can be semantically related, e.g. via functional relationships (night  dark) or when they are antonyms (high  low). 2In this paper, word denotes the graphemic form of a token and concept refers to a particular sense of a word.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,evaluating wordnetbased measures of semantic distance,2006,Alexander Budanitsky; Graeme Hirst
1388,D13-1115,External_23656,[0],related work,Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <TARGET_CITATION/> .,"More recently, <CITATION/> show that visual attribute classifiers, which have been immensely successful in object recognition <CITATION/>, act as excellent substitutes for featurenorms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <TARGET_CITATION/> . Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>. norms. More recently, <CITATION/> show that visual attribute classifiers, which have been immensely successful in object recognition <CITATION/>, act as excellent substitutes for feature",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,6cd2d1058023fbf9dc9fb12e5c2bbd88037e8a0a,Learning the abstract motion semantics of verbs from captioned videos,2008,Stefan Mathe; A. Fazly; Sven J. Dickinson; S. Stevenson
1389,J05-3003,P98-2184,[1],,It has been shown <TARGET_CITATION/> that the subcategorization tendencies of verbs vary across linguistic domains .,"PennIII consists of the WSJ section from PennII as well as a parseannotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown <TARGET_CITATION/> that the subcategorization tendencies of verbs vary across linguistic domains . It has been shown <CITATION/> that the subcategorization tendencies of verbs vary across linguistic domains. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. PennIII consists of the WSJ section from PennII as well as a parseannotated subset of the Brown corpus.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,b4213d9ffabd486fad674075bf3f3486271fab3d,How Verb Subcategorization Frequencies are Affected by Corpus Choice,1998,Douglas Roland; Dan Jurafsky
1390,Q13-1020,N04-1035,[2],experiments,"In the system , we extract both the minimal GHKM rules <TARGET_CITATION/> , and the rules of SPMT Model 1 <CITATION/> with phrases up to length L = 5 on the source side .","The translation system used for testing the effectiveness of our Utrees is our inhouse stringtotree system (abbreviated as s2t). The system is implemented based on <CITATION/>. In the system , we extract both the minimal GHKM rules <TARGET_CITATION/> , and the rules of SPMT Model 1 <CITATION/> with phrases up to length L = 5 on the source side . In the system, we extract both the minimal GHKM rules <CITATION/>, and the rules of SPMT Model 1 <CITATION/> with phrases up to length L=5 on the source side. The system is implemented based on <CITATION/>. The translation system used for testing the effectiveness of our Utrees is our inhouse stringtotree system (abbreviated as s2t).",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,a7e925a65860e90b2b4eb427a8bc497f76b2fe6e,What’s in a translation rule?,2004,Michel Galley; Mark Hopkins; Kevin Knight; D. Marcu
1391,J03-3004,External_28667,[0],introduction,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; <TARGET_CITATION/> ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed .","When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; <TARGET_CITATION/> ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed . Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cfXXX Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,gaijin a bootstrapping templatedriven approach to examplebased machine translation,1997,Tony Veale; Andy Way
1392,J02-3002,External_3643,[0],,"One of the betterknown approaches is described in <TARGET_CITATION/> , which suggested that abbreviations first be extracted from a corpus using abbreviationguessing heuristics akin to those described in Section 6 and then reused in further processing .","12.2.3 Disambiguation of Abbreviations. Not much information has been published on abbreviation identification. One of the betterknown approaches is described in <TARGET_CITATION/> , which suggested that abbreviations first be extracted from a corpus using abbreviationguessing heuristics akin to those described in Section 6 and then reused in further processing . One of the betterknown approaches is described in <CITATION/>, which suggested that abbreviations first be extracted from a corpus using abbreviationguessing heuristics akin to those described in Section 6 and then reused in further processing. Not much information has been published on abbreviation identification. 12.2.3 Disambiguation of Abbreviations.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,e727c7fd2bf3460a36934eae64c8c5716bc28980,"What is a word, What is a sentence? Problems of Tokenization",1994,G. Grefenstette; P. Tapanainen
1393,J00-2004,External_42502,[4],method,"Unlike the models proposed by <TARGET_CITATION/> ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .","Now, a bagtobag translation model can be fully specified by the distributions of 1 and trans.The probability distribution trans (ii, it') is a wordtoword translation model. Unlike the models proposed by <TARGET_CITATION/> ) , this model is symmetric , because both word bags are generated together from a joint probability distribution . Unlike the models proposed by <CITATION/>, this model is symmetric, because both word bags are generated together from a joint probability distribution. The probability distribution trans (ii, it') is a wordtoword translation model. Now, a bagtobag translation model can be fully specified by the distributions of 1 and trans.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,fc593d91a7974bb1d3fac1ffe47b787ce1853a88,But Dictionaries Are Data Too,1993,P. Brown; S. D. Pietra; V. D. Pietra; Meredith J. Goldsmith; Jan Hajic; R. Mercer; Surya Mohanty
1394,P13-3018,External_63932,[2],method,"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in <TARGET_CITATION/> for Bangla morphologically complex words ."," We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in <TARGET_CITATION/> for Bangla morphologically complex words . We apply two different priming experiments namely, the cross modal priming and masked priming experiment discussed in <CITATION/> for Bangla morphologically complex words.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,df408a7acf643fead01848986fec63f26da09bbb,Morphology and meaning in the English mental lexicon.,1994,W. Marslen-Wilson; L. Tyler; Rachelle Waksler; Lianne Older
1395,D12-1037,D11-1125,[0],related work,<TARGET_CITATION/> proposed other optimization objectives by introducing a marginbased and rankingbased indirect loss functions .,Several works have proposed discriminative techniques to train loglinear model for SMT. <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. <TARGET_CITATION/> proposed other optimization objectives by introducing a marginbased and rankingbased indirect loss functions . <CITATION/> proposed other optimization objectives by introducing a marginbased and rankingbased indirect loss functions. <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,a13d46125ef505d4e687e25ded74b794efc18323,Tuning as Ranking,2011,Mark Hopkins; Jonathan May
1396,A00-1009,A92-1006,[3],introduction,"Our work extends directions taken in systems such as Ariane <CITATION/> , FoG <CITATION/> , JOYCE ( Rambow and <TARGET_CITATION/> ) , and LFS <CITATION/> .","In this paper we present a linguistically motivated framework for uniform lexicostructural processing. It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT). Our work extends directions taken in systems such as Ariane <CITATION/> , FoG <CITATION/> , JOYCE ( Rambow and <TARGET_CITATION/> ) , and LFS <CITATION/> . Our work extends directions taken in systems such as Ariane <CITATION/>, FoG <CITATION/>, JOYCE <CITATION/>, and LFS <CITATION/>. It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT). In this paper we present a linguistically motivated framework for uniform lexicostructural processing.",6602edbc2f35e085dc4ee0361da214c4f14c5a07,A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing,2000,Benoit Lavoie; R. Kittredge; Tanya Korelsky; Owen Rambow,37100857c059c190fdca074fabe6d6856b480c51,Applied Text Generation,1992,Owen Rambow; Tanya Korelsky
1397,D08-1034,External_9407,[0],introduction,"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering <CITATION/> , Information Extraction <CITATION/> , and Machine Translation <TARGET_CITATION/> .","The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering <CITATION/> , Information Extraction <CITATION/> , and Machine Translation <TARGET_CITATION/> . Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering <CITATION/>, Information Extraction <CITATION/>, and Machine Translation<CITATION/>. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,fa97b93a4162c393862cdf2fa8ba5dae63a88081,Bilingual FrameNet Dictionaries for Machine Translation,2002,H. Boas
1398,W06-3813,P02-1032,[0],related work,Such systems extract information from some types of syntactic units ( clauses in <CITATION/> ; noun phrases in <TARGET_CITATION/> ) .,"Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>. In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Such systems extract information from some types of syntactic units ( clauses in <CITATION/> ; noun phrases in <TARGET_CITATION/> ) . Such systems extract information from some types of syntactic units (clauses in <CITATION/>; noun phrases in <CITATION/>). In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,94d2efafe5fd6f47b62e6165e361888ded570e0a,"The Descent of Hierarchy, and Selection in Relational Semantics",2002,Barbara Rosario; Marti A. Hearst; C. Fillmore
1399,D13-1115,External_9238,[2],experiments,Association Norms ( AN ) is a collection of association norms collected by Schulte im <TARGET_CITATION/> ., Association Norms ( AN ) is a collection of association norms collected by Schulte im <TARGET_CITATION/> . Association Norms (AN) is a collection of association norms collected by Schulte im <CITATION/>.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,e313d41d975cf45b3ba1ef5e069fdea38d38cf22,Association Norms of German Noun Compounds,2012,Sabine Schulte im Walde; S. Borgwaldt; Ronny Jauch
1400,W01-1510,External_328,[0],introduction,"Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project <TARGET_CITATION/> .","The center of Figure 6 shows a rule application to can run'' and we''. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts <CITATION/>. Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project <TARGET_CITATION/> . Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project <CITATION/>. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts <CITATION/>. The center of Figure 6 shows a rule application to can run'' and we''.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,on building a more efficient grammar by exploiting types,2000,Dan Flickinger
1401,J06-3002,External_28319,[1],method,The changes made were inspired by those described in <TARGET_CITATION/> ) .,"Entropy is 4 The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files. Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet. The changes made were inspired by those described in <TARGET_CITATION/> ) . The changes made were inspired by those described in <CITATION/>. Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet. Entropy is 4 The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.",7c2aaf831d92750a683e979ec282f1c74fe43083,The Notion of Argument in Prepositional Phrase Attachment,2006,Paola Merlo; Eva Esteve Ferrer,31ea50a9a54050decad35d775788afa7a4615ced,Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary,1997,Jiri Stetina; M. Nagao
1402,J87-3002,External_12390,[3],,The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of <TARGET_CITATION/> ) .,"Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of <TARGET_CITATION/> ) . The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of Quirk et al. (1972, 1985). Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,a grammar of contemporary english longman group limited,1972,Randolph Quirk; Sidney Greenbaum; Geoffrey Leech; Jan Svartvik
1403,J87-3002,External_40336,[0],,"Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English <TARGET_CITATION/> .","Finally, the complexity of the data structures stored on disc should not be constrained in any way by the method of access, as we do not have a very clear idea what form the restructured dictionary may eventually take. Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the sexpression, we adopted the approach of converting the tape source into a set of list structures, one per entry. Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English <TARGET_CITATION/> . Our task was made possible by the fact that while far from being a database in the accepted sense of the word, the LDOCE typesetting tape is the only truly computerised dictionary of English <CITATION/>.Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the sexpression, we adopted the approach of converting the tape source into a set of list structures, one per entry. Finally, the complexity of the data structures stored on disc should not be constrained in any way by the method of access, as we do not have a very clear idea what form the restructured dictionary may eventually take.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,automatic analysis of texts in informatics 7,1983,Archibal Michiels
1404,J86-1002,External_33228,[0],,A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <TARGET_CITATION/> .,"Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <TARGET_CITATION/> . A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,cede7e5150ecfbd5024a8cdc441010ad61299a4e,"The acquisition and use of dialogue expectation in speech recognition (natural language processing, artificial intelligence)",1983,P. Fink
1405,Q13-1020,N04-1035,[2],method,"Using the GHKM algorithm <TARGET_CITATION/> , we can get two different STSG derivations from the two Utrees based on the fixed word alignment .","Otherwise, we change its state to the right state (P=1), and transform the Utree to Figure 3(b) accordingly.Obviously, towards an snode for sampling, the two values of P would define two different Utrees. Using the GHKM algorithm <TARGET_CITATION/> , we can get two different STSG derivations from the two Utrees based on the fixed word alignment . Using the GHKM algorithm <CITATION/>, we can get two different STSG derivations from the two Utrees based on the fixed word alignment. Obviously, towards an snode for sampling, the two values of P would define two different Utrees. Otherwise, we change its state to the right state (P=1), and transform the Utree to Figure 3(b) accordingly.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,a7e925a65860e90b2b4eb427a8bc497f76b2fe6e,What’s in a translation rule?,2004,Michel Galley; Mark Hopkins; Kevin Knight; D. Marcu
1406,W06-3309,External_74943,[0],introduction,The need for information systems to support physicians at the point of care has been well studied <TARGET_CITATION/> .,"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>. For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied <TARGET_CITATION/> . The need for information systems to support physicians at the point of care has been well studied <CITATION/>. For a variety of reasons, medicine is an interesting domain of research. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,7a46c1a0db880c4ef9d96faedcaeca683def50ed,Research Paper: Answering Physicians' Clinical Questions: Obstacles and Potential Solutions,2005,John W. Ely; J. Osheroff; M. Lee Chambliss; M. Ebell; Marcy E. Rosenbaum
1407,W06-1639,External_27080,[0],introduction,"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) .","In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) . Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,4446061b1ed0bfb66805de8442db31ae1980f685,On the Computation of Point of View,1994,Warren Sack
1408,P97-1063,External_12276,[0],introduction,"Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools ( e.g. <CITATION/> ) , concordancing for bilingual lexicography <TARGET_CITATION/> , computerassisted language learning , corpus linguistics ( Melby .","Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including  crummy '' MT on the World Wide Web <CITATION/> , certain machineassisted translation tools ( e.g. <CITATION/> ) , concordancing for bilingual lexicography <TARGET_CITATION/> , computerassisted language learning , corpus linguistics ( Melby . Table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including crummy'' MT on the World Wide Web <CITATION/>, certain machineassisted translation tools (e.g. <CITATION/>), concordancing for bilingual lexicography <CITATION/>, computerassisted language learning, corpus linguistics (Melby. However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation <CITATION/>.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,deriving translation data from bilingual textsquot,1993,R Catizone; G Russell; S Warwick
1409,P10-1143,P07-1107,[1],method,Our HDP extension is also inspired from the Bayesian model proposed by <TARGET_CITATION/> .,"We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L. Our HDP extension is also inspired from the Bayesian model proposed by <TARGET_CITATION/> . Our HDP extension is also inspired from the Bayesian model proposed by <CITATION/>. We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L.",791198ab24bea86944265887136843e00352e13f,Unsupervised Event Coreference Resolution with Rich Linguistic Features,2010,C. Bejan; S. Harabagiu,d0e498810f28462ea4d172f1f6000b7c1cf2870c,Unsupervised Coreference Resolution in a Nonparametric Bayesian Model,2007,A. Haghighi; D. Klein
1411,W06-1639,External_57193,[4],method,"As has been previously observed and exploited in the NLP literature <TARGET_CITATION/> , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","where c(s) is the opposite'' class from c(s). A minimumcost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individualdocument classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature <TARGET_CITATION/> , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs . As has been previously observed and exploited in the NLP literature <CITATION/>, the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. A minimumcost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individualdocument classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. where c(s) is the opposite'' class from c(s).",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,3497dcdac3db7db37aabc1db94b516780e89ea8e,Sentiment Analysis : A New Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be Classified .,2005,Alekh Agarwal
1412,P11-1134,External_4256,[2],,"To combine the phrasal matching scores obtained at each ngram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight <TARGET_CITATION/> , using each score as a feature .","Once matching for each ngram level has been concluded, the number of matches (Mn) and the number of phrases in the hypothesis (Nn) are used to estimate the portion of phrases in H that are matched at each level (n). The phrasal matching score for each ngram level is calculated as follows: To combine the phrasal matching scores obtained at each ngram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight <TARGET_CITATION/> , using each score as a feature . To combine the phrasal matching scores obtained at each ngram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight <CITATION/>, using each score as a feature.The phrasal matching score for each ngram level is calculated as follows:Once matching for each ngram level has been concluded, the number of matches (Mn) and the number of phrases in the hypothesis (Nn) are used to estimate the portion of phrases in H that are matched at each level (n).",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,,making largescale support vector machine learning practical,1999,Thorsten Joachims
1413,J01-4001,External_95543,[0],,"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> .","The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> . Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,improving pronoun resolution in two languages by means of bilingual corpora,2000,Ruslan Mitkov; Catalina Barbu
1415,P97-1063,External_9618,[5],conclusion,Another interesting extension is to broaden the definition of a  word '' to include multiword lexical units <TARGET_CITATION/> .,"Even better accuracy can be achieved with a more finegrained link class structure. Promising features for classification include part of speech, frequency of cooccurrence, relative word position, and translational entropy <CITATION/>. Another interesting extension is to broaden the definition of a  word '' to include multiword lexical units <TARGET_CITATION/> . Another interesting extension is to broaden the definition of a word'' to include multiword lexical units <CITATION/>. Promising features for classification include part of speech, frequency of cooccurrence, relative word position, and translational entropy <CITATION/>. Even better accuracy can be achieved with a more finegrained link class structure.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,how to compile a bilingual collocational lexicon automaticallyquot,1992,F Smadja
1416,W03-0806,External_67123,[0],introduction,"For example , 10 million words of the American National Corpus <TARGET_CITATION/> will have manually corrected POS tags , a tenfold increase over the Penn Treebank <CITATION/> , currently used for training POS taggers .","NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus <TARGET_CITATION/> will have manually corrected POS tags , a tenfold increase over the Penn Treebank <CITATION/> , currently used for training POS taggers . For example, 10 million words of the American National Corpus <CITATION/> will have manually corrected POS tags, a tenfold increase over the Penn Treebank <CITATION/>, currently used for training POS taggers. Some of this new data will be manually annotated. NLP is experiencing an explosion in the quantity of electronic text available.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,818a43e2f16cee9fb6dcbcadfbe78a9efc7bd0d3,The American National Corpus: More Than the Web Can Provide,2002,Nancy Ide; R. Reppen; Keith Suderman
1417,W02-0309,External_44553,[0],introduction,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> .","When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> . From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexicosemantic aspects of dederivation and decomposition <CITATION/>. This is particularly true for the medical domain. When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,,morphosemantic analysis of compound word forms denoting surgical procedures methods ofinformation in medicine,1983,L Norton; M Pacak
1418,J00-2001,W94-0306,[0],,"This approach has occasionally been taken , as in <CITATION/> and , at least implicitly , in <TARGET_CITATION/> et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .","On the other hand, it precludes making decisions involving interactions between text planning and linguistic issues. One possible response would be to abandon the separation; the generator could be a single component that handles all of the work. This approach has occasionally been taken , as in <CITATION/> and , at least implicitly , in <TARGET_CITATION/> ; however , under this approach , all of the flexibility and simplicity of modular design is lost . This approach has occasionally been taken, as in <CITATION/> and, at least implicitly, in <CITATION/>; however, under this approach, all of the flexibility and simplicity of modular design is lost. One possible response would be to abandon the separation; the generator could be a single component that handles all of the work. On the other hand, it precludes making decisions involving interactions between text planning and linguistic issues.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,5b744278494048d090d1d22999217cb82a53bb93,"Intentions, Structure and Expression in Multi-Lingual Instructions",1994,Cécile Paris; D. Scott
1419,W06-2933,External_18316,[0],experiments,"Typical examples are Bulgarian <CITATION/> , Chinese <TARGET_CITATION/> , Danish <CITATION/> , and Swedish <CITATION/> .","before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. Typical examples are Bulgarian <CITATION/> , Chinese <TARGET_CITATION/> , Danish <CITATION/> , and Swedish <CITATION/> . Typical examples are Bulgarian <CITATION/>, Chinese <CITATION/>, Danish <CITATION/>, and Swedish <CITATION/>. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 959085, for arcs of length 1, 2 and 36. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,ef904a08b7926ce9f4548d5a2907669a36da71ff,"Sinica Treebank: Design Criteria, Representational Issues and Implementation",2004,Kuen Chen; Chu-Ren Huang; Frank Y. Chen; C. Luo; Min-cheng Chang; Chien-Hung Chen
1420,K15-1002,D13-1057,[2],related work,"In this paper , we use the Constrained Latent LeftLinking Model ( CL3M ) described in <TARGET_CITATION/> in our experiments .","<CITATION/>). The introduction of ILP methods has influenced the coreference area too <CITATION/>. In this paper , we use the Constrained Latent LeftLinking Model ( CL3M ) described in <TARGET_CITATION/> in our experiments . In this paper, we use the Constrained Latent LeftLinking Model (CL3M) described in <CITATION/> in our experiments. The introduction of ILP methods has influenced the coreference area too <CITATION/>. <CITATION/>).",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,be2f82cfd32c41d6493dc3ffb414de27b4f9e15b,A Constrained Latent Variable Model for Coreference Resolution,2013,Kai-Wei Chang; Rajhans Samdani; D. Roth
1421,K15-1002,N07-1011,[2],experiments,"We use a standard split of 268 training documents , 68 development documents , and 106 testing documents <TARGET_CITATION/> .","Datasets The ACE2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents <TARGET_CITATION/> . We use a standard split of 268 training documents, 68 development documents, and 106 testing documents <CITATION/>. Datasets The ACE2004 dataset contains 443 documents.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,a957e6cbc55f004f94b468f23c5149f1b0fd3389,First-Order Probabilistic Models for Coreference Resolution,2007,A. Culotta; Michael L. Wick; A. McCallum
1422,J05-3003,External_13650,[0],introduction,"Aside from the extraction of theoryneutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG <TARGET_CITATION/> .","<CITATION/> argues that, aside from missing domainspecific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theoryneutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG <TARGET_CITATION/> . Aside from the extraction of theoryneutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG <CITATION/>. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. <CITATION/> argues that, aside from missing domainspecific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,d4943720cc393626ed1ff87e6bad9622e69cb284,Extracting Tree Adjoining Grammars from Bracketed Corpora,2009,Fei Xia; Chung-hye Han; A. Joshi; Martha Palmer; C. Prolo; Anoop Sarkar
1423,J90-3003,External_2221,[4],experiments,"An alternative representation based on <TARGET_CITATION/> is presented in <CITATION/> , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .","Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on <TARGET_CITATION/> is presented in <CITATION/> , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree . An alternative representation based on <CITATION/> is presented in <CITATION/>, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,b8ec853894551c0e7a822df50dc04eccd613d46f,On stress and linguistic rhythm,1977,M. Liberman; A. Prince
1425,D09-1056,External_37184,[0],introduction,"According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people <TARGET_CITATION/> .","com are examples of sites which perform web people search, although with limited disambiguation capabilities. A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 1117% of the queries were composed of a person name with additional terms and 4% were identified as person names <CITATION/>. According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people <TARGET_CITATION/> . According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people <CITATION/>. A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 1117% of the queries were composed of a person name with additional terms and 4% were identified as person names <CITATION/>. com are examples of sites which perform web people search, although with limited disambiguation capabilities.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,db8fc6e3192fa0d9703dc9ad4500d07e91a6a68b,A testbed for people searching strategies in the WWW,2005,J. Artiles; Julio Gonzalo; M. Verdejo
1426,J02-3002,External_5983,[0],,"Since then this idea has been applied to several tasks , including word sense disambiguation <TARGET_CITATION/> and namedentity recognition <CITATION/> .","It has been applied not only to the identification of proper names, as described in this article, but also to their classification <CITATION/>. <CITATION/> showed that words strongly tend to exhibit only one sense in a document or discourse (one sense per discourse''). Since then this idea has been applied to several tasks , including word sense disambiguation <TARGET_CITATION/> and namedentity recognition <CITATION/> . Since then this idea has been applied to several tasks, including word sense disambiguation <CITATION/> and namedentity recognition <CITATION/>. <CITATION/> showed that words strongly tend to exhibit only one sense in a document or discourse (one sense per discourse''). It has been applied not only to the identification of proper names, as described in this article, but also to their classification <CITATION/>.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,944cba683d10d8c1a902e05cd68e32a9f47b372e,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,1995,David Yarowsky
1427,W06-2933,H92-1026,[2],introduction, Historybased feature models for predicting the next parser action <TARGET_CITATION/> .,Our methodology for performing this task is based on four essential components: A deterministic algorithm for building labeled projective dependency graphs <CITATION/>. Historybased feature models for predicting the next parser action <TARGET_CITATION/> .  Historybased feature models for predicting the next parser action <CITATION/>.  A deterministic algorithm for building labeled projective dependency graphs <CITATION/>. Our methodology for performing this task is based on four essential components:,f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,571596a4075c408e3738e858d9b781847825fe5c,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1992,Ezra Black; Frederick Jelinek; John D. Lafferty; David M. Magerman; R. Mercer; S. Roukos
1428,P97-1063,External_6119,[2],method,"For each cooccurring pair of word types u and v , these likelihoods are initially set proportional to their cooccurrence frequency (  , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following <TARGET_CITATION/> 2 .","The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u, v) represents the likelihood that u and v can be mutual translations. For each cooccurring pair of word types u and v , these likelihoods are initially set proportional to their cooccurrence frequency (  , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following <TARGET_CITATION/> 2 . For each cooccurring pair of word types u and v, these likelihoods are initially set proportional to their cooccurrence frequency (,v) and inversely proportional to their marginal frequencies n(u) and n(v) 1, following <CITATION/>2. L(u, v) represents the likelihood that u and v can be mutual translations. The two hidden parameters are the probabilities of the model generating true and false positives in the data.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,025464b73f805e76689a7a20a48a9e9c0f4ff3ef,Accurate Methods for the Statistics of Surprise and Coincidence,1993,T. Dunning
1429,J03-3004,External_8487,[2],experiments,"However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on <TARGET_CITATION/> .","The system searches for marker words within the string and retrieves their translations.10 In this case, the marker word in the string is the and its translation can be one of le, la, l', or les, depending on the context. The system simply attaches the translation with the highest weight to the existing chunk ordinateurs personnels to produce the mistranslation in (50): (50) *la ordinateurs personnels The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on <TARGET_CITATION/> . However, rather than output this wrong translation directly, we use a post hoc validation and (if required) correction process based on <CITATION/>. The system simply attaches the translation with the highest weight to the existing chunk ordinateurs personnels to produce the mistranslation in (50): (50) *la ordinateurs personnels The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. The system searches for marker words within the string and retrieves their translations.10 In this case, the marker word in the string is the and its translation can be one of le, la, l', or les, depending on the context.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,7e3e4c011f2dfe28a706cfb8d32571948192ab89,The World Wide Web as a Resource for Example-Based Machine Translation Tasks,1999,G. Grefenstette
1430,W06-3813,External_6941,[0],related work,"Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <TARGET_CITATION/> .","The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <TARGET_CITATION/> . Most approaches rely on VerbNet <CITATION/> and FrameNet <CITATION/> to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions <CITATION/> and also <CITATION/>. In current work on semantic relation analysis, the focus is on semantic roles  relations between verbs and their arguments. The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,95c2195e9fbebc8b5dbb4bb9797d1dd2851b0463,"Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing",2005,Lei Shi; Rada Mihalcea
1431,J97-4003,External_77925,[0],related work,The powerful mechanism of lexical rules <TARGET_CITATION/> has been used in many natural language processing systems ., The powerful mechanism of lexical rules <TARGET_CITATION/> has been used in many natural language processing systems . The powerful mechanism of lexical rules <CITATION/> has been used in many natural language processing systems.,d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,f80c3eaf4ba8773cabd337b5b8642017a5e63708,The Generative Power of Categorial Grammars and Head-Driven Phrase Structure Grammars with Lexical Rules,1991,Bob Carpenter
1432,J00-4002,J90-1001,[4],,"The version proposed here combines a basic insight from <CITATION/> with higherorder unification to give an analysis that has a strong resemblance to that proposed in <TARGET_CITATION/> ) , with some differences that are commented on below .","We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. The version proposed here combines a basic insight from <CITATION/> with higherorder unification to give an analysis that has a strong resemblance to that proposed in <TARGET_CITATION/> ) , with some differences that are commented on below . The version proposed here combines a basic insight from <CITATION/> with higherorder unification to give an analysis that has a strong resemblance to that proposed in Pereira (1990, 1991), with some differences that are commented on below. We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,8a1053957e26b4fccd9105ab94835e6d06cd659d,Categorial Semantics and Scoping,1990,Fernando C Pereira
1433,D11-1138,P06-1096,[4],related work,<TARGET_CITATION/> presented a perceptronbased algorithm for learning the phrasetranslation parameters in a statistical machine translation system .,"This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. <TARGET_CITATION/> presented a perceptronbased algorithm for learning the phrasetranslation parameters in a statistical machine translation system . <CITATION/> presented a perceptronbased algorithm for learning the phrasetranslation parameters in a statistical machine translation system. Furthermore, we also evaluate the method on alternate extrinsic loss functions. This allows us to give guarantees of convergence.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,10d21ca7728cb3dd15731accedda9ea711d8a0f4,An End-to-End Discriminative Approach to Machine Translation,2006,P. Liang; A. Bouchard-Côté; D. Klein; B. Taskar
1434,J06-2002,External_15998,[0],,"NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; <TARGET_CITATION/> ) : The selected expression should also be felicitous ."," NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; <TARGET_CITATION/> ) : The selected expression should also be felicitous . NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,9f37a927fc0769207bfac25a5b3e04c50ad14b61,Cooking Up Referring Expressions,1989,R. Dale
1435,J03-3004,External_61553,[0],introduction,<TARGET_CITATION/> ) conducts some small experiments using his METLA system to show the viability of this approach for English  > French and English  > Urdu .,"Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of (source, target) chunks. <TARGET_CITATION/> ) conducts some small experiments using his METLA system to show the viability of this approach for English  > French and English  > Urdu . Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English > French and English > Urdu. Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of (source, target) chunks.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,a psycholinguistic approach to corpusbased machine translation,1994,Patrick Juola
1436,J97-4003,External_25076,[4],introduction,"This idea of preserving properties can be considered an instance of the wellknown frame problem in AT <TARGET_CITATION/> , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .","This is so since the lexical rule in Figure 2 ''(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.'' (Pollard and Sag [1994, 3141, following Flickinger [19871). This idea of preserving properties can be considered an instance of the wellknown frame problem in AT <TARGET_CITATION/> , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule . This idea of preserving properties can be considered an instance of the wellknown frame problem in AT <CITATION/>, and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. (Pollard and Sag [1994, 3141, following Flickinger [19871). This is so since the lexical rule in Figure 2 ''(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.''",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,some philosophical problems from the standpoint of artificial intelligence,1969,John McCarthy; Patrick Hayes
1437,W06-2933,External_4592,[2],introduction,"All experiments have been performed using MaltParser <TARGET_CITATION/> , version 0.4 , which is made available together with the suite of programs used for preand postprocessing .1"," Support vector machines for mapping histories to parser actions <CITATION/>.  Graph transformations for recovering nonprojective structures <CITATION/>. All experiments have been performed using MaltParser <TARGET_CITATION/> , version 0.4 , which is made available together with the suite of programs used for preand postprocessing .1 All experiments have been performed using MaltParser <CITATION/>, version 0.4, which is made available together with the suite of programs used for preand postprocessing.1 Graph transformations for recovering nonprojective structures <CITATION/>. Support vector machines for mapping histories to parser actions <CITATION/>.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,,maltparser a datadriven parsergenerator for dependency parsing,2006,J Nivre; J Hall; J Nilsson
1438,J06-2002,P00-1012,[0],,One area of current interest concerns the lefttoright arrangement of premodifying adjectives within an NP <TARGET_CITATION/> .,"An inference phase, during which the list L is transformed; 3. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things. One area of current interest concerns the lefttoright arrangement of premodifying adjectives within an NP <TARGET_CITATION/> . One area of current interest concerns the lefttoright arrangement of premodifying adjectives within an NP <CITATION/>. A realization phase, during which the choice between base, superlative, and comparative forms is made, among other things.An inference phase, during which the list L is transformed; 3.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf
1439,D09-1087,P05-1022,[5],conclusion,Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <TARGET_CITATION/> for self training .,"We show for the first time that selftraining is able to significantly improve the performance of a PCFGLA parser, a single generative parser, on both small and large amounts of labeled training data. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by overfitting. Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <TARGET_CITATION/> for self training . Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <CITATION/> for self training. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by overfitting. We show for the first time that selftraining is able to significantly improve the performance of a PCFGLA parser, a single generative parser, on both small and large amounts of labeled training data.",5bfd8d40bc071fffaf93685a46974b122ee4239d,Self-Training PCFG Grammars with Latent Annotations Across Languages,2009,Zhongqiang Huang; M. Harper,0ecb33ced5b0976accdf13817151f80568b6fdcb,Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking,2005,Eugene Charniak; Mark Johnson
1440,D09-1056,D07-1074,[0],related work,Some researchers <TARGET_CITATION/> have explored the use of Wikipedia information to improve the disambiguation process .,"Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <CITATION/>. Some researchers <TARGET_CITATION/> have explored the use of Wikipedia information to improve the disambiguation process . Some researchers <CITATION/> have explored the use of Wikipedia information to improve the disambiguation process. Other representations use the link structure <CITATION/> or generate graph representations of the extracted features <CITATION/>. Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,1c909ac1c331c0c246a88da047cbdcca9ec9b7e7,Large-Scale Named Entity Disambiguation Based on Wikipedia Data,2007,Silviu Cucerzan
1441,W06-1639,External_94528,[0],related work,Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <TARGET_CITATION/> .,"There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. An exception is <CITATION/>, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <TARGET_CITATION/> . Others have applied the NLP technologies of nearduplicate detection and topicbased text categorization to politically oriented text <CITATION/>. An exception is <CITATION/>, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,,nearduplicate detection for erulemaking,2005,H Yang; J Callan
1442,P08-1101,W04-3236,[2],experiments,We chose to follow <TARGET_CITATION/> and split the sentences evenly to facilitate further comparison .,"However, the comparison is indirect because our partitions of the CTB corpus are different. <CITATION/> also chunked the sentences before doing 10fold cross validation, but used an uneven split. We chose to follow <TARGET_CITATION/> and split the sentences evenly to facilitate further comparison . We chose to follow <CITATION/> and split the sentences evenly to facilitate further comparison. <CITATION/> also chunked the sentences before doing 10fold cross validation, but used an uneven split. However, the comparison is indirect because our partitions of the CTB corpus are different.",3594af2ebf510609651bf282dfea65c8e837b1a7,Joint Word Segmentation and POS Tagging Using a Single Perceptron,2008,Yue Zhang; S. Clark,35640547b3ba7989b5abbb9d269055e736d9dff3,Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based?,2004,H. Ng; Jin Kiat Low
1443,W00-1312,External_11755,[0],method,"That is , a document that contains terms al , a2 and a3 may be ranked higher than a document which contains terms al and b.f. However , the second document is more likely to be relevant since correct translations of the query terms are more likely to cooccur <TARGET_CITATION/> .","Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common. Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query. That is , a document that contains terms al , a2 and a3 may be ranked higher than a document which contains terms al and b.f. However , the second document is more likely to be relevant since correct translations of the query terms are more likely to cooccur <TARGET_CITATION/> . That is, a document that contains terms al, a2 and a3 may be ranked higher than a document which contains terms al and b.f. However, the second document is more likely to be relevant since correct translations of the query terms are more likely to cooccur <CITATION/>. Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query. Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,resolving ambiguity for crosslanguage retrievalquot,1998,L Ballesteros; W B Croft
1444,J01-4001,External_48126,[5],,"The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field <TARGET_CITATION/> .","Finally, in his squib Kibble discusses a reformulation of the centering transitions (Continue, Retain, and Shift), which specify the center movement across sentences. Instead of defining a total preference ordering, Kibble argues that a partial ordering emerges from the interaction among cohesion (maintaining the same center), salience (realizing the center as subject), and cheapness (realizing the anticipated center of a following utterance as subject). The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field <TARGET_CITATION/> . The last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field <CITATION/>. Instead of defining a total preference ordering, Kibble argues that a partial ordering emerges from the interaction among cohesion (maintaining the same center), salience (realizing the center as subject), and cheapness (realizing the anticipated center of a following utterance as subject). Finally, in his squib Kibble discusses a reformulation of the centering transitions (Continue, Retain, and Shift), which specify the center movement across sentences.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,3ba21326db5c2dc1708057b420dd31654baf966c,Outstanding Issues in Anaphora Resolution (Invited Talk),2001,R. Mitkov
1445,J97-4003,External_48954,[2],introduction,"Using an accumulator passing technique <TARGET_CITATION/> , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .","In fact, one can view the representations as notational variants of one another. Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause. Using an accumulator passing technique <TARGET_CITATION/> , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules . Using an accumulator passing technique (O'Keefe 1990), we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules. Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause. In fact, one can view the representations as notational variants of one another.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,d8db14be4944eca43ee0cec117cd3b2cd4344835,The Craft of Prolog,1990,Richard A. O'Keefe
1446,W04-1805,W02-1403,[4],introduction,"However , most strategies are based on  internal '' or  external methods '' <TARGET_CITATION/> , i.e. methods that rely on the form of terms or on the information gathered from contexts .","Recent literature in computational terminology has shown an increasing interest in identifying various semantic relationships between terms. Different strategies have been developed in order to identify pairs of terms that share a specific semantic relationship (such as hyperonymy or meronymy) or to build classes of terms. However , most strategies are based on  internal '' or  external methods '' <TARGET_CITATION/> , i.e. methods that rely on the form of terms or on the information gathered from contexts . However, most strategies are based on internal'' or external methods'' <CITATION/>, i.e. methods that rely on the form of terms or on the information gathered from contexts. Different strategies have been developed in order to identify pairs of terms that share a specific semantic relationship (such as hyperonymy or meronymy) or to build classes of terms. Recent literature in computational terminology has shown an increasing interest in identifying various semantic relationships between terms.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,51dd8780f090e58bc617b2baf549b1434a53907f,Lexically-Based Terminology Structuring: Some Inherent Limits,2002,N. Grabar; Pierre Zweigenbaum
1447,P97-1063,External_19462,[0],introduction,"With the exception of <CITATION/> , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts <TARGET_CITATION/> .","The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy.2 Cooccurrence With the exception of <CITATION/> , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts <TARGET_CITATION/> . With the exception of <CITATION/>, previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts <CITATION/>. 2 CooccurrenceThe hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,automatic evaluation and uniform filter cascades for inducing nbest translation lexiconsquot,1995,I D Melamed
1448,W02-1601,External_76422,[2],,A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in <TARGET_CITATION/> .,"It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (01) for John'', (12) for picks'', (23) for the'', (34) for box'' and (45) for up''. A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in <TARGET_CITATION/> . A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in <CITATION/>.An interval is assigned to each word in the sentence, i.e. (01) for John'', (12) for picks'', (23) for the'', (34) for box'' and (45) for up''. It contains a nonprojective correspondence.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,b195aa91260fbb9e90cc8198cde19408860c9180,Natural Languages Analysis in Machine Translation (MT) Based on the STCG (String-Tree Correspondence Grammar),1995,E. Tang; Yusoff Zaharin
1449,D09-1056,W07-2012,[0],introduction,"The Web People Search task , as defined in the first WePS evaluation campaign <TARGET_CITATION/> , consists of grouping search results for a given name according to the different people that share it .","The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task , as defined in the first WePS evaluation campaign <TARGET_CITATION/> , consists of grouping search results for a given name according to the different people that share it . The Web People Search task, as defined in the first WePS evaluation campaign <CITATION/>, consists of grouping search results for a given name according to the different people that share it. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,fcc301c51ce866dcb7db73d6a3711ceccb8f2aa3,The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task,2007,J. Artiles; Julio Gonzalo; S. Sekine
1451,D08-1034,J08-2004,[2],experiments,"We use the same data setting with <TARGET_CITATION/> , however a bit different from <CITATION/> .","fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with <TARGET_CITATION/> , however a bit different from <CITATION/> . We use the same data setting with <CITATION/>, however a bit different from <CITATION/>.The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. fid.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,03541f4c7b737964289b3cb2cde4b6ac08a1c29d,Labeling Chinese Predicates with Semantic Roles,2008,Nianwen Xue
1452,D08-1007,External_4260,[0],related work,<CITATION/> compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and <TARGET_CITATION/> ) 's informationtheoretic metric work best .,"In contrast, <CITATION/> generalizes by substituting similar arguments, while <CITATION/> use the crossproduct of similar pairs. One key issue is how to define the set of similar words, SIMS(w). <CITATION/> compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and <TARGET_CITATION/> ) 's informationtheoretic metric work best . <CITATION/> compared a number of techniques for creating similarword sets and found that both the Jaccard coefficient and <CITATION/>'s informationtheoretic metric work best. One key issue is how to define the set of similar words, SIMS(w). In contrast, <CITATION/> generalizes by substituting similar arguments, while <CITATION/> use the crossproduct of similar pairs.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
1453,K15-1003,W14-1615,[3],introduction,We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger <TARGET_CITATION/> .,"pler categories by encoding a notion of category simplicity into a prior <CITATION/>. Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger <TARGET_CITATION/> . We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger <CITATION/>. Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. pler categories by encoding a notion of category simplicity into a prior <CITATION/>.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,05bbf7436bfcb1e49e1132c16f25706b61de6c93,Weakly-Supervised Bayesian Learning of a CCG Supertagger,2014,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith
1454,J06-2002,External_40449,[0],introduction,"4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a typerelated property if none is present yet ( cfXXX , <TARGET_CITATION/> ) .","Suppose the target is c4:3 The degree of precision of the measurement (James et al. 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. 4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a typerelated property if none is present yet ( cfXXX , <TARGET_CITATION/> ) . 4 To turn this likelihood into a certainty, one can add a test at the end of the algorithm, which adds a typerelated property if none is present yet (cfXXX, Dale and Reiter 1995). 3 The degree of precision of the measurement (James et al. 1996, Section 1.5) determines which objects can be described by the GRE algorithm, since it determines which objects count as having the same size. Suppose the target is c4:",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,a32c486987fb5df4d8dc9133180d51cee899478a,Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions,1995,R. Dale; Ehud Reiter
1455,E03-1007,External_22209,[0],,For descriptions of SMT systems see for example <TARGET_CITATION/> .,"If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example <TARGET_CITATION/> . For descriptions of SMT systems see for example <CITATION/>.In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. If necessary, the inverse of these transformations will be applied to the generated output string.",339ec71f2e0907ba376a3c8e7b7a89c592be3fdd,Using POS Information for SMT into Morphologically Rich Languages,2003,Nicola Ueffing; H. Ney,8b0495331238da6c0e7be0bfdb9b5453b33c1f98,Improved Alignment Models for Statistical Machine Translation,1999,F. Och; Christoph Tillmann; H. Ney
1456,P02-1001,External_876,[0],introduction,"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisychannel decoding , ' including classic models for speech recognition <TARGET_CITATION/> and machine translation <CITATION/> .","An artificial example will appear in §2. The availability of toolkits for this weighted case (<CITATION/>; van <CITATION/>) promises to unify much of statistical NLP. Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisychannel decoding , ' including classic models for speech recognition <TARGET_CITATION/> and machine translation <CITATION/> . Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisychannel decoding,' including classic models for speech recognition <CITATION/> and machine translation <CITATION/>. The availability of toolkits for this weighted case (<CITATION/>; van <CITATION/>) promises to unify much of statistical NLP. An artificial example will appear in §2.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,0bf2c2ca87256956b3e51bec4845c4fa28d4de7b,Speech Recognition by Composition of Weighted Finite Automata,1996,Fernando C Pereira; M. Riley
1457,P97-1063,External_20588,[4],method,It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <TARGET_CITATION/> .,"Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u, v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <TARGET_CITATION/> . It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <CITATION/>. This step significantly reduces the computational burden of the algorithm. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u, v) < 1.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,d609fd5e328a06a67f883c08e609bc57583100f0,Building Probabilistic Models for Natural Language,1996,Stanley F. Chen
1459,J09-4010,W04-1017,[2],method,"After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by <TARGET_CITATION/> to penalize redundant sentences in cohesive clusters .","In order to ensure the relevance of the generated replies, we have placed tight restrictions on prediction probability and cluster cohesion (Table 3), which cause the SentPred method to often return partial responses. Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by <TARGET_CITATION/> to penalize redundant sentences in cohesive clusters . After calculating the raw score of each sentence, we use a modified version of the Adaptive Greedy Algorithm by <CITATION/> to penalize redundant sentences in cohesive clusters. Removing redundant sentences. In order to ensure the relevance of the generated replies, we have placed tight restrictions on prediction probability and cluster cohesion (Table 3), which cause the SentPred method to often return partial responses.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,f2a665c064f84a301d64a0ab79f9ed6047dc74e2,Event-Based Extractive Summarization,2004,Elena Filatova; V. Hatzivassiloglou
1460,J00-2004,External_3270,[2],,"If each word 's translation is treated as a sense tag <TARGET_CITATION/> , then  translational '' collocations have the unique property that the collocate and the word sense are one and the same !","The ratio links(u, v) I cooc (u, v), for several values of cooc(u, v). in bitext space is another kind of collocation. If each word 's translation is treated as a sense tag <TARGET_CITATION/> , then  translational '' collocations have the unique property that the collocate and the word sense are one and the same ! If each word's translation is treated as a sense tag <CITATION/>, then translational'' collocations have the unique property that the collocate and the word sense are one and the same! in bitext space is another kind of collocation. The ratio links(u, v) I cooc (u, v), for several values of cooc(u, v).",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,812c2ad5d26f474d1c499c2665ba1a9e4fd74436,A Perspective on Word Sense Disambiguation Methods and Their Evaluation,2002,P. Resnik
1461,J09-4010,External_42507,[2],method,"The values of a vector correspond to the presence or absence of each ( lemmatized ) corpus word in the document in question ( after removing stopwords and words with very low frequency ) .4 The predictive model is a Decision Graph <TARGET_CITATION/> , which , like Snob , is based on the MML principle .","We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the SentPred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each ( lemmatized ) corpus word in the document in question ( after removing stopwords and words with very low frequency ) .4 The predictive model is a Decision Graph <TARGET_CITATION/> , which , like Snob , is based on the MML principle . The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stopwords and words with very low frequency).4 The predictive model is a Decision Graph <CITATION/>, which, like Snob, is based on the MML principle. The input to Snob is a set of binary vectors, one vector per response document. We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the SentPred method, Section 3.2.2).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,73f1d17df0e1232da9e2331878a802a941f351c6,Decision Graphs - An Extension of Decision Trees,1993,Jonathan J. Oliver
1464,J92-1004,External_91850,[2],conclusion,"One , the VOYAGER domain <TARGET_CITATION/> , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .","The perplexity (average number of words that can follow a given word) decreased from 70 to 28 to 8 when the grammar changed from wordpair (derived from the same grammar) to parser without probabilities to parser with probabilities. We currently have two application domains that can carry on a spoken dialog with a user. One , the VOYAGER domain <TARGET_CITATION/> , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . One, the VOYAGER domain <CITATION/>, answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. We currently have two application domains that can carry on a spoken dialog with a user. The perplexity (average number of words that can follow a given word) decreased from 70 to 28 to 8 when the grammar changed from wordpair (derived from the same grammar) to parser without probabilities to parser with probabilities.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,the voyager speech understanding system preliminary development and evaluationquot,1990,V Zue; J Glass; D Goodine; H Leung; M Phillips; J Polifroni; S Seneff
1465,W06-3309,W00-1302,[4],related work,"Our task is closer to the work of <TARGET_CITATION/> , who looked at the problem of intellectual attribution in scientific texts .","Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <CITATION/>. Our task is closer to the work of <TARGET_CITATION/> , who looked at the problem of intellectual attribution in scientific texts . Our task is closer to the work of <CITATION/>, who looked at the problem of intellectual attribution in scientific texts.Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <CITATION/>. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,9a66167f0ec32ecec86eb1021be12ea611aa3d46,What’s Yours and What’s Mine: Determining Intellectual Attribution in Scientific Text,2000,Simone Teufel; M. Moens
1466,W06-1104,External_1728,[0],related work,"In the seminal work by <TARGET_CITATION/> , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards ."," In the seminal work by <TARGET_CITATION/> , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards . In the seminal work by <CITATION/>, similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,7ef3ac14cdb484aaa2b039850093febd5cf73a21,Contextual correlates of synonymy,1965,H. Rubenstein; J. Goodenough
1467,D12-1037,P02-1038,[4],related work,<TARGET_CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it .,Several works have proposed discriminative techniques to train loglinear model for SMT. <TARGET_CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it . <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,37fadfb6d60e83e24c72d8a90da5644b39d6e8f0,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,2002,F. Och; H. Ney
1470,J87-3002,External_29754,[0],,<TARGET_CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .,"The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, IS, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to IS if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. <TARGET_CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description . <CITATION/> provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, IS, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to IS if the verb is felt to be semantically two place rather than one place, such as know versus appear.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,designing a computerised lexicon for linguistic purposes,1985,Erik Akkerman; Pieter Masereeuw; Willem Meijs
1471,D08-1007,External_4260,[2],experiments,"We parsed the 3 GB AQUAINT corpus <CITATION/> using Minipar <TARGET_CITATION/> , and collected verbobject and verbsubject frequencies , building an empirical MI model from this data ."," We parsed the 3 GB AQUAINT corpus <CITATION/> using Minipar <TARGET_CITATION/> , and collected verbobject and verbsubject frequencies , building an empirical MI model from this data . We parsed the 3 GB AQUAINT corpus <CITATION/> using Minipar <CITATION/>, and collected verbobject and verbsubject frequencies, building an empirical MI model from this data.",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,fd1901f34cc3673072264104885d70555b1a4cdc,Automatic Retrieval and Clustering of Similar Words,1998,Dekang Lin
1472,W06-2933,External_380,[2],method,"More specifically , we use LIBSVM <TARGET_CITATION/> with a quadratic kernel K ( xZ , xj ) = (  yxT xj + r ) 2 and the builtin oneversusall strategy for multiclass classification .","We use support vector machines3 to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM <TARGET_CITATION/> with a quadratic kernel K ( xZ , xj ) = (  yxT xj + r ) 2 and the builtin oneversusall strategy for multiclass classification . More specifically, we use LIBSVM <CITATION/> with a quadratic kernel K(xZ, xj) = (yxT xj +r)2 and the builtin oneversusall strategy for multiclass classification. We use support vector machines3 to predict the next parser action from a feature vector representing the history.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,,libsvm a library for support vector machines software available at httpwwwcsientuedutw cjlinlibsvm,2001,C-C Chang; C-J Lin
1473,P02-1001,External_24070,[0],,"It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an cclosure step <TARGET_CITATION/> that implements the allpairs version of algebraic path , whereas all we need is the singlesource version .","Let Ti = xiofoyi. Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an cclosure step <TARGET_CITATION/> that implements the allpairs version of algebraic path , whereas all we need is the singlesource version . It is wasteful to compute ti as suggested earlier, by minimizing (cxxi)of o(yixE), since then the real work is done by an cclosure step <CITATION/> that implements the allpairs version of algebraic path, whereas all we need is the singlesource version. Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and unweighted). Let Ti = xiofoyi.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,generic epsilonremoval and input epsilonnormalization algorithms for weighted transducers,2002,M Mohri
1474,P07-1068,P02-1014,[4],,"positional features that have been employed by highwe can see , the baseline achieves an Fmeasure of performing resolvers such as <TARGET_CITATION/> 57.0 and a resolution accuracy of 48.4 .","Following previous work (e.g., <CITATION/>), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1. Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highwe can see , the baseline achieves an Fmeasure of performing resolvers such as <TARGET_CITATION/> 57.0 and a resolution accuracy of 48.4 . positional features that have been employed by highwe can see, the baseline achieves an Fmeasure of performing resolvers such as <CITATION/> 57.0 and a resolution accuracy of 48.4. Each instance is represented by 33 lexical, grammatical, semantic, andFollowing previous work (e.g., <CITATION/>), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,08c81389b3ac4b8253d718a7cebe04a5536efa78,Improving Machine Learning Approaches to Coreference Resolution,2002,Vincent Ng; Claire Gardent
1475,W06-3309,External_17763,[5],conclusion,Such a component would serve as the first stage of a clinical question answering system <CITATION/> or summarization system <TARGET_CITATION/> .,"The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsicallyvalid measure of performance. Such a component would serve as the first stage of a clinical question answering system <CITATION/> or summarization system <TARGET_CITATION/> . Such a component would serve as the first stage of a clinical question answering system <CITATION/> or summarization system <CITATION/>. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsicallyvalid measure of performance. The true utility of content models is to structure abstracts that have no structure to begin with.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,1a54ba4062ac41fd06e93d022c54ac8c73c1d684,Leveraging a common representation for personalized search and summarization in a medical digital library,2003,K. McKeown; Noémie Elhadad; V. Hatzivassiloglou
1476,J05-3003,P87-1027,[0],related work,"<CITATION/> predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX <CITATION/> and ANLT <TARGET_CITATION/> dictionaries and adding around 30 frames found by manual inspection .","The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following <CITATION/>. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verbsubcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. <CITATION/> predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX <CITATION/> and ANLT <TARGET_CITATION/> dictionaries and adding around 30 frames found by manual inspection . <CITATION/> predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX <CITATION/> and ANLT <CITATION/> dictionaries and adding around 30 frames found by manual inspection. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verbsubcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following <CITATION/>.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,583cfbb0410fcd87a4833255013475694d202a25,The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of Contemporary English,1987,B. Boguraev; Ted Briscoe; John A. Carroll; D. Carter; Claire Grover
1477,W06-3813,External_15775,[0],introduction,The idea resurfaced forcefully at several points in the more recent history of linguistic research <TARGET_CITATION/> .,The first chronicled endeavour to connect text elements and organise connections between them goes back to the 51h century B.C. and the work of Paninil. He was a grammarian who analysed Sanskrit <CITATION/>. The idea resurfaced forcefully at several points in the more recent history of linguistic research <TARGET_CITATION/> . The idea resurfaced forcefully at several points in the more recent history of linguistic research <CITATION/>. He was a grammarian who analysed Sanskrit <CITATION/>. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 51h century B.C. and the work of Paninil.,f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,5c79d503a1f38a92f5e8f3f8c5225d0967b59971,Studies in lexical relations,1965,Jeffrey Gruber
1478,J09-4010,External_36090,[0],introduction,It is therefore no surprise that early attempts at response automation were knowledgedriven <TARGET_CITATION/> .,"1 http://customercare.telephonyonline.com/ar/telecom next generation customer.circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledgedriven <TARGET_CITATION/> . It is therefore no surprise that early attempts at response automation were knowledgedriven <CITATION/>. circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. 1 http://customercare.telephonyonline.com/ar/telecom next generation customer.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,knowledge harvesting articulation and delivery the hewlettpackard journal,1998,K A Delic; D Lahaix
1479,W03-0806,External_5988,[0],experiments,"For example , the suite of LT tools <TARGET_CITATION/> perform tokenization , tagging and chunking on XML markedup text directly .","This allows components to be highly configurable and simplifies the addition of new components to the system. A number of standalone tools have also been developed. For example , the suite of LT tools <TARGET_CITATION/> perform tokenization , tagging and chunking on XML markedup text directly . For example, the suite of LT tools <CITATION/> perform tokenization, tagging and chunking on XML markedup text directly. A number of standalone tools have also been developed. This allows components to be highly configurable and simplifies the addition of new components to the system.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,082dd9f93c88993b866aa55ed06eda4ace7f36c8,LT TTT - A Flexible Tokenisation Tool,2000,Claire Grover; C. Matheson; Andrei Mikheev; M. Moens
1480,D09-1056,W07-2012,[2],related work,It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <TARGET_CITATION/> .,The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>. Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <TARGET_CITATION/> . It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own <CITATION/>. Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>.,a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,fcc301c51ce866dcb7db73d6a3711ceccb8f2aa3,The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task,2007,J. Artiles; Julio Gonzalo; S. Sekine
1481,D12-1037,External_21949,[2],,"To retrieve translation examples for a test sentence , <TARGET_CITATION/> defined a metric based on the combination of edit distance and TFIDF ( Manning and Sch  utze , 1999 ) as follows :","We assume that the metric satisfy the property: more similar the test sentence and translation examples are, the better translation result one obtains when decoding the test sentence with the weight trained on the translation examples. The metric we consider here is derived from an examplebased machine translation. To retrieve translation examples for a test sentence , <TARGET_CITATION/> defined a metric based on the combination of edit distance and TFIDF ( Manning and Sch  utze , 1999 ) as follows : To retrieve translation examples for a test sentence, <CITATION/> defined a metric based on the combination of edit distance and TFIDF <CITATION/> as follows:The metric we consider here is derived from an examplebased machine translation. We assume that the metric satisfy the property: more similar the test sentence and translation examples are, the better translation result one obtains when decoding the test sentence with the weight trained on the translation examples.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,,examplebased decoding for statistical machine translation,2003,Taro Watanabe; Eiichiro Sumita
1482,P97-1063,External_1429,[0],,"By using the EM algorithm <TARGET_CITATION/> , they can guarantee convergence towards the globally optimum parameter set .","It is not clear how the precision/recall tradeoff can be controlled in the IBM models. One advantage that Brown et al.'s Model 1 has over our wordtoword model is that their objective function has no local maxima. By using the EM algorithm <TARGET_CITATION/> , they can guarantee convergence towards the globally optimum parameter set . By using the EM algorithm <CITATION/>, they can guarantee convergence towards the globally optimum parameter set. One advantage that Brown et al.'s Model 1 has over our wordtoword model is that their objective function has no local maxima. It is not clear how the precision/recall tradeoff can be controlled in the IBM models.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,d36efb9ad91e00faa334b549ce989bfae7e2907a,Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper,1977,A. Dempster; N. Laird; D. Rubin
1483,J86-1002,External_43441,[5],conclusion," use of low level knowledge from the speech recognition phase ,  use of high level knowledge about the domain in particular and the dialogue task in general ,  a  continue '' facility and an  autoloop '' facility as described by <TARGET_CITATION/> ,  a  conditioning '' facility as described by <CITATION/> ,  implementation of new types of paraphrasing ,  checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and  examining interspeaker dialogue patterns .","However, there are many possible extensions that could be examined in the future and added to the implementation if the investigation indicates that it would create a yet more usable system. These include the following: use of low level knowledge from the speech recognition phase ,  use of high level knowledge about the domain in particular and the dialogue task in general ,  a  continue '' facility and an  autoloop '' facility as described by <TARGET_CITATION/> ,  a  conditioning '' facility as described by <CITATION/> ,  implementation of new types of paraphrasing ,  checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and  examining interspeaker dialogue patterns .  use of low level knowledge from the speech recognition phase,  use of high level knowledge about the domain in particular and the dialogue task in general,  a continue'' facility and an autoloop'' facility as described by <CITATION/>,  a conditioning'' facility as described by <CITATION/>,  implementation of new types of paraphrasing,  checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and  examining interspeaker dialogue patterns.These include the following:However, there are many possible extensions that could be examined in the future and added to the implementation if the investigation indicates that it would create a yet more usable system.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,construction of programs from example computations,1976,A Biermann; R Krishnaswamy
1484,D13-1038,W12-1621,[3],,"Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions <TARGET_CITATION/> .","Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features <CITATION/>. Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions <TARGET_CITATION/> . Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions <CITATION/>. Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features <CITATION/>.",e065f4c930f5dad1f6f82a14c180815d418ff765,Towards Situated Dialogue: Revisiting Referring Expression Generation,2013,Rui Fang; Changsong Liu; Lanbo She; J. Chai,6c53c56064c460bac13efb02d30874024730a673,Towards Mediating Shared Perceptual Basis in Situated Dialogue,2012,Changsong Liu; Rui Fang; J. Chai
1485,D09-1056,W07-2041,[0],related work,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> .","The most used feature for the Web People Search task, however, are NEs. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents  see for instance <TARGET_CITATION/> . In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents see for instance <CITATION/>. <CITATION/> introduced a rulebased approach that tackles both variation and ambiguity analysing the structure of names. The most used feature for the Web People Search task, however, are NEs.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,03b6cd177ccf8d73c05a5898ad23d74a049612f3,IRST-BP: Web People Search Using Name Entities,2007,Octavian Popescu; B. Magnini
1486,W03-0806,External_49343,[2],experiments,"To provide the required configurability in the static version of the code we will use policy templates <TARGET_CITATION/> , and for the dynamic version we will use configuration classes .","Templates will be used heavily to provide generality without significantly impacting on efficiency. However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces. To provide the required configurability in the static version of the code we will use policy templates <TARGET_CITATION/> , and for the dynamic version we will use configuration classes . To provide the required configurability in the static version of the code we will use policy templates <CITATION/>, and for the dynamic version we will use configuration classes. However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces. Templates will be used heavily to provide generality without significantly impacting on efficiency.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,b516d9bde1885a335711b3013b31e174cd432ca8,Modern C++ design: generic programming and design patterns applied,2001,Andrei Alexandrescu
1488,J00-2004,W95-0115,[3],,"In informal experiments described elsewhere <TARGET_CITATION/> , I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02 .","The statistical interdependence between two word types can be estimated more robustly by considering the whole table. For example, Gale and Church (1991, 154) suggest that 02, a x2like statistic, seems to be a particularly good choice because it makes good use of the offdiagonal cells'' in the contingency table. In informal experiments described elsewhere <TARGET_CITATION/> , I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02 . In informal experiments described elsewhere <CITATION/>, I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02. For example, Gale and Church (1991, 154) suggest that 02, a x2like statistic, seems to be a particularly good choice because it makes good use of the offdiagonal cells'' in the contingency table.The statistical interdependence between two word types can be estimated more robustly by considering the whole table.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,42fd4d469c53e4eedd7eb76e7859e3270367f795,Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best Translation Lexicons,1995,I. D. Melamed
1489,J03-3004,External_61553,[0],introduction," language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <TARGET_CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different languagerelated tasks, including language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <TARGET_CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )  language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002)The marker hypothesis has been used for a number of different languagerelated tasks, includingThe marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,a psycholinguistic approach to corpusbased machine translation,1994,Patrick Juola
1490,D11-1138,D11-1017,[4],related work,A recent study by <TARGET_CITATION/> also investigates the task of training parsers to improve MT reordering ., A recent study by <TARGET_CITATION/> also investigates the task of training parsers to improve MT reordering . A recent study by <CITATION/> also investigates the task of training parsers to improve MT reordering.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,37c74abd5287796d711a86d1564d4a782b1a6f24,Training a Parser for Machine Translation Reordering,2011,Jason Katz-Brown; Slav Petrov; Ryan T. McDonald; F. Och; David Talbot; Hiroshi Ichikawa; Masakazu Seno; H. Kazawa
1491,Q13-1020,N04-1035,[0],introduction,"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <TARGET_CITATION/> ) .","In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <TARGET_CITATION/> ) . Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, treebased translation models have shown promising progress in improving translation quality (<CITATION/>, 2009; <CITATION/>, 2006; <CITATION/>). In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,a7e925a65860e90b2b4eb427a8bc497f76b2fe6e,What’s in a translation rule?,2004,Michel Galley; Mark Hopkins; Kevin Knight; D. Marcu
1492,J97-4003,C96-1092,[0],introduction,15 <TARGET_CITATION/> show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .,13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 <TARGET_CITATION/> show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention . 15 <CITATION/> show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention. 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 13 A more detailed presentation can be found in Minnen (in preparation).,d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,c2680f599e198e385d2b28ebf27ec83156736cd0,Applying Lexical Rules Under Subsumption,1996,E. Hinrichs; Tsuneko Nakazawa
1493,D13-1115,External_351,[0],method,"Latent Dirichlet Allocation <TARGET_CITATION/> , or LDA , is an unsupervised Bayesian probabilistic model of text documents ."," Latent Dirichlet Allocation <TARGET_CITATION/> , or LDA , is an unsupervised Bayesian probabilistic model of text documents . Latent Dirichlet Allocation <CITATION/>, or LDA, is an unsupervised Bayesian probabilistic model of text documents.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4574d77fff19e093782178595a8988a7f3aa1969,Latent Dirichlet Allocation,2009,D. Blei; A. Ng; Michael I. Jordan
1494,D09-1143,P06-1117,[5],conclusion,"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <CITATION/> or shallow semantic trees , <TARGET_CITATION/> .","For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <CITATION/> or shallow semantic trees , <TARGET_CITATION/> . Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNetbased features <CITATION/> or shallow semantic trees, <CITATION/>. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task.",a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi,0f96123f18df8ce0656c6adb4c37edd10ee87d30,"Semantic Role Labeling via FrameNet, VerbNet and PropBank",2006,Ana-Maria Giuglea; Alessandro Moschitti
1499,W06-1705,External_25127,[0],introduction,"Most webderived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like HyppiaBTE , Tidy and Parcels3 <TARGET_CITATION/> .","We can evaluate annotation speed gains of our approach comparatively against the single server version by utilising processing power in computer labs at Lancaster University and the United States Naval Academy (USNA) and we will call for volunteers from the corpus community to be involved in the evaluation as well. A key aspect of our case study research will be to investigate extending corpus collection to new document types. Most webderived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like HyppiaBTE , Tidy and Parcels3 <TARGET_CITATION/> . Most webderived corpora have exploited raw text or HTML pages, so efforts have focussed on boilerplate removal and cleanup of these formats with tools like HyppiaBTE, Tidy and Parcels3 <CITATION/>. A key aspect of our case study research will be to investigate extending corpus collection to new document types. We can evaluate annotation speed gains of our approach comparatively against the single server version by utilising processing power in computer labs at Lancaster University and the United States Naval Academy (USNA) and we will call for volunteers from the corpus community to be involved in the evaluation as well.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,creating specialized and general corpora using automated search engine queries web as corpus workshop,2005,M Baroni; S Sharoff
1500,P02-1001,External_220,[0],introduction,Undesirable consequences of this fact have been termed  label bias '' <TARGET_CITATION/> .," An easy approach is to normalize the options at each state to make the FST Markovian. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed  label bias '' <TARGET_CITATION/> . Undesirable consequences of this fact have been termed label bias'' <CITATION/>. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.  An easy approach is to normalize the options at each state to make the FST Markovian.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,f4ba954b0412773d047dc41231c733de0c1f4926,Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,2001,J. Lafferty; A. McCallum; Fernando Pereira
1503,P13-3018,External_359,[0],introduction,"Their computational significance arises from the issue of their storage in lexical resources like WordNet <TARGET_CITATION/> and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .","A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. Their computational significance arises from the issue of their storage in lexical resources like WordNet <TARGET_CITATION/> and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency . Their computational significance arises from the issue of their storage in lexical resources like WordNet <CITATION/> and raises the questions like, how to store morphologically complex words, in a lexical resource like WordNet keeping in mind the storage and access efficiency. Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications. A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,d53bcbac7ea19173e95d3bd855b998fab765737d,WordNet: An Electronic Lexical Database,1998,Dekang Lin
1504,E03-1005,External_16080,[0],introduction,"Fortunately , there exists a compact PCFGreduction of DOP1 that generates the same trees with the same probabilities , as shown by <TARGET_CITATION/> ) .","Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of singlelevel rules to counts of entire trees. A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Fortunately , there exists a compact PCFGreduction of DOP1 that generates the same trees with the same probabilities , as shown by <TARGET_CITATION/> ) . Fortunately, there exists a compact PCFGreduction of DOP1 that generates the same trees with the same probabilities, as shown by Goodman (1996, 2002). A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account. Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of singlelevel rules to counts of entire trees.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa,Efficient Algorithms for Parsing the DOP Model,1996,J. Goodman
1505,J90-3003,External_73666,[0],introduction,"<TARGET_CITATION/> claims that prosodic phrase boundaries will cooccur with grammatical functions such as subject , predicate , modifier , and adjunct .","The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. <TARGET_CITATION/> claims that prosodic phrase boundaries will cooccur with grammatical functions such as subject , predicate , modifier , and adjunct . <CITATION/> claims that prosodic phrase boundaries will cooccur with grammatical functions such as subject, predicate, modifier, and adjunct. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,217f96773aebf0f4ad5e3b84b5e567e4e3cc0520,Prosodic Systems and Intonation in English,1969,D. Crystal
1506,J00-1003,P97-1058,[2],,"We rephrase the method of <TARGET_CITATION/> as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above ."," We rephrase the method of <TARGET_CITATION/> as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above . We rephrase the method of <CITATION/> as follows: First, we construct the approximating finite automaton according to the unparameterized RTN method above.",b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof,5001ed3166af70a8ef296893b26f975a4f1de253,Approximating Context-Free Grammars with a Finite-State Calculus,1997,Edmund Grimley-Evans
1507,W06-2933,External_62,[4],experiments,6The analysis is reminiscent of the treatment of coordination in the Collins parser <TARGET_CITATION/> .,"In Turkish, very essential syntactic information is contained in the rich morphological structure, where 6The analysis is reminiscent of the treatment of coordination in the Collins parser <TARGET_CITATION/> . 6The analysis is reminiscent of the treatment of coordination in the Collins parser <CITATION/>.In Turkish, very essential syntactic information is contained in the rich morphological structure, where",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,,headdriven statistical models for natural language parsing,1999,M Collins
1508,W04-1805,External_89824,[1],introduction,The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information <TARGET_CITATION/> .,"The work reported here infers specific semantic relationships based on sets of examples and counterexamples. In this paper, the method is applied to a French corpus on computing to find nounverb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information <TARGET_CITATION/> . The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information (L'<CITATION/>). In this paper, the method is applied to a French corpus on computing to find nounverb combinations in which verbs convey a meaning of realization. The work reported here infers specific semantic relationships based on sets of examples and counterexamples.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,9cad9f686a89dcb8588790c2c19dcc64f09b1005,Sélection de termes dans un dictionnaire d’informatique : comparaison de corpus et critères lexico-sémantiques,2004,M.-C. L'homme
1509,A00-1016,A97-1001,[0],introduction,"CornmandTalk <TARGET_CITATION/> , Circuit FixIt Shop <CITATION/> and TRAINS96 <CITATION/> are spoken language systems but they interface to simulation or help facilities rather than semiautonomous agents .","More recent work on spoken language interfaces to semiautonomous robots include SRI's Flakey robot <CITATION/> and NCARAI's InterBOT project <CITATION/>. A number of other systems have addressed part of the task. CornmandTalk <TARGET_CITATION/> , Circuit FixIt Shop <CITATION/> and TRAINS96 <CITATION/> are spoken language systems but they interface to simulation or help facilities rather than semiautonomous agents . CornmandTalk <CITATION/>, Circuit FixIt Shop <CITATION/> and TRAINS96 <CITATION/> are spoken language systems but they interface to simulation or help facilities rather than semiautonomous agents. A number of other systems have addressed part of the task. More recent work on spoken language interfaces to semiautonomous robots include SRI's Flakey robot <CITATION/> and NCARAI's InterBOT project <CITATION/>.",229eebe84ed75649e83ebebdd283f248a913aaef,A Compact Architecture for Dialogue Management Based on Scripts and Meta-Outputs,2000,Manny Rayner; Beth Ann Hockey; Frankie James,b35076035130ea7683fb30b3314957fe9684bde3,CommandTalk: A Spoken-Language Interface for Battlefield Simulations,1997,Robert C. Moore; J. Dowding; H. Bratt; J. Gawron; Y. Gorfu; Adam Cheyer
1510,J05-3003,External_13798,[2],method,"The annotation procedure is dependent on locating the head daughter , for which an amended version of <TARGET_CITATION/> is used .","We utilize the automatic annotation algorithm of <CITATION/> and Cahill, McCarthy, et al. (2004) to derive a version of PennII in which each node in each tree is annotated with LFG functional annotations in the form of attributevalue structure equations. The algorithm uses categorial, configurational, local head, and PennII functional and trace information. The annotation procedure is dependent on locating the head daughter , for which an amended version of <TARGET_CITATION/> is used . The annotation procedure is dependent on locating the head daughter, for which an amended version of <CITATION/> is used. The algorithm uses categorial, configurational, local head, and PennII functional and trace information. We utilize the automatic annotation algorithm of <CITATION/> and Cahill, McCarthy, et al. (2004) to derive a version of PennII in which each node in each tree is annotated with LFG functional annotations in the form of attributevalue structure equations.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce,Natural Language Parsing as Statistical Pattern Recognition,1994,David M. Magerman
1511,J97-4003,External_24530,[4],related work,<TARGET_CITATION/> ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .,"Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification <TARGET_CITATION/> . Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,word formation in lexical type hierarchies a case study of baradjectives in german masters thesis,1993,Susanne Riehemann
1512,J00-2003,J97-4001,[0],introduction,"For instance , <TARGET_CITATION/> recently wrote :  To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) .","It is also conceivable that datadriven techniques can actually outperform traditional rules. However, this possibility is not usually given much credence. For instance , <TARGET_CITATION/> recently wrote :  To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) . For instance, <CITATION/> recently wrote: To our knowledge, learning algorithms, although promising, have not (yet) reached the level of rule sets developed by humans'' (p. 520). However, this possibility is not usually given much credence. It is also conceivable that datadriven techniques can actually outperform traditional rules.",18ff4f15416e34d9a56142e6f5d491567934e4fb,A multistrategy approach to improving pronunciation by analogy,2000,Y. Marchand; R. Damper,2a792467c88452a0013c2b61aecfb23cf178e015,Algorithms for Grapheme-Phoneme Translation for English and French: Applications for Database Searches and Speech Synthesis,1997,M. Divay; Anthony J. Vitale
1514,A00-1024,External_32929,[2],experiments,We use an inhouse statistical tagger ( based on <TARGET_CITATION/> ) to tag the text in which the unknown word occurs .,These identify two features of the unknown word itself as well as two features for each of the two preceding and two following words. The first feature represents the part of speech of the word. We use an inhouse statistical tagger ( based on <TARGET_CITATION/> ) to tag the text in which the unknown word occurs . We use an inhouse statistical tagger (based on <CITATION/>) to tag the text in which the unknown word occurs. The first feature represents the part of speech of the word. These identify two features of the unknown word itself as well as two features for each of the two preceding and two following words.,caa11f45ef1d8cdd6683a34b22407ec0f2c55d77,Categorizing Unknown Words: Using Decision Trees to Identify Names and Misspellings,2000,J. Toole,a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10,A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,1988,Kenneth Ward Church
1515,W04-0910,External_51288,[2],introduction,"Based on a computational grammar that associates natural language expressions with both a syntactic and a semantic representation , a paraphrastic gram  As we shall briefly discuss in section 4 , the grammar is developed with the help of a metagrammar <TARGET_CITATION/> thus ensuring an additional level of abstraction .","On the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low. We chose to investigate an alternative research direction by aiming to develop a paraphrastic grammar'' that is, a grammar which captures the paraphrastic relations between linguistic structuress. Based on a computational grammar that associates natural language expressions with both a syntactic and a semantic representation , a paraphrastic gram  As we shall briefly discuss in section 4 , the grammar is developed with the help of a metagrammar <TARGET_CITATION/> thus ensuring an additional level of abstraction . Based on a computational grammar that associates natural language expressions with both a syntactic and a semantic representation, a paraphrastic gramAs we shall briefly discuss in section 4, the grammar is developed with the help of a metagrammar <CITATION/> thus ensuring an additional level of abstraction. We chose to investigate an alternative research direction by aiming to develop a paraphrastic grammar'' that is, a grammar which captures the paraphrastic relations between linguistic structuress. On the other hand, the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,un outil multilingue de generation de ltag  application au francais et a l’italien,1999,M H Candito
1516,J09-4010,External_52005,[4],,There are very few reported attempts at corpusbased automation of helpdesk responses <TARGET_CITATION/> .,"In contrast, the techniques examined in this article are corpusbased and datadriven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpusbased automation of helpdesk responses <TARGET_CITATION/> . There are very few reported attempts at corpusbased automation of helpdesk responses <CITATION/>. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. In contrast, the techniques examined in this article are corpusbased and datadriven.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,mercure towards an automatic email followup system,2003,G Lapalme; L Kosseim
1518,D09-1056,External_2273,[0],related work,The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) <CITATION/> and Crossdocument Coreference ( CDC ) <TARGET_CITATION/> .,"In this section we will discuss (i) the state of the art in Web People Search in general, focusing on which features are used to solve the problem; and (ii) lessons learnt from the WePS evaluation campaign where most approaches to the problem have been tested and compared. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) <CITATION/> and Crossdocument Coreference ( CDC ) <TARGET_CITATION/> . The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) <CITATION/> and Crossdocument Coreference (CDC) <CITATION/>. In this section we will discuss (i) the state of the art in Web People Search in general, focusing on which features are used to solve the problem; and (ii) lessons learnt from the WePS evaluation campaign where most approaches to the problem have been tested and compared.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,,entitybased crossdocument coreferencing using the vector space model,1998,Amit Bagga; Breck Baldwin
1519,D12-1084,W11-1208,[3],related work,Our own work <TARGET_CITATION/> extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .,<CITATION/> extract subsentential translation pairs from comparable corpora using the loglikelihoodratio of word translation probability. <CITATION/> extract fragments using a generative model of noisy translations. Our own work <TARGET_CITATION/> extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora . Our own work <CITATION/> extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. <CITATION/> extract fragments using a generative model of noisy translations. <CITATION/> extract subsentential translation pairs from comparable corpora using the loglikelihoodratio of word translation probability.,a59a0185e02bf46b9f03274da718e87a24e7b8a8,Using Discourse Information for Paraphrase Extraction,2012,Michaela Regneri; Rui Wang,2a2a7b8a93ca9e7dad6c2223a6ebac7f33616869,Paraphrase Fragment Extraction from Monolingual Comparable Corpora,2011,Rui Wang; Chris Callison-Burch
1520,P11-1134,External_33256,[1],,"They proved to be useful in a number of NLP applications such as natural language generation <TARGET_CITATION/> , multidocument summarization <CITATION/> , automatic evaluation of MT <CITATION/> , and TE <CITATION/> .","1http://www.statmt.org/wmt10/Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation <TARGET_CITATION/> , multidocument summarization <CITATION/> , automatic evaluation of MT <CITATION/> , and TE <CITATION/> . They proved to be useful in a number of NLP applications such as natural language generation <CITATION/>, multidocument summarization <CITATION/>, automatic evaluation of MT <CITATION/>, and TE <CITATION/>. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. 1http://www.statmt.org/wmt10/",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,6ea4193bc13b1bdc1afcdbd53b4242f789f8aec5,Lexical Selection and Paraphrase in a Meaning-Text Generation Model,1991,L. Iordanskaja; R. Kittredge; A. Polguère
1521,J04-3001,External_62,[0],introduction,Current stateoftheart statistical parsers <TARGET_CITATION/> are all trained on large annotated corpora such as the Penn Treebank <CITATION/> .,"Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data <CITATION/>. Current stateoftheart statistical parsers <TARGET_CITATION/> are all trained on large annotated corpora such as the Penn Treebank <CITATION/> . Current stateoftheart statistical parsers <CITATION/> are all trained on large annotated corpora such as the Penn Treebank <CITATION/>. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data <CITATION/>. Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,headdriven statistical models for natural language parsing,1999,M Collins
1522,W06-2933,P05-1013,[0],method,"Although the parser only derives projective graphs , the fact that graphs are labeled allows nonprojective dependencies to be captured using the pseudoprojective approach of <TARGET_CITATION/> ."," RIGHTARC(r): Add an arc labeled r from top to next; push next onto the stack.  LEFTARC(r): Add an arc labeled r from next to top; pop the stack. Although the parser only derives projective graphs , the fact that graphs are labeled allows nonprojective dependencies to be captured using the pseudoprojective approach of <TARGET_CITATION/> . Although the parser only derives projective graphs, the fact that graphs are labeled allows nonprojective dependencies to be captured using the pseudoprojective approach of <CITATION/> .  LEFTARC(r): Add an arc labeled r from next to top; pop the stack. RIGHTARC(r): Add an arc labeled r from top to next; push next onto the stack.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,b92c0e898b1fee243864176e18a2b50105be3e54,Pseudo-Projective Dependency Parsing,2005,Joakim Nivre; Jens Nilsson
1523,J97-4003,External_91306,[4],related work,Riehemann 1993 ; Oliva 1994 ; <TARGET_CITATION/> ; Opalka 1995 ; Sanfilippo 1995 ) .,"Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification <TARGET_CITATION/> . Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,2dc0b06f2ad00153a78bcf888b6a7b4c3a5e6916,Verb Second by Lexical Rule or by Underspecification,1994,A. Frank
1524,W06-3813,External_4542,[0],introduction,The idea resurfaced forcefully at several points in the more recent history of linguistic research <TARGET_CITATION/> .,The first chronicled endeavour to connect text elements and organise connections between them goes back to the 51h century B.C. and the work of Paninil. He was a grammarian who analysed Sanskrit <CITATION/>. The idea resurfaced forcefully at several points in the more recent history of linguistic research <TARGET_CITATION/> . The idea resurfaced forcefully at several points in the more recent history of linguistic research <CITATION/>. He was a grammarian who analysed Sanskrit <CITATION/>. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 51h century B.C. and the work of Paninil.,f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,8c31301fb9f339e2b496a572a7933956d2260154,The Case for the Case,2016,J. Toomasian
1525,J02-3002,J97-2002,[0],introduction,A detailed introduction to the SBD problem can be found in <TARGET_CITATION/> .,"In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the endofsentence indicator (fullstop). A detailed introduction to the SBD problem can be found in <TARGET_CITATION/> . A detailed introduction to the SBD problem can be found in <CITATION/>. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the endofsentence indicator (fullstop). In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,30154464f549643e825ccf60072a17a3e55291d3,To Appear in Computational Linguistics Adaptive Multilingual Sentence Boundary Disambiguation,2004,D. Palmer; Marti A. Hearst
1526,W06-1639,External_17115,[0],introduction,"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) .","In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) . Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,829d1090da72af223944f7e576bbedaba53417c7,Tracking Point of View in Narrative,1994,Janyce Wiebe
1527,P10-4003,External_25453,[0],introduction,The BEETLE II system architecture is designed to overcome these limitations <TARGET_CITATION/> .,author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual handauthored remediations. The BEETLE II system architecture is designed to overcome these limitations <TARGET_CITATION/> . The BEETLE II system architecture is designed to overcome these limitations <CITATION/>. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual handauthored remediations. author a different remediation dialogue for every possible dialogue state.,1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,4d92807ccaaa241d2b42dfbc48b14f9e488cedd0,The Beetle and BeeDiff tutoring systems,2007,Charles B. Callaway; M. Dzikovska; Elaine Farrow; Manuel Marques-Pita; C. Matheson; Johanna D. Moore
1528,E03-1004,External_8193,[2],introduction,For the evaluation of the results we use the BLEU score <TARGET_CITATION/> .,"The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Section 7. For the evaluation of the results we use the BLEU score <TARGET_CITATION/> . For the evaluation of the results we use the BLEU score <CITATION/>. tion 7. The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec",55559a2ee9693969d30237534ac290f4b0077a3a,Czech-English Dependency Tree-based Machine Translation,2003,Martin Cmejrek; J. Curín; Jirí Havelka,d7da009f457917aa381619facfa5ffae9329a6e9,Bleu: a Method for Automatic Evaluation of Machine Translation,2002,K. Papineni; Salim Roukos; T. Ward; Wei-Jing Zhu
1529,J09-4010,External_9392,[4],,There are very few reported attempts at corpusbased automation of helpdesk responses <TARGET_CITATION/> .,"In contrast, the techniques examined in this article are corpusbased and datadriven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. There are very few reported attempts at corpusbased automation of helpdesk responses <TARGET_CITATION/> . There are very few reported attempts at corpusbased automation of helpdesk responses <CITATION/>. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus. In contrast, the techniques examined in this article are corpusbased and datadriven.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,e1d2a76d5934b016f90447cb1940e2cc209f2b8f,Learning from Message Pairs for Automatic Email Answering,2004,S. Bickel; T. Scheffer
1530,E03-1007,External_3692,[0],,"For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance <TARGET_CITATION/> or <CITATION/> ."," the verb contained in a transformed lexicon entry (e.g. go' for you_go' or you_will_go): hs, ,v(s ,t) = S(s. s')  V erb(t, v) , whereThis enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry. For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance <TARGET_CITATION/> or <CITATION/> . For an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance <CITATION/> or <CITATION/>.This enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry.  the verb contained in a transformed lexicon entry (e.g. go' for you_go' or you_will_go): hs, ,v(s ,t) = S(s. s')  V erb(t, v) , where",339ec71f2e0907ba376a3c8e7b7a89c592be3fdd,Using POS Information for SMT into Morphologically Rich Languages,2003,Nicola Ueffing; H. Ney,fb486e03369a64de2d5b0df86ec0a7b55d3907db,A Maximum Entropy Approach to Natural Language Processing,1996,Adam L. Berger; S. D. Pietra; V. D. Pietra
1531,W06-3813,External_27696,[2],,This design idea was adopted from TANKA <TARGET_CITATION/> .,"Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text. This design idea was adopted from TANKA <TARGET_CITATION/> . This design idea was adopted from TANKA <CITATION/>. Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,fee1b8b15216f52a641fd9b72eba311dff57da40,Systematic construction of a versatile case system,1997,Ken Barker; T. Copeck; Stan Szpakowicz; S. Delisle
1532,J86-1002,J83-3001,[4],,The problem of handling illformed input has been studied by <TARGET_CITATION/> .,"While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. The problem of handling illformed input has been studied by <TARGET_CITATION/> . The problem of handling illformed input has been studied by <CITATION/>. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in <CITATION/>. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,b33e8093c332f185f8c7e074b73cd7c79838b16a,Recovery Strategies for Parsing Extragrammatical Language,1983,J. Carbonell; P. Hayes
1533,W04-1805,External_54387,[1],method,"In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques <TARGET_CITATION/> .","understanding of the results, that is, why a specific element has been retrieved or not; 2. highlighting of the corpusspecific structures conveying the target element. In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques <TARGET_CITATION/> . In addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques <CITATION/>.highlighting of the corpusspecific structures conveying the target element.understanding of the results, that is, why a specific element has been retrieved or not; 2.",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,8530983891523cc09310daec5adfa0f9161ca454,Acquisition of Qualia Elements from Corpora - Evaluation of a Symbolic Learning Method,2002,P. Bouillon; V. Claveau; Cécile Fabre; P. Sébillot
1536,D10-1100,D09-1143,[4],experiments,"Here , the PET and GR kernel perform similar : this is different from the results of <TARGET_CITATION/> where GR performed much worse than PET for ACE data .","As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of <TARGET_CITATION/> where GR performed much worse than PET for ACE data . Here, the PET and GR kernel perform similar: this is different from the results of <CITATION/> where GR performed much worse than PET for ACE data. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. As in the baseline system, a combination of structures performs best.",330d82cedd1567515d42163b766197944adf6647,Automatic Detection and Classification of Social Events,2010,Apoorv Agarwal; Owen Rambow,a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi
1537,J05-3003,External_12085,[0],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; <TARGET_CITATION/> ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; <TARGET_CITATION/> ] , headdriven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexical functional grammar volume 34 of syntax and semantics,2001,Mary Dalrymple
1538,D13-1115,External_14208,[0],related work,"Some works abstract perception via the usage of symbolic logic representations <TARGET_CITATION/> , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>. Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using highlevel representations instead. Some works abstract perception via the usage of symbolic logic representations <TARGET_CITATION/> , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Some works abstract perception via the usage of symbolic logic representations <CITATION/>, while others choose to employ concepts elicited from psycholinguistic and cognition studies. Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using highlevel representations instead. The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,3954a3f80cf1b1f76430d80e924d85f2f1ba6799,Learning to Parse Natural Language Commands to a Robot Control System,2012,Cynthia Matuszek; E. Herbst; Luke Zettlemoyer; D. Fox
1539,J02-3002,A00-2035,[4],conclusion,"This is where robust syntactic systems like SATZ <CITATION/> or the POS tagger reported in <TARGET_CITATION/> , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .","optical character readergenerated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ <CITATION/> or the POS tagger reported in <TARGET_CITATION/> , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage . This is where robust syntactic systems like SATZ <CITATION/> or the POS tagger reported in <CITATION/>, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. optical character readergenerated texts.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,ec2f036f1e6f56ad6400e42cf1eb14f4ebe122c6,Tagging Sentence Boundaries,2000,Andrei Mikheev
1540,K15-1002,D13-1057,[0],,Our work is inspired by the latent leftlinking model in <TARGET_CITATION/> and the ILP formulation from <CITATION/> .,This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent leftlinking model in <TARGET_CITATION/> and the ILP formulation from <CITATION/> . Our work is inspired by the latent leftlinking model in <CITATION/> and the ILP formulation from <CITATION/>. This section describes our joint coreference resolution and mention head detection framework.,f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,be2f82cfd32c41d6493dc3ffb414de27b4f9e15b,A Constrained Latent Variable Model for Coreference Resolution,2013,Kai-Wei Chang; Rajhans Samdani; D. Roth
1541,J00-3006,External_98665,[0],,"For example , <TARGET_CITATION/> proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not contextfree , which implies that Chinese is not a contextfree language and thus might parse in exponential worstcase time .","Specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible timeconsuming behavior of the algorithm never happens for this grammar. Average, since it can happen that the grammar does admit hardtoparse sentences that are not used (or at least not frequently used) in the real corpus. For example , <TARGET_CITATION/> proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not contextfree , which implies that Chinese is not a contextfree language and thus might parse in exponential worstcase time . For example, <CITATION/> proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not contextfree, which implies that Chinese is not a contextfree language and thus might parse in exponential worstcase time. Average, since it can happen that the grammar does admit hardtoparse sentences that are not used (or at least not frequently used) in the real corpus. Specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible timeconsuming behavior of the algorithm never happens for this grammar.",e43ac61f3ab081525bdcda46636e842e3463aa42,Book Reviews: Foundations of Computational Linguistics: Man-Machine Communication in Natural Language,2000,Alexander F. Gelbukh,,chinese numbernames tree adjoining languages and mild contextsensitivity,1991,Daniel Radzinsky
1542,N10-1084,P01-1008,[0],experiments,<TARGET_CITATION/> also note that the applicability of paraphrases is strongly influenced by context .,"Moreover, there are some (phrase, paraphrase) pairs which are only suitable in particular contexts. For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year. <TARGET_CITATION/> also note that the applicability of paraphrases is strongly influenced by context . <CITATION/> also note that the applicability of paraphrases is strongly influenced by context. For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year. Moreover, there are some (phrase, paraphrase) pairs which are only suitable in particular contexts.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,8d7a692d8763a38283db54e19f1dcc694a34e706,Extracting Paraphrases from a Parallel Corpus,2001,R. Barzilay; K. McKeown
1543,P97-1063,External_19460,[5],conclusion,"Promising features for classification include part of speech , frequency of cooccurrence , relative word position , and translational entropy <TARGET_CITATION/> .","Unlike other translation models, the wordtoword model can automatically produce dictionarysized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more finegrained link class structure. Promising features for classification include part of speech , frequency of cooccurrence , relative word position , and translational entropy <TARGET_CITATION/> . Promising features for classification include part of speech, frequency of cooccurrence, relative word position, and translational entropy <CITATION/>. Even better accuracy can be achieved with a more finegrained link class structure. Unlike other translation models, the wordtoword model can automatically produce dictionarysized translation lexicons, and it can do so with over 99% accuracy.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,measuring semantic entropyquot,1997,I D Melamed
1544,E03-1007,External_6864,[0],,For descriptions of SMT systems see for example <TARGET_CITATION/> .,"If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example <TARGET_CITATION/> . For descriptions of SMT systems see for example <CITATION/>.In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. If necessary, the inverse of these transformations will be applied to the generated output string.",339ec71f2e0907ba376a3c8e7b7a89c592be3fdd,Using POS Information for SMT into Morphologically Rich Languages,2003,Nicola Ueffing; H. Ney,dd5514876b7e1c09b6d2f931d90bb34aa3501441,Fast Decoding and Optimal Decoding for Machine Translation,2001,Ulrich Germann; Michael E. Jahr; Kevin Knight; D. Marcu; Kenji Yamada
1545,D11-1138,External_25749,[0],experiments,like information extraction <TARGET_CITATION/> and textual entailment <CITATION/> .,"When training with ALS (labeled and unlabeled), we see an improvement in UAS, LAS, and ALS. Furthermore, if we use a labeledALS as the metric for augmentedloss training, we also see a considerable increase in LAS. like information extraction <TARGET_CITATION/> and textual entailment <CITATION/> . like information extraction <CITATION/> and textual entailment <CITATION/>. Furthermore, if we use a labeledALS as the metric for augmentedloss training, we also see a considerable increase in LAS. When training with ALS (labeled and unlabeled), we see an improvement in UAS, LAS, and ALS.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,a1b19bb17697133a87e43c312f25f1e3ddd026cb,Unsupervised Methods for Determining Object and Relation Synonyms on the Web,2014,A. Yates; Oren Etzioni
1546,J03-3004,External_49743,[2],introduction,"In a final processing stage , we generalize over the marker lexicon following a process found in <TARGET_CITATION/> .","<DET> the board : le conseil <DET> the : le <PREP> to <QUANT> 14 members : a 14 membres <QUANT> 14 members : 14 membres <LEX> expanding : augmente <LEX> board : conseil <PREP> to : a <LEX> members : membresWe ignore here the trivially true lexical chunk <QUANT> 14 : 14.'' In a final processing stage , we generalize over the marker lexicon following a process found in <TARGET_CITATION/> . In a final processing stage, we generalize over the marker lexicon following a process found in <CITATION/>. We ignore here the trivially true lexical chunk <QUANT> 14 : 14.'' <DET> the board : le conseil <DET> the : le <PREP> to <QUANT> 14 members : a 14 membres <QUANT> 14 members : 14 membres <LEX> expanding : augmente <LEX> board : conseil <PREP> to : a <LEX> members : membres",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,examplebased incremental synchronous interpretation,2000,Hans-Ulrich Block
1547,J00-3002,External_55710,[0],,"One approach to this problem consists in defining , within the Cutfree atomicid space , normal form derivations in which the succession of rule application is regulated <TARGET_CITATION/> .","But even in the Cutfree atomicid calculus there is spurious ambiguity: equivalent derivations differing only in irrelevant rule ordering. For example, composition as above has the following alternative derivation: One approach to this problem consists in defining , within the Cutfree atomicid space , normal form derivations in which the succession of rule application is regulated <TARGET_CITATION/> . One approach to this problem consists in defining, within the Cutfree atomicid space, normal form derivations in which the succession of rule application is regulated <CITATION/>. For example, composition as above has the following alternative derivation:But even in the Cutfree atomicid calculus there is spurious ambiguity: equivalent derivations differing only in irrelevant rule ordering.",1cf5e98f01f6aa2ccd2a68682c49cb93d152f152,Incremental processing and acceptability,2000,G. Morrill,3fbf3ac92afd7cb1c7a653cedcfbeebca970ca31,Parsing as Natural Deduction,1989,E. König
1549,W06-3309,External_42488,[4],introduction,<TARGET_CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .,"(NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. <TARGET_CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques . <CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. (NLM), which also serves as a readily available corpus of abstracts for our experiments.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,72de308314b88b53aef6cb86bd8390f334a6bd24,Categorization of Sentence Types in Medical Abstracts,2003,Lawrence K. McKnight; P. Srinivasan
1550,W04-0910,External_41701,[5],,"For complementing this database and for converse constructions , the LADL tables <TARGET_CITATION/> can furthermore be resorted to , which list detailed syntacticosemantic descriptions for 5 000 verbs and 25 000 verbal expressions .","However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources. For shuffling paraphrases, french alternations are partially described in <CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions , the LADL tables <TARGET_CITATION/> can furthermore be resorted to , which list detailed syntacticosemantic descriptions for 5 000 verbs and 25 000 verbal expressions . For complementing this database and for converse constructions, the LADL tables <CITATION/> can furthermore be resorted to, which list detailed syntacticosemantic descriptions for 5 000 verbs and 25 000 verbal expressions. For shuffling paraphrases, french alternations are partially described in <CITATION/> and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. However as for other types of synonymy, distributional analysis and clustering techniques can be used to develop such resources.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,m´ethodes en syntase,1975,M Gross
1552,J03-3004,External_49743,[2],introduction,"Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on <TARGET_CITATION/> to permit a limited form of insertion in the translation process .","<CITATION/> observes that a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its casemarking), can capture this sort of result quite handily.'' Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determinernoun sequence onto a Japanese nouncase marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on <TARGET_CITATION/> to permit a limited form of insertion in the translation process . Following construction of the marker lexicon, the (source, target) chunks are generalized further using a methodology based on <CITATION/> to permit a limited form of insertion in the translation process. Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determinernoun sequence onto a Japanese nouncase marker segment, once one has identified the sets of marker tags in the languages to be translated. <CITATION/> observes that a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its casemarking), can capture this sort of result quite handily.''",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,examplebased incremental synchronous interpretation,2000,Hans-Ulrich Block
1553,W01-1510,External_329,[0],introduction,ment <TARGET_CITATION/> .,"There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environ1In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. ment <TARGET_CITATION/> . ment <CITATION/>. 1In this paper, we use the term LTAG to refer to FBLTAG, if not confusing.There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environ",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,0aedc08cce73ec1f05e229338db47a2866f8bd59,Evolution of the XTAG System,2000,Christine Doran; Beth Ann Hockey; Anoop Sarkar; B. Srinivas; Fei Xia
1554,W06-3309,External_315,[0],introduction,"The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/> , information retrieval <CITATION/> , information extraction <TARGET_CITATION/> , and question answering .","As an example, scientific abstracts across many differentfields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/> , information retrieval <CITATION/> , information extraction <TARGET_CITATION/> , and question answering . The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/>, information retrieval <CITATION/>, information extraction <CITATION/>, and question answering. fields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. As an example, scientific abstracts across many different",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,9bf4a4bccf6baa1c3d29367a46f08c8632058119,Zone analysis in biology articles as a basis for information extraction,2006,Y. Mizuta; A. Korhonen; Tony Mullen; Nigel Collier
1555,J87-3002,External_24080,[0],introduction,"Recent developments in linguistics , and especially on grammatical theory  for example , Generalised Phrase Structure Grammar ( GPSG ) <CITATION/> , Lexical Functional Grammar ( LFG ) <CITATION/>  and on natural language parsing frameworks  for example , Functional Unification Grammar ( FUG ) <CITATION/> , PATRII <TARGET_CITATION/>  make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .","The grammar coding system employed by the Longman Dictionary of Contemporary English (henceforth LDOCE) is the most comprehensive description of grammatical properties of words to be found in any published dictionary available in machine readable form. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. Recent developments in linguistics , and especially on grammatical theory  for example , Generalised Phrase Structure Grammar ( GPSG ) <CITATION/> , Lexical Functional Grammar ( LFG ) <CITATION/>  and on natural language parsing frameworks  for example , Functional Unification Grammar ( FUG ) <CITATION/> , PATRII <TARGET_CITATION/>  make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language . Recent developments in linguistics, and especially on grammatical theory  for example, Generalised Phrase Structure Grammar (GPSG) <CITATION/>, Lexical Functional Grammar (LFG) <CITATION/>  and on natural language parsing frameworks  for example, Functional Unification Grammar (FUG) <CITATION/>, PATRII <CITATION/>  make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language. This paper describes the extraction of this, and other, information from LDOCE and discusses the utility of the coding system for automated natural language processing. The grammar coding system employed by the Longman Dictionary of Contemporary English (henceforth LDOCE) is the most comprehensive description of grammatical properties of words to be found in any published dictionary available in machine readable form.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,decd25cc661adb7c0769588a2c0bf243caacb49b,The Design of a Computer Language for Linguistic Information,1984,Stuart M. Shieber
1556,W06-1104,External_3087,[0],,"The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation <TARGET_CITATION/> or malapropism detection <CITATION/> .","According to <CITATION/>, there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric <CITATION/>.4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation <TARGET_CITATION/> or malapropism detection <CITATION/> . The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation <CITATION/> or malapropism detection <CITATION/>. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric <CITATION/>.4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. According to <CITATION/>, there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,eaa82498a122bb846f13c6419d5752f1e2109320,Using Measures of Semantic Relatedness for Word Sense Disambiguation,2003,Siddharth Patwardhan; Satanjeev Banerjee; Ted Pedersen
1557,W06-1104,External_31966,[0],,"<TARGET_CITATION/> argue for applicationspecific evaluation of similarity measures , because measures are always used for some task .","Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric <CITATION/>.4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation <CITATION/> or malapropism detection <CITATION/>. <TARGET_CITATION/> argue for applicationspecific evaluation of similarity measures , because measures are always used for some task . <CITATION/> argue for applicationspecific evaluation of similarity measures, because measures are always used for some task. The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation <CITATION/> or malapropism detection <CITATION/>. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric <CITATION/>.4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3411ee1422bca9cb481ff8c6f0821724539cd500,Computing similarity distances between rankings,2013,Farzad Farnoud; O. Milenkovic; Gregory J. Puleo; Lili Su
1558,D08-1022,P08-1023,[3],related work,The first direct application of parse forest in translation is our previous work <TARGET_CITATION/> which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .,"Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forestbased fragmentation (Algorithm 1) which we think is nontrivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models <CITATION/>. The first direct application of parse forest in translation is our previous work <TARGET_CITATION/> which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) . The first direct application of parse forest in translation is our previous work <CITATION/> which translates a packed forest from a parser; it is also the base system in our experiments (see below). The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models <CITATION/>. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forestbased fragmentation (Algorithm 1) which we think is nontrivial.",90840ced4df35ddbf5d85282a717f949ea9b3dcf,Forest-based Translation Rule Extraction,2008,Haitao Mi; Liang Huang,c0a785bc32a724e70840a71e1f60ac5901775ae2,Forest-Based Translation,2008,Haitao Mi; Liang Huang; Qun Liu
1559,J97-4003,External_51366,[0],introduction,4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and <TARGET_CITATION/> ; Gerdemann 1995 ) .,"Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and <TARGET_CITATION/> ; Gerdemann 1995 ) . 4 This interpretation of the signature is sometimes referred to as closed world <CITATION/>. To avoid confusion, we will only use the terminology introduced in the text. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,an expanded logical formalism for headdriven phrase structure grammar,1994,Paul King
1560,J03-3004,External_28667,[4],introduction,"In their Gaijin system , <TARGET_CITATION/> give a result of 63 % accurate translations obtained for English  > German on a test set of 791 sentences from CorelDRAW manuals .","For English > <CITATION/> notes that the system learned the original training corpus ... perfectly and could reproduce it without errors''; that is, it scored 100% accuracy when tested against the training corpus. On novel test sentences, he gives results of 72% correct translation. In their Gaijin system , <TARGET_CITATION/> give a result of 63 % accurate translations obtained for English  > German on a test set of 791 sentences from CorelDRAW manuals . In their Gaijin system, <CITATION/> give a result of 63% accurate translations obtained for English > German on a test set of 791 sentences from CorelDRAW manuals. On novel test sentences, he gives results of 72% correct translation. For English > <CITATION/> notes that the system learned the original training corpus ... perfectly and could reproduce it without errors''; that is, it scored 100% accuracy when tested against the training corpus.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,gaijin a bootstrapping templatedriven approach to examplebased machine translation,1997,Tony Veale; Andy Way
1561,J00-1003,P94-1011,[0],,"This method can be generalized , inspired by <TARGET_CITATION/> , who derive Ngram probabilities from stochastic contextfree grammars .","Our implementation is made slightly more sophisticated by taking EA to be LCBX I 3B, a, /3[B E N, A B aX0 AX N]RCB, for each A such that A E N, and recursive(N) = self, for some i. That is, each X E EA is a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable from set N through B E N. This method can be generalized , inspired by <TARGET_CITATION/> , who derive Ngram probabilities from stochastic contextfree grammars . This method can be generalized, inspired by <CITATION/>, who derive Ngram probabilities from stochastic contextfree grammars. That is, each X E EA is a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable from set N through B E N. Our implementation is made slightly more sophisticated by taking EA to be LCBX I 3B, a, /3[B E N, A B aX0 AX N]RCB, for each A such that A E N, and recursive(N) = self, for some i.",b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof,823233093025cdc6aec13dbc17d9e4116f686eba,Precise N-Gram Probabilities From Stochastic Context-Free Grammars,1994,A. Stolcke; Jonathan Segal
1562,P00-1007,External_555,[4],,"The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <TARGET_CITATION/> , and became a common testbed ."," The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by <TARGET_CITATION/> , and became a common testbed . The system was trained on the Penn Treebank <CITATION/> WSJ Sections 221 and tested on Section 23 (Table 1), same as used by <CITATION/>, and became a common testbed.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,0ffa423a5283396c88ff3d4033d541796bd039cc,"Three Generative, Lexicalised Models for Statistical Parsing",1997,M. Collins
1563,J06-2002,W03-0603,[0],experiments,"The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX <TARGET_CITATION/> ; Thorisson 1994 , for other plans ) .","For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX <TARGET_CITATION/> ; Thorisson 1994 , for other plans ) . The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cfXXX Gorniak and Roy 2003; Thorisson 1994, for other plans). Many alternative strategies are possible. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,703ea3c46856869a165f5f46aac6d271d3e43ffb,Understanding Complex Visually Referring Utterances,2003,Peter Gorniak; D. Roy
1564,N10-1084,D08-1021,[2],experiments,"The paraphrase dictionary that we use was generated for us by Chris CallisonBurch , using the technique described in <TARGET_CITATION/> , which exploits a parallel corpus and methods developed for statistical machine translation .","The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank <CITATION/>. Hence we require possible paraphrases for phrases that occur in Section 00. The paraphrase dictionary that we use was generated for us by Chris CallisonBurch , using the technique described in <TARGET_CITATION/> , which exploits a parallel corpus and methods developed for statistical machine translation . The paraphrase dictionary that we use was generated for us by Chris CallisonBurch, using the technique described in <CITATION/>, which exploits a parallel corpus and methods developed for statistical machine translation. Hence we require possible paraphrases for phrases that occur in Section 00. The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank <CITATION/>.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,cadc3dbd73f0cbbe04b2a66f832c3cf34c877b41,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,2008,Chris Callison-Burch
1565,D13-1115,External_9237,[0],related work,"To name a few examples , <TARGET_CITATION/> et al. ( 2013 ) show how semantic information from text can be used to improve zeroshot classification ( i.e. , classifying neverbeforeseen objects ) , and <CITATION/> show that verb clusters can be used to improve activity recognition in videos .","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>. The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , <TARGET_CITATION/> show how semantic information from text can be used to improve zeroshot classification ( i.e. , classifying neverbeforeseen objects ) , and <CITATION/> show that verb clusters can be used to improve activity recognition in videos . To name a few examples, <CITATION/> show how semantic information from text can be used to improve zeroshot classification (i.e., classifying neverbeforeseen objects), and <CITATION/> show that verb clusters can be used to improve activity recognition in videos.The Computer Vision community has also benefited greatly from efforts to unify the two modalities. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4a4b5ae5793696b861aa009932e7a121d36ad67a,What helps where – and why? Semantic relatedness for knowledge transfer,2010,Marcus Rohrbach; Michael Stark; György Szarvas; Iryna Gurevych; B. Schiele
1566,E03-1005,External_6640,[1],introduction,"This approach has now gained wide usage , as exemplified by the work of <TARGET_CITATION/> ) , Charniak ( 1996 , 1997 ) , <CITATION/> , and many others .","Waegner 1992; Pereira and Schabes 1992). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage , as exemplified by the work of <TARGET_CITATION/> ) , Charniak ( 1996 , 1997 ) , <CITATION/> , and many others . This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), <CITATION/>, and many others. The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. Waegner 1992; Pereira and Schabes 1992).",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,3764baa7465201f054083d02b58fa75f883c4461,A New Statistical Parser Based on Bigram Lexical Dependencies,1996,M. Collins
1567,W02-1601,External_19998,[0],,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> .","Similar limitations also appear in synchronous CFGs <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning <CITATION/> , ( kaji et al. , 1992 ) , and examplebase machine translation EBMT3 <TARGET_CITATION/> . For example, such schema can serve as a mean to represent translation examples, or find structural correspondences for the purpose of transfer grammar learning <CITATION/>, (kaji et al., 1992), and examplebase machine translation EBMT3 <CITATION/>.Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured StringTree Correspondence (SSSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. Similar limitations also appear in synchronous CFGs <CITATION/>.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,,achieving commercialquality translation with examplebased methods,2001,S Richardson; W Dolan; A Menezes; J Pinkham
1568,W06-1705,W02-1030,[0],introduction,"Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem <TARGET_CITATION/> .","Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (<CITATION/>: 56) unless the web is used as a corpus <CITATION/>. Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem <TARGET_CITATION/> . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem <CITATION/>. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (<CITATION/>: 56) unless the web is used as a corpus <CITATION/>. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,79883c30922037c93392ddbbecc6fd35674a6a1c,Using the Web to Overcome Data Sparseness,2002,Frank Keller; Maria Lapata; Olga Ourioupina
1570,J97-4003,External_77176,[0],introduction,"de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <TARGET_CITATION/> to operate on those raised elements .","email: LCBdm,minnenRCB@sfs.nphil.unituebingen. de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <TARGET_CITATION/> to operate on those raised elements . de URL: http://www.sfs.nphil.unituebingen.de/sfb /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements. nphil.unituebingen.email: LCBdm,minnenRCB@sfs.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,french clitic climbing without clitics or climbing,1993,Philip Miller; Ivan Sag
1571,A00-1024,A92-1015,[4],,Research that is more similar in goal to that outlined in this paper is Vosse <TARGET_CITATION/> .,"Naturally, these are not appropriate comparisons for the work reported here. However, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree. Research that is more similar in goal to that outlined in this paper is Vosse <TARGET_CITATION/> . Research that is more similar in goal to that outlined in this paper is Vosse <CITATION/>. However, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree. Naturally, these are not appropriate comparisons for the work reported here.",caa11f45ef1d8cdd6683a34b22407ec0f2c55d77,Categorizing Unknown Words: Using Decision Trees to Identify Names and Misspellings,2000,J. Toole,06f3428dcf8703e49ce9cf29981b0f1904262cc7,Detecting and Correcting Morpho-syntactic Errors in Real Texts,1992,T. Vosse
1572,J02-3002,External_57340,[2],,<TARGET_CITATION/> pointed out that little attention had been paid in the namedentity recognition field to the discourse properties of proper names .,"Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. <TARGET_CITATION/> pointed out that little attention had been paid in the namedentity recognition field to the discourse properties of proper names . <CITATION/> pointed out that little attention had been paid in the namedentity recognition field to the discourse properties of proper names. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,86661a897c1421874087fbca76c67c5b1b775e16,Identifying Unknown Proper Names in Newswire Text,1993,I. Mani; T. MacMillan; S. Luperfoy; E. Lusher; S. Laskowski
1575,P07-1068,N06-1025,[2],,"Following <TARGET_CITATION/> , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .","As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonlyused MUC scorer <CITATION/>, and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following <TARGET_CITATION/> , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . Following <CITATION/>, we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonlyused MUC scorer <CITATION/>, and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,6d375f8b45d8e5e961875bf74a89f32394759b89,"Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution",2006,Simone Paolo Ponzetto; M. Strube
1576,W02-0309,External_34418,[0],introduction,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> .","When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist. This is particularly true for the medical domain. From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexicosemantic aspects of dederivation and decomposition <TARGET_CITATION/> . From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexicosemantic aspects of dederivation and decomposition <CITATION/>. This is particularly true for the medical domain. When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domainspecific algorithms exist.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,b7427a210eb40724c7d5c468e5945f1b10aa4730,Morphosemantic Analysis of -ITIS Forms in Medical Language,1980,M. G. PAeAN; L. M. Norton; G. Dunham; Dr. M. G. Pacak; Dr. L. M. Nor ton
1577,W06-1705,External_24318,[0],related work,The Gsearch system <TARGET_CITATION/> also selects sentences by syntactic criteria from large online text collections .,"The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system <TARGET_CITATION/> also selects sentences by syntactic criteria from large online text collections . The Gsearch system <CITATION/> also selects sentences by syntactic criteria from large online text collections. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The second method pushes the new collection onto a queue for the LSE annotator to analyse.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,0e2b5bc7bfdfe5a1860b0398a60305279562b643,Finding Syntactic Structure in Unparsed Corpora The Gsearch Corpus Query System,2001,S. Corley; M. Corley; Frank Keller; M. Crocker; Shari Trewin
1579,J00-2003,External_94114,[0],introduction,"See also the work of <TARGET_CITATION/> , which considers computerbased pronunciation by analogy but does not mention the possible application to texttospeech synthesis .","Pronunciation by analogy (PbA) is a datadriven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by <CITATION/>. It was first proposed for ITS applications over a decade ago by Dedina and Nusbaum (1986, 1991). See also the work of <TARGET_CITATION/> , which considers computerbased pronunciation by analogy but does not mention the possible application to texttospeech synthesis . See also the work of <CITATION/>, which considers computerbased pronunciation by analogy but does not mention the possible application to texttospeech synthesis. It was first proposed for ITS applications over a decade ago by Dedina and Nusbaum (1986, 1991). Pronunciation by analogy (PbA) is a datadriven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by <CITATION/>.",18ff4f15416e34d9a56142e6f5d491567934e4fb,A multistrategy approach to improving pronunciation by analogy,2000,Y. Marchand; R. Damper,3d5eabf859c29670e3b91adf6f0193a8d7bc4981,Using an On-Line Dictionary to Find Rhyming Words and Pronunciations for Unknown Words,1985,Roy J. Byrd; M. Chodorow
1580,W06-1104,External_2004,[2],experiments,The resulting list of POStagged lemmas is weighted using the SMART  ltc ' 8 tf.idfweighting scheme <TARGET_CITATION/> .,"Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps (tokenization, POStagging, lemmatization) are performed using TreeTagger <CITATION/>. The resulting list of POStagged lemmas is weighted using the SMART  ltc ' 8 tf.idfweighting scheme <TARGET_CITATION/> . The resulting list of POStagged lemmas is weighted using the SMART ltc'8 tf.idfweighting scheme <CITATION/>. The three preprocessing steps (tokenization, POStagging, lemmatization) are performed using TreeTagger <CITATION/>. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f,"Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer",1989,G. Salton
1581,J90-3003,External_65918,[0],introduction,"The psycholinguistic studies of <TARGET_CITATION/> , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency ."," The psycholinguistic studies of <TARGET_CITATION/> , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . The psycholinguistic studies of <CITATION/>, responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,dae4c4b0c58678cb99a83190378abbd5503a434d,Speech Rhythm: Its Relation to Performance Universals and Articulatory Timing.,1975,G. Allen
1582,D12-1037,P03-1021,[0],introduction,"Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one .","where f and e (e') are source and target sentences, respectively. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. Some methods are based on likelihood <CITATION/> , error rate <TARGET_CITATION/> , margin <CITATION/> and ranking <CITATION/> , and among which minimum error rate training ( MERT ) <CITATION/> is the most popular one . Some methods are based on likelihood <CITATION/>, error rate <CITATION/>, margin <CITATION/> and ranking <CITATION/>, and among which minimum error rate training (MERT) <CITATION/> is the most popular one. h is a feature vector which is scaled by a weight W. Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W. where f and e (e') are source and target sentences, respectively.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,1f12451245667a85d0ee225a80880fc93c71cc8b,Minimum Error Rate Training in Statistical Machine Translation,2003,F. Och
1583,D08-1007,J03-3005,[4],experiments,Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times <TARGET_CITATION/> .,"Passive subjects (the car was bought) were converted to objects (bought car). We set the MIthreshold, T, to be 0, and the negativetopositive ratio, K, to be 2. Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times <TARGET_CITATION/> . Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times <CITATION/>. We set the MIthreshold, T, to be 0, and the negativetopositive ratio, K, to be 2. Passive subjects (the car was bought) were converted to objects (bought car).",94ad3c0d561f29f0c4989b5b6a2e6c7f1570ef05,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,S. Bergsma; Dekang Lin; R. Goebel,5dfed29550d75cca99019aa52d40038dcb23b3cb,Using the Web to Obtain Frequencies for Unseen Bigrams,2003,Frank Keller; Mirella Lapata
1584,J00-2004,External_81899,[0],method," crosslanguage information retrieval <CITATION/> ,  multilingual document filtering <CITATION/> ,  computerassisted language learning <TARGET_CITATION/> ,  certain machineassisted translation tools <CITATION/> ,  concordancing for bilingual lexicography <CITATION/> ,","Empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications. Applications where word order is not essential include crosslanguage information retrieval <CITATION/> ,  multilingual document filtering <CITATION/> ,  computerassisted language learning <TARGET_CITATION/> ,  certain machineassisted translation tools <CITATION/> ,  concordancing for bilingual lexicography <CITATION/> ,  crosslanguage information retrieval <CITATION/>,  multilingual document filtering <CITATION/>,  computerassisted language learning <CITATION/>,  certain machineassisted translation tools <CITATION/>,  concordancing for bilingual lexicography <CITATION/>,Applications where word order is not essential includeEmpirically estimated models of translational equivalence among word types can play a central role in both kinds of applications.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,b1a6a25bd90379e124c73327fa698181e011782c,Reading more into Foreign Languages,1997,J. Nerbonne; L. Karttunen; E. Paskaleva; Gábor Prószéky; Tiit Roosmaa
1585,J97-4003,External_105,[0],introduction,"One would think that the type information ti , which is more specific than that 16 A linguistic example based on the signature given by <TARGET_CITATION/> would be a lexical rule deriving predicative signs from nonpredicative ones , i.e. , changing the PRD value of substantive signs from  to  F , much like the lexical rule for NPs given by Pollard and Sag ( 1994 , p. 360 , fn .","For example, the lexical rule 1 of Figure 6 applies to word objects with tj as their c value and to those having t2 as their c value. With respect to frame specification this means that there can be lexical entries, such as the one in Figure 7, for which we need to make sure that tj as the value of c gets transferred.' One would think that the type information ti , which is more specific than that 16 A linguistic example based on the signature given by <TARGET_CITATION/> would be a lexical rule deriving predicative signs from nonpredicative ones , i.e. , changing the PRD value of substantive signs from  to  F , much like the lexical rule for NPs given by Pollard and Sag ( 1994 , p. 360 , fn . One would think that the type information ti, which is more specific than that 16 A linguistic example based on the signature given by <CITATION/> would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from  to F, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. With respect to frame specification this means that there can be lexical entries, such as the one in Figure 7, for which we need to make sure that tj as the value of c gets transferred.' For example, the lexical rule 1 of Figure 6 applies to word objects with tj as their c value and to those having t2 as their c value.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,headdriven phrase structure grammar,1994,Carl Pollard; Ivan Sag
1586,J03-3004,External_69584,[0],introduction," language learning <TARGET_CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different languagerelated tasks, including language learning <TARGET_CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )  language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002)The marker hypothesis has been used for a number of different languagerelated tasks, includingThe marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,ff47eb67ba7aad057ec11d9486606653678f689e,The Role of Syntax Markers and Semantic Referents in Learning an Artificial Language.,1983,Kazuo Mori; Shannon Dawn Moeser
1587,W06-1639,External_7807,[0],related work,Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> .,Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitinterdocument references in the form of hyperlinks <CITATION/>. Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> . Notable early papers on graphbased semisupervised learning include <CITATION/>. interdocument references in the form of hyperlinks <CITATION/>. Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicit,dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,0eedbab3ae55fd6a4e7bbc75fcc261293384f883,Learning from Labeled and Unlabeled Data using Graph Mincuts,2001,Avrim Blum; Shuchi Chawla
1588,P07-1068,N04-1038,[0],introduction,"As a result , researchers have readopted the oncepopular knowledgerich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , <CITATION/> ) , their semantic similarity as computed using WordNet ( e.g. , <CITATION/> ) or Wikipedia <CITATION/> , and the contextual role played by an NP <TARGET_CITATION/> .","While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have readopted the oncepopular knowledgerich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , <CITATION/> ) , their semantic similarity as computed using WordNet ( e.g. , <CITATION/> ) or Wikipedia <CITATION/> , and the contextual role played by an NP <TARGET_CITATION/> . As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>). In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. While these approaches have been reasonably successful (see <CITATION/>), <CITATION/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,cf0a7ec9f002f0388476ed2110116de90a553bc7,Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution,2004,David L. Bean; E. Riloff
1590,W02-0309,External_44555,[5],experiments,"Alternatively , we may think of usercentered comparative studies <TARGET_CITATION/> .","It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of usercentered comparative studies <TARGET_CITATION/> . Alternatively, we may think of usercentered comparative studies <CITATION/>.This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,77a4f4716bc3e45ad46a4babe99674372fd7ce49,Towards new measures of information retrieval evaluation,1995,W. Hersh; D. Elliot; D. Hickam; S. Wolf; A. Molnár; Christine Leichtenstien
1592,J05-3003,H94-1020,[0],method,"However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the PennII Treebank <TARGET_CITATION/> , containing more than 1,000,000 words and 49,000 sentences .","Fstructures are attributevalue structures which represent abstract syntactic information, approximating to basic predicateargumentmodifier structures. Most of the early work on automatic fstructure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept. However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the PennII Treebank <TARGET_CITATION/> , containing more than 1,000,000 words and 49,000 sentences . However, more recent work (Cahill et al. 2002; Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the PennII Treebank <CITATION/>, containing more than 1,000,000 words and 49,000 sentences. Most of the early work on automatic fstructure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept. Fstructures are attributevalue structures which represent abstract syntactic information, approximating to basic predicateargumentmodifier structures.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b,The Penn Treebank: Annotating Predicate Argument Structure,1994,Mitchell P. Marcus; Grace Kim; Mary Ann Marcinkiewicz; R. MacIntyre; Ann Bies; Mark Ferguson; Karen Katz; Britta Schasberger
1593,J86-1002,External_21696,[4],,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <TARGET_CITATION/> , assertional statements as in <CITATION/> , or semantic nets as in <CITATION/> .","It self activates to bias recognition toward historically observed patterns but is not otherwise observable. The VNLCE processor may be considered to be a learning system of the tradition described, for example, in <CITATION/>. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <TARGET_CITATION/> , assertional statements as in <CITATION/> , or semantic nets as in <CITATION/> . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>. The VNLCE processor may be considered to be a learning system of the tradition described, for example, in <CITATION/>. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,perceptrons,1969,M Minsky; S Papert
1594,P00-1004,P96-1021,[2],,It also shows the structural identity to bilingual grammars as used in <TARGET_CITATION/> .,"We write the labels at first position as these translations patterns can be used in the reverse direction, i.e. from target language to source language. In section 2.4 this property is used to convert a bilingual corpus into a set of translation patterns which are formulated in terms of words and category labels. It also shows the structural identity to bilingual grammars as used in <TARGET_CITATION/> . It also shows the structural identity to bilingual grammars as used in <CITATION/>.In section 2.4 this property is used to convert a bilingual corpus into a set of translation patterns which are formulated in terms of words and category labels. We write the labels at first position as these translations patterns can be used in the reverse direction, i.e. from target language to source language.",9ddf3d2e52789255fc4d9692cffff95af3b10628,Translation with Cascaded Finite State Transducers,2000,S. Vogel; H. Ney,4711ff01d8eff9b9d10deeb3b68f366f7944c208,A Polynomial-Time Algorithm for Statistical Machine Translation,1996,Dekai Wu
1595,W06-1639,External_8502,[0],related work,"Relationships between the unlabeled items <TARGET_CITATION/> consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .","Also relevant is work on the general problems of dialogact tagging <CITATION/>, citation analysis <CITATION/>, and computational rhetorical analysis <CITATION/>. We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Relationships between the unlabeled items <TARGET_CITATION/> consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations . Relationships between the unlabeled items <CITATION/> consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations. We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. Also relevant is work on the general problems of dialogact tagging <CITATION/>, citation analysis <CITATION/>, and computational rhetorical analysis <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,ab627ba77dced941f9f45eeaee17bc6644308d89,"On the collective classification of email ""speech acts""",2005,Vitor R. Carvalho; William W. Cohen
1596,A00-1022,A97-1031,[2],experiments,100000 word stems of German <TARGET_CITATION/> .,"MorphAna: Morphological Analysis provided by smes yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words. We are using a lexicon of approx. 100000 word stems of German <TARGET_CITATION/> . 100000 word stems of German <CITATION/>. We are using a lexicon of approx. MorphAna: Morphological Analysis provided by smes yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.",79a78e6a86b296679dbe9a98cf52fea75f6f7d7a,Message Classification in the Call Center,2000,Stephan Busemann; S. Schmeier; Roman Georg Arens,042b49ddd96847a17d72fdfec7e9189c7af0d06b,An Information Extraction Core System for Real World German Text Processing,1997,G. Neumann; R. Backofen; Judith Baur; Markus Becker; Christian Braun
1597,J97-4003,External_41778,[3],introduction,"The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and <TARGET_CITATION/> , 1997b ) for encoding the main building block of HPSG grammars  the implicative constraints  as a logic program .","Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency. The resulting encoding allows the execution of lexical rules onthefly, i.e., coroutined with other constraints at some time after lexical lookup. The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and <TARGET_CITATION/> ) for encoding the main building block of HPSG grammars  the implicative constraints  as a logic program . The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and Meurers (1995, 1996, 1997b) for encoding the main building block of HPSG grammarsthe implicative constraintsas a logic program. The resulting encoding allows the execution of lexical rules onthefly, i.e., coroutined with other constraints at some time after lexical lookup. Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,towards a semantics for lexical rules as used in hpsg,1995,Detmar Meurers
1598,D11-1138,W02-1001,[0],introduction,"Identical to the standard perceptron proof , e.g. , <TARGET_CITATION/> , by inserting in lossseparability for normal separability .","If training is run indefinitely, then m < R2 2. Proof. Identical to the standard perceptron proof , e.g. , <TARGET_CITATION/> , by inserting in lossseparability for normal separability . Identical to the standard perceptron proof, e.g., <CITATION/>, by inserting in lossseparability for normal separability. Proof. If training is run indefinitely, then m < R2 2.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,5a7958b418bceb48a315384568091ab1898b1640,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,2002,M. Collins
1599,A00-1015,P98-2195,[4],,The goal of the JAVOX toolkit is to speechenable traditional desktop applications  this is similar to the goals of the MELISSA project <TARGET_CITATION/> .,"JAVOX has been successfully demonstrated with several GUIbased applications. Previous systems to assist in the development of spokenlanguage systems (SLSs) have focused on building standalone, customized applications, such as <CITATION/>. The goal of the JAVOX toolkit is to speechenable traditional desktop applications  this is similar to the goals of the MELISSA project <TARGET_CITATION/> . The goal of the JAVOX toolkit is to speechenable traditional desktop applications  this is similar to the goals of the MELISSA project <CITATION/>. Previous systems to assist in the development of spokenlanguage systems (SLSs) have focused on building standalone, customized applications, such as <CITATION/>. JAVOX has been successfully demonstrated with several GUIbased applications.",66087769e3379f06aa0eef1f5d06a7ef6e9fc7a7,Javox: A Toolkit for Building Speech-Enabled Applications,2000,Michael S. Fulkerson; A. Biermann,f3b0af104dc51db4e6388e92dd4a437f398e3b08,Natural Language Access to Software Applications,1998,P. Schmidt; S. Rieder; A. Theofilidis; M. Groenendijk; P. Phelan; Henrik Schulz; Thierry Declerck; A. Bredenkamp
1600,P00-1012,External_69922,[0],experiments,The simplest strategy for ordering adjectives is what <TARGET_CITATION/> call the direct evidence method ., The simplest strategy for ordering adjectives is what <TARGET_CITATION/> call the direct evidence method . The simplest strategy for ordering adjectives is what <CITATION/> call the direct evidence method.,a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf,4eec0a1ce645151c4acc89b151814b3deb53af43,Ordering Among Premodifiers,1999,James Shaw; V. Hatzivassiloglou
1601,J06-2002,C04-1181,[0],introduction,"The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed <TARGET_CITATION/> : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .","(3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm)Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cfXXX, Kennedy 1999). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed <TARGET_CITATION/> : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large . The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed <CITATION/>: (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large. Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cfXXX, Kennedy 1999). (3) The large mouse (= the one whose size is 10 cm) (4) The two large mice (= the two whose sizes are 8 and 10 cm)",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,14e8bfecc1b27f7720712e57ddfab74576eca092,Interpreting Vague Utterances in Context,2004,David DeVault; Matthew Stone
1602,W06-3813,External_14918,[0],related work,Such systems extract information from some types of syntactic units ( clauses in <TARGET_CITATION/> ; noun phrases in <CITATION/> ) .,"Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>. In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Such systems extract information from some types of syntactic units ( clauses in <TARGET_CITATION/> ; noun phrases in <CITATION/> ) . Such systems extract information from some types of syntactic units (clauses in <CITATION/>; noun phrases in <CITATION/>). In other methods, lexical resources are specifically tailored to meet the requirements of the domain <CITATION/> or the system <CITATION/>. Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.",f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,,framenet and lexicographic relevance,1998,Charles Fillmore; Beryl T Atkins
1604,J02-3002,A00-2035,[2],,"Unlike other POS taggers , this POS tagger <TARGET_CITATION/> was also trained to disambiguate sentence boundaries .","To test our hypothesis that DCA can be used as a complement to a localcontext approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger. Unlike other POS taggers , this POS tagger <TARGET_CITATION/> was also trained to disambiguate sentence boundaries . Unlike other POS taggers, this POS tagger <CITATION/> was also trained to disambiguate sentence boundaries.To test our hypothesis that DCA can be used as a complement to a localcontext approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,ec2f036f1e6f56ad6400e42cf1eb14f4ebe122c6,Tagging Sentence Boundaries,2000,Andrei Mikheev
1605,P13-3018,External_83583,[0],introduction,"On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent <TARGET_CITATION/> .","are morphologically richer than many of their IndoEuropean cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent <TARGET_CITATION/> . On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent <CITATION/>. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. are morphologically richer than many of their IndoEuropean cousins.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,0c3e2e233a752d09cd29b4fe9bd727bd0c2a1780,Morphological Decomposition and the Reverse Base Frequency Effect,2004,M. Taft
1606,W04-1805,External_1172,[0],method,ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework <TARGET_CITATION/> and called qualia relations <CITATION/> .,inference of extraction patterns with ASARES; and 3. extraction of NV pairs from the corpus with the inferred patterns. ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework <TARGET_CITATION/> and called qualia relations <CITATION/> . ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework <CITATION/> and called qualia relations <CITATION/>. extraction of NV pairs from the corpus with the inferred patterns.inference of extraction patterns with ASARES; and 3.,f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,259d0304adcb49e40436137684b78a80c9ef097b,The Generative Lexicon,1991,J. Pustejovsky
1607,W06-1639,External_62831,[0],introduction,"People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length <TARGET_CITATION/> .","Regardless of whether one views such claims as clearsighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process. Evaluative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text. People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length <TARGET_CITATION/> . People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S.) bills often reach several hundred pages in length <CITATION/>. Evaluative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text. Regardless of whether one views such claims as clearsighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,a509c85d68fb1e871609b3d37743f2b6f8efc337,The American Congress,2019,S. L. Hagedorn; Michael C. LeMay
1608,W04-0910,External_41698,[0],,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena <CITATION/> ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information <TARGET_CITATION/> .","While corpus driven efforts along the PARSEVAL lines <CITATION/> are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena <CITATION/> ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information <TARGET_CITATION/> . For english, there is for instance the 15 year old HewlettPackard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena <CITATION/>; and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information <CITATION/>. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. While corpus driven efforts along the PARSEVAL lines <CITATION/> are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,,towards systematic grammar profiling test suite technology 10 years after computer speech and language,1998,S Oepen; D Flickinger
1609,W06-1705,External_8426,[0],introduction,"Its significance is reflected both in the growing interest in annotation software for word sense tagging <TARGET_CITATION/> and in the longstanding use of partofspeech taggers , parsers and morphological analysers for data from English and many other languages .","Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse. Its significance is reflected both in the growing interest in annotation software for word sense tagging <TARGET_CITATION/> and in the longstanding use of partofspeech taggers , parsers and morphological analysers for data from English and many other languages . Its significance is reflected both in the growing interest in annotation software for word sense tagging <CITATION/> and in the longstanding use of partofspeech taggers, parsers and morphological analysers for data from English and many other languages. Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,d8e247ae8ebfbc042b84e9f254d6e611611ab01a,Introduction to the special issue on evaluating word sense disambiguation systems,2002,P. Edmonds; A. Kilgarriff
1610,J06-2002,External_40449,[0],introduction,"Each of these Values has equal status , so the notion of a basiclevel Value can not play a role ( cfXXX , <TARGET_CITATION/> ) .","For one thing, it is convenient to view properties of the form size(x) <  as belonging to a different Attribute than those of the form size(x) > , because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on. More importantly, it will now become normal for an object to have many Values for the same Attribute; c4, for example, has the Values > 6 cm, > 10 cm, and > 12 cm. Each of these Values has equal status , so the notion of a basiclevel Value can not play a role ( cfXXX , <TARGET_CITATION/> ) . Each of these Values has equal status, so the notion of a basiclevel Value cannot play a role (cfXXX, Dale and Reiter 1995). More importantly, it will now become normal for an object to have many Values for the same Attribute; c4, for example, has the Values > 6 cm, > 10 cm, and > 12 cm. For one thing, it is convenient to view properties of the form size(x) <  as belonging to a different Attribute than those of the form size(x) > , because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,a32c486987fb5df4d8dc9133180d51cee899478a,Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions,1995,R. Dale; Ehud Reiter
1611,D08-1034,J08-2004,[0],introduction,"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <TARGET_CITATION/> .","With the efforts of many researchers (Carreras and M rquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <TARGET_CITATION/> . Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as <CITATION/>. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. With the efforts of many researchers (Carreras and M rquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,03541f4c7b737964289b3cb2cde4b6ac08a1c29d,Labeling Chinese Predicates with Semantic Roles,2008,Nianwen Xue
1612,D08-1001,J02-1002,[2],experiments,"Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by <TARGET_CITATION/> ."," Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by <TARGET_CITATION/> . Accuracy is not the best measure to assess segmentation quality, therefore we also conducted experiments using the WindowDiff measure as proposed by <CITATION/>.",8eea098583cb18298b10a277a7c452208300923f,Revealing the Structure of Medical Dictations with Conditional Random Fields,2008,Jeremy Jancsary; Johannes Matiasek; Harald Trost,52dc40d50a891c6e4b5fa6a046d7009adf63c740,A Critique and Improvement of an Evaluation Metric for Text Segmentation,2002,L. Pevzner; Marti A. Hearst
1613,N04-2004,External_40237,[4],,The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program <TARGET_CITATION/> .,My theory of verbal argument structure can be implemented in a unified morphosyntactic parsing model that interleaves syntactic and semantic parsing. The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program <TARGET_CITATION/> . The system is in the form of an agendadriven chartbased parser whose foundation is similar to previous formalizations of Chomsky's Minimalist Program <CITATION/>.My theory of verbal argument structure can be implemented in a unified morphosyntactic parsing model that interleaves syntactic and semantic parsing.,1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,f755b8b47a1ba1935c7133cf8f489f792c3bf250,A Minimalist Implementation of Verb Subcategorization,2001,Sourabh A. Niyogi
1614,D11-1138,D07-1013,[1],experiments,"Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies <TARGET_CITATION/> and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks","The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies <TARGET_CITATION/> and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies <CITATION/> and these dependencies are typically the most meaningful for downstream tasks, e.g., main verb dependencies for tasksThe labeled version of this score requires that the labels of the arc are also correct. The score is normalized by the summed arc lengths for the sentence.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,ef06ccbdab742b88f5eff8b46a61fa4cf7cfcbd8,Characterizing the Errors of Data-Driven Dependency Parsing Models,2007,Ryan T. McDonald; Joakim Nivre
1615,W10-4005,W10-3908,[4],conclusion,"Whereas <TARGET_CITATION/> dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .","pus, whereby the same corpus is used for several language pairs. The basic idea underlying our work is to look at foreign words, to compute their cooccurrencebased associations, and to consider these as translations of the respective words. Whereas <TARGET_CITATION/> dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora . Whereas <CITATION/> dealt only with an English corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora. The basic idea underlying our work is to look at foreign words, to compute their cooccurrencebased associations, and to consider these as translations of the respective words. pus, whereby the same corpus is used for several language pairs.",305ea15f21fb788f20d9b56cfdad590dcc62200c,The Noisier the Better: Identifying Multilingual Word Translations Using a Single Monolingual Corpus,2010,R. Rapp; M. Zock; A. Trotman; Yue Xu,b1b748fd1f1fa3485be04077b1c62a38b99f1cc3,Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation,2010,R. Rapp; M. Zock
1616,N10-1084,J07-4004,[2],introduction,"We also experiment with a CCG parser <TARGET_CITATION/> , requiring that the contexts surrounding the original phrase and paraphrase are assigned","We find that using larger contexts leads to a high precision system (100% when using 5grams), but at the cost of a reduced recall. This precisionrecall tradeoff reflects the inherent tradeoff between imperceptibility and payload in a Linguistic Steganography system. We also experiment with a CCG parser <TARGET_CITATION/> , requiring that the contexts surrounding the original phrase and paraphrase are assigned We also experiment with a CCG parser <CITATION/>, requiring that the contexts surrounding the original phrase and paraphrase are assignedThis precisionrecall tradeoff reflects the inherent tradeoff between imperceptibility and payload in a Linguistic Steganography system. We find that using larger contexts leads to a high precision system (100% when using 5grams), but at the cost of a reduced recall.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,2d45f21c9deb17987a6be71b3c9a2758791540a2,Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models,2007,S. Clark; J. Curran
1617,D08-1042,W02-1010,[5],related work,"Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) <TARGET_CITATION/> .","But as was also noted in <CITATION/>, this kernel is opaque i.e. it is not obvious what the implicit features are and the authors do not describe it either. In contrast, our dependencybased word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths. Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) <TARGET_CITATION/> . Their kernel is also very time consuming and in their more general sparse setting it requires O(mn3) time and O(mn2) space, where m and n are the number of nodes of the two trees (m >= n) <CITATION/>. In contrast, our dependencybased word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths. But as was also noted in <CITATION/>, this kernel is opaque i.e. it is not obvious what the implicit features are and the authors do not describe it either.",57e2f24f7cd6c2a80b1171f93cc4ba7f3798dad9,A Dependency-based Word Subsequence Kernel,2008,Rohit J. Kate,cc1cad12521b5aab43fdda5b4dec67586aef1f87,Kernel Methods for Relation Extraction,2002,D. Zelenko; Chinatsu Aone; A. Richardella
1619,P11-1134,W04-3205,[0],introduction,"ones , DIRT <CITATION/> , VerbOcean <TARGET_CITATION/> , FrameNet <CITATION/> , and Wikipedia <CITATION/> .","Besides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>. These include, just to mention the most popular ones , DIRT <CITATION/> , VerbOcean <TARGET_CITATION/> , FrameNet <CITATION/> , and Wikipedia <CITATION/> . ones, DIRT <CITATION/>, VerbOcean <CITATION/>, FrameNet <CITATION/>, and Wikipedia <CITATION/>. These include, just to mention the most popularBesides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,0c739b915d633cc3c162e4ef1e57b796c2dc2217,VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations,2004,Timothy Chklovski; Patrick Pantel
1620,W04-1610,External_29099,[0],related work,"include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in <TARGET_CITATION/> , respectively .","They1 Most modern Arabic writing (web, novels, articles) are written without vowels. include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in <TARGET_CITATION/> , respectively . include decision tree learning and Bayesian learning, nearest neighbor learning, and artificial neural networks, early such works may be found in <CITATION/>, respectively. 1 Most modern Arabic writing (web, novels, articles) are written without vowels.They",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,,comparison of two learning algorithms for text categorizationquot,1994,D Lewis; M Ringnette
1621,W06-2807,External_98156,[0],,"In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object <TARGET_CITATION/> .","AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML. In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object <TARGET_CITATION/> . In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS, Document Object Model (or DOM), the XMLHTTPRequest object <CITATION/>. AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,wikipedia from wikipedia the free encyclopedia,2005,Wikipedia
1622,W04-0910,External_10315,[5],,"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <TARGET_CITATION/> .","Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <CITATION/>. Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <TARGET_CITATION/> . For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <CITATION/>. Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available <CITATION/>.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,a69b04113c8f890a09e07ef86d0ccfe6982b8289,40 80 11 v 1 2 2 A ug 1 99 4 DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS,2011,Fernando C Pereira
1623,W06-2933,External_97,[0],method,"converted to numerical features using the standard technique of binarization , and we split values of the FEATS field into its atomic components .4 For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy <TARGET_CITATION/> .","2The fields PHEAD and PDEPREL have not been used at all, since we rely on pseudoprojective parsing for the treatment of nonprojective structures. 3We also ran preliminary experiments with memorybased learning but found that this gave consistently lower accuracy. converted to numerical features using the standard technique of binarization , and we split values of the FEATS field into its atomic components .4 For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy <TARGET_CITATION/> . converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components.4 For some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy <CITATION/>. 3We also ran preliminary experiments with memorybased learning but found that this gave consistently lower accuracy.2The fields PHEAD and PDEPREL have not been used at all, since we rely on pseudoprojective parsing for the treatment of nonprojective structures.",f3719ad02a3164e87e5ef0b1de215354e4c9de92,Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines,2006,Joakim Nivre; Johan Hall; Jens Nilsson; Gülşen Eryiğit; Svetoslav Marinov,f0e1883cf9d1b3c911125f46359f908557fc5827,Statistical Dependency Analysis with Support Vector Machines,2003,H. Yamada; Yuji Matsumoto
1624,J03-3004,External_61551,[0],introduction," language learning <CITATION/>  monolingual grammar induction <TARGET_CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","The marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages. The marker hypothesis has been used for a number of different languagerelated tasks, including language learning <CITATION/>  monolingual grammar induction <TARGET_CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )  language learning <CITATION/>  monolingual grammar induction <CITATION/>  grammar optimization <CITATION/>  insights into universal grammar <CITATION/>  machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way, and Hearne 2002)The marker hypothesis has been used for a number of different languagerelated tasks, includingThe marker hypothesis is arguably universal in presuming that concepts and structures like these have similar morphological or structural marking in all languages.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,55ecd739ed7717067191535a8f7cc318842a10d0,On Psycholinguistic Grammars,1998,P. Juola
1625,A00-1022,A97-1031,[2],,"Linguistic preprocessing of text documents is carried out by reusing smes , an information extraction core system for realworld German text processing <TARGET_CITATION/> ."," Linguistic preprocessing of text documents is carried out by reusing smes , an information extraction core system for realworld German text processing <TARGET_CITATION/> . Linguistic preprocessing of text documents is carried out by reusing smes, an information extraction core system for realworld German text processing <CITATION/>.",79a78e6a86b296679dbe9a98cf52fea75f6f7d7a,Message Classification in the Call Center,2000,Stephan Busemann; S. Schmeier; Roman Georg Arens,042b49ddd96847a17d72fdfec7e9189c7af0d06b,An Information Extraction Core System for Real World German Text Processing,1997,G. Neumann; R. Backofen; Judith Baur; Markus Becker; Christian Braun
1626,D09-1143,J08-2003,[5],conclusion,"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <CITATION/> or shallow semantic trees , <TARGET_CITATION/> .","For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNetbased features <CITATION/> or shallow semantic trees , <TARGET_CITATION/> . Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNetbased features <CITATION/> or shallow semantic trees, <CITATION/>. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the stateoftheart in RE. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task.",a1435f9443794a882be226393dabaa2c6de0e6d3,"Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction",2009,Truc-Vien T. Nguyen; Alessandro Moschitti; G. Riccardi,d45129b6ecda3b6941befa34da49ab1ee4248679,Tree Kernels for Semantic Role Labeling,2008,Alessandro Moschitti; Daniele Pighin; Roberto Basili
1627,J03-3004,External_61552,[0],introduction,"For English  > Urdu , <TARGET_CITATION/> ) notes that  the system learned the original training corpus ... perfectly and could reproduce it without errors '' ; that is , it scored 100 % accuracy when tested against the training corpus .","Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English > French and English > Urdu. For the English > French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English  > Urdu , <TARGET_CITATION/> ) notes that  the system learned the original training corpus ... perfectly and could reproduce it without errors '' ; that is , it scored 100 % accuracy when tested against the training corpus . For English > <CITATION/> notes that the system learned the original training corpus ... perfectly and could reproduce it without errors''; that is, it scored 100% accuracy when tested against the training corpus. For the English > French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English > French and English > Urdu.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,corpusbased acquisition of transfer functions using psycholinguistic principles,1997,Patrick Juola
1629,W03-0806,P01-1005,[0],introduction,Recent work <TARGET_CITATION/> has suggested that some tasks will benefit from using significantly more data .,"This will require more efficient learning algorithms and implementations. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data <CITATION/>). Recent work <TARGET_CITATION/> has suggested that some tasks will benefit from using significantly more data . Recent work <CITATION/> has suggested that some tasks will benefit from using significantly more data. However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data <CITATION/>). This will require more efficient learning algorithms and implementations.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,7628b62d64d2e5c33a13a5a473bc41b2391c1ebc,Scaling to Very Very Large Corpora for Natural Language Disambiguation,2001,Michele Banko; Eric Brill
1630,J00-1003,W98-1302,[0],,See <TARGET_CITATION/> for a variant of this approximation that constructs finite transducers rather than finite automata .,"For the sake of presentational convenience, the above describes a construction working on the complete grammar. However, our implementation applies the construction separately for each nonterminal in a set N, such that recursive(N) = self, which leads to a separate subautomaton of the compact representation (Section 3). See <TARGET_CITATION/> for a variant of this approximation that constructs finite transducers rather than finite automata . See <CITATION/> for a variant of this approximation that constructs finite transducers rather than finite automata. However, our implementation applies the construction separately for each nonterminal in a set N, such that recursive(N) = self, which leads to a separate subautomaton of the compact representation (Section 3). For the sake of presentational convenience, the above describes a construction working on the complete grammar.",b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof,aef7597a77c5057f4c6fe00d410d40c793a3acfc,Context-Free Parsing through Regular Approximation,1998,M. Nederhof
1631,Q13-1020,P05-1034,[0],introduction,"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <TARGET_CITATION/> , 2006 ; <CITATION/> ) .","In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <CITATION/> , 2009 ; <TARGET_CITATION/> ) . Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, treebased translation models have shown promising progress in improving translation quality (<CITATION/>, 2009; <CITATION/>, 2006; <CITATION/>). In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,d9f2186e2e3aab0e697532aeec0d1b88a61af02d,Dependency Treelet Translation: Syntactically Informed Phrasal SMT,2005,Chris Quirk; Arul Menezes; Colin Cherry
1632,J00-2001,External_80545,[0],,"These include devices such as interleaving the components <CITATION/> , backtracking on failure <CITATION/> , allowing the linguistic component to interrogate the planner <CITATION/> , and Hovy 's notion of restrictive ( i.e. , bottomup ) planning ( <TARGET_CITATION/> , 1988c ) .","Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity. There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components <CITATION/> , backtracking on failure <CITATION/> , allowing the linguistic component to interrogate the planner <CITATION/> , and Hovy 's notion of restrictive ( i.e. , bottomup ) planning <TARGET_CITATION/> . These include devices such as interleaving the components <CITATION/>, backtracking on failure <CITATION/>, allowing the linguistic component to interrogate the planner <CITATION/>, and Hovy's notion of restrictive (i.e., bottomup) planning (Hovy 1988a, 1988c). There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,,generating natural language linder pragmatic constraints lawrence erlbaum,1988,Eduard H Hovy
1635,J04-3001,External_33281,[0],,"Some wellknown approaches include rulebased models <TARGET_CITATION/> , backedoff models <CITATION/> , and a maximumentropy model <CITATION/> .","One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some wellknown approaches include rulebased models <TARGET_CITATION/> , backedoff models <CITATION/> , and a maximumentropy model <CITATION/> . Some wellknown approaches include rulebased models <CITATION/>, backedoff models <CITATION/>, and a maximumentropy model <CITATION/>. Researchers have proposed many computational models for resolving PPattachment ambiguities. One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,a rule based approach to pp attachment disambiguation,1994,Eric Brill; Philip S Resnik
1636,P00-1007,External_29000,[0],introduction,"One approach to partial parsing was presented by <TARGET_CITATION/> , who extended a shallowparsing technique to partial parsing .","The approach presented here, of trainable partial parsing, attempts to reduce the gap between shallow and full parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. One approach to partial parsing was presented by <TARGET_CITATION/> , who extended a shallowparsing technique to partial parsing . One approach to partial parsing was presented by <CITATION/>, who extended a shallowparsing technique to partial parsing. It is an extension of shallow parsing towards handling composite and multiple patterns, while maintaining the local nature of the task, and simplicity of training material. The approach presented here, of trainable partial parsing, attempts to reduce the gap between shallow and full parsing.",d58542faa89032c785e46aabc60ca18d4872332d,Incorporating Compositional Evidence in Memory-Based Partial Parsing,2000,Yuval Krymolowski; Ido Dagan,5db4b1405a1e77311a4a78414fe7eedc7712c4ed,Cascaded Grammatical Relation Assignment,1999,S. Buchholz; Jorn Veenstra; Walter Daelemans
1637,J09-4010,External_2296,[4],,"<TARGET_CITATION/> compared a predictive approach ( statistical translation ) , a retrieval approach based on a languagemodel , and a hybrid approach which combines statistical chunking and traditional retrieval .","<CITATION/> compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de <CITATION/> compared different variants of retrieval techniques. <TARGET_CITATION/> compared a predictive approach ( statistical translation ) , a retrieval approach based on a languagemodel , and a hybrid approach which combines statistical chunking and traditional retrieval . <CITATION/> compared a predictive approach (statistical translation), a retrieval approach based on a languagemodel, and a hybrid approach which combines statistical chunking and traditional retrieval. Jijkoun and de <CITATION/> compared different variants of retrieval techniques. <CITATION/> compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,1c78809a7fd22c95f4d60cd707c32280a019940f,Automatic question answering using the web: Beyond the Factoid,2006,Radu Soricut; Eric Brill
1638,W04-1805,External_1519,[4],introduction,"Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by <TARGET_CITATION/> with WoRDNET relations ) .","The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information (L'<CITATION/>). Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by <TARGET_CITATION/> with WoRDNET relations ) . Indeed, such rich semantic links can be used to extend indices or reformulate queries (similar to the work by <CITATION/> with WoRDNET relations).Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information (L'<CITATION/>).",f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,query expansion using lexicalsemantic relations,1994,Ellen M Voorhees
1639,J05-3003,External_105,[0],introduction,"In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] <TARGET_CITATION/> , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."," In modern syntactic theories ( e.g. , lexicalfunctional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , headdriven phrase structure grammar [ HPSG ] <TARGET_CITATION/> , treeadjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information . In modern syntactic theories (e.g., lexicalfunctional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], headdriven phrase structure grammar [HPSG] [Pollard and Sag 1994], treeadjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,headdriven phrase structure grammar,1994,Carl Pollard; Ivan Sag
1640,J00-2005,W94-0319,[0],introduction,"Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by <TARGET_CITATION/> .","The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by <TARGET_CITATION/> . Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by <CITATION/>. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.",f10e6b08a31d42bd0c6f51808cfa1058d170fd49,Pipelines and size constraints,2000,Ehud Reiter,48f426fe2018022838bebe3744cc728c0b6053f9,"Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible?",1994,Ehud Reiter
1641,J03-3004,External_42692,[0],introduction,"Other similar approaches include those of Cicekli and G  uvenir ( 1996 ) , <TARGET_CITATION/> , inter alia .","<CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. <CITATION/> combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G  uvenir ( 1996 ) , <TARGET_CITATION/> , inter alia . Other similar approaches include those of <CITATION/>, inter alia. <CITATION/> combines lexical and dependency mappings to form his generalizations. <CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,inducing translation templates for examplebased machine translation,1999,Michael Carl
1643,J87-3002,External_40340,[0],,"In addition to headwords , dictionary search through the pronunciation field is available ; <CITATION/> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <TARGET_CITATION/> .","While no application currently makes use of this facility, the motivation for such an approach to dictionary access comes from envisaging a parser which will operate on the basis of the online LDOCE; and any serious parser must be able to recognise compounds before it segments its input into separate words. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; <CITATION/> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <TARGET_CITATION/> . In addition to headwords, dictionary search through the pronunciation field is available; <CITATION/> has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <CITATION/>. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. While no application currently makes use of this facility, the motivation for such an approach to dictionary access comes from envisaging a parser which will operate on the basis of the online LDOCE; and any serious parser must be able to recognise compounds before it segments its input into separate words.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,8943f36be3e2665cb6fa7a0ee676a1eb853137f6,Phonotactic and Lexical Constraints in Speech Recognition,1983,D. Huttenlocher; Victor W. Sue
1644,J05-3003,External_13650,[2],,"Following <TARGET_CITATION/> , we extract a reference lexicon from Sections 02  21 of the WSJ .","In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced. This can be expressed as a measure of the coverage of the induced lexicon on new data. Following <TARGET_CITATION/> , we extract a reference lexicon from Sections 02  21 of the WSJ . Following <CITATION/>, we extract a reference lexicon from Sections 0221 of the WSJ. This can be expressed as a measure of the coverage of the induced lexicon on new data. In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,d4943720cc393626ed1ff87e6bad9622e69cb284,Extracting Tree Adjoining Grammars from Bracketed Corpora,2009,Fei Xia; Chung-hye Han; A. Joshi; Martha Palmer; C. Prolo; Anoop Sarkar
1645,W98-1124,W97-0713,[3],,"The only disambiguation metric that we used in our previous work <TARGET_CITATION/> was the shapebased metric , according to which the  best '' trees are those that are skewed to the right .","In the light of the rhetoricalclusteringbased metric, we consider that a discourse tree A is better'' than another discourse tree B if the sum of the rhetoricalclustering scores associated with the nodes of .4 is higher than the sum of the rhetoricalclustering scores associated with the nodes of B. The shapebased metric. The only disambiguation metric that we used in our previous work <TARGET_CITATION/> was the shapebased metric , according to which the  best '' trees are those that are skewed to the right . The only disambiguation metric that we used in our previous work <CITATION/> was the shapebased metric, according to which the best'' trees are those that are skewed to the right. The shapebased metric. In the light of the rhetoricalclusteringbased metric, we consider that a discourse tree A is better'' than another discourse tree B if the sum of the rhetoricalclustering scores associated with the nodes of .4 is higher than the sum of the rhetoricalclustering scores associated with the nodes of B.",6d5e3c29d0ce466eec6bc62a6e3f905bc6f98eb3,Improving summarization through rhetorical parsing tuning,1998,Daniel Marcu,1daf375141571501ca8c30b62d7c14269d566762,From discourse structures to text summaries,1997,D. Marcu
1646,W06-3309,External_20681,[0],introduction,The need for information systems to support physicians at the point of care has been well studied <TARGET_CITATION/> .,"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>. For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied <TARGET_CITATION/> . The need for information systems to support physicians at the point of care has been well studied <CITATION/>. For a variety of reasons, medicine is an interesting domain of research. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,897faff173c3881e7c57017b1b70d2e88ddbf829,Information needs in office practice: are they being met?,1985,D. Covell; G. Uman; P. R. Manning
1647,W06-2807,External_91885,[0],related work,"Every arc always has a definite direction , i.e. arcs are arrows <TARGET_CITATION/> .","Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel. Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs. Every arc always has a definite direction , i.e. arcs are arrows <TARGET_CITATION/> . Every arc always has a definite direction, i.e. arcs are arrows <CITATION/>. Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs. Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,learning creating and using knowledge concept maps as facilitative tools in schools and corporations lawrence erlbaum associates,1998,Joseph Donald Novak
1648,J04-3001,External_33266,[2],,"In the first experiment , we use an induction algorithm <TARGET_CITATION/> based on the expectationmaximization ( EM ) principle that induces parsers for PLTIGs .","This study is repeated for two different models, the PLTIG parser and Collins's Model 2 parser. 4.2.1 An ExpectationMaximizationBased Learner. In the first experiment , we use an induction algorithm <TARGET_CITATION/> based on the expectationmaximization ( EM ) principle that induces parsers for PLTIGs . In the first experiment, we use an induction algorithm <CITATION/> based on the expectationmaximization (EM) principle that induces parsers for PLTIGs. 4.2.1 An ExpectationMaximizationBased Learner. This study is repeated for two different models, the PLTIG parser and Collins's Model 2 parser.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,5420574cf620ce6325a8963ce70efb2b72b55c4c,Learning probabilistic lexicalized grammars for natural language processing,2001,Stuart M. Shieber; R. Hwa
1649,Q13-1020,W06-3119,[2],experiments,"To create the baseline system , we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased ( HPB ) system , and a syntaxaugmented MT ( SAMT ) 11 system <TARGET_CITATION/> respectively .","The translation quality is evaluated by caseinsensitive BLEU4 with the shortest length penalty. The statistical significance test is performed by the resampling approach <CITATION/>. To create the baseline system , we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased ( HPB ) system , and a syntaxaugmented MT ( SAMT ) 11 system <TARGET_CITATION/> respectively . To create the baseline system, we use the opensource Joshua 4.0 system <CITATION/> to build a hierarchical phrasebased (HPB) system, and a syntaxaugmented MT (SAMT) 11 system <CITATION/> respectively. The statistical significance test is performed by the resampling approach <CITATION/>. The translation quality is evaluated by caseinsensitive BLEU4 with the shortest length penalty.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,56bc078f0b7b4c6112001af12527b3f7fcf4f021,Syntax Augmented Machine Translation via Chart Parsing,2006,Andreas Zollmann; Ashish Venugopal
1650,J09-4010,W06-0706,[2],method,"6 For SentPred we also experimented with grammatical and sentencebased syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person <TARGET_CITATION/> , but the simple binary bagoflemmas representation yielded similar results .","We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the 6 For SentPred we also experimented with grammatical and sentencebased syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person <TARGET_CITATION/> , but the simple binary bagoflemmas representation yielded similar results . 6 For SentPred we also experimented with grammatical and sentencebased syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person <CITATION/>, but the simple binary bagoflemmas representation yielded similar results. During theWe use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests.7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,1f1daf9c36c3370ccd206c794fe1f9d630292036,Automating Help-desk Responses: A Comparative Study of Information-gathering Approaches,2006,Yuval Marom; Ingrid Zukerman
1651,P10-4003,External_11756,[5],experiments,"At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to <TARGET_CITATION/> .","The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on <CITATION/>, outputs a diagnosis which consists of lists of correct, contradictory and nonmentioned objects and relations from the student's answer. At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to <TARGET_CITATION/> . At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to <CITATION/>.The diagnoser, based on <CITATION/>, outputs a diagnosis which consists of lists of correct, contradictory and nonmentioned objects and relations from the student's answer. The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,learning to assess lowlevel conceptual understanding,2008,Rodney D Nielsen; Wayne Ward; James H Martin
1652,J00-4002,J90-1001,[4],,It is interesting to compare this analysis with that described in <TARGET_CITATION/> ) .," It is interesting to compare this analysis with that described in <TARGET_CITATION/> ) . It is interesting to compare this analysis with that described in <CITATION/> and Pereira (1990, 1991).",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,8a1053957e26b4fccd9105ab94835e6d06cd659d,Categorial Semantics and Scoping,1990,Fernando C Pereira
1653,E03-1005,A00-2018,[0],introduction,The importance of including nonheadwords has become uncontroversial <TARGET_CITATION/> .,"However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of <CITATION/> restricted the fragments to the locality of headwords, later models showed the importance of including context from higher nodes in the tree <CITATION/>. The importance of including nonheadwords has become uncontroversial <TARGET_CITATION/> . The importance of including nonheadwords has become uncontroversial <CITATION/>. While the models of <CITATION/> restricted the fragments to the locality of headwords, later models showed the importance of including context from higher nodes in the tree <CITATION/>. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.",a9d59174cc50b119ee4be19b3e65177431e37003,An efficient implementation of a new DOP model,2003,R. Bod,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
1654,W06-3813,External_1008,[0],related work,Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <TARGET_CITATION/> ., Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <TARGET_CITATION/> . Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts <CITATION/>.,f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,547f23597f9ec8a93f66cedaa6fbfb73960426b1,The Berkeley FrameNet Project,1998,Collin F. Baker; C. Fillmore; John B. Lowe
1655,J92-1004,External_10588,[4],,"The example used to illustrate the power of ATNs <TARGET_CITATION/> ,  John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb  believed '' acts as both an absorber and a ( re ) generator .","Meanwhile, the [participialphrase] passes along the original FLOATOBJECT (which hospital'') to its right sibling, the adverbial prepositional phrase, to [object].'' The phrase which hospital'' is finally absorbed by the preposition's object. The example used to illustrate the power of ATNs <TARGET_CITATION/> ,  John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb  believed '' acts as both an absorber and a ( re ) generator . The example used to illustrate the power of ATNs <CITATION/>, John was believed to have been shot,'' also parses correctly, because the [object] node following the verb believed'' acts as both an absorber and a (re)generator. The phrase which hospital'' is finally absorbed by the preposition's object. Meanwhile, the [participialphrase] passes along the original FLOATOBJECT (which hospital'') to its right sibling, the adverbial prepositional phrase, to [object].''",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,semantics and quantification in natural language question answeringquot,1986,W A Woods
1656,J08-1003,E03-1025,[1],introduction,"Problems such as these have motivated research on more abstract , dependencybased parser evaluation <TARGET_CITATION/> .","3. Because a treebased gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as handcrafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract , dependencybased parser evaluation <TARGET_CITATION/> . Problems such as these have motivated research on more abstract, dependencybased parser evaluation <CITATION/>. Because a treebased gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as handcrafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser.3.",9c8e756fda6c46d9f78430ee4f7bbce66b168921,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,2008,A. Cahill; Michael Burke; Ruth O'Donovan; S. Riezler; Josef van Genabith; Andy Way,716edcdc92b0c33242395f1e08ea87d30b8bd4f7,Using Grammatical Relations to Compare Parsers,2003,Judita Preiss
1657,W01-1510,External_9268,[0],experiments,"LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system <TARGET_CITATION/> .","The RenTAL system is implemented in LiLFeS <CITATION/>2. LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system <TARGET_CITATION/> . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system <CITATION/>. The RenTAL system is implemented in LiLFeS <CITATION/>2.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,1518f674f1a27f011d61f18521004d6a86573838,An HPSG parser with CFG filtering,2000,Kentaro Torisawa; K. Nishida; Yusuke Miyao; Junichi Tsujii
1659,D10-1074,W09-3930,[3],introduction,"To address this limitation , our previous work <TARGET_CITATION/> has initiated an investigation on the problem of conversation entailment .","Hypothesis: (1) B's mother is eightythree. (2) B is eightythree. To address this limitation , our previous work <TARGET_CITATION/> has initiated an investigation on the problem of conversation entailment . To address this limitation, our previous work <CITATION/> has initiated an investigation on the problem of conversation entailment. (2) B is eightythree.Hypothesis: (1) B's mother is eightythree.",3d0adc6fca3a0669c108958c5d5204e2695ea4db,Towards Conversation Entailment: An Empirical Investigation,2010,Chen Zhang; J. Chai,ee9fb0cb4487d277233f61286eb40637e82dbb5e,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,Chen Zhang; J. Chai
1660,P02-1001,External_8687,[3],introduction," A brief version of this work , with some additional material , first appeared as <TARGET_CITATION/> .","Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed. It is common to define further useful operations (as macros), which modify existing relations not by editing their source code but simply by operating on them from outside.'' A brief version of this work , with some additional material , first appeared as <TARGET_CITATION/> . A brief version of this work, with some additional material, first appeared as <CITATION/>. It is common to define further useful operations (as macros), which modify existing relations not by editing their source code but simply by operating on them from outside.'' Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,expectation semirings flexible em for finitestate transducers,2001,Jason Eisner
1661,J09-4010,External_60687,[0],,"They also proposed two major categories of metalearning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by <TARGET_CITATION/> as follows .","This kind of metalearning is referred to as stacking by the Data Mining community <CITATION/>. <CITATION/> implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of metalearning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by <TARGET_CITATION/> as follows . They also proposed two major categories of metalearning approaches for recommender systems, merging and ensemble, each subdivided into the more specific subclasses suggested by <CITATION/> as follows. <CITATION/> implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. This kind of metalearning is referred to as stacking by the Data Mining community <CITATION/>.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,hybrid recommender systems user modeling and useradapted interaction,2002,R Burke
1662,N04-2004,External_40238,[2],,"Here , I adopt the model proposed by <TARGET_CITATION/> and decompose lexical verbs into verbalizing heads and verbal roots .","They correspond to the primitives BE, BECOME, and DO proposed by a variety of linguists; let us adopt these conceptual primitives as the basic vocabulary of our lexical semantic representation. Following the nonlexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as socalled light verbs. Here , I adopt the model proposed by <TARGET_CITATION/> and decompose lexical verbs into verbalizing heads and verbal roots . Here, I adopt the model proposed by <CITATION/> and decompose lexical verbs into verbalizing heads and verbal roots. Following the nonlexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as socalled light verbs. They correspond to the primitives BE, BECOME, and DO proposed by a variety of linguists; let us adopt these conceptual primitives as the basic vocabulary of our lexical semantic representation.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,7aabefa98b54d9897d96fe0fca4392405f34b354,No escape from syntax: Don't try morphological analysis in the privacy of your own lexicon,1997,A. Marantz
1664,N04-2004,External_1008,[0],introduction,"This approach has its roots in Fillmore 's Case <CITATION/> , and serves as the foundation for two current largescale semantic annotation projects : FrameNet <TARGET_CITATION/> and PropBank <CITATION/> .","A common lexical semantic representation in the computational linguistics literature is a framebased model where syntactic arguments are associated with various semantic roles (essentially frame slots). Verbs are viewed as simple predicates over their arguments. This approach has its roots in Fillmore 's Case <CITATION/> , and serves as the foundation for two current largescale semantic annotation projects : FrameNet <TARGET_CITATION/> . This approach has its roots in Fillmore's Case <CITATION/>, and serves as the foundation for two current largescale semantic annotation projects: FrameNet <CITATION/> and PropBank <CITATION/>. Verbs are viewed as simple predicates over their arguments. A common lexical semantic representation in the computational linguistics literature is a framebased model where syntactic arguments are associated with various semantic roles (essentially frame slots).",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,547f23597f9ec8a93f66cedaa6fbfb73960426b1,The Berkeley FrameNet Project,1998,Collin F. Baker; C. Fillmore; John B. Lowe
1665,J90-3003,External_85697,[1],experiments,"Many investigators <TARGET_CITATION/> have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a texttospeech synthesizer. Existing texttospeech systems perform well on word pronunciation and short sentences,12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators <TARGET_CITATION/> have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . Many investigators <CITATION/> have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech. Existing texttospeech systems perform well on word pronunciation and short sentences,12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a texttospeech synthesizer.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,,capacity demands in shortterm memory for synthetic and natural speech,1983,P A Luce; T C Feustel; D B Pisoni
1666,W06-3309,External_3427,[0],introduction,"The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/> , information retrieval <TARGET_CITATION/> , information extraction <CITATION/> , and question answering .","As an example, scientific abstracts across many differentfields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/> , information retrieval <TARGET_CITATION/> , information extraction <CITATION/> , and question answering . The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization <CITATION/>, information retrieval <CITATION/>, information extraction <CITATION/>, and question answering. fields generally follow the pattern of introduction'', methods'', results'', and conclusions'' <CITATION/>. As an example, scientific abstracts across many different",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,0cd2281d0aeb1013ad0bd7c06dbb07377fbcb9b9,Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the MEDLINE digital library,2006,I. Tbahriti; C. Chichester; F. Lisacek; Patrick Ruch
1668,N04-2004,External_1951,[0],,There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <TARGET_CITATION/> ; Rappaport <CITATION/> ) .,"The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <TARGET_CITATION/> ; Rappaport <CITATION/> ) . There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structurerepresentations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity (<CITATION/>; Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>).",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,97571ce3fec251a2edcaee9d3dce75eda433169f,Word Meaning And Montague Grammar,2016,Lena Osterhagen
1671,J92-1004,External_84961,[0],introduction,"Representative systems are described in <CITATION/> , De <TARGET_CITATION/> .","In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in <CITATION/> , De <TARGET_CITATION/> . Representative systems are described in <CITATION/>, De <CITATION/>.Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,the use of a semantic network in speech dialoguequot,1989,G Th Niedermair
1673,N10-1084,External_56140,[0],related work,<TARGET_CITATION/> all belong to the syntactic transformation category .,"<CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <TARGET_CITATION/> all belong to the syntactic transformation category . <CITATION/> all belong to the syntactic transformation category. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,cc2dd20b3d23aa38c19c065f06065200c80af617,A Natural Language Watermarking Based on Chinese Syntax,2005,Yuling Liu; Xingming Sun; Yong Wu
1674,E03-1002,External_13541,[2],experiments,We used a publicly available tagger <TARGET_CITATION/> to tag the words and then used these in the input to the system .,"We then tested the best nonlexicalized and the best lexicalized models on the testing set.6 Standard measures of performance are shown in table 1.7 The top panel of table 1 lists the results for the nonlexicalized model (SSNTags) and the available results for three other models which only use partofspeech tags as inputs, another neural network parser <CITATION/>, an earlier statis5In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the handcorrected tags which come with the corpus. We used a publicly available tagger <TARGET_CITATION/> to tag the words and then used these in the input to the system . We used a publicly available tagger <CITATION/> to tag the words and then used these in the input to the system. 5In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the handcorrected tags which come with the corpus. We then tested the best nonlexicalized and the best lexicalized models on the testing set.6 Standard measures of performance are shown in table 1.7 The top panel of table 1 lists the results for the nonlexicalized model (SSNTags) and the available results for three other models which only use partofspeech tags as inputs, another neural network parser <CITATION/>, an earlier statis",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,,a maximum entropy model for partofspeech tagging,1996,Adwait Ratnaparkhi
1675,D13-1115,External_1533,[0],related work,"Some works abstract perception via the usage of symbolic logic representations <TARGET_CITATION/> , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>. Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using highlevel representations instead. Some works abstract perception via the usage of symbolic logic representations <TARGET_CITATION/> , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Some works abstract perception via the usage of symbolic logic representations <CITATION/>, while others choose to employ concepts elicited from psycholinguistic and cognition studies. Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using highlevel representations instead. The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4b17662f21c6313907a022af20f88616d11620eb,Learning to Interpret Natural Language Navigation Instructions from Observations,2011,David L. Chen; R. Mooney
1676,P11-1134,External_33257,[0],introduction,"Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet <TARGET_CITATION/> ) have been created for several languages , with different degrees of coverage .","This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet <TARGET_CITATION/> ) have been created for several languages , with different degrees of coverage . Multilingual lexical databases aligned with the English WordNet (e.g. MultiWordNet <CITATION/>) have been created for several languages, with different degrees of coverage. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English. This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,,multiwordnet developing and aligned multilingual database,2002,Emanuele Pianta; Luisa Bentivogli; Christian Girardi
1677,J86-1002,External_40052,[0],experiments,"An offtheshelf speech recognition device , a Nippon Electric Corporation DP200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) <TARGET_CITATION/> .","The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An offtheshelf speech recognition device , a Nippon Electric Corporation DP200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) <TARGET_CITATION/> . An offtheshelf speech recognition device, a Nippon Electric Corporation DP200, was added to an existing natural language processing system, the Natural Language Computer (NLC) <CITATION/>. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,semantic processing for a natural language programming system,1979,B Ballard
1678,D10-1101,P07-1034,[5],conclusion,"For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation <TARGET_CITATION/> , perform in comparison to our approach .","Our CRFbased approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation <TARGET_CITATION/> , perform in comparison to our approach . For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation <CITATION/>, perform in comparison to our approach. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. Our CRFbased approach also yields promising results in the crossdomain setting.",4d135641931a6efce82bd9c1d69d86e08d3cd28d,Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields,2010,Niklas Jakob; Iryna Gurevych,b672ef69f60aea81220d658963445c41e60bb0e3,Instance Weighting for Domain Adaptation in NLP,2007,Jing Jiang; ChengXiang Zhai
1680,J04-3001,External_5769,[0],,"That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example <TARGET_CITATION/> .","2. Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly. That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example <TARGET_CITATION/> . That is, if the current hypothesis is unable to label a candidate or is uncertain about it, then the candidate might be a good training example <CITATION/>. Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly. 2.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,b69e0cce79eb288ffb43ad7ae3b99b8dea9ac5ac,Heterogeneous Uncertainty Sampling for Supervised Learning,1994,D. Lewis; J. Catlett
1681,J01-4001,P99-1079,[0],,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <TARGET_CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form <TARGET_CITATION/> ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Charniak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form <CITATION/>; and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a, 2001b). Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,54744d6219a0722ffe68621730f61a0ac068af4c,Analysis of Syntax-Based Pronoun Resolution Methods,1999,Joel R. Tetreault
1682,P13-3018,External_90449,[0],related work,"For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model <TARGET_CITATION/> .","The affixes are stripped away from the root form, which in turn are used to access the mental lexicon <CITATION/>. Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model <TARGET_CITATION/> . For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model <CITATION/>. Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon <CITATION/>.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,9ba4b5ac12496b14e9d2e571515fad89904d19fa,Lexical access and inflectional morphology,1988,A. Caramazza; A. Laudanna; C. Romani
1683,A00-2036,P98-1035,[5],conclusion,"We perceive that these results can be extended to other language models that properly embed bilexical contextfree grammars , as for instance the more general historybased models used in <TARGET_CITATION/> .","In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical contextfree grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical contextfree grammars , as for instance the more general historybased models used in <TARGET_CITATION/> . We perceive that these results can be extended to other language models that properly embed bilexical contextfree grammars, as for instance the more general historybased models used in <CITATION/>. Our results hold for bilexical contextfree grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). In this paper we have provided an original mathematical argument in favour of this thesis.",01c9ea0ebd7b342fa28de34293687d3b038ebd4a,Left-To-Right Parsing and Bilexical Context-Free Grammars,2000,M. Nederhof; G. Satta,0922ec1bf7a00391a53e8f3db93e72a51e8cc56c,Exploiting Syntactic Structure for Language Modeling,1998,Ciprian Chelba; F. Jelinek
1684,W01-1510,External_9270,[0],experiments,Another paper <TARGET_CITATION/> describes the detailed analysis on the factor of the difference of parsing performance .,parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper <TARGET_CITATION/> describes the detailed analysis on the factor of the difference of parsing performance . Another paper <CITATION/> describes the detailed analysis on the factor of the difference of parsing performance.We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. parsing.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,,efficient ltag parsing using hpsg parsers,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Jun’ichi Tsujii
1685,W01-1510,J93-2004,[2],experiments,The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus <TARGET_CITATION/> 6 ( the average length is 6.32 words ) .,"Table 1 shows the classifications of elementary tree templates of the XTAG English grammar, according to the conditions we introduced in Section 3, and also shows the number of corresponding HPSG lexical entry templates. Conversion took about 25 minutes CPU time on a 700 Mhz Pentium III Xeon with four gigabytes main memory. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus <TARGET_CITATION/> 6 ( the average length is 6.32 words ) . The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus <CITATION/>6 (the average length is 6.32 words). Conversion took about 25 minutes CPU time on a 700 Mhz Pentium III Xeon with four gigabytes main memory. Table 1 shows the classifications of elementary tree templates of the XTAG English grammar, according to the conditions we introduced in Section 3, and also shows the number of corresponding HPSG lexical entry templates.",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,0b44fcbeea9415d400c5f5789d6b892b6f98daff,Building a Large Annotated Corpus of English: The Penn Treebank,1993,Mitchell P. Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz
1686,A00-2028,P99-1040,[1],,The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors <TARGET_CITATION/> .,"The DM features also include running tallies for the number of reprompts (numreprompts), number of confirmation prompts (numconfirms), and number of subdialogue prompts (numsubdials), that had been played up to each point in the dialogue, as well as running percentages (percentreprompts, percent. confirms, percentsubdials). The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors <TARGET_CITATION/> . The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors <CITATION/>. confirms, percentsubdials). The DM features also include running tallies for the number of reprompts (numreprompts), number of confirmation prompts (numconfirms), and number of subdialogue prompts (numsubdials), that had been played up to each point in the dialogue, as well as running percentages (percentreprompts, percent.",628176f850f7ba7bb0db10c9e458d80b1cd46766,Learning to Predict Problematic Situations in a Spoken Dialogue System: Experiments with How May I Help You?,2000,M. Walker; Irene Langkilde-Geary; Jeremy H. Wright; A. Gorin; D. Litman,22404884bd6e2dc80691b968a1e571f873b0a774,Automatic Detection of Poor Speech Recognition at the Dialogue Level,1999,D. Litman; M. Walker; Michael Kearns
1687,J87-3002,External_43916,[0],,"The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as <TARGET_CITATION/> ) .","Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.LDOCE provides considerably more syntactic information than a traditional dictionary. The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as <TARGET_CITATION/> ) . The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information, usually to be found only in large descriptive grammars of English (such as <CITATION/>). LDOCE provides considerably more syntactic information than a traditional dictionary. Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,,a comprehensive grammar of english longman group limited,1985,Randolph Quirk; Sidney Greenbaum; Geoffrey Leech; Jan Svartvik
1689,W02-0309,External_34420,[0],conclusion,Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories <TARGET_CITATION/> .,"There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>. The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories <TARGET_CITATION/> . Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories <CITATION/>. The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. There has been some controversy, at least for simple stemmers <CITATION/>, about the effectiveness of morphological analysis for document retrieval <CITATION/>.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,39068aa74104a44ed84b3c45cf48627bb9c995b5,"Effective use of natural language processing techniques for automatic conflation of multi-word terms: the role of derivational morphology, part of speech tagging, and shallow parsing",1997,E. Tzoukermann; Judith L. Klavans; C. Jacquemin
1691,W11-1402,P11-2088,[3],experiments,( Details of how the averageexpert model performs can be found in our prior work <TARGET_CITATION/> . ),"To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the averageexpert model performs can be found in our prior work <TARGET_CITATION/> . ) (Details of how the averageexpert model performs can be found in our prior work <CITATION/>.)Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.",ef2ff38f2b4b1bacc252d42056ddcef3014f3fed,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,2011,Wenting Xiong; D. Litman,006ac84e4432ce8dab7a6ff0c6260d13c4a77e1f,Automatically Predicting Peer-Review Helpfulness,2011,Wenting Xiong; D. Litman
1692,D09-1087,D08-1091,[5],conclusion,"Selftraining should also benefit other discriminatively trained parsers with latent annotations <TARGET_CITATION/> , although training would be much slower compared to using generative models , as in our case .","We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by overfitting. Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <CITATION/> for self training. Selftraining should also benefit other discriminatively trained parsers with latent annotations <TARGET_CITATION/> , although training would be much slower compared to using generative models , as in our case . Selftraining should also benefit other discriminatively trained parsers with latent annotations <CITATION/>, although training would be much slower compared to using generative models, as in our case. Better results would be expected by combining the PCFGLA parser with discriminative reranking approaches <CITATION/> for self training. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by overfitting.",5bfd8d40bc071fffaf93685a46974b122ee4239d,Self-Training PCFG Grammars with Latent Annotations Across Languages,2009,Zhongqiang Huang; M. Harper,082aa12d63d6196815ae4ebe24c88a3e9521cdd1,Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing,2008,Slav Petrov; D. Klein
1694,D11-1138,W01-0521,[0],introduction,"In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> . In most cases, the accuracy of parsers degrades when run on outofdomain data <CITATION/>. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50,Corpus Variation and Parser Performance,2001,D. Gildea
1695,J05-3003,J93-2002,[0],related work,<TARGET_CITATION/> relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .,"We will divide moregeneral approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and handcorrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. <TARGET_CITATION/> relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames . <CITATION/> relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. We will divide moregeneral approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and handcorrected treebank data as their input.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,b0e5ab189f770b7e106db429f2980510065ef125,From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax,1993,M. Brent
1696,Q13-1020,External_71978,[2],experiments,"To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser <TARGET_CITATION/> .","In the system, we extract both the minimal GHKM rules <CITATION/>, and the rules of SPMT Model 1 <CITATION/> with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser <TARGET_CITATION/> . To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser <CITATION/>. We then obtain the composed rules by composing two or three adjacent minimal rules. In the system, we extract both the minimal GHKM rules <CITATION/>, and the rules of SPMT Model 1 <CITATION/> with phrases up to length L=5 on the source side.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,f52de7242e574b70410ca6fb70b79c811919fc00,"Learning Accurate, Compact, and Interpretable Tree Annotation",2006,Slav Petrov; Leon Barrett; R. Thibaux; D. Klein
1697,D11-1138,External_63,[0],introduction,One obvious approach to this problem is to employ parser reranking <TARGET_CITATION/> .,"While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domainspecific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure. One obvious approach to this problem is to employ parser reranking <TARGET_CITATION/> . One obvious approach to this problem is to employ parser reranking <CITATION/>. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domainspecific data sets which could help direct our search for optimal parameters during parser training.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,844db702be4bc149b06b822b47247e15f5894cc3,Discriminative Reranking for Natural Language Parsing,2000,M. Collins; Terry Koo
1698,J87-3002,External_43911,[5],,"Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( <TARGET_CITATION/>:460 ff . )","source of error in the case of the Object Raising rule. Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( <TARGET_CITATION/>:460 ff . ) Ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed (<CITATION/>:460ff.)Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. source of error in the case of the Object Raising rule.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,1b4c0c5b3eae6aa57cdebeb6ee2d411687b6c255,Syntactic Argumentation and the Structure of English,1979,David M. Perlmutter; S. Soames
1699,W06-3309,External_42488,[4],experiments,"The table also presents the closest comparable experimental results reported by <TARGET_CITATION/> .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .","Table 2(a) reports the multiway classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and Fmeasure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by <TARGET_CITATION/> .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 . The table also presents the closest comparable experimental results reported by <CITATION/>.1 McKnight and Srinivasan (henceforth, M&S) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001. Table 2(b) reports accuracy, precision, recall, and Fmeasure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). Table 2(a) reports the multiway classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,72de308314b88b53aef6cb86bd8390f334a6bd24,Categorization of Sentence Types in Medical Abstracts,2003,Lawrence K. McKnight; P. Srinivasan
1700,D13-1038,W12-1621,[3],introduction,How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work <TARGET_CITATION/> .,"Although physically copresent, a joint perceptual basis between the human and the robot cannot be established <CITATION/>. Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work <TARGET_CITATION/> . How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work <CITATION/>. Thus, referential communication between the human and the robot becomes difficult. Although physically copresent, a joint perceptual basis between the human and the robot cannot be established <CITATION/>.",e065f4c930f5dad1f6f82a14c180815d418ff765,Towards Situated Dialogue: Revisiting Referring Expression Generation,2013,Rui Fang; Changsong Liu; Lanbo She; J. Chai,6c53c56064c460bac13efb02d30874024730a673,Towards Mediating Shared Perceptual Basis in Situated Dialogue,2012,Changsong Liu; Rui Fang; J. Chai
1701,D13-1115,External_32478,[0],related work,"<TARGET_CITATION/> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .","cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>). <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. <TARGET_CITATION/> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . <CITATION/> furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. <CITATION/> helped pave the path for cognitivelinguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis <CITATION/> in the prediction of association norms. cue word and name the first (or several) associated words that come to mind (e.g., <CITATION/>), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., <CITATION/>).",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1b3616e10fc5f810243f20b84ffc72acbf773cc3,Integrating experiential and distributional data to learn semantic representations.,2009,Mark Andrews; G. Vigliocco; D. Vinson
1702,W00-1312,External_27894,[2],experiments,A cooccurrence based stemmer <TARGET_CITATION/> was used to stem Spanish words .,"For Spanish, we downloaded a bilingual EnglishSpanish lexicon from the Internet (http://www.activa.arralcis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer <TARGET_CITATION/> was used to stem Spanish words . A cooccurrence based stemmer <CITATION/> was used to stem Spanish words. Each English word has around 1.5 translations on average. For Spanish, we downloaded a bilingual EnglishSpanish lexicon from the Internet (http://www.activa.arralcis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,corpusbased stemming using cooccurrence of word variantsquot,1998,J Xu; W B Croft
1703,J02-3002,M95-1012,[4],,The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <TARGET_CITATION/> : a 0.5 % error rate .,Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. Stateoftheart machine learning and rulebased SBD systems achieve an error rate of 0.81.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <TARGET_CITATION/> : a 0.5 % error rate . The best performance on the WSJ corpus was achieved by a combination of the SATZ system <CITATION/> with the Alembic system <CITATION/>: a 0.5% error rate. Stateoftheart machine learning and rulebased SBD systems achieve an error rate of 0.81.5% measured on the Brown corpus and the WSJ corpus. Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.,3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,14fee4e396303e9eb00beff50ee018ca9f7aff1a,MITRE: Description of the Alembic System Used for MUC-6,1995,J. Aberdeen; J. Burger; David S. Day; L. Hirschman; Patricia Robinson; M. Vilain
1704,J97-4003,External_69617,[4],related work,"In a number of proposals , lexical generalizations are captured using lexical underspecification ( <TARGET_CITATION/> ; Krieger and Nerbonne 1992 ;","Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( <TARGET_CITATION/> ; Krieger and Nerbonne 1992 ; In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,passive without lexical rules in,1994,Andreas Kathol
1705,J90-3003,P87-1020,[4],experiments,"Although a grid may be more descriptively suitable for some aspects of prosody ( for example , <TARGET_CITATION/> use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .","Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on <CITATION/> is presented in <CITATION/>, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody ( for example , <TARGET_CITATION/> use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing . Although a grid may be more descriptively suitable for some aspects of prosody (for example, <CITATION/> use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing. An alternative representation based on <CITATION/> is presented in <CITATION/>, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,63ab2f9ee7a0f00e338508f4b1b6e28ecbd0f1ce,Toward Treating English Noniinals Correctly,1987,R. Sproat; M. Liberman
1706,E03-1002,A00-2018,[4],experiments,The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSNFreq > 200 ) and five recent statistical parsers <TARGET_CITATION/> .,The Tags model achieves performance which is better than any previously published results on parsing with a nonlexicalized model. The Tags model also does much better than the only other broad coverage neural network parser <CITATION/>. The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSNFreq > 200 ) and five recent statistical parsers <TARGET_CITATION/> . The bottom panel of table 1 lists the results for the chosen lexicalized model (SSNFreq>200) and five recent statistical parsers <CITATION/>. The Tags model also does much better than the only other broad coverage neural network parser <CITATION/>. The Tags model achieves performance which is better than any previously published results on parsing with a nonlexicalized model.,adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,76d5e3fa888bee872b7adb7fa810089aa8ab1d58,A Maximum-Entropy-Inspired Parser,2000,Eugene Charniak
1708,N10-1084,External_56144,[4],introduction,"linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words <TARGET_CITATION/> .","Note that we are concerned with transformations which are2The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits. linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words <TARGET_CITATION/> . linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words <CITATION/>. 2The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits.Note that we are concerned with transformations which are",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,8d0c5b76a1f05cef982fa15582473485b70d69c3,WhiteSteg: a new scheme in information hiding using text steganography,2008,L. Y. Por; T. F. Ang; B. Delina
1709,J92-1004,External_84963,[2],,The second version ( RM ) concerns the Resource Management task <TARGET_CITATION/> that has been popular within the DARPA community in recent years .,"To date, four distinct domainspecific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database <CITATION/>. The second version ( RM ) concerns the Resource Management task <TARGET_CITATION/> that has been popular within the DARPA community in recent years . The second version (RM) concerns the Resource Management task <CITATION/> that has been popular within the DARPA community in recent years. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database <CITATION/>. To date, four distinct domainspecific versions of TINA have been implemented.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,4bb05d370090a32800b3cc246d0624730bcea2fa,Benchmark tests for DARPA resource management database performance evaluations,1989,D. S. Pallett
1710,W14-2106,W13-4006,[4],related work,Another line of research that is correlated with ours is recognition of agreement/disagreement <TARGET_CITATION/> and classification of stances <CITATION/> in online forums .,Our work contributes a new principled method for building annotated corpora for online interactions. The corpus and guidelines will also be shared with the research community. Another line of research that is correlated with ours is recognition of agreement/disagreement <TARGET_CITATION/> and classification of stances <CITATION/> in online forums . Another line of research that is correlated with ours is recognition of agreement/disagreement <CITATION/> and classification of stances <CITATION/> in online forums. The corpus and guidelines will also be shared with the research community. Our work contributes a new principled method for building annotated corpora for online interactions.,2b8b59a74d815a70bbb31892ce484510480be6fe,Analyzing Argumentative Discourse Units in Online Interactions,2014,Debanjan Ghosh; S. Muresan; Nina Wacholder; Mark Aakhus; M. Mitsui,05f06634d0557530bfc56b45e74aa203f56cea91,Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue,2013,Amita Misra; M. Walker
1711,D08-1036,P07-1094,[4],,The studies presented by <TARGET_CITATION/> differed in the number of states that they used .,Then we use the dynamic programming sampler deaccuracy. The studies presented by <TARGET_CITATION/> differed in the number of states that they used . The studies presented by <CITATION/> differed in the number of states that they used. accuracy. Then we use the dynamic programming sampler de,bbf3ddb68cc14e886c5cf8be6cc2efc133a89063,A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers,2008,Jianfeng Gao; Mark Johnson,469d720411f8d8d75d2352b170dbe4611b508cff,A fully Bayesian approach to unsupervised part-of-speech tagging,2007,S. Goldwater; T. Griffiths
1712,A00-1004,External_9616,[0],method,"A number of alignment techniques have been proposed , varying from statistical methods <TARGET_CITATION/> to lexical methods <CITATION/> .","Some are highly parallel and easy to align while others can be very noisy. Aligning EnglishChinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages. A number of alignment techniques have been proposed , varying from statistical methods <TARGET_CITATION/> to lexical methods <CITATION/> . A number of alignment techniques have been proposed, varying from statistical methods <CITATION/> to lexical methods <CITATION/>. Aligning EnglishChinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages. Some are highly parallel and easy to align while others can be very noisy.",14ffbd58082d1197ea454ec9162b5cfd36cac9f9,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,2000,Jiang Chen; Jian-Yun Nie,5d7cf89392ce857475f8e699e11f8ac3c60dab47,Aligning Sentences in Parallel Corpora,1991,P. Brown; J. Lai; R. Mercer
1713,D08-1113,N07-1047,[5],conclusion,"We would like to use features that look at wide context on the input side , which is inexpensive <TARGET_CITATION/> .","The model's errors are often reasonable misgeneralizations (e.g., assume regular conjugation where irregular would have been correct), and it is able to use even a small number of latent variables (including the latent alignment) to capture useful linguistic properties. In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and stringtransduction tasks. We would like to use features that look at wide context on the input side , which is inexpensive <TARGET_CITATION/> . We would like to use features that look at wide context on the input side, which is inexpensive <CITATION/>. In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and stringtransduction tasks. The model's errors are often reasonable misgeneralizations (e.g., assume regular conjugation where irregular would have been correct), and it is able to use even a small number of latent variables (including the latent alignment) to capture useful linguistic properties.",62ac088d966d4a9122959f13301759c6bbda6c36,Latent-Variable Modeling of String Transductions with Finite-State Methods,2008,Markus Dreyer; Jason R. Smith; Jason Eisner,592f0d4f2520525476c979ec53c9b44badfcb37b,Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion,2007,Sittichai Jiampojamarn; Grzegorz Kondrak; Tarek Sherif
1714,J01-4001,C88-1021,[0],,"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <TARGET_CITATION/> , which was difficult both to represent and to process , and which required considerable human input .","The drive toward corpusbased robust NLP solutions further stimulated interest in alternative and/or dataenriched approaches. Last, but not least, applicationdriven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <TARGET_CITATION/> , which was difficult both to represent and to process , and which required considerable human input . Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. Last, but not least, applicationdriven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. The drive toward corpusbased robust NLP solutions further stimulated interest in alternative and/or dataenriched approaches.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,08c4176dcc331cf479c73f77e40dccb7e15496fb,Anaphora Resolution: A Multi-Strategy Approach,1988,Jaime G. Carbonell; Ralf D. Brown
1715,D08-1034,J08-2004,[0],introduction,Experiments on Chinese SRL <TARGET_CITATION/> reassured these findings .,"They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL <TARGET_CITATION/> reassured these findings . Experiments on Chinese SRL <CITATION/> reassured these findings. For semantic analysis, developing features that capture the right kind of information is crucial. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,03541f4c7b737964289b3cb2cde4b6ac08a1c29d,Labeling Chinese Predicates with Semantic Roles,2008,Nianwen Xue
1716,J03-3004,External_49743,[2],introduction,"This is then generalized , following a methodology based on <TARGET_CITATION/> , to generate the  generalized marker lexicon . ''","From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on <TARGET_CITATION/> , to generate the  generalized marker lexicon . '' This is then generalized, following a methodology based on <CITATION/>, to generate the generalized marker lexicon.'' First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,examplebased incremental synchronous interpretation,2000,Hans-Ulrich Block
1717,J86-1002,External_21720,[4],,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , <TARGET_CITATION/> , Walker 1978 , and Wolf and Woods 1980 ) ."," A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , <TARGET_CITATION/> , Walker 1978 , and Wolf and Woods 1980 ) . A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,5cc6ba30820e7c3b36d314f4deee990dc5655afb,"Speech Recognition by Machine, A Review",2010,M. Anusuya; S. Katti
1718,N10-1084,D08-1021,[2],introduction,"Our proposed method is based on the automatically acquired paraphrase dictionary described in <TARGET_CITATION/> , in which the application of paraphrases from the dictionary encodes secret bits .","2The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits.linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words <CITATION/>. Our proposed method is based on the automatically acquired paraphrase dictionary described in <TARGET_CITATION/> , in which the application of paraphrases from the dictionary encodes secret bits . Our proposed method is based on the automatically acquired paraphrase dictionary described in <CITATION/>, in which the application of paraphrases from the dictionary encodes secret bits. linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words <CITATION/>. 2The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,cadc3dbd73f0cbbe04b2a66f832c3cf34c877b41,Syntactic Constraints on Paraphrases Extracted from Parallel Corpora,2008,Chris Callison-Burch
1719,P97-1063,External_19460,[0],method,"For example , frequent words are translated less consistently than rare words <TARGET_CITATION/> .","In the basic wordtoword model, the hidden parameters A+ and Adepend only on the distributions of link frequencies generated by the competitive linking algorithm. More accurate models can be induced by taking into account various features of the linked tokens. For example , frequent words are translated less consistently than rare words <TARGET_CITATION/> . For example, frequent words are translated less consistently than rare words <CITATION/>. More accurate models can be induced by taking into account various features of the linked tokens. In the basic wordtoword model, the hidden parameters A+ and Adepend only on the distributions of link frequencies generated by the competitive linking algorithm.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,measuring semantic entropyquot,1997,I D Melamed
1720,P10-4003,External_34149,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <TARGET_CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <TARGET_CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,simulated tutors in immersive learning environments empiricallyderived design principles,2007,N B Steinhauser; L A Butler; G E Campbell
1721,P07-1007,W02-1006,[3],,These knowledge sources were effectively used to build a stateoftheart WSD program in one of our prior work <TARGET_CITATION/> .,"For our experiments, we use naive Bayes as the learning algorithm. The knowledge sources we use include partsofspeech, local collocations, and surrounding words. These knowledge sources were effectively used to build a stateoftheart WSD program in one of our prior work <TARGET_CITATION/> . These knowledge sources were effectively used to build a stateoftheart WSD program in one of our prior work <CITATION/>. The knowledge sources we use include partsofspeech, local collocations, and surrounding words. For our experiments, we use naive Bayes as the learning algorithm.",689b4bec1cf9dbfd6af293eff2aeefd8c18085b2,Domain Adaptation with Active Learning for Word Sense Disambiguation,2007,Yee Seng Chan; H. Ng,4c2a642effd543babace0c565b48cadcb6fce14f,An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation,2002,Yoong Keok Lee; H. Ng
1722,D13-1115,W11-2503,[0],introduction,Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> .,"Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <TARGET_CITATION/> . Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,1003bccaddda851c19a1127b23691e17bc6a334b,Distributional semantics from text and images,2011,Elia Bruni; G. Tran; Marco Baroni
1723,N04-2004,External_22037,[0],introduction,"Due to advances in statistical syntactic parsing techniques <TARGET_CITATION/> , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .","The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content. Due to advances in statistical syntactic parsing techniques <TARGET_CITATION/> , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences . Due to advances in statistical syntactic parsing techniques <CITATION/>, attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences. The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,,three generative lexicalized models for statistical parsing,1997,Michael Collins
1724,J02-3002,J97-2002,[4],conclusion,"This is where robust syntactic systems like SATZ <TARGET_CITATION/> or the POS tagger reported in <CITATION/> , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .","optical character readergenerated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ <TARGET_CITATION/> or the POS tagger reported in <CITATION/> , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage . This is where robust syntactic systems like SATZ <CITATION/> or the POS tagger reported in <CITATION/>, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. optical character readergenerated texts.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,30154464f549643e825ccf60072a17a3e55291d3,To Appear in Computational Linguistics Adaptive Multilingual Sentence Boundary Disambiguation,2004,D. Palmer; Marti A. Hearst
1725,K15-1003,D12-1075,[2],method,category relationships from the weak supervision : the tag dictionary and raw corpus <TARGET_CITATION/> .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of rawcorpus occurrences of each word in the dictionary evenly across all of its associated tags .,We employ the same procedure as our previous work for setting the terminal production prior dis category relationships from the weak supervision : the tag dictionary and raw corpus <TARGET_CITATION/> .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of rawcorpus occurrences of each word in the dictionary evenly across all of its associated tags . category relationships from the weak supervision: the tag dictionary and raw corpus <CITATION/>.4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of rawcorpus occurrences of each word in the dictionary evenly across all of its associated tags. We employ the same procedure as our previous work for setting the terminal production prior dis,39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,e388d24f72988f5b62cf42598e638b4f3c184fcb,Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries,2012,Dan Garrette; Jason Baldridge
1726,W00-1017,External_46370,[3],experiments,"WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system <CITATION/> , a videorecording programming system , a schedule management system <CITATION/> , and a weather infomiation system <TARGET_CITATION/> .","This makes it easy to find errors in the domain specifications.$ Implementation WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system <CITATION/> , a videorecording programming system , a schedule management system <CITATION/> , and a weather infomiation system <TARGET_CITATION/> . WIT has been implemented in Common Lisp and C on UNIX, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system <CITATION/>, a videorecording programming system, a schedule management system <CITATION/>, and a weather infomiation system <CITATION/>. $ ImplementationThis makes it easy to find errors in the domain specifications.",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,a719b3ae720093b658b0ba1487a8fe3379108738,An Efficient Dialogue Control Method An Efficient Dialogue Control Method An Efficient Dialogue Control Method An Efficient Dialogue Control Method under System under System under System under System’’’’s Limited Knowledge s Limited Knowledge s Limited Knowledge s Limited Knowledge,2021,Kohji Dohsaka; Norihito Yasuda; Noboru Miyazaki; Mikio Nakano; Kiyoaki Aikawa
1727,J05-3003,External_198,[0],,Lexical functional grammar <TARGET_CITATION/> is a member of the family of constraintbased grammars ., Lexical functional grammar <TARGET_CITATION/> is a member of the family of constraintbased grammars . Lexical functional grammar <CITATION/> is a member of the family of constraintbased grammars.,ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,e17117dbee804d7d177d8eb9fadf0bda1ebc4d22,Lexical Functional Grammar A Formal System for Grammatical Representation,2004,Ronald M. Kaplan
1728,J05-3003,C00-2100,[0],related work,<TARGET_CITATION/> present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic,"In general, argumenthood was preferred over adjuncthoood. As <CITATION/> does not include an evaluation, currently it is impossible to say how effective their technique is. <TARGET_CITATION/> present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic <CITATION/> present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (HajicAs <CITATION/> does not include an evaluation, currently it is impossible to say how effective their technique is. In general, argumenthood was preferred over adjuncthoood.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,1a9bcf583a0a0fb643ab296874e8f29ad509875d,Automatic Extraction of Subcategorization Frames for Czech,2000,Anoop Sarkar; Daniel Zeman
1729,A00-1014,A00-2027,[3],experiments,A companion paper describes the evaluation process and results in further detail <TARGET_CITATION/> .,"We compared MIMIC with two control systems: MIMICSI, a systeminitiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMICMI, a nonadaptive mixedinitiative version of MIMIC that resembles the behavior of many existing dialogue systems. In this section we summarize these experiments and their results. A companion paper describes the evaluation process and results in further detail <TARGET_CITATION/> . A companion paper describes the evaluation process and results in further detail <CITATION/>. In this section we summarize these experiments and their results. We compared MIMIC with two control systems: MIMICSI, a systeminitiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMICMI, a nonadaptive mixedinitiative version of MIMIC that resembles the behavior of many existing dialogue systems.",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,fda82250a7de8b1054110a42dc7c2403a9f82f76,Evaluating Automatic Dialogue Strategy Adaptation for a Spoken Dialogue System,2000,Jennifer Chu-Carroll
1731,W02-0309,External_34414,[0],introduction,"Hence , enumerating morphological variants in a semiautomatically generated lexicon , such as proposed for French <TARGET_CITATION/> , turns out to be infeasible , at least for German and related languages .","This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., Blut druck mess gerdt' translates to device for measuring blood pressure'). The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents. Hence , enumerating morphological variants in a semiautomatically generated lexicon , such as proposed for French <TARGET_CITATION/> , turns out to be infeasible , at least for German and related languages . Hence, enumerating morphological variants in a semiautomatically generated lexicon, such as proposed for French <CITATION/>, turns out to be infeasible, at least for German and related languages.The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents. This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., Blut druck mess gerdt' translates to device for measuring blood pressure').",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,8c884345a809c630dfdf5c91d003a00f7ac1af46,The contribution of morphological knowledge to French MeSH mapping for information retrieval,2001,Pierre Zweigenbaum; S. Darmoni; N. Grabar
1732,P97-1063,External_6470,[4],method,It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <TARGET_CITATION/> .,"Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u, v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <TARGET_CITATION/> . It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values <CITATION/>. This step significantly reduces the computational burden of the algorithm. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u, v) < 1.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,,a statistical approach to machine translationquot,1990,P F Brown; J Cocke; S Della Pietra; V Della Pietra; F Jelinek; R Mercer; P Roossin
1733,P11-1134,E09-1025,[1],,"They proved to be useful in a number of NLP applications such as natural language generation <CITATION/> , multidocument summarization <CITATION/> , automatic evaluation of MT <CITATION/> , and TE <TARGET_CITATION/> .","1http://www.statmt.org/wmt10/Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation <CITATION/> , multidocument summarization <CITATION/> , automatic evaluation of MT <CITATION/> , and TE <TARGET_CITATION/> . They proved to be useful in a number of NLP applications such as natural language generation <CITATION/>, multidocument summarization <CITATION/>, automatic evaluation of MT <CITATION/>, and TE <CITATION/>. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. 1http://www.statmt.org/wmt10/",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,ba95602cfee50860366d062c7e2c9caa39850d26,Inference Rules and their Application to Recognizing Textual Entailment,2009,Georgiana Dinu; Rui Wang
1734,P10-2059,External_81763,[4],introduction,"The results , which partly confirm those obtained on a smaller dataset in <TARGET_CITATION/> , must be seen in light of the fact that our gesture annotation scheme comprises more finegrained categories than most of the studies mentioned earlier for both head movements and face expressions .","Our data are made up by a collection of eight videorecorded maptask dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in <TARGET_CITATION/> , must be seen in light of the fact that our gesture annotation scheme comprises more finegrained categories than most of the studies mentioned earlier for both head movements and face expressions . The results, which partly confirm those obtained on a smaller dataset in <CITATION/>, must be seen in light of the fact that our gesture annotation scheme comprises more finegrained categories than most of the studies mentioned earlier for both head movements and face expressions. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. Our data are made up by a collection of eight videorecorded maptask dialogues in Danish, which were annotated with phonetic and prosodic information.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,,feedback in head gesture and speech to appear in,2010,Patrizia Paggio; Costanza Navarretta
1735,D12-1037,External_2214,[2],,We employ the idea of ultraconservative update <TARGET_CITATION/> to propose two incremental methods for local training in Algorithm 2 as follows .,"Our goal is to find an optimal weight, denoted by Wi, which is a local weight and used for decoding the sentence ti. Unlike the global method which performs tuning on the whole development set Dev + Di as in Algorithm 1, Wi can be incrementally learned by optimizing on Di based on Wb. We employ the idea of ultraconservative update <TARGET_CITATION/> to propose two incremental methods for local training in Algorithm 2 as follows . We employ the idea of ultraconservative update <CITATION/> to propose two incremental methods for local training in Algorithm 2 as follows. Unlike the global method which performs tuning on the whole development set Dev + Di as in Algorithm 1, Wi can be incrementally learned by optimizing on Di based on Wb. Our goal is to find an optimal weight, denoted by Wi, which is a local weight and used for decoding the sentence ti.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,28b9bacde6499f8cc6f7e70feee4232107211e39,Ultraconservative Online Algorithms for Multiclass Problems,2003,K. Crammer; Y. Singer
1736,D13-1115,P13-1056,[0],method,It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers <TARGET_CITATION/> .,Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <CITATION/>. Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together <CITATION/>. It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers <TARGET_CITATION/> . It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers <CITATION/>. Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together <CITATION/>. Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by <CITATION/>.,75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,4adca62f888226d3a16654ca499bf2a7d3d11b71,Models of Semantic Representation with Visual Attributes,2013,Carina Silberer; V. Ferrari; Mirella Lapata
1737,W06-1104,External_61155,[4],experiments,<TARGET_CITATION/> reported a correlation of r = .69 .,"<CITATION/> reported a correlation of r=.9026.10 The results are not directly comparable, because he only used nounnoun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. <CITATION/> did not report intersubject correlation for their larger dataset. <TARGET_CITATION/> reported a correlation of r = .69 . <CITATION/> reported a correlation of r=.69. <CITATION/> did not report intersubject correlation for their larger dataset. <CITATION/> reported a correlation of r=.9026.10 The results are not directly comparable, because he only used nounnoun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3b5b95038c6b065f29649c1b11ea3e7855c00a53,Thinking beyond the nouns - computing semantic relatedness across parts of speech,2006,Iryna Gurevych
1738,P02-1001,External_24068,[0],introduction,"A more subtle example is weighted FSAs that approximate PCFGs <TARGET_CITATION/> , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","P(v, z) def = Ew,x,y P(vw)P(w, x)P(yx)P(zy), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic <CITATION/>. Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 )arcs). A more subtle example is weighted FSAs that approximate PCFGs <TARGET_CITATION/> , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . A more subtle example is weighted FSAs that approximate PCFGs <CITATION/>, or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation. Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 )arcs). P(v, z) def = Ew,x,y P(vw)P(w, x)P(yx)P(zy), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic <CITATION/>.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,,regular approximation of contextfree grammars through transformation,2001,M Mohri; M-J Nederhof
1739,W06-1639,H05-1042,[4],method,"As has been previously observed and exploited in the NLP literature <TARGET_CITATION/> , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","where c(s) is the opposite'' class from c(s). A minimumcost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individualdocument classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature <TARGET_CITATION/> , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs . As has been previously observed and exploited in the NLP literature <CITATION/>, the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. A minimumcost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individualdocument classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. where c(s) is the opposite'' class from c(s).",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,b2c1efcaac5e99083639994d0daa4ae05b3839d8,Collective Content Selection for Concept-to-Text Generation,2005,R. Barzilay; Mirella Lapata
1740,W00-1017,P99-1026,[2],introduction,WIT features an incremental understanding method <TARGET_CITATION/> that makes it possible to build a robust and realtime system .,"This paper presents WIT', which is a toolkit WIT is an acronym of Workable spoken dialogue Interfor building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output. WIT features an incremental understanding method <TARGET_CITATION/> that makes it possible to build a robust and realtime system . WIT features an incremental understanding method <CITATION/> that makes it possible to build a robust and realtime system. for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output. This paper presents WIT', which is a toolkit WIT is an acronym of Workable spoken dialogue Inter",143c89043402241b9db0d37b79632823f2fa70ee,WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems,2000,Mikio Nakano; Noboru Miyazaki; Norihito Yasuda; Akira Sugiyama; Jun-ichi Hirasawa; Kohji Dohsaka; K. Aikawa,b9317d1aa658d94f18fe7cbad6c8ab3ac64b73f9,Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems,1999,Mikio Nakano; Noboru Miyazaki; Jun-ichi Hirasawa; Kohji Dohsaka; T. Kawabata
1741,W14-1619,D12-1086,[1],introduction,"This choice is inspired by recent work on learning syntactic categories <TARGET_CITATION/> , which successfully utilized such language models to represent word window contexts of target words .","This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard ngram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories <TARGET_CITATION/> , which successfully utilized such language models to represent word window contexts of target words . This choice is inspired by recent work on learning syntactic categories <CITATION/>, which successfully utilized such language models to represent word window contexts of target words. To capture syntagmatic patterns, we choose in this work standard ngram language models as the basis for a concrete model implementing our scheme. This provides grounds to expect that such model has the potential to excel for verbs.",fae0dd3a8350fad208089a1b3b7ac3a366939d68,Probabilistic Modeling of Joint-context in Distributional Similarity,2014,Oren Melamud; Ido Dagan; J. Goldberger; Idan Szpektor; Deniz Yuret,02ef54074c7765825af96e94f8d41a21df7f7d35,Learning Syntactic Categories Using Paradigmatic Representations of Word Context,2012,M. Yatbaz; Enis Sert; Deniz Yuret
1743,W00-1312,External_1389,[4],related work,Other studies which view lR as a query generation process include <TARGET_CITATION/> .," Other studies which view lR as a query generation process include <TARGET_CITATION/> . Other studies which view lR as a query generation process include <CITATION/>; Miller et al, 1999.",e68a7773495e3407d8e040fff8c67d614df08265,Cross-lingual Information Retrieval Using Hidden Markov Models,2000,Jinxi Xu; R. Weischedel,,a language modeling approach to information retrievalquot,1998,J Ponte; W B Croft
1744,J09-4010,External_42504,[2],method,"In Section 5 , we discuss the difficulties associated with such user studies , and describe a humanbased evaluation we conducted for a small subset of the responses generated by our system <TARGET_CITATION/> .","Our evaluation is performed by measuring the quality of the generated responses. Quality is a subjective measure, which is best judged by the users of the system (i.e., the helpdesk customers or operators). In Section 5 , we discuss the difficulties associated with such user studies , and describe a humanbased evaluation we conducted for a small subset of the responses generated by our system <TARGET_CITATION/> . In Section 5, we discuss the difficulties associated with such user studies, and describe a humanbased evaluation we conducted for a small subset of the responses generated by our system <CITATION/>. Quality is a subjective measure, which is best judged by the users of the system (i.e., the helpdesk customers or operators). Our evaluation is performed by measuring the quality of the generated responses.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,evaluation of a largescale email response system,2007,Y Marom; I Zukerman
1745,P13-3018,External_90444,[0],related work,<TARGET_CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries .,"Compound verbs are a special phenomena that are abundantly found in IndoEuropean languages like Indian languages. A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. <TARGET_CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries . <CITATION/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries. A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. Compound verbs are a special phenomena that are abundantly found in IndoEuropean languages like Indian languages.",97139bdadb6349ff46a4d970aaa8d8621cdc7cc8,Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words,2013,Tirthankar Dasgupta,,hindi structures intermediate levelquot michigan papers on south and,1981,P E Hook
1746,P97-1063,External_15612,[4],method,"2We could just as easily use other symmetric  association '' measures , such as 02 <TARGET_CITATION/> or the Dice coefficient <CITATION/> .","If uk and vk are indeed mutual translations, then their tendency to The cooccurrence frequency of a word type pair is simply the number of times the pair cooccurs in the corpus. However, n(u) = Ev n(u,v), which is not the same as the frequency of u, because each token of u can cooccur with several differentv's. 2We could just as easily use other symmetric  association '' measures , such as 02 <TARGET_CITATION/> or the Dice coefficient <CITATION/> . 2We could just as easily use other symmetric association'' measures, such as 02 <CITATION/> or the Dice coefficient <CITATION/>.However, n(u) = Ev n(u,v), which is not the same as the frequency of u, because each token of u can cooccur with several differentv's. If uk and vk are indeed mutual translations, then their tendency to The cooccurrence frequency of a word type pair is simply the number of times the pair cooccurs in the corpus.",b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695,A Word-to-Word Model of Translational Equivalence,1997,I. D. Melamed,4fe2a45babab10c1bfae05d2464363f4e52bbaf9,A Program for Aligning Sentences in Bilingual Corpora,1993,W. Gale; Kenneth Ward Church
1747,J00-2004,External_42502,[1],,"In this situation , <TARGET_CITATION/> , 293 ) recommend  evaluating the expectations using only a single , probable alignment . ''","Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation , <TARGET_CITATION/> , 293 ) recommend  evaluating the expectations using only a single , probable alignment . '' In this situation, Brown et al. (1993b, 293) recommend evaluating the expectations using only a single, probable alignment.'' This is why we must make do with approximations to the EM algorithm. Barring such a decomposition method, the MLE approach is infeasible.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,fc593d91a7974bb1d3fac1ffe47b787ce1853a88,But Dictionaries Are Data Too,1993,P. Brown; S. D. Pietra; V. D. Pietra; Meredith J. Goldsmith; Jan Hajic; R. Mercer; Surya Mohanty
1748,W06-1705,External_43370,[0],related work,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( <TARGET_CITATION/> , 2004b ) and received a special issue of the journal Computational Linguistics <CITATION/> .","Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times <TARGET_CITATION/> and received a special issue of the journal Computational Linguistics <CITATION/> . The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing singleserver systems.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,61bf612e6456380d93d2240984391de303c4e54e,The Web as a Corpus A Multilingual Multipurpose Corpus,2019,Xavier Gil Bouzou
1749,J03-3004,External_7391,[0],introduction, Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <TARGET_CITATION/>  Localization ( Sch  aler 1996 ),"Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <TARGET_CITATION/>  Localization ( Sch  aler 1996 )  Learnability <CITATION/>  Text generation <CITATION/>  Speech generation <CITATION/>  Localization <CITATION/>More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:Accordingly, they generate lexical correspondences by means of cooccurrence measures and string similarity metrics.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,b441a87df61e3874d72a48eb1ef776250d4786ab,Hybrid language processing in the Spoken Language Translator,1997,Manny Rayner; D. Carter
1750,D14-1083,P13-2142,[3],experiments,"Following our previous work on stance classification <TARGET_CITATION/> , we employ three types of features computed based on the framesemantic parse of each sentence in a post obtained from SEMAFOR <CITATION/> .","Framesemantic features. While dependencybased features capture the syntactic dependencies, framesemantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification <TARGET_CITATION/> , we employ three types of features computed based on the framesemantic parse of each sentence in a post obtained from SEMAFOR <CITATION/> . Following our previous work on stance classification <CITATION/>, we employ three types of features computed based on the framesemantic parse of each sentence in a post obtained from SEMAFOR <CITATION/>. While dependencybased features capture the syntactic dependencies, framesemantic features encode the semantic representation of the concepts in a sentence. Framesemantic features.",3543efd388e2c1af4b64ec69d4b8510570c01f8c,Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates,2014,K. Hasan; Vincent Ng,b634a489624ead65075b3b9caae77eab9344174d,Extra-Linguistic Constraints on Stance Recognition in Ideological Debates,2013,K. Hasan; Vincent Ng
1751,N10-1084,External_36164,[0],introduction,"However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media <TARGET_CITATION/> .","Since the difference between 11111111 and 11111110 in the value for red/green/blue intensity is likely to be undetectable by the human eye, the LSB can be used to hide information other than colour, without being perceptable by a human observer.1 A key question for any steganography system is the choice of cover medium. Given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider. However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media <TARGET_CITATION/> . However, the literature on Linguistic Steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media <CITATION/>. Given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider. Since the difference between 11111111 and 11111110 in the value for red/green/blue intensity is likely to be undetectable by the human eye, the LSB can be used to hide information other than colour, without being perceptable by a human observer.1 A key question for any steganography system is the choice of cover medium.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,7b2ddf3a5aa9894f63482ac514bc4aed136614a6,A comprehensive bibliography of linguistic steganography,2007,Richard Bergmair
1752,W02-0309,External_34419,[0],introduction,"This has been reported for other languages , too , dependent on the generality of the chosen approach ( J  appinen and Niemist  o , 1988 ; <CITATION/> ; Ekmekc  ioglu et al. , 1995 ; <TARGET_CITATION/> ) .","For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator. mers <CITATION/> demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J  appinen and Niemist  o , 1988 ; <CITATION/> ; Ekmekc  ioglu et al. , 1995 ; <TARGET_CITATION/> ) . This has been reported for other languages, too, dependent on the generality of the chosen approach <CITATION/>. mers <CITATION/> demonstrably improve retrieval performance. For English, known for its limited number of inflection patterns, lexiconfree generalpurpose stem1 ' denotes the string concatenation operator.",ae30310b31ea97fdac5215a59434a7de2a97bd99,Biomedical text retrieval in languages with a complex morphology,2002,S. Schulz; Martin Honeck; U. Hahn,500766341ceb43a836d2a12a476d89cbcf3776ab,Morphological typology of languages for IR,2001,Ari Pirkola
1753,P02-1001,External_16023,[0],introduction,"We offer a theorem that highlights the broad applicability of these modeling techniques .4 If f ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ; ( 2 ) f can be computed by a Markovian FST that halts with probability 1 ; ( 3 ) f can be expressed as a probabilistic regexp , i.e. , a regexp built up from atomic expressions a : b ( for a E E U LCB E RCB , b E A U LCB E RCB ) using concatenation , probabilistic union + p , and probabilistic closure * p. For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules <TARGET_CITATION/> , ( 3 ) by compilation of decision trees <CITATION/> , ( 4 ) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation ( Gerdemann and van <CITATION/> ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","These 4 parameters have global effects on Fig. 1a, thanks to complex parameter tying: arcs  b:p ) @,  b:q )  in Fig. 1b get respective probabilities (1  A) and (1  ), which covary with  and vary oppositely with . Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a. We offer a theorem that highlights the broad applicability of these modeling techniques .4 If f ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ; ( 2 ) f can be computed by a Markovian FST that halts with probability 1 ; ( 3 ) f can be expressed as a probabilistic regexp , i.e. , a regexp built up from atomic expressions a : b ( for a E E U LCB E RCB , b E A U LCB E RCB ) using concatenation , probabilistic union + p , and probabilistic closure * p. For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules <TARGET_CITATION/> , ( 3 ) by compilation of decision trees <CITATION/> , ( 4 ) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation ( Gerdemann and van <CITATION/> ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below . We offer a theorem that highlights the broad applicability of these modeling techniques.4 If f(input, output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a E E U LCBERCB, b E A U LCBERCB) using concatenation, probabilistic union +p, and probabilistic closure *p. For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules <CITATION/>, (3) by compilation of decision trees <CITATION/>, (4) as a relation that performs contextual lefttoright replacement of input substrings by a smaller conditional relation (Gerdemann and van <CITATION/>),5 (5) by conditionalization of a joint relation as discussed below. Each of these probabilities in turn affects multiple arcs in the composed FST of Fig. 1a. These 4 parameters have global effects on Fig. 1a, thanks to complex parameter tying: arcs  b:p ) @,  b:q )  in Fig. 1b get respective probabilities (1  A) and (1  ), which covary with  and vary oppositely with .",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,30cd6bab1201af143cd7c7e1520c3d334642c9ce,An Efficient Compiler for Weighted Rewrite Rules,1996,Mehryar Mohri; R. Sproat
1754,N10-1084,J07-4004,[2],method,We use the <TARGET_CITATION/> CCG parser to analyse the sentence before and after paraphrasing .,"In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method. We use the <TARGET_CITATION/> CCG parser to analyse the sentence before and after paraphrasing . We use the <CITATION/> CCG parser to analyse the sentence before and after paraphrasing. In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,2d45f21c9deb17987a6be71b3c9a2758791540a2,Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models,2007,S. Clark; J. Curran
1755,J06-2002,External_40448,[0],experiments,"For example , consider a relational description ( cfXXX , <TARGET_CITATION/> ) involving a gradable adjective , as in the dog in the large shed .","Extensions of the Approach 9.1 Relational DescriptionsSome generalizations of our method are fairly straightforward. For example , consider a relational description ( cfXXX , <TARGET_CITATION/> ) involving a gradable adjective , as in the dog in the large shed . For example, consider a relational description (cfXXX, Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed. Some generalizations of our method are fairly straightforward. Extensions of the Approach 9.1 Relational Descriptions",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,,generating referring expressions containing relations,1991,Robbert Dale; Nickolas Haddock
1756,J90-3003,P86-1022,[3],introduction,"Previous versions of our work , as described in <TARGET_CITATION/> also assume that phrasing is dependent on predicateargument structure .","<CITATION/> claims that prosodic phrase boundaries will cooccur with grammatical functions such as subject, predicate, modifier, and adjunct. <CITATION/> take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in <TARGET_CITATION/> also assume that phrasing is dependent on predicateargument structure . Previous versions of our work, as described in <CITATION/> also assume that phrasing is dependent on predicateargument structure. <CITATION/> take a similar approach, but within a different theoretical framework. <CITATION/> claims that prosodic phrase boundaries will cooccur with grammatical functions such as subject, predicate, modifier, and adjunct.",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,826c7abff3ea69c74ac1dc16acd1c381516b1712,The Contribution of Parsing to Prosodic Phrasing in an Experimental Text-to-Speech System,1986,J. Bachenko; Eileen Fitzpatrick; C. Wright
1757,D11-1138,P10-1124,[0],experiments,like information extraction <CITATION/> and textual entailment <TARGET_CITATION/> .,"When training with ALS (labeled and unlabeled), we see an improvement in UAS, LAS, and ALS. Furthermore, if we use a labeledALS as the metric for augmentedloss training, we also see a considerable increase in LAS. like information extraction <CITATION/> and textual entailment <TARGET_CITATION/> . like information extraction <CITATION/> and textual entailment <CITATION/>. Furthermore, if we use a labeledALS as the metric for augmentedloss training, we also see a considerable increase in LAS. When training with ALS (labeled and unlabeled), we see an improvement in UAS, LAS, and ALS.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,79c68d0efc13e8be8f8bf20e7fdc253df2c266ee,Global Learning of Focused Entailment Graphs,2010,Jonathan Berant; Ido Dagan; J. Goldberger
1758,J01-4001,M95-1015,[0],,"The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC6 and MUC7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in <TARGET_CITATION/> .","From simple cooccurrence rules <CITATION/> through training decision trees to identify anaphorantecedent pairs <CITATION/> to genetic algorithms to optimize the resolution factors <CITATION/>, the successful performance of more and more modern approaches was made possible by the availability of suitable corpora. While the shift toward knowledgepoor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC6 and MUC7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in <TARGET_CITATION/> . The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>. While the shift toward knowledgepoor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research. From simple cooccurrence rules <CITATION/> through training decision trees to identify anaphorantecedent pairs <CITATION/> to genetic algorithms to optimize the resolution factors <CITATION/>, the successful performance of more and more modern approaches was made possible by the availability of suitable corpora.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,9b8c468e9cedbf1db1a1def91e53b8e40ad70167,University of Pennsylvania: Description of the University of Pennsylvania System Used for MUC-6,2005,B. Baldwin; M. Collins; Jason Eisner; A. Ratnaparkhi; Joseph Rosenzweig; Anoop Sarkar
1759,K15-1002,External_16514,[0],related work,<CITATION/> model entity coreference and event coreference jointly ; <TARGET_CITATION/> consider joint coreference and entitylinking .,"Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not. Several recent works suggest studying coreference jointly with other tasks. <CITATION/> model entity coreference and event coreference jointly ; <TARGET_CITATION/> consider joint coreference and entitylinking . <CITATION/> model entity coreference and event coreference jointly; <CITATION/> consider joint coreference and entitylinking. Several recent works suggest studying coreference jointly with other tasks. Our joint framework provides similar insights, where the added mention decision variable partly reflects if the mention is singleton or not.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,28eb033eee5f51c5e5389cbb6b777779203a6778,"A Joint Model for Entity Analysis: Coreference, Typing, and Linking",2014,Greg Durrett; D. Klein
1760,P07-1068,J01-4004,[0],introduction,"However , learningbased resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , <TARGET_CITATION/> ) .","As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. However , learningbased resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC <TARGET_CITATION/> . However, learningbased resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., <CITATION/>). Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs. As a result, researchers have readopted the oncepopular knowledgerich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., <CITATION/>), their semantic similarity as computed using WordNet (e.g., <CITATION/>) or Wikipedia <CITATION/>, and the contextual role played by an NP (see <CITATION/>).",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,a20bfec3c95aad003dcb45a21a220c19cca8bb66,A Machine Learning Approach to Coreference Resolution of Noun Phrases,2001,Wee Meng Soon; H. Ng; Chung Yong Lim
1761,D11-1138,P07-1036,[4],introduction,"This includes work on generalized expectation <CITATION/> , posterior regularization <CITATION/> and constraint driven learning <TARGET_CITATION/> .","We call our algorithm augmentedloss training as it optimizes multiple losses to augment the traditional supervised parser loss. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation <CITATION/> , posterior regularization <CITATION/> and constraint driven learning <TARGET_CITATION/> . This includes work on generalized expectation <CITATION/>, posterior regularization <CITATION/> and constraint driven learning <CITATION/>. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. We call our algorithm augmentedloss training as it optimizes multiple losses to augment the traditional supervised parser loss.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,dc9f999632bf6d82882cc54e2d2cc2d32eaed932,Guiding Semi-Supervision with Constraint-Driven Learning,2007,Ming-Wei Chang; Lev-Arie Ratinov; D. Roth
1763,J00-2001,External_80545,[0],,Hovy has described another text planner that builds similar plans <TARGET_CITATION/> .,"Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory <CITATION/>. In IGEN, the plans can involve any goals or actions that could be achieved via communication. Hovy has described another text planner that builds similar plans <TARGET_CITATION/> . Hovy has described another text planner that builds similar plans <CITATION/>. In IGEN, the plans can involve any goals or actions that could be achieved via communication. Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory <CITATION/>.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,,generating natural language linder pragmatic constraints lawrence erlbaum,1988,Eduard H Hovy
1764,W06-1639,W02-1011,[0],introduction,"In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> .","Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> . In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,12d0353ce8b41b7e5409e5a4a611110aee33c7bc,Thumbs up? Sentiment Classification using Machine Learning Techniques,2002,B. Pang; Lillian Lee; Shivakumar Vaithyanathan
1765,D11-1138,J08-4003,[2],experiments,transitionbased dependency parsing framework <TARGET_CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8 .,For our experiments we focus on two dependency parsers. Transitionbased: An implementation of the transitionbased dependency parsing framework <TARGET_CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8 . transitionbased dependency parsing framework <CITATION/> using an arceager transition strategy and are trained using the perceptron algorithm as in <CITATION/> with a beam size of 8.  Transitionbased: An implementation of theFor our experiments we focus on two dependency parsers.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,053f1cf10ced2321c1853f307075f0a6a83b6840,Algorithms for Deterministic Incremental Dependency Parsing,2008,Joakim Nivre
1766,J00-2004,External_42502,[2],,"Just as easily , we can model link types that coincide with entries in an online bilingual dictionary separately from those that do not ( cfXXX <TARGET_CITATION/> ) .","Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily , we can model link types that coincide with entries in an online bilingual dictionary separately from those that do not ( cfXXX <TARGET_CITATION/> ) . Just as easily, we can model link types that coincide with entries in an online bilingual dictionary separately from those that do not (cfXXX Brown et al. 1993). A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Similarly, the auxiliary parameters can be conditioned on the linked parts of speech.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,fc593d91a7974bb1d3fac1ffe47b787ce1853a88,But Dictionaries Are Data Too,1993,P. Brown; S. D. Pietra; V. D. Pietra; Meredith J. Goldsmith; Jan Hajic; R. Mercer; Surya Mohanty
1768,D12-1037,P03-1021,[4],related work,<CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it .,Several works have proposed discriminative techniques to train loglinear model for SMT. <CITATION/> used maximum likelihood estimation to learn weights for MT. <TARGET_CITATION/> employed an evaluation metric as a loss function and directly optimized it . <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,1f12451245667a85d0ee225a80880fc93c71cc8b,Minimum Error Rate Training in Statistical Machine Translation,2003,F. Och
1769,E03-1004,External_10615,[4],introduction,"We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder <TARGET_CITATION/> , trained on the same parallel corpus .","For the evaluation of the results we use the BLEU score <CITATION/>. Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder <TARGET_CITATION/> , trained on the same parallel corpus . We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder <CITATION/>, trained on the same parallel corpus.Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. For the evaluation of the results we use the BLEU score <CITATION/>.",55559a2ee9693969d30237534ac290f4b0077a3a,Czech-English Dependency Tree-based Machine Translation,2003,Martin Cmejrek; J. Curín; Jirí Havelka,c9214ebe91454e6369720136ab7dd990d52a07d4,Improved Statistical Alignment Models,2000,F. Och; H. Ney
1770,D13-1115,External_9735,[0],introduction,"Some efforts have tackled tasks such as automatic image caption generation <CITATION/> , text illustration <TARGET_CITATION/> , or automatic location identification of Twitter users <CITATION/> .","Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some efforts have tackled tasks such as automatic image caption generation <CITATION/> , text illustration <TARGET_CITATION/> , or automatic location identification of Twitter users <CITATION/> . Some efforts have tackled tasks such as automatic image caption generation <CITATION/>, text illustration <CITATION/>, or automatic location identification of Twitter users <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,f7d6366c90fe8f1fe87c3ad3abbed5945bf9a979,The Story Picturing Engine---a system for automatic text illustration,2006,D. Joshi; James Ze Wang; Jia Li
1771,K15-1003,P02-1017,[1],method,"Since we are not generating from the model , this does not introduce difficulties <TARGET_CITATION/> .","Finally, since it is possible to generate a supertag context category that does not match the actual category generated by the neighboring constituent, we must allow our process to reject such invalid trees and reattempt to sample. Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Since we are not generating from the model , this does not introduce difficulties <TARGET_CITATION/> . Since we are not generating from the model, this does not introduce difficulties <CITATION/>. Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Finally, since it is possible to generate a supertag context category that does not match the actual category generated by the neighboring constituent, we must allow our process to reject such invalid trees and reattempt to sample.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,77021fb48704b860fa850dd103b79db4dcf920ee,A Generative Constituent-Context Model for Improved Grammar Induction,2002,D. Klein; Christopher D. Manning
1772,J05-3003,External_555,[0],related work,"The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <TARGET_CITATION/> .","The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. <CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <TARGET_CITATION/> . The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of <CITATION/>. <CITATION/> also presents a similar method for the extraction of a TAG from the Penn Treebank. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,0ffa423a5283396c88ff3d4033d541796bd039cc,"Three Generative, Lexicalised Models for Statistical Parsing",1997,M. Collins
1773,W06-1104,I05-1067,[2],experiments,"As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother :  brother , male sibling '' vs.  brother , comrade , friend '' <TARGET_CITATION/> .","It is the most complete resource of this type for German. GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother :  brother , male sibling '' vs.  brother , comrade , friend '' <TARGET_CITATION/> . As they are required to enable test subjects to distinguish between senses, we use artificial glosses composed from synonyms and hypernyms as a surrogate, e.g. for brother: brother, male sibling'' vs. brother, comrade, friend'' <CITATION/>. GermaNet contains only a few conceptual glosses. It is the most complete resource of this type for German.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,2ebe88fab46e53e980b5aef28c414c04d133fc6f,Using the Structure of a Conceptual Network in Computing Semantic Relatedness,2005,Iryna Gurevych
1774,P10-4003,External_397,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,6f0c3c03b89afa5299cffe66dde99bcf4b2b7caa,Towards Tutorial Dialog to Support Self- Explanation: Adding Natural Language Understanding to a Cognitive Tutor *,2001,V. Aleven; Octav Popescu; K. Koedinger
1775,J05-3003,External_12085,[1],method,"<TARGET_CITATION/> argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .","For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]).3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. <TARGET_CITATION/> argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . <CITATION/> argues that there are cases, albeit exceptional ones, in which constraints on syntactic category are an issue in subcategorization. As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]).3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4).",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,lexical functional grammar volume 34 of syntax and semantics,2001,Mary Dalrymple
1776,A00-1024,A92-1015,[2],experiments,Corpus frequency : <TARGET_CITATION/> differentiates between misspellings and neologisms ( new words ) in terms of their frequency .,"The features we use are derived from previous research, including our own previous research on misspelling identification. An abridged list of the features that are used in the training data is listed in Table 2 and discussed below. Corpus frequency : <TARGET_CITATION/> differentiates between misspellings and neologisms ( new words ) in terms of their frequency . Corpus frequency: <CITATION/> differentiates between misspellings and neologisms (new words) in terms of their frequency. An abridged list of the features that are used in the training data is listed in Table 2 and discussed below. The features we use are derived from previous research, including our own previous research on misspelling identification.",caa11f45ef1d8cdd6683a34b22407ec0f2c55d77,Categorizing Unknown Words: Using Decision Trees to Identify Names and Misspellings,2000,J. Toole,06f3428dcf8703e49ce9cf29981b0f1904262cc7,Detecting and Correcting Morpho-syntactic Errors in Real Texts,1992,T. Vosse
1777,D12-1037,P05-1033,[2],experiments,"We use an inhouse developed hierarchical phrasebased translation <TARGET_CITATION/> as our baseline system , and we denote it as InHiero .","In our experiments the translation performances are measured by caseinsensitive BLEU4 metric <CITATION/> and we use mtevalv13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap resampling <CITATION/>. We use an inhouse developed hierarchical phrasebased translation <TARGET_CITATION/> as our baseline system , and we denote it as InHiero . We use an inhouse developed hierarchical phrasebased translation <CITATION/> as our baseline system, and we denote it as InHiero. The significance testing is performed by paired bootstrap resampling <CITATION/>. In our experiments the translation performances are measured by caseinsensitive BLEU4 metric <CITATION/> and we use mtevalv13a.pl as the evaluation tool.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,ad3d2f463916784d0c14a19936c1544309a0a440,A Hierarchical Phrase-Based Model for Statistical Machine Translation,2005,David Chiang
1778,J86-1002,External_40052,[0],experiments,"The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions <TARGET_CITATION/> .","The expectation parser uses an ATNlike representation for its grammar <CITATION/>. Its strategy is topdown. The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions <TARGET_CITATION/> . The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions <CITATION/>. Its strategy is topdown. The expectation parser uses an ATNlike representation for its grammar <CITATION/>.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,,semantic processing for a natural language programming system,1979,B Ballard
1780,P11-1134,P05-1074,[0],,One of the proposed methods to extract paraphrases relies on a pivotbased approach using phrase alignments in a bilingual parallel corpus <TARGET_CITATION/> .,"Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation <CITATION/>, multidocument summarization <CITATION/>, automatic evaluation of MT <CITATION/>, and TE <CITATION/>. One of the proposed methods to extract paraphrases relies on a pivotbased approach using phrase alignments in a bilingual parallel corpus <TARGET_CITATION/> . One of the proposed methods to extract paraphrases relies on a pivotbased approach using phrase alignments in a bilingual parallel corpus <CITATION/>. They proved to be useful in a number of NLP applications such as natural language generation <CITATION/>, multidocument summarization <CITATION/>, automatic evaluation of MT <CITATION/>, and TE <CITATION/>. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,36e8b8b35e51e5b39fcafdb5c2bc763796d0672e,Paraphrasing with Bilingual Parallel Corpora,2005,Colin Bannard; Chris Callison-Burch
1781,J06-2002,External_20234,[0],experiments,"The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( <TARGET_CITATION/> ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .","For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( <TARGET_CITATION/> ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) . The Nash arbitration plan, for example, would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors (Nash 1950; cfXXX Gorniak and Roy 2003; Thorisson 1994, for other plans). Many alternative strategies are possible. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,c38e70179b9c33a19824fd157adb8291cfbef1c2,THE BARGAINING PROBLEM,1950,J. Nash
1782,W06-1705,W04-0858,[0],related work,<TARGET_CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler .,"The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>. Studies have used several different methods to mine web data. <TARGET_CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler . <CITATION/> extracts word cooccurrence probabilities from unlabelled text collected from a web crawler. Studies have used several different methods to mine web data. The use of the web as a corpus for teaching and research on language has been proposed a number of times (<CITATION/>, 2004b) and received a special issue of the journal Computational Linguistics <CITATION/>.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,bc0176c5dcdeb1b1ee2680e69eb52afee3b0b4f0,Word Sense Disambiguation by Web mining for word co-occurrence probabilities,2004,Peter D. Turney
1783,W06-1639,External_30065,[0],related work,Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> .,Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitinterdocument references in the form of hyperlinks <CITATION/>. Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> . Notable early papers on graphbased semisupervised learning include <CITATION/>. interdocument references in the form of hyperlinks <CITATION/>. Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicit,dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,68500a8b8b3dbabfa971db607bb8bfc49fa28f8d,Correlation Clustering with Vertex Splitting,2024,Matthias Bentert; Alex Crane; Paal Gronaas Drange; F. Reidl; Blair D. Sullivan
1785,W04-0910,External_390,[0],,"Thus for instance , <CITATION/> describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and <TARGET_CITATION/> show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","Semantic grammars'' already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , <CITATION/> describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and <TARGET_CITATION/> show how to equip Lexical Functional grammar ( LFG ) with a glue semantics . Thus for instance, <CITATION/> describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and <CITATION/> show how to equip Lexical Functional grammar (LFG) with a glue semantics. Semantic grammars'' already exist which describe not only the syntax but also the semantics of natural language.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,156f008a434be0aa3192061493cfaa6f9ed480bc,Semantics and syntax in lexical functional grammar : the resource logic approach,1999,M. Dalrymple
1786,P10-4003,External_22831,[2],experiments,"The contextual interpreter then uses a reference resolution approach similar to <CITATION/> , and an ontology mapping mechanism <TARGET_CITATION/> to produce a domainspecific semantic representation of the student 's output .","We use the TRIPS dialogue parser <CITATION/> to parse the utterances. The parser provides a domainindependent semantic representation including highlevel word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to <CITATION/> , and an ontology mapping mechanism <TARGET_CITATION/> to produce a domainspecific semantic representation of the student 's output . The contextual interpreter then uses a reference resolution approach similar to <CITATION/>, and an ontology mapping mechanism <CITATION/> to produce a domainspecific semantic representation of the student's output. The parser provides a domainindependent semantic representation including highlevel word senses and semantic role labels. We use the TRIPS dialogue parser <CITATION/> to parse the utterances.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,linking semantic and knowledge representations in a multidomain dialogue system,2008,Myroslava O Dzikovska; James F Allen; Mary D Swift
1787,J00-4002,J87-1005,[0],,"only the available five relative scopings of the quantifiers are produced ( <TARGET_CITATION/> , 47 ) , but without the need for a free variable constraint  the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;  the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ;  partial scopings are permitted ( see Reyle [ 19961 )  scoping can be freely interleaved with other types of reference resolution ;  unscoped or partially scoped forms are available for inference or for generation at every stage .","This is a rather oversimplified treatment of quantifier scope, which we will refine a little shortly, but even as it stands the treatment has several advantages: in classic examples like: (21) Every representative in a company saw most samples. only the available five relative scopings of the quantifiers are produced ( <TARGET_CITATION/> , 47 ) , but without the need for a free variable constraint  the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;  the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ;  partial scopings are permitted ( see Reyle [ 19961 )  scoping can be freely interleaved with other types of reference resolution ;  unscoped or partially scoped forms are available for inference or for generation at every stage . only the available five relative scopings of the quantifiers are produced (Hobbs and Shieber 1987, 47), but without the need for a free variable constraintthe HOU algorithm will not produce any solutions in which a previously bound variable becomes free;  the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms;  partial scopings are permitted (see Reyle [19961)  scoping can be freely interleaved with other types of reference resolution;  unscoped or partially scoped forms are available for inference or for generation at every stage. in classic examples like: (21) Every representative in a company saw most samples. This is a rather oversimplified treatment of quantifier scope, which we will refine a little shortly, but even as it stands the treatment has several advantages:",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,0f6dc7400f9c319f09bbbc0ffb0f33dbdbbef3d2,An Algorithm for Generating Quantifier Scopings,1987,Jerry R. Hobbs; Stuart M. Shieber
1788,J00-1004,W97-0408,[2],,"In the transducers produced by the training method described in this paper , the source and target positions are in the set LCB 1 , 0,1 RCB , though we have also used handcoded transducers <TARGET_CITATION/> and automatically trained transducers <CITATION/> with a larger range of positions .","We can now define the stringtostring transduction function for a head transducer to be the function that maps an input string to the output string produced by the lowestcost valid derivation taken over all initial states and initial symbols. (Formally, the function is partial in that it is not defined on an input when there are no derivations or when there are multiple outputs with the same minimal cost.) In the transducers produced by the training method described in this paper , the source and target positions are in the set LCB 1 , 0,1 RCB , though we have also used handcoded transducers <TARGET_CITATION/> and automatically trained transducers <CITATION/> with a larger range of positions . In the transducers produced by the training method described in this paper, the source and target positions are in the set LCB1, 0,1RCB, though we have also used handcoded transducers <CITATION/> and automatically trained transducers <CITATION/> with a larger range of positions.(Formally, the function is partial in that it is not defined on an input when there are no derivations or when there are multiple outputs with the same minimal cost.) We can now define the stringtostring transduction function for a head transducer to be the function that maps an input string to the output string produced by the lowestcost valid derivation taken over all initial states and initial symbols.",355c46c066f29dc91f25d303df6e128bb69858c2,Learning dependency translation models as collections of finite state head transducers,2000,H. Alshawi; Srinivas Bangalore; Shona Douglas,ed6c40bf6016af891f091d51cb7962c9a3f2232f,English-to-Mandarin Speech Translation with Head Transducers,1997,H. Alshawi
1789,J04-3001,External_713,[0],related work,"Similar approaches are being explored for parsing ( Steedman , <TARGET_CITATION/> ; Hwa et al. 2003 ) .","The work of <CITATION/> and Steedman, Osborne, et al. (2003) suggests that cotraining can be helpful for statistical parsing. <CITATION/> have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for largescale training. Similar approaches are being explored for parsing ( Steedman , <TARGET_CITATION/> ; Hwa et al. 2003 ) . Similar approaches are being explored for parsing (Steedman, Hwa, et al. 2003; Hwa et al. 2003).<CITATION/> have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for largescale training. The work of <CITATION/> and Steedman, Osborne, et al. (2003) suggests that cotraining can be helpful for statistical parsing.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,,corrected cotraining for statistical parsers,2003,Rebecca Hwa; Miles Osborne; Anoop Sarkar; Mark Steedman
1790,W06-1639,External_68,[0],introduction,"In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> .","Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). In particular , since we treat each individual speech within a debate as a single  document '' , we are considering a version of documentlevel sentimentpolarity classification , namely , automatically distinguishing between positive and negative documents <TARGET_CITATION/> . In particular, since we treat each individual speech within a debate as a single document'', we are considering a version of documentlevel sentimentpolarity classification, namely, automatically distinguishing between positive and negative documents <CITATION/>. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,338a891907dce447da9a0fa2f27221bd35164163,Mining the peanut gallery: opinion extraction and semantic classification of product reviews,2003,Kushal Dave; S. Lawrence; David M. Pennock
1792,W10-2910,W05-0601,[0],conclusion,"The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. <TARGET_CITATION/> .","We were not able to achieve the same performance using tree kernels as with manually extracted features. It is possible that this could be improved with a better strategy for representing dependency structure for tree kernels, or if the tree kernels could be incorporated into the structural learning framework. The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. <TARGET_CITATION/> . The flexible architecture we have presented enables interesting future research: (i) a straightforward improvement is the use of lexical similarity to reduce data sparseness, e.g. <CITATION/>. It is possible that this could be improved with a better strategy for representing dependency structure for tree kernels, or if the tree kernels could be incorporated into the structural learning framework. We were not able to achieve the same performance using tree kernels as with manually extracted features.",24b82384ba7f45167073fd47437759f08a331d34,Syntactic and Semantic Structure for Opinion Expression Detection,2010,Richard Johansson; Alessandro Moschitti,91c0bfb0b7ecd4cad4fc815862ad17e3f318eb39,Effective use of WordNet Semantics via Kernel-Based Learning,2005,Roberto Basili; Marco Cammisa; Alessandro Moschitti
1793,A00-1014,External_85932,[5],,"5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results <TARGET_CITATION/> , which we leave for future work .","Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur 5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results <TARGET_CITATION/> , which we leave for future work . 5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results <CITATION/>, which we leave for future work.Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the curThus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,,evaluating response strategies in a webbased spoken dialogue agent,1998,Diane J Litman; Shimei Pan; Marilyn A Walker
1794,P02-1001,External_8253,[0],introduction,Such approaches have been tried recently in restricted cases <TARGET_CITATION/> .,"This allows meaningful parameter tying: if certain arcs such asu:i *, *, and a:ae o:e * share a contextual vowelfronting'' feature, then their weights rise and fall together with the strength of that feature. The resulting machine must be normalized, either perstate or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases <TARGET_CITATION/> . Such approaches have been tried recently in restricted cases <CITATION/>. The resulting machine must be normalized, either perstate or globally, to obtain a joint or a conditional distribution as desired. This allows meaningful parameter tying: if certain arcs such asu:i *, *, and a:ae o:e * share a contextual vowelfronting'' feature, then their weights rise and fall together with the strength of that feature.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,bece46ed303f8eaef2affae2cba4e0aef51fe636,Maximum Entropy Markov Models for Information Extraction and Segmentation,2000,A. McCallum; Dayne Freitag; Fernando C Pereira
1796,J09-4010,External_45354,[0],introduction,It is therefore no surprise that early attempts at response automation were knowledgedriven <TARGET_CITATION/> .,"1 http://customercare.telephonyonline.com/ar/telecom next generation customer.circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. It is therefore no surprise that early attempts at response automation were knowledgedriven <TARGET_CITATION/> . It is therefore no surprise that early attempts at response automation were knowledgedriven <CITATION/>. circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer. 1 http://customercare.telephonyonline.com/ar/telecom next generation customer.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,applying casebased reasoning techniques for enterprise systems,1997,I Watson
1797,P02-1001,J00-1003,[0],introduction,"A more subtle example is weighted FSAs that approximate PCFGs <TARGET_CITATION/> , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","P(v, z) def = Ew,x,y P(vw)P(w, x)P(yx)P(zy), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic <CITATION/>. Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 )arcs). A more subtle example is weighted FSAs that approximate PCFGs <TARGET_CITATION/> , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . A more subtle example is weighted FSAs that approximate PCFGs <CITATION/>, or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation. Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 )arcs). P(v, z) def = Ew,x,y P(vw)P(w, x)P(yx)P(zy), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic <CITATION/>.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof
1798,P11-1134,N10-1146,[0],introduction,"ones , DIRT <CITATION/> , VerbOcean <CITATION/> , FrameNet <CITATION/> , and Wikipedia <TARGET_CITATION/> .","Besides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>. These include, just to mention the most popular ones , DIRT <CITATION/> , VerbOcean <CITATION/> , FrameNet <CITATION/> , and Wikipedia <TARGET_CITATION/> . ones, DIRT <CITATION/>, VerbOcean <CITATION/>, FrameNet <CITATION/>, and Wikipedia <CITATION/>. These include, just to mention the most popularBesides WordNet, the RTE literature documents the use of a variety of lexical information sources <CITATION/>.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,4876b9d79886960e034a5d52adcdad640b363c76,Syntactic/Semantic Structures for Textual Entailment Recognition,2010,Yashar Mehdad; Alessandro Moschitti; Fabio Massimo Zanzotto
1799,W06-2807,External_98153,[2],,The Ruby on <TARGET_CITATION/> framework permits us to quickly develop web applications without rewriting common functions and classes .,"Messages, data and metadata are exchanged between layers using the capability of this language. This allows to treat data and metadata on different level of abstraction. The Ruby on <TARGET_CITATION/> framework permits us to quickly develop web applications without rewriting common functions and classes . The Ruby on <CITATION/> framework permits us to quickly develop web applications without rewriting common functions and classes. This allows to treat data and metadata on different level of abstraction. Messages, data and metadata are exchanged between layers using the capability of this language.",48aec60cf83eb18975aaec50ef52fcbbb948603e,"Novelle, a collaborative open source writing tool software",2006,F. Gobbo; Michele Chinosi; Massimiliano Pepe,,ruby on rails web developement that doesn’t hurt url httpwwwrubyonrailsorg retrieved the 03rd of january,2006,Ruby on Rails
1800,K15-1003,J07-4004,[2],,We further add rules for combining with punctuation to the left and right and allow for the merge rule X  X X of <TARGET_CITATION/> .,"A category (s\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\) with a noun phrase (np) that serves as its subject.We follow <CITATION/> in allowing a small set of generic, linguisticallyplausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X  X X of <TARGET_CITATION/> . We further add rules for combining with punctuation to the left and right and allow for the merge rule X  X X of <CITATION/>.We follow <CITATION/> in allowing a small set of generic, linguisticallyplausible unary and binary grammar rules. A category (s\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\) with a noun phrase (np) that serves as its subject.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,2d45f21c9deb17987a6be71b3c9a2758791540a2,Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models,2007,S. Clark; J. Curran
1801,W06-1639,External_71381,[0],related work,"Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit <TARGET_CITATION/> ."," Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit <TARGET_CITATION/> . Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,fd69ee460ae899b08866f728f548a630597c4b51,Using natural language processing to improve eRulemaking: project highlight,2006,Claire Cardie; Cynthia Farina; Thomas Bruce
1802,D08-1034,W05-0630,[0],introduction,<TARGET_CITATION/> has made some preliminary attempt on the idea of hierarchical semantic,"This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank <CITATION/> was built, <CITATION/> have produced more complete and systematic research on Chinese SRL. <TARGET_CITATION/> has made some preliminary attempt on the idea of hierarchical semantic <CITATION/> has made some preliminary attempt on the idea of hierarchical semanticAfter the PropBank <CITATION/> was built, <CITATION/> have produced more complete and systematic research on Chinese SRL. This paper made the first attempt on Chinese SRL and produced promising results.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,41dca48ae5074f9d51c61a2c7c5e0f3f4b1c8109,Hierarchical Semantic Role Labeling,2005,Alessandro Moschitti; Ana-Maria Giuglea; Bonaventura Coppola; Roberto Basili
1803,W06-1104,W04-2607,[0],related work,<TARGET_CITATION/> pointed out that many relations between words in a text are nonclassical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .,"Furthermore, manually selected word pairs are often biased towards highly related pairs <CITATION/>, because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpusbased selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexicalsemantic relations. <TARGET_CITATION/> pointed out that many relations between words in a text are nonclassical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity . <CITATION/> pointed out that many relations between words in a text are nonclassical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Automatic corpusbased selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexicalsemantic relations. Furthermore, manually selected word pairs are often biased towards highly related pairs <CITATION/>, because human annotators tend to select only highly related pairs connected by relations they are aware of.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,3319f87827b77d6eff9101b4d8beb913a0035c3f,Non-Classical Lexical Semantic Relations,2004,Jane Morris; Graeme Hirst
1805,D09-1056,External_8770,[0],related work,"In 2009 , the second WePS campaign showed similar trends regarding the use of NE features <TARGET_CITATION/> .","This makes NEs the second most common type of feature; only the BoW feature was more popular. Other features used by the systems include noun phrases <CITATION/>, word ngrams <CITATION/>, emails and URLs (del <CITATION/>), etc.. In 2009 , the second WePS campaign showed similar trends regarding the use of NE features <TARGET_CITATION/> . In 2009, the second WePS campaign showed similar trends regarding the use of NE features <CITATION/>. Other features used by the systems include noun phrases <CITATION/>, word ngrams <CITATION/>, emails and URLs (del <CITATION/>), etc.. This makes NEs the second most common type of feature; only the BoW feature was more popular.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,420e7adc1596b726fee0d0d016f5c5e62a8a8574,WePS 2 Evaluation Campaign: Overview of the Web People Search Clustering Task,2009,J. Artiles; Julio Gonzalo; S. Sekine
1806,J04-3001,External_13798,[2],,The head words can be automatically extracted using a heuristic table lookup in the manner described by <TARGET_CITATION/> .,"preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attachverb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by <TARGET_CITATION/> . The head words can be automatically extracted using a heuristic table lookup in the manner described by <CITATION/>. For example, (wrote a book in three days, attachverb) would be annotated as (wrote, book, in, days, verb). preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce,Natural Language Parsing as Statistical Pattern Recognition,1994,David M. Magerman
1807,J00-2001,W94-0319,[0],,Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation <TARGET_CITATION/> .,"For example, DIOGENES <CITATION/>, EPICURE <CITATION/>, SPOKESMAN <CITATION/>, Sibun's work on local organization of text <CITATION/>, and COMET <CITATION/> all are organized this way. McDonald has even argued for extending the model to a large number of components <CITATION/>, and several systems have indeed added an additional component between the planner and the linguistic component <CITATION/>. Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation <TARGET_CITATION/> . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation <CITATION/>. McDonald has even argued for extending the model to a large number of components <CITATION/>, and several systems have indeed added an additional component between the planner and the linguistic component <CITATION/>. For example, DIOGENES <CITATION/>, EPICURE <CITATION/>, SPOKESMAN <CITATION/>, Sibun's work on local organization of text <CITATION/>, and COMET <CITATION/> all are organized this way.",dd600a130a7572d26432f3fbe9c19faaebf4ff7d,Integrating Text planning and linguistic choice without abandoning modularity: the IGEN generator,2000,Robert Rubinoff,48f426fe2018022838bebe3744cc728c0b6053f9,"Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible?",1994,Ehud Reiter
1808,W03-0806,External_44163,[0],experiments,"For example , the suite of LT tools <TARGET_CITATION/> perform tokenization , tagging and chunking on XML markedup text directly .","This allows components to be highly configurable and simplifies the addition of new components to the system. A number of standalone tools have also been developed. For example , the suite of LT tools <TARGET_CITATION/> perform tokenization , tagging and chunking on XML markedup text directly . For example, the suite of LT tools <CITATION/> perform tokenization, tagging and chunking on XML markedup text directly. A number of standalone tools have also been developed. This allows components to be highly configurable and simplifies the addition of new components to the system.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,c98cdccba7dcd5404a4477a87f07a0ec764707b8,XML Tools And Architecture for Named Entity Recognition,1999,Andrei Mikheev; Claire Grover; M. Moens
1809,N10-1084,External_56139,[0],related work,<TARGET_CITATION/> all belong to the syntactic transformation category .,"<CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <TARGET_CITATION/> all belong to the syntactic transformation category . <CITATION/> all belong to the syntactic transformation category. In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences. <CITATION/> embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.",b751fbbcdb05973856b18362baf8da581a4edfc3,Linguistic Steganography Using Automatically Generated Paraphrases,2010,Ching-Yun Chang; S. Clark,1d72b8262b932d3222b3d38c14285a9af435f184,Syntactic tools for text watermarking,2007,H. Meral; E. Sevinç; Ersin Ünkar; B. Sankur; A. S. Özsoy; Tunga Güngör
1810,D09-1067,P07-1028,[5],conclusion,"<CITATION/> have showed that WordNetbased approaches do not always outperform simple frequencybased models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach <TARGET_CITATION/> .","In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to WordNet classes. <CITATION/> have showed that WordNetbased approaches do not always outperform simple frequencybased models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach <TARGET_CITATION/> . <CITATION/> have showed that WordNetbased approaches do not always outperform simple frequencybased models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach <CITATION/>. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to WordNet classes. In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.",1cdfaff988038c2f9cd16b39922a2a3a0b4f320b,Improving Verb Clustering with Automatically Acquired Selectional Preferences,2009,Lin Sun; A. Korhonen,3cce57b4eb593359862817e93eb47e8655520652,"A Simple, Similarity-based Model for Selectional Preferences",2007,K. Erk
1811,W01-1510,P98-2132,[2],experiments,The RenTAL system is implemented in LiLFeS <TARGET_CITATION/> 2 ., The RenTAL system is implemented in LiLFeS <TARGET_CITATION/> 2 . The RenTAL system is implemented in LiLFeS <CITATION/>2.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,9f4744e8f2beb883c5a5dafad48aac31ab236d0c,LiLFeS - Towards a Practical HPSG Parser,1998,Takaki Makino; Minoru Yoshida; Kentaro Torisawa; Junichi Tsujii
1812,J05-3003,External_238,[5],,"In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank <TARGET_CITATION/> .","We believe our semantic forms are finegrained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank <TARGET_CITATION/> . In the future, we hope to evaluate the automatic annotations and extracted lexicon against Propbank <CITATION/>. Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. We believe our semantic forms are finegrained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,fc090a68e45e0e6337136777d21c87b76a90ae72,From TreeBank to PropBank,2002,Paul R. Kingsbury; Martha Palmer
1813,J03-3004,External_59887,[2],introduction,"Each set of translations is stored separately , and for each set the  marker hypothesis '' <TARGET_CITATION/> is used to segment the phrasal lexicon into a  marker lexicon . ''","In Section 2, we describe how we automatically obtain a hierarchy of lexical resources that are used sequentially by our EBMT system, wEBMT, to translate new input. The primary resource gathered is a phrasal lexicon,'' constructed by extracting over 200,000 phrases from the Penn Treebank and having them translated into French by three Webbased machine translation (MT) systems. Each set of translations is stored separately , and for each set the  marker hypothesis '' <TARGET_CITATION/> is used to segment the phrasal lexicon into a  marker lexicon . '' Each set of translations is stored separately, and for each set the marker hypothesis'' <CITATION/> is used to segment the phrasal lexicon into a marker lexicon.'' The primary resource gathered is a phrasal lexicon,'' constructed by extracting over 200,000 phrases from the Penn Treebank and having them translated into French by three Webbased machine translation (MT) systems. In Section 2, we describe how we automatically obtain a hierarchy of lexical resources that are used sequentially by our EBMT system, wEBMT, to translate new input.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,1824dd0d62577c0dff87cdc4b69eb07c22249885,The necessity of syntax markers: Two experiments with artificial languages,1979,T. Green
1814,A00-1019,External_1559,[2],method,"One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple nounphrase contraints ( <TARGET_CITATION/> ; hua Chen and Chen , 94 ; <CITATION/> ) ."," One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple nounphrase contraints ( <TARGET_CITATION/> ; hua Chen and Chen , 94 ; <CITATION/> ) . One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple nounphrase contraints (<CITATION/>; hua Chen and Chen, 94; <CITATION/>).",7f61670dcf65a166cb9772b2b53870410159722c,Unit Completion for a Computer-aided Translation Typing System,2000,P. Langlais; George F. Foster; G. Lapalme,b1bf3d314ad996c394949f88c4091a4832ce0c9b,An Algorithm for Finding Noun Phrase Correspondences in Bilingual Corpora,1993,J. Kupiec
1815,W06-3309,External_3918,[4],related work,"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <TARGET_CITATION/> .","Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <TARGET_CITATION/> . Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements <CITATION/>. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,641122f8703ea0df8535ab0365f6797dbeb17e3c,Text generation: using discourse strategies and focus constraints to generate natural language text,1985,K. McKeown
1817,W06-1639,P88-1016,[0],introduction,"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) .","In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation. Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinionoriented language ( early work includes <TARGET_CITATION/> ; see <CITATION/> for an active bibliography ) . Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinionoriented language (early work includes <CITATION/>; see <CITATION/> for an active bibliography). Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records. In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech'' (continuous singlespeaker segment of text) represents support for or opposition to a proposed piece of legislation.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,f92e1eae13bfce2264602d6dd70a5255aa423d8a,A Computational Theory of Perspective and Reference in Narrative,1988,Janyce Wiebe; W. Rapaport
1819,J87-3002,E87-1011,[0],,"In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information <TARGET_CITATION/> .","From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords, dictionary search through the pronunciation field is available; <CITATION/> has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <CITATION/>. In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information <TARGET_CITATION/> . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information <CITATION/>. In addition to headwords, dictionary search through the pronunciation field is available; <CITATION/> has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure <CITATION/>. From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,9d733811a52c9ec95dfd6ef95c63725d32b59709,A Multi-Purpose Interface to an On-line Dictionary,1987,B. Boguraev; D. Carter; Ted Briscoe
1820,J02-3002,External_27962,[3],,"<TARGET_CITATION/> developed a way of incorporating standard ngrams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last","The DCA system is similar in spirit to such dynamic adaptation: it applies word ngrams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. <TARGET_CITATION/> developed a way of incorporating standard ngrams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last <CITATION/> developed a way of incorporating standard ngrams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's lastBut unlike the cache model, it uses a multipass strategy. The DCA system is similar in spirit to such dynamic adaptation: it applies word ngrams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,e4efb4b4c07d02b4031111b1cb97a7a13b5c928a,Language model adaptation using mixtures and an exponentially decaying cache,1997,P. Clarkson; A. J. Robinson
1821,J97-4003,External_24535,[4],related work,Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; <TARGET_CITATION/> ) .,"Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification <TARGET_CITATION/> . Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992;Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,6139a6506954d9fc31ab599d2e9115dd7c2a9671,Lexical Polymorphism and Word Disambiguation,1995,A. Sanfilippo
1822,W03-0806,External_20439,[1],,"Machine learning methods should be interchangeable : Transformationbased learning ( TBL ) <TARGET_CITATION/> and Memorybased learning ( MBL ) <CITATION/> have been applied to many different problems , so a single interchangeable component should be used to represent each method .","It also ensures components are maximally composable and extensible. This is particularly important in NLP because of the high redundancy across tasks and approaches. Machine learning methods should be interchangeable : Transformationbased learning ( TBL ) <TARGET_CITATION/> learning ( MBL ) <CITATION/> have been applied to many different problems , so a single interchangeable component should be used to represent each method . Machine learning methods should be interchangeable: Transformationbased learning (TBL) <CITATION/> and Memorybased learning (MBL) <CITATION/> have been applied to many different problems, so a single interchangeable component should be used to represent each method. This is particularly important in NLP because of the high redundancy across tasks and approaches. It also ensures components are maximally composable and extensible.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,a corpusbased appreach to language learning,1993,Eric Brill
1823,W06-1639,External_29176,[0],related work,"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> .","Notable early papers on graphbased semisupervised learning include <CITATION/>. <CITATION/> maintains a survey of this area. Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed <TARGET_CITATION/> . Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed <CITATION/>. <CITATION/> maintains a survey of this area. Notable early papers on graphbased semisupervised learning include <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,4cc1ce96bfa2ad8af16dbd0c2356a2cb5a476c24,Learning Probabilistic Models of Relational Structure,2001,L. Getoor; N. Friedman; D. Koller; B. Taskar
1824,J87-3002,External_43918,[2],,We tested the classification of verbs into semantic types using a verb list of 139 preclassified items drawn from the lists published in <TARGET_CITATION/> et al. ( 1973 ) .,"Therefore, we have undertaken a limited test of both the accuracy of the assignment of the LDOCE codes in the source dictionary and the reliability of the more ambitious (and potentially controversial) aspects of the grammar code transformation rules. It is not clear, in particular, that the rules for computing semantic types for verbs are well enough motivated linguistically or that the LDOCE lexicographers were sensitive enough to the different transformational potential of the various classes of verbs to make a rule such as our one for Object Raising viable. We tested the classification of verbs into semantic types using a verb list of 139 preclassified items drawn from the lists published in <TARGET_CITATION/> . We tested the classification of verbs into semantic types using a verb list of 139 preclassified items drawn from the lists published in <CITATION/>. It is not clear, in particular, that the rules for computing semantic types for verbs are well enough motivated linguistically or that the LDOCE lexicographers were sensitive enough to the different transformational potential of the various classes of verbs to make a rule such as our one for Object Raising viable. Therefore, we have undertaken a limited test of both the accuracy of the assignment of the LDOCE codes in the source dictionary and the reliability of the more ambitious (and potentially controversial) aspects of the grammar code transformation rules.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,d4f9f40a4485135d4eebeabda60c939b6e9dce1e,The grammar of English predicate complement constructions,1967,P. Rosenbaum
1825,J00-3003,External_32929,[4],,"The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <CITATION/> and tagging <TARGET_CITATION/> .","Stolcke et al.. Dialogue Act Modeling sequence with the highest posterior probability: The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <CITATION/> and tagging <TARGET_CITATION/> . The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <CITATION/> and tagging <CITATION/>. Dialogue Act Modeling sequence with the highest posterior probability:Stolcke et al..",22d45dadde6b5837eff11dc031045754bc5901c3,Dialogue act modeling for automatic tagging and recognition of conversational speech,2000,A. Stolcke; K. Ries; N. Coccaro; Elizabeth Shriberg; R. Bates; Dan Jurafsky; P. Taylor; Rachel Martin; C. V. Ess-Dykema; M. Meteer,a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10,A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,1988,Kenneth Ward Church
1826,P11-1134,External_4172,[0],introduction,"Crosslingual Textual Entailment ( CLTE ) has been proposed by <CITATION/> as an extension of Textual Entailment <TARGET_CITATION/> that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T ."," Crosslingual Textual Entailment ( CLTE ) has been proposed by <CITATION/> as an extension of Textual Entailment <TARGET_CITATION/> that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T . Crosslingual Textual Entailment (CLTE) has been proposed by <CITATION/> as an extension of Textual Entailment <CITATION/> that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T.",0d9a1c73077952d65c9c27550853f07a8fa164fd,Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment,2011,Yashar Mehdad; Matteo Negri; Marcello Federico,da236f03773145a3a1e3246c94c5e0e526fcbe4b,PROBABILISTIC TEXTUAL ENTAILMENT: GENERIC APPLIED MODELING OF LANGUAGE VARIABILITY,2004,Ido Dagan; Oren Glickman
1828,W06-1705,External_25124,[0],,"Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by <TARGET_CITATION/> ) .","We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in webbased corpus studies based in general. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by <TARGET_CITATION/> ) . Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data, issues considered at length by <CITATION/>. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in webbased corpus studies based in general.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,facilitating the compilation and dissemination of adhoc web corpora,2004,W H Fletcher
1829,D11-1138,External_14311,[0],introduction,"This includes work on generalized expectation <TARGET_CITATION/> , posterior regularization <CITATION/> and constraint driven learning <CITATION/> .","We call our algorithm augmentedloss training as it optimizes multiple losses to augment the traditional supervised parser loss. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation <TARGET_CITATION/> , posterior regularization <CITATION/> and constraint driven learning <CITATION/> . This includes work on generalized expectation <CITATION/>, posterior regularization <CITATION/> and constraint driven learning <CITATION/>. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. We call our algorithm augmentedloss training as it optimizes multiple losses to augment the traditional supervised parser loss.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,b62c81269e1d13bcae9c15db335887db990e5860,Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data,2010,Gideon S. Mann; A. McCallum
1830,J00-1003,W98-1302,[4],,"A very similar formulation , for another grammar transformation , is given in <TARGET_CITATION/> .","A set of such nonterminals can therefore be treated as the corresponding case from Figure 2, assuming the value right. The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here. A very similar formulation , for another grammar transformation , is given in <TARGET_CITATION/> . A very similar formulation, for another grammar transformation, is given in <CITATION/>.The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here. A set of such nonterminals can therefore be treated as the corresponding case from Figure 2, assuming the value right.",b4846ad03c170c5779c24bf91c0fe002a0f8023d,Practical Experiments with Regular Approximation of Context-Free Languages,1999,M. Nederhof,aef7597a77c5057f4c6fe00d410d40c793a3acfc,Context-Free Parsing through Regular Approximation,1998,M. Nederhof
1831,D12-1037,External_10615,[2],experiments,We run GIZA + + <TARGET_CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair .,"The training data is FBIS corpus consisting of about 240k sentence pairs. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. We run GIZA + + <TARGET_CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair . We run GIZA++ <CITATION/> on the training corpus in both directions <CITATION/> to obtain the word alignment for each sentence pair. The development set is NIST02 evaluation data, and the test datasets are NIST05, NIST06,and NIST08. The training data is FBIS corpus consisting of about 240k sentence pairs.",413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,c9214ebe91454e6369720136ab7dd990d52a07d4,Improved Statistical Alignment Models,2000,F. Och; H. Ney
1832,W03-0806,N01-1006,[0],experiments,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit <TARGET_CITATION/> which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging <CITATION/> .","However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit <TARGET_CITATION/> which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging <CITATION/> . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit <CITATION/> which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging <CITATION/>. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. However, it will be increasingly important as techniques become more complex and corpus sizes grow.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,c52f80f056a2de8f503bf912e8025413ec2111ec,Transformation Based Learning in the Fast Lane,2001,G. Ngai; Radu Florian
1833,J00-2003,P98-1029,[0],experiments,"Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems  using a plethora of theories , techniques , and tools  including some applications in computational linguistics ( e.g. , <TARGET_CITATION/> ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .","Methods of information fusion include voting methods, Bayesian inference, DempsterShafer 's method, generalized evidence processing theory, and various ad hoc techniques'' (Hall 1992, 135). Clearly, the above characterization is very wide ranging. Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems  using a plethora of theories , techniques , and tools  including some applications in computational linguistics ( e.g. , <TARGET_CITATION/> ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) . Consequently, fusion has been applied to a wide variety of pattern recognition and decision theoretic problems using a plethora of theories, techniques, and toolsincluding some applications in computational linguistics (e.g., Brill and Wu 1998; van Halteren, Zavrel, and Daelemans 1998) and speech technology (e.g., Bowles and Damper 1989; Romary and Pierre11989). Clearly, the above characterization is very wide ranging. Methods of information fusion include voting methods, Bayesian inference, DempsterShafer 's method, generalized evidence processing theory, and various ad hoc techniques'' (Hall 1992, 135).",18ff4f15416e34d9a56142e6f5d491567934e4fb,A multistrategy approach to improving pronunciation by analogy,2000,Y. Marchand; R. Damper,8e824aaf67f4f4f068455c6dbb7a6ed877794bd6,Classifier Combination for Improved Lexical Disambiguation,1998,Eric Brill; Jun Wu
1834,J90-3003,External_8904,[0],introduction,"This observation has led some researchers , e.g. , <TARGET_CITATION/> , to claim a direct mapping between the syntactic phrase and the prosodic phrase .","Who did you speak to? (to pronounced /tu/) When it comes to sentencelevel prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes. This observation has led some researchers , e.g. , <TARGET_CITATION/> , to claim a direct mapping between the syntactic phrase and the prosodic phrase . This observation has led some researchers, e.g., <CITATION/>, to claim a direct mapping between the syntactic phrase and the prosodic phrase. (to pronounced /tu/) When it comes to sentencelevel prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes. Who did you speak to?",678083540de63943000e9e0f19ac07e5e3678d70,A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English,1990,J. Bachenko; Eileen Fitzpatrick,29a444766187ac50d263f2c1f3b018d343ff91a9,Syntax and speech,1984,J. Bernstein
1835,P10-2002,W08-0302,[1],method,These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work <TARGET_CITATION/> : 1 .,ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative contextbased features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work <TARGET_CITATION/> : 1 . These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work <CITATION/>: 1. We incorporate into the ME based model the following informative contextbased features to train CBSM and CBTM. ME approach has the merit of easily combining different features to predict the probability of each class.,4899e043c69f8ab61c760b1f56961be9f1a5be9a,A Joint Rule Selection Model for Hierarchical Phrase-Based Translation,2010,Lei Cui; Dongdong Zhang; Mu Li; M. Zhou; T. Zhao,a694ee9b9255b3e8cd16a6a49c49e3863e3eec79,Rich Source-Side Context for Statistical Machine Translation,2008,Kevin Gimpel; Noah A. Smith
1836,D10-1074,W09-3930,[3],,"In our previous work <TARGET_CITATION/> , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 § ... § dm , and a hypothesis H represented by another set of clauses H = h1 § ... § hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows ."," In our previous work <TARGET_CITATION/> , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 § ... § dm , and a hypothesis H represented by another set of clauses H = h1 § ... § hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows . In our previous work <CITATION/>, conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d1 § ... § dm, and a hypothesis H represented by another set of clauses H = h1 § ... § hn, the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows.",3d0adc6fca3a0669c108958c5d5204e2695ea4db,Towards Conversation Entailment: An Empirical Investigation,2010,Chen Zhang; J. Chai,ee9fb0cb4487d277233f61286eb40637e82dbb5e,What do We Know about Conversation Participants: Experiments on Conversation Entailment,2009,Chen Zhang; J. Chai
1837,W06-1639,External_63491,[0],related work,Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> .,Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicitinterdocument references in the form of hyperlinks <CITATION/>. Notable early papers on graphbased semisupervised learning include <TARGET_CITATION/> . Notable early papers on graphbased semisupervised learning include <CITATION/>. interdocument references in the form of hyperlinks <CITATION/>. Previous sentimentanalysis work in different domains has considered interdocument similarity <CITATION/> or explicit,dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,6320770fe216ebbba769b9f0a006669b616a03d0,Diffusion Kernels on Graphs and Other Discrete Input Spaces,2002,R. Kondor; J. Lafferty
1838,W14-2106,P04-1085,[1],,"In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement <TARGET_CITATION/> .","Compared to the all unigrams baseline, the MIbased unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6). The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification. In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement <TARGET_CITATION/> . In addition, we consider several types of lexical features (LexF) inspired by previous work on agreement and disagreement <CITATION/>.The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification. Compared to the all unigrams baseline, the MIbased unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).",2b8b59a74d815a70bbb31892ce484510480be6fe,Analyzing Argumentative Discourse Units in Online Interactions,2014,Debanjan Ghosh; S. Muresan; Nina Wacholder; Mark Aakhus; M. Mitsui,18d079a6d72e3f0b0c9214f597b6b178265b05ee,Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies,2004,Michel Galley; K. McKeown; Julia Hirschberg; Elizabeth Shriberg
1839,J09-4010,External_12994,[0],,<TARGET_CITATION/> compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .,"In FA<CITATION/> employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. <TARGET_CITATION/> compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) . <CITATION/> compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. In FA<CITATION/> employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,bridging the lexical chasm statistical approaches to answerfinding,2000,A Berger; R Caruana; D Cohn; D Freitag; V Mittal
1840,D13-1115,External_14208,[0],introduction,"Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions <CITATION/> or robot commands <TARGET_CITATION/> .","The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions <CITATION/> or robot commands <TARGET_CITATION/> . Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions <CITATION/> or robot commands <CITATION/>. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning <CITATION/>. The language grounding problem has come in many different flavors with just as many different approaches.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,3954a3f80cf1b1f76430d80e924d85f2f1ba6799,Learning to Parse Natural Language Commands to a Robot Control System,2012,Cynthia Matuszek; E. Herbst; Luke Zettlemoyer; D. Fox
1841,P10-4003,External_25453,[5],experiments,"The dialogue state is represented by a cumulative answer analysis which tracks , over multiple turns , the correct , incorrect , and notyetmentioned parts 1Other factors such as student confidence could be considered as well <TARGET_CITATION/> .","Interaction between components is coordinated by the dialogue manager which uses the informationstate approach <CITATION/>. The dialogue state is represented by a cumulative answer analysis which tracks , over multiple turns , the correct , incorrect , and notyetmentioned parts 1Other factors such as student confidence could be considered as well <TARGET_CITATION/> . The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and notyetmentioned parts 1Other factors such as student confidence could be considered as well <CITATION/>. Interaction between components is coordinated by the dialogue manager which uses the informationstate approach <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,4d92807ccaaa241d2b42dfbc48b14f9e488cedd0,The Beetle and BeeDiff tutoring systems,2007,Charles B. Callaway; M. Dzikovska; Elaine Farrow; Manuel Marques-Pita; C. Matheson; Johanna D. Moore
1842,J05-3003,P98-2184,[1],,"As noted above , it is well documented <TARGET_CITATION/> that subcategorization frames ( and their frequencies ) vary across domains .","The first mapping is essentially a conflation of our more finegrained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible. Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented <TARGET_CITATION/> that subcategorization frames ( and their frequencies ) vary across domains . As noted above, it is well documented <CITATION/> that subcategorization frames (and their frequencies) vary across domains. Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. The first mapping is essentially a conflation of our more finegrained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,b4213d9ffabd486fad674075bf3f3486271fab3d,How Verb Subcategorization Frequencies are Affected by Corpus Choice,1998,Douglas Roland; Dan Jurafsky
1843,J05-3003,External_317,[0],related work,<TARGET_CITATION/> explore a number of related approaches to the extraction of a lexicalized TAG from the PennII Treebank with the aim of constructing a statistical model for parsing .,"Work has been carried out on the extraction of formalismspecific lexical resources from the PennII Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. <TARGET_CITATION/> explore a number of related approaches to the extraction of a lexicalized TAG from the PennII Treebank with the aim of constructing a statistical model for parsing . <CITATION/> explore a number of related approaches to the extraction of a lexicalized TAG from the PennII Treebank with the aim of constructing a statistical model for parsing. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Work has been carried out on the extraction of formalismspecific lexical resources from the PennII Treebank, in particular TAG, CCG, and HPSG.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,d48edcd14d6d5163e96d61a2867c0a6d2b64feda,Automated Extraction of TAGs from the Penn Treebank,2000,Johnathan M. Chen; Vijay K. Shanker
1844,J00-1004,J90-2002,[4],conclusion,"At the same time , we believe our method has advantages over the approach developed initially at IBM <TARGET_CITATION/> for training translation systems automatically .","The reduction of effort results, in large part, from being able to do without artificial intermediate representations of meaning; we do not require the development of semantic mapping rules (or indeed any rules) or the creation of a corpus including semantic annotations. Compared with lefttoright transduction, middleout transduction also aids robustness because, when complete derivations are not available, partial derivations tend to have meaningful headwords. At the same time , we believe our method has advantages over the approach developed initially at IBM <TARGET_CITATION/> for training translation systems automatically . At the same time, we believe our method has advantages over the approach developed initially at IBM <CITATION/> for training translation systems automatically. Compared with lefttoright transduction, middleout transduction also aids robustness because, when complete derivations are not available, partial derivations tend to have meaningful headwords. The reduction of effort results, in large part, from being able to do without artificial intermediate representations of meaning; we do not require the development of semantic mapping rules (or indeed any rules) or the creation of a corpus including semantic annotations.",355c46c066f29dc91f25d303df6e128bb69858c2,Learning dependency translation models as collections of finite state head transducers,2000,H. Alshawi; Srinivas Bangalore; Shona Douglas,a1066659ec1afee9dce586f6f49b7d44527827e1,A Statistical Approach to Machine Translation,1990,P. Brown; J. Cocke; S. D. Pietra; V. D. Pietra; F. Jelinek; J. Lafferty; R. Mercer; P. Roossin
1845,W06-3309,N04-1015,[0],introduction,"Building on the work of <CITATION/> in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX <TARGET_CITATION/> .","Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks. <CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of <CITATION/> in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX <TARGET_CITATION/> . Building on the work of <CITATION/> in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cfXXX <CITATION/>. <CITATION/> have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) <CITATION/>, and the availability of software that leverages this knowledge MetaMap <CITATION/> for concept identification and SemRep <CITATION/> for relation extractionprovide a foundation for studying the role of semantics in various tasks.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,984efc8932edb635d09ec1a5fd8fc1d1ceccad45,"Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",2004,R. Barzilay; Lillian Lee
1846,W03-0806,W03-0407,[0],,"The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines <TARGET_CITATION/> .","The Python interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler. Finally, since Python can produce standalone executables directly, it will be possible to create distributable code that does not require the entire infrastructure or Python interpreter to be installed. The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines <TARGET_CITATION/> . The basic Python reflection has already been implemented and used for large scale experiments with POS tagging, using pyMPI (a message passing interface library for Python) to coordinate experiments across a cluster of over 100 machines <CITATION/>. Finally, since Python can produce standalone executables directly, it will be possible to create distributable code that does not require the entire infrastructure or Python interpreter to be installed. The Python interface allows the components to be dynamically composed, configured and extended in any operating system environment without the need for a compiler.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,906765f7ac46011123cca59de775216f1ee9b451,Bootstrapping POS-taggers using unlabelled data,2003,S. Clark; J. Curran; M. Osborne
1848,D13-1115,External_61631,[0],related work,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <TARGET_CITATION/> ) ."," The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von <CITATION/> ) , computing power , improved computer vision models <CITATION/> and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; <TARGET_CITATION/> ) . The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von <CITATION/>), computing power, improved computer vision models <CITATION/> and neurological evidence of ties between the language, perceptual and motor systems in the brain <CITATION/>.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,27614ad6d8323113dd74c1ba253bd23d39ca2497,Congruent Embodied Representations for Visually Presented Actions and Linguistic Phrases Describing Actions,2006,L. Aziz-Zadeh; Stephen M. Wilson; G. Rizzolatti; M. Iacoboni
1850,J87-3002,J87-3001,[0],,"As <TARGET_CITATION/> points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage , this simple and convenComputational Linguistics , Volume 13 , Numbers 34 , JulyDecember 1987 205 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing tional access strategy is perfectly adequate .","They all make use of an efficient dictionary access system which services requests for sexpression entries made by client programs. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As <TARGET_CITATION/> points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage , this simple and convenComputational Linguistics , Volume 13 , Numbers 34 , JulyDecember 1987 205 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing tional access strategy is perfectly adequate . As <CITATION/> points out, given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and convenComputational Linguistics, Volume 13, Numbers 34, JulyDecember 1987 205 Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing tional access strategy is perfectly adequate. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. They all make use of an efficient dictionary access system which services requests for sexpression entries made by client programs.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5,Processing Dictionary Definitions with Phrasal Pattern Hierarchies,1987,H. Alshawi
1852,W01-1510,External_1855,[0],introduction,This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar ( FBLTAG1 ) <TARGET_CITATION/> and HeadDriven Phrase Structure Grammar ( HPSG ) <CITATION/> by a method of grammar conversion ., This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar ( FBLTAG1 ) <TARGET_CITATION/> Phrase Structure Grammar ( HPSG ) <CITATION/> by a method of grammar conversion . This paper describes an approach for sharing resources in various grammar formalisms such as FeatureBased Lexicalized Tree Adjoining Grammar (FBLTAG1) <CITATION/> and HeadDriven Phrase Structure Grammar (HPSG) <CITATION/> by a method of grammar conversion.,9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,84c8d41dfdd41b1bd735182148857dfd0dd28f58,A study of tree adjoining grammars,1987,K. Vijayashanker; A. Joshi
1853,Q13-1020,D07-1078,[1],method,This is because the binary structure has been verified to be very effective for treebased translation <TARGET_CITATION/> .,"To generate frag, <CITATION/> used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multiword nonterminal node must have two child nodes. This is because the binary structure has been verified to be very effective for treebased translation <TARGET_CITATION/> . This is because the binary structure has been verified to be very effective for treebased translation <CITATION/>. Differently, we require that each multiword nonterminal node must have two child nodes. To generate frag, <CITATION/> used a geometric prior to decide how many child nodes to assign each node.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,d07db07e9f98e3dd97544bd835619357683fc936,Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy,2007,Wei Wang; Kevin Knight; D. Marcu
1854,N04-2004,External_3166,[0],,There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <TARGET_CITATION/> ; Rappaport <CITATION/> ) .,"The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure  representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( <TARGET_CITATION/> ; Rappaport <CITATION/> ) . There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structurerepresentations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity (<CITATION/>; Rappaport <CITATION/>). Fixed roles are too coarsegrained to account for certain semantic distinctionsthe only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxtosemantics mapping. The actual inventory of semantic roles, along with precise definitions and diagnostics, remains an unsolved problem; see (Levin and Rappaport <CITATION/>).",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,c414875a916a05f6490aac594cf08f8cee7fb227,Semantics and Cognition,1991,R. Jackendoff
1855,D11-1138,W06-2920,[2],experiments,measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) <TARGET_CITATION/> .,We show empirical results for two extrinsic lossfunctions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic lossfunction: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) <TARGET_CITATION/> . measure the standard intrinsic parser metrics unlabeled attachment score (UAS) and labeled attachment score (LAS) <CITATION/>. For some experiments we alsoWe show empirical results for two extrinsic lossfunctions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic lossfunction: an arclength parsing score.,2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,1deed1a4a03e07aee3b8b8e4716f35033c715a57,CoNLL-X Shared Task on Multilingual Dependency Parsing,2006,S. Buchholz; E. Marsi
1857,J03-3004,External_49743,[4],introduction,"Accordingly , we convert examples such as ( 27 ) into their generalized equivalents , as in ( 28 ) : ( 28 ) <DET> good man : bon homme That is , where <TARGET_CITATION/> substitutes variables for various words in his templates , we replace certain lexical items with their marker tag .","Other similar approaches include those of <CITATION/>, inter alia. In our system, in some cases the smallest chunk obtainable via the markerbased segmentation process may be something like (27): (27) <DET> the good man: le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly , we convert examples such as ( 27 ) into their generalized equivalents , as in ( 28 ) : ( 28 ) <DET> good man : bon homme That is , where <TARGET_CITATION/> substitutes variables for various words in his templates , we replace certain lexical items with their marker tag . Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man: bon homme That is, where <CITATION/> substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. In our system, in some cases the smallest chunk obtainable via the markerbased segmentation process may be something like (27): (27) <DET> the good man: le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Other similar approaches include those of <CITATION/>, inter alia.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,,examplebased incremental synchronous interpretation,2000,Hans-Ulrich Block
1858,J86-1002,External_33228,[0],experiments,How it is done is beyond the scope of this paper but is explained in detail in <TARGET_CITATION/> .,"Thus, both options are imperfect in terms of the error correction capabilities that they can provide. The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second. How it is done is beyond the scope of this paper but is explained in detail in <TARGET_CITATION/> . How it is done is beyond the scope of this paper but is explained in detail in <CITATION/>. The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second. Thus, both options are imperfect in terms of the error correction capabilities that they can provide.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,cede7e5150ecfbd5024a8cdc441010ad61299a4e,"The acquisition and use of dialogue expectation in speech recognition (natural language processing, artificial intelligence)",1983,P. Fink
1859,W03-0806,External_3025,[2],experiments,These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing <TARGET_CITATION/> .,"We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing <TARGET_CITATION/> . These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing <CITATION/>. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a,A Gaussian Prior for Smoothing Maximum Entropy Models,1999,Stanley F. Chen; R. Rosenfeld
1860,W06-1639,J02-4002,[0],related work,"Also relevant is work on the general problems of dialogact tagging <CITATION/> , citation analysis <CITATION/> , and computational rhetorical analysis <TARGET_CITATION/> .","Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement. More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Also relevant is work on the general problems of dialogact tagging <CITATION/> , citation analysis <CITATION/> , and computational rhetorical analysis <TARGET_CITATION/> . Also relevant is work on the general problems of dialogact tagging <CITATION/>, citation analysis <CITATION/>, and computational rhetorical analysis <CITATION/>. More sophisticated approaches have been proposed <CITATION/>, including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments <CITATION/>. Detecting agreement We used a simple method to learn to identify crossspeaker references indicating agreement.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,54eafbf7621337724c6591cc13e604986243293a,Articles Summarizing Scientific Articles: Experiments with Relevance and Rhetorical Status,2002,Simone Teufel; M. Moens
1861,J06-2002,External_1249,[0],experiments,The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model <TARGET_CITATION/> .,"Much is still unknown, differences between speakers abound, and the experimental methodology for advancing the state of the art in this area is not without its problems (van Deemter 2004). Architecture (Section 6). The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model <TARGET_CITATION/> . The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model <CITATION/>. Architecture (Section 6). Much is still unknown, differences between speakers abound, and the experimental methodology for advancing the state of the art in this area is not without its problems (van Deemter 2004).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,d7ec2f43258ff2032c8dd13f8777f221221340b1,Building Natural-Language Generation Systems,1996,Ehud Reiter
1862,N04-2004,External_40237,[3],,<TARGET_CITATION/> has developed an agendadriven chart parser for the featuredriven formalism described above ; please refer to his paper for a description of the parsing algorithm .,"The +x denotes a need to discharge features, and the x denotes a need for features. A simple example of this is the case assignment involved in building a prepositional phrase, i.e., prepositions must assign case, and DPs much receive case. <TARGET_CITATION/> has developed an agendadriven chart parser for the featuredriven formalism described above ; please refer to his paper for a description of the parsing algorithm . <CITATION/> has developed an agendadriven chart parser for the featuredriven formalism described above; please refer to his paper for a description of the parsing algorithm. A simple example of this is the case assignment involved in building a prepositional phrase, i.e., prepositions must assign case, and DPs much receive case.The +x denotes a need to discharge features, and the x denotes a need for features.",1f921759e67ad6e6e85cf4d70028d4ff85de029b,A Computational Framework for Non-Lexicalist Semantics,2004,Jimmy J. Lin,f755b8b47a1ba1935c7133cf8f489f792c3bf250,A Minimalist Implementation of Verb Subcategorization,2001,Sourabh A. Niyogi
1864,N01-1002,P00-1019,[0],introduction,"In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent <TARGET_CITATION/> ."," In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent <TARGET_CITATION/> . In addition to a referring function, noun phrases (NP) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent <CITATION/>.",a463350fc2e38decfe736e0801d465874cef0891,Corpus-based NP Modifier Generation,2001,Hua Cheng; Massimo Poesio; R. Henschel; C. Mellish,b01a5af2b986dfd9252eef0623cb8e28f6956996,Can Nominal Expressions Achieve Multiple Goals?: An Empirical Study,2000,Pamela W. Jordan
1865,J04-3001,External_21527,[0],,"Some wellknown approaches include rulebased models <CITATION/> , backedoff models <CITATION/> , and a maximumentropy model <TARGET_CITATION/> .","One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some wellknown approaches include rulebased models <CITATION/> , backedoff models <CITATION/> , and a maximumentropy model <TARGET_CITATION/> . Some wellknown approaches include rulebased models <CITATION/>, backedoff models <CITATION/>, and a maximumentropy model <CITATION/>. Researchers have proposed many computational models for resolving PPattachment ambiguities. One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.",6c70170f55b7e9c724b8c02db9c0f41c05254a3c,Sample Selection for Statistical Parsing,2004,R. Hwa,6f64da7e650d155806aebcb852a9d3aed22cc2a7,Statistical Models for Unsupervised Prepositional Phrase Attachment,1998,A. Ratnaparkhi
1866,J86-1002,External_43440,[4],,There is some literature on procedure acquisition such as the LISP synthesis work described in <TARGET_CITATION/> and the PROLOG synthesis method of <CITATION/> .,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in <TARGET_CITATION/> and the PROLOG synthesis method of <CITATION/> . There is some literature on procedure acquisition such as the LISP synthesis work described in <CITATION/> and the PROLOG synthesis method of <CITATION/>. That is, the current system learns procedures rather than data structures. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in <CITATION/>, assertional statements as in <CITATION/>, or semantic nets as in <CITATION/>.",cb2fb1a39449b902fca83206637edae16e0a7cf4,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,1986,Pamela E. Fink; A. Biermann,0f58bdf1e4fa6af9315957bc1b5dc8225c1af73e,Automatic Program Construction Techniques,1984,A. Biermann; Y. Kodratoff; G. Guiho
1867,J01-4001,A00-1020,[0],,"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> .","The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years <TARGET_CITATION/> . Against the background of a growing interest in multilingual NLP, multilingual anaphora /coreference resolution has gained considerable momentum in recent years <CITATION/>. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC6 and MUC7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in <CITATION/>.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,76894392818a9a360feaf2f1a797bbe1eaac82b0,Multilingual Coreference Resolution,2000,S. Harabagiu; Steven J. Maiorano
1868,W06-3813,External_27696,[2],experiments,The list of semantic relations with which we work is based on extensive literature study <TARGET_CITATION/> ., The list of semantic relations with which we work is based on extensive literature study <TARGET_CITATION/> . The list of semantic relations with which we work is based on extensive literature study <CITATION/>.,f20ef3f9f3b5a42699d7da9fb273017b65e8bb28,Matching syntactic-semantic graphs for semantic relation assignment,2006,Vivi Nastase; Stan Szpakowicz,fee1b8b15216f52a641fd9b72eba311dff57da40,Systematic construction of a versatile case system,1997,Ken Barker; T. Copeck; Stan Szpakowicz; S. Delisle
1869,J87-3002,External_29751,[0],introduction,"The research described below is taking place in the context of three collaborative projects <TARGET_CITATION/> to develop a generalpurpose , wide coverage morphological and syntactic analyser for English .","These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system. Realtime parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. The research described below is taking place in the context of three collaborative projects <TARGET_CITATION/> to develop a generalpurpose , wide coverage morphological and syntactic analyser for English . The research described below is taking place in the context of three collaborative projects <CITATION/> to develop a generalpurpose, wide coverage morphological and syntactic analyser for English. Realtime parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words. These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.",998dbb7344086edaf050ec9dcfc886d359f18458,Large Lexicons for Natural Language Processing: Utilising the Grammar Coding System of LDOCE,1987,B. Boguraev,59ce79b3ea0b345b211c9156a7798f9520d70e55,A Natural Language Toolkit: Reconciling Theory with Practice,1988,B. Boguraev
1870,D11-1138,D10-1069,[0],introduction,"In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> .","The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. In most cases , the accuracy of parsers degrades when run on outofdomain data <TARGET_CITATION/> . In most cases, the accuracy of parsers degrades when run on outofdomain data <CITATION/>. This includes work on question answering <CITATION/>, sentiment analysis <CITATION/>, MT reordering <CITATION/>, and many other tasks. The accuracy and speed of stateoftheart dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",2db7160ade8868b4613448f55682fcee28b160d2,Training dependency parsers by jointly optimizing multiple objectives,2011,Keith B. Hall; Ryan T. McDonald; Jason Katz-Brown; Michael Ringgaard,6c9b8a8d8b615d7b70a08f8bfaa66d65c8b93d3a,Uptraining for Accurate Deterministic Question Parsing,2010,Slav Petrov; Pi-Chuan Chang; Michael Ringgaard; H. Alshawi
1871,W06-3309,External_56172,[0],introduction,"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the foursection pattern discussed above <TARGET_CITATION/> .","For example, <CITATION/> experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in Fscore. <CITATION/> found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the foursection pattern discussed above <TARGET_CITATION/> . This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>. <CITATION/> found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. For example, <CITATION/> experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in Fscore.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,,discoursal movements in medical english abstracts and their linguistic exponents a genre analysis study,1990,Franc¸oise Salanger-Meyer
1872,J09-4010,External_56350,[4],,"Following <TARGET_CITATION/> , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .","In other words, our metalevel process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request <CITATION/>. Following <TARGET_CITATION/> , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases . Following <CITATION/>, one approach for achieving this objective consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases. These predictions enable our system to recommend a particular method for handling a new (unseen) request <CITATION/>. In other words, our metalevel process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.",a39cfe8ad45aebf7d6cb4f152a95d1187a9662d8,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,2009,Yuval Marom; Ingrid Zukerman,,a hybrid approach for improving predictive accuracy of collaborative filtering algorithms user modeling and useradapted interaction,2007,G Lekakos; G M Giaglis
1874,Q13-1020,P06-1077,[0],introduction,"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <TARGET_CITATION/> , 2006 ; <CITATION/> ) .","In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , treebased translation models have shown promising progress in improving translation quality ( <TARGET_CITATION/> ) . Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, treebased translation models have shown promising progress in improving translation quality (<CITATION/>, 2009; <CITATION/>, 2006; <CITATION/>). In recent years, treebased translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,7e982f360b44094552264010781a476d85ac78a7,Tree-to-String Alignment Template for Statistical Machine Translation,2006,Yang Liu; Qun Liu; Shouxun Lin
1876,P00-1012,External_63377,[5],conclusion,"In particular , boosting <TARGET_CITATION/> offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .","The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature <CITATION/>. In particular , boosting <TARGET_CITATION/> offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly . In particular, boosting <CITATION/> offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly.It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature <CITATION/>. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data.",a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf,5db4c0f527adc11923c97a05e947e21711572a97,Boosting Applied to Tagging and PP Attachment,1999,S. Abney; R. Schapire; Y. Singer
1879,N01-1024,W00-0712,[3],method,"In order to obtain semantic representations of each word , we apply our previous strategy <TARGET_CITATION/> .","The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is welldescribed in the literature (Landauer, et al., 1998; <CITATION/>). In order to obtain semantic representations of each word , we apply our previous strategy <TARGET_CITATION/> . In order to obtain semantic representations of each word, we apply our previous strategy (<CITATION/>). This methodology is welldescribed in the literature (Landauer, et al., 1998; <CITATION/>). The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace.",30545fe538a773b57e06b4217cd495ef84230bc8,Knowledge-Free Induction of Inflectional Morphologies,2001,Patrick Schone; Dan Jurafsky,879e8af1770dd65d8237acbdf0964e0d0031635a,Knowledge-Free Induction of Morphology Using Latent Semantic Analysis,2000,Patrick Schone; Dan Jurafsky
1880,W06-1104,External_20456,[0],related work,"<TARGET_CITATION/> annotated a larger set of word pairs ( 353 ) , too .","She used an adapted experimental setup where test subjects had to assign discrete values LCB0,1,2,3,4RCB and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in <CITATION/>. <TARGET_CITATION/> annotated a larger set of word pairs ( 353 ) , too . <CITATION/> annotated a larger set of word pairs (353), too. This setup is also scalable to a higher number of word pairs (350) as was shown in <CITATION/>. She used an adapted experimental setup where test subjects had to assign discrete values LCB0,1,2,3,4RCB and word pairs were presented in isolation.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,e0c01df98a6b633b25c96c1a99b713ac96f1c5be,Placing search in context: the concept revisited,2002,L. Finkelstein; E. Gabrilovich; Yossi Matias; E. Rivlin; Zach Solan; G. Wolfman; E. Ruppin
1881,J06-2002,External_9768,[0],,"A more flexible approach is used by <TARGET_CITATION/> , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .","The FOG weatherforecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994). FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. A more flexible approach is used by <TARGET_CITATION/> , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on . A more flexible approach is used by <CITATION/>, where users can specify boundary values for attributes like rainfall, specifying, for example, rain counts as moderate above 7 mm/h, as heavy above 20 mm/h, and so on. FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead. The FOG weatherforecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,6e862065b597f791b66a8861bc625bce7a8240d9,Should Corpora Texts Be Gold Standards for NLG?,2002,Ehud Reiter; S. Sripada
1882,A00-1014,P97-1035,[2],experiments,"Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme <TARGET_CITATION/> , were automatically logged , derived , or manually annotated .","Each experiment involved eight users interacting with MIMIC and MIMICSI or MIMICMI to perform a set of tasks, each requiring the user to obtain specific movie information. User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system. Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme <TARGET_CITATION/> , were automatically logged , derived , or manually annotated . Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme <CITATION/>, were automatically logged, derived, or manually annotated. User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system. Each experiment involved eight users interacting with MIMIC and MIMICSI or MIMICMI to perform a set of tasks, each requiring the user to obtain specific movie information.",80ed6bf3a0a7cf1ce7f98585954ac6d62230da78,MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries,2000,Jennifer Chu-Carroll,2aa2db01525c36f5717de5abf83f04ad32481331,PARADISE: A Framework for Evaluating Spoken Dialogue Agents,1997,M. Walker; D. Litman; C. Kamm; A. Abella
1883,P00-1012,External_69922,[0],experiments,<TARGET_CITATION/> propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .,"If the reverse is true, and (b,a) is found more often than (a,b), then b  a. If neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned. <TARGET_CITATION/> propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation . <CITATION/> propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation . If neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned. If the reverse is true, and (b,a) is found more often than (a,b), then b  a.",a8d028b04c6c73f17e688c14a2cf9d0975c3ffb6,The Order of Prenominal Adjectives in Natural Language Generation,2000,Robert Malouf,4eec0a1ce645151c4acc89b151814b3deb53af43,Ordering Among Premodifiers,1999,James Shaw; V. Hatzivassiloglou
1884,E03-1004,External_8193,[2],experiments,"We evaluated our translations with IBM 's BLEU evaluation metric <TARGET_CITATION/> , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."," We evaluated our translations with IBM 's BLEU evaluation metric <TARGET_CITATION/> , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) . We evaluated our translations with IBM's BLEU evaluation metric <CITATION/>, using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP (Haji 6 et al., 2002).",55559a2ee9693969d30237534ac290f4b0077a3a,Czech-English Dependency Tree-based Machine Translation,2003,Martin Cmejrek; J. Curín; Jirí Havelka,d7da009f457917aa381619facfa5ffae9329a6e9,Bleu: a Method for Automatic Evaluation of Machine Translation,2002,K. Papineni; Salim Roukos; T. Ward; Wei-Jing Zhu
1885,W03-0806,External_424,[0],experiments,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit <CITATION/> which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging <TARGET_CITATION/> .","However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit <CITATION/> which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging <TARGET_CITATION/> . Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit <CITATION/> which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging <CITATION/>. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by <CITATION/> that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly <CITATION/>. However, it will be increasingly important as techniques become more complex and corpus sizes grow.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,,deterministic partofspeech tagging with finitestate transducers,1997,Emmanuel Roche; Yves Schabes
1887,P10-2059,External_81764,[2],,The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) <TARGET_CITATION/> .,"These two sets of data were used for automatic dialogue act classification, which was run in the Weka system <CITATION/>. We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table. The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) <TARGET_CITATION/> . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) <CITATION/>. We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table. These two sets of data were used for automatic dialogue act classification, which was run in the Weka system <CITATION/>.",b05885c63b300c02f9fc1523875af242526b2ba9,Classification of Feedback Expressions in Multimodal Data,2010,Costanza Navarretta; Patrizia Paggio,4ea281016640f2463261191fcda615757ac8d365,Insurance Claim Fraud Detection Using Hidden Naive Bayes,2024,Gundu Preetham; Kodavath Siddu; Boda Ramesh; M. A. Jabbar; Swati Sucharita
1888,J97-4003,External_105,[0],introduction,"de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <TARGET_CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements .","email: LCBdm,minnenRCB@sfs.nphil.unituebingen. de URL : http://www.sfs.nphil.unituebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <TARGET_CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements . de URL: http://www.sfs.nphil.unituebingen.de/sfb /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement <CITATION/> that also use lexical rules such as the Complement Extraction Lexical Rule <CITATION/> or the Complement Cliticization Lexical Rule <CITATION/> to operate on those raised elements. nphil.unituebingen.email: LCBdm,minnenRCB@sfs.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,,headdriven phrase structure grammar,1994,Carl Pollard; Ivan Sag
1889,J01-4001,External_71492,[0],,"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; <TARGET_CITATION/> ) .","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; <TARGET_CITATION/> ) . A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledgepoor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledgepoor anaphora resolution strategies. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge <CITATION/>, which was difficult both to represent and to process, and which required considerable human input.",7fb0c0d26debc523e39590f15935cf6a5e6433b8,Introduction to the Special Issue on Computational Anaphora Resolution,2001,R. Mitkov; B. Boguraev; Shalom Lappin,,pronoun resolution the practical alternative presented at the discourse anaphora and anaphor resolution colloquium daarc1,1996,Ruslan Mitkov
1890,W04-1805,External_76201,[0],method,ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework <CITATION/> and called qualia relations <TARGET_CITATION/> .,inference of extraction patterns with ASARES; and 3. extraction of NV pairs from the corpus with the inferred patterns. ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework <CITATION/> and called qualia relations <TARGET_CITATION/> . ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework <CITATION/> and called qualia relations <CITATION/>. extraction of NV pairs from the corpus with the inferred patterns.inference of extraction patterns with ASARES; and 3.,f0f0f7a282f76327e2d1b943b0053c9099e67a25,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,2004,V. Claveau; Marie-Claude LHomme,,using partofspeech and semantic tagging for the corpusbased learning of qualia structure elements,2001,Pierrette Bouillon; Vincent Claveau; Cecile Fabre; Pascale Sebillot
1891,D14-1222,D13-1077,[3],experiments,mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work <TARGET_CITATION/> on bridging anaphora recognition and antecedent selection .,mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rulebased system. All rules from the rulebased system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work <TARGET_CITATION/> on bridging anaphora recognition and antecedent selection . mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work <CITATION/> on bridging anaphora recognition and antecedent selection. All rules from the rulebased system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rulebased system.,e1284218dd71d220c02121ff9278e80de4cd6acd,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,2014,Yufang Hou; K. Markert; M. Strube,8de4164931eea9c4ebea42091e31e334e9acef11,Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set,2013,Yufang Hou; K. Markert; M. Strube
1892,J02-3002,External_62876,[4],,The description of the EAGLE workbench for linguistic engineering <TARGET_CITATION/> mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lowercased in the same document .,"In capitalizedword disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (ngrams). This is similar to one sense per collocation'' idea of <CITATION/>. The description of the EAGLE workbench for linguistic engineering <TARGET_CITATION/> mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lowercased in the same document . The description of the EAGLE workbench for linguistic engineering <CITATION/> mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lowercased in the same document. This is similar to one sense per collocation'' idea of <CITATION/>. In capitalizedword disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (ngrams).",3d9db1146acd2da5ad7b85d81c737f9260576c37,"Periods, Capitalized Words, etc.",2002,Andrei Mikheev,aa66512348329c295839e5af2dd33ede9e7defc2,EAGLE: An Extensible Architecture for General Linguistic Engineering,1997,B. Baldwin; Christine Doran; Jeffrey C. Reynar; Michael Niv; S. Bangalore
1893,W06-1639,External_36926,[0],related work,"An exception is <TARGET_CITATION/> , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .","Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>. There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. An exception is <TARGET_CITATION/> , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site . An exception is <CITATION/>, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site. There has also been work focused upon determining the political leaning (e.g., liberal'' vs. conservative'') of a document or author, where most previouslyproposed methods make no direct use of relationships between the documents to be classified (the unlabeled'' texts) <CITATION/>. Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,9d8f6567b84e04439d17b8ac0405684fa3360673,Coupling Niche Browsers and Affect Analysis for an Opinion Mining Application,2004,G. Grefenstette; Yan Qu; J. Shanahan; David A. Evans
1894,J00-2004,External_42502,[4],,"Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models <TARGET_CITATION/> .","method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models <TARGET_CITATION/> . Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models <CITATION/>. 6.1.1 Experiment 1. method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,fc593d91a7974bb1d3fac1ffe47b787ce1853a88,But Dictionaries Are Data Too,1993,P. Brown; S. D. Pietra; V. D. Pietra; Meredith J. Goldsmith; Jan Hajic; R. Mercer; Surya Mohanty
1895,W06-1104,External_2373,[2],experiments,"The three preprocessing steps ( tokenization , POStagging , lemmatization ) are performed using TreeTagger <TARGET_CITATION/> .","They cover a wide range of topics from bio genetics to computer science and contain many technical terms. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. The three preprocessing steps ( tokenization , POStagging , lemmatization ) are performed using TreeTagger <TARGET_CITATION/> . The three preprocessing steps (tokenization, POStagging, lemmatization) are performed using TreeTagger <CITATION/>. Due to the special structure of presentations, this corpus will be particularly demanding with respect to the required preprocessing components of an information retrieval system. They cover a wide range of topics from bio genetics to computer science and contain many technical terms.",e69b28380b4efc695a60054f85e8c46a9731c35b,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,Torsten Zesch; Iryna Gurevych,,probabilistic partofspeech tagging using decision trees,1995,Helmut Schmid
1896,E03-1002,External_62,[0],introduction,"Many statistical parsers <TARGET_CITATION/> are based on a historybased probability model <CITATION/> , where the probability of each decision in a parse is conditioned on the previous decisions in the parse ."," Many statistical parsers <TARGET_CITATION/> are based on a historybased probability model <CITATION/> , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . Many statistical parsers <CITATION/> are based on a historybased probability model <CITATION/>, where the probability of each decision in a parse is conditioned on the previous decisions in the parse.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,,headdriven statistical models for natural language parsing,1999,M Collins
1897,J00-2004,External_6119,[0],,"In informal experiments described elsewhere <CITATION/> , I found that the G2 statistic suggested by <TARGET_CITATION/> slightly outperforms 02 .","The statistical interdependence between two word types can be estimated more robustly by considering the whole table. For example, Gale and Church (1991, 154) suggest that 02, a x2like statistic, seems to be a particularly good choice because it makes good use of the offdiagonal cells'' in the contingency table. In informal experiments described elsewhere <CITATION/> , I found that the G2 statistic suggested by <TARGET_CITATION/> slightly outperforms 02 . In informal experiments described elsewhere <CITATION/>, I found that the G2 statistic suggested by <CITATION/> slightly outperforms 02. For example, Gale and Church (1991, 154) suggest that 02, a x2like statistic, seems to be a particularly good choice because it makes good use of the offdiagonal cells'' in the contingency table.The statistical interdependence between two word types can be estimated more robustly by considering the whole table.",38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1,Models of translation equivalence among words,2000,I. D. Melamed,025464b73f805e76689a7a20a48a9e9c0f4ff3ef,Accurate Methods for the Statistics of Surprise and Coincidence,1993,T. Dunning
1898,E03-1002,P99-1054,[0],,"For rightbranching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial <CITATION/> , as has conditioning on the leftcorner child <TARGET_CITATION/> .","For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the leftcorner ancestor of top, (which is below top, on the stack), top 's leftcorner child (its leftmost child, if any), and top 's most recent child (which was top,_1, if any). For rightbranching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial <CITATION/> , as has conditioning on the leftcorner child <TARGET_CITATION/> . For rightbranching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial <CITATION/>, as has conditioning on the leftcorner child <CITATION/>. These nodes are the leftcorner ancestor of top, (which is below top, on the stack), top 's leftcorner child (its leftmost child, if any), and top 's most recent child (which was top,_1, if any). For this reason, D(top) includes nodes which are structurally local to top,.",adcf1552e759f9cade8ef9e59ecf6159e25a055e,Neural Network Probability Estimation for Broad Coverage Parsing,2003,James Henderson,48b88fb145da84df2f61756a73d65bf22475a0ad,Efficient probabilistic top-down and left-corner parsing,1999,Brian Roark; Mark Johnson
1900,Q13-1020,J10-2004,[0],introduction,This indicates that parse trees are usually not the optimal choice for training treebased translation models <TARGET_CITATION/> .,"However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Treebank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training treebased translation models <TARGET_CITATION/> . This indicates that parse trees are usually not the optimal choice for training treebased translation models <CITATION/>. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Treebank resources for training.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,01266bd71f553fcfc822ea3d7ce5ef13bc4ad4c7,"Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation",2010,Wei Wang; Jonathan May; Kevin Knight; D. Marcu
1901,J05-3003,External_14416,[0],,"In <TARGET_CITATION/> et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .","Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%). The rate of accession may also be represented graphically. In <TARGET_CITATION/> , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . In <CITATION/>, it was observed that treebank grammars (CFGs extracted from treebanks) are very large and grow with the size of the treebank. The rate of accession may also be represented graphically. Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,f5bb34e38e3403054d4396fc48882f02eae1ffcc,Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification,1998,Claire Cardie; D. Pierce
1902,P02-1001,External_692,[0],introduction,"For example , the forwardbackward algorithm <TARGET_CITATION/> trains only Hidden Markov Models , while <CITATION/> trains only stochastic edit distance .","Currently, finitestate practitioners derive weights using exogenous training methods, then patch them onto transducer arcs. Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. For example , the forwardbackward algorithm <TARGET_CITATION/> trains only Hidden Markov Models , while <CITATION/> trains only stochastic edit distance . For example, the forwardbackward algorithm <CITATION/> trains only Hidden Markov Models, while <CITATION/> trains only stochastic edit distance. Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens. Currently, finitestate practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.",683305450fcb46f6832108308fc436df1b9eb80e,Parameter Estimation for Probabilistic Finite-State Transducers,2002,Jason Eisner,539036ab9e8f038c8a948596e77cc0dfcfa91fb3,An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process,1972,L. Baum
1903,W01-1510,P98-2132,[0],introduction,ment <TARGET_CITATION/> .,"There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environ1In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. ment <TARGET_CITATION/> . ment <CITATION/>. 1In this paper, we use the term LTAG to refer to FBLTAG, if not confusing.There have been many studies on parsing techniques <CITATION/>, ones on disambiguation models <CITATION/>, and ones on programming/grammardevelopment environ",9acc772280c3edeaf9057efaa689f33f211ed78e,Resource Sharing Amongst HPSG and LTAG Communities by a Method of Grammar Conversion between FB-LTAG and HPSG,2001,Naoki Yoshinaga; Yusuke Miyao; Kentaro Torisawa; Junichi Tsujii,9f4744e8f2beb883c5a5dafad48aac31ab236d0c,LiLFeS - Towards a Practical HPSG Parser,1998,Takaki Makino; Minoru Yoshida; Kentaro Torisawa; Junichi Tsujii
1904,J05-3003,External_13799,[1],introduction,"<TARGET_CITATION/> argues that , aside from missing domainspecific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .","However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre <CITATION/>. <TARGET_CITATION/> argues that , aside from missing domainspecific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . <CITATION/> argues that, aside from missing domainspecific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. In addition, subcategorization requirements may vary across linguistic domain or genre <CITATION/>. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.",ea681d3cc082bb8297d93f118f4355d4bd76f94c,Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks,2005,Ruth O'Donovan; Michael Burke; A. Cahill; Josef van Genabith; Andy Way,,automatic acquisition of a large subcategorisation dictionary from corpora,1993,Christopher Manning
1905,W03-0806,W02-0109,[0],experiments,"Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python <TARGET_CITATION/> .","The Weka package <CITATION/> provides a common framework for several existing machine learning methods including decision trees and support vector machines. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python <TARGET_CITATION/> . Finally, the Natural Language Toolkit (NLTK) is a package of NLP components implemented in Python <CITATION/>. This library has been very popular because it allows researchers to experiment with different methods without having to modify code or reformat data. The Weka package <CITATION/> provides a common framework for several existing machine learning methods including decision trees and support vector machines.",7f9945be2f027d721e0bb5e5ae708a9dfc16dc46,Blueprint for a High Performance NLP Infrastructure,2003,J. Curran,77a02706edbdca6ad63ddf903ebd559dd7d423ac,NLTK: The Natural Language Toolkit,2002,E. Loper; Steven Bird
1906,W06-1639,External_55771,[0],related work,"Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit <TARGET_CITATION/> ."," Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit <TARGET_CITATION/> . Politicallyoriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit <CITATION/>.",dc832b298290e316d1218266f6f33de97c9b5679,Get out the vote: Determining support or opposition from Congressional floor-debate transcripts,2006,Matt Thomas; B. Pang; Lillian Lee,fd44d2aa04591f2e265b07f4b79b9df9552cbcbb,Language processing technologies for electronic rulemaking: a project highlight,2005,Stuart W. Shulman; E. Hovy; Jamie Callan; S. Zavestoski
1907,Q13-1020,N10-1015,[4],related work,<TARGET_CITATION/> focused on joint parsing and alignment .,"This study differs from their work because we concentrate on constructing tree structures for treebased translation models. Our Utrees are learned based on STSG, which is more appropriate for treebased translation models than SCFG. <TARGET_CITATION/> focused on joint parsing and alignment . <CITATION/> focused on joint parsing and alignment. Our Utrees are learned based on STSG, which is more appropriate for treebased translation models than SCFG. This study differs from their work because we concentrate on constructing tree structures for treebased translation models.",aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,844133147a3e0859419f53daba00c130747d22b0,Joint Parsing and Alignment with Weakly Synchronized Grammars,2010,David Burkett; John Blitzer; D. Klein
1908,J00-4002,E95-1001,[2],,"But the general outlines are reasonably clear , and we can adapt some of the UDRS <TARGET_CITATION/> work to our own framework .","Information extraction systems typically carry out such reasoning in a way that is, in Jerry Hobbs' phrase, unhindered by theory. Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. But the general outlines are reasonably clear , and we can adapt some of the UDRS <TARGET_CITATION/> work to our own framework . But the general outlines are reasonably clear, and we can adapt some of the UDRS <CITATION/> work to our own framework. Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. Information extraction systems typically carry out such reasoning in a way that is, in Jerry Hobbs' phrase, unhindered by theory.",c6c97edc947e0e6bf935459ea57e8827854ae2bf,Bidirectional Contextual Resolution,2000,S. Pulman,00bc2373074293d9f743ebfda0c626b4709fb682,On Reasoning with Ambiguities,1995,Uwe Reyle
1911,W06-3309,External_74945,[0],introduction,The need for information systems to support physicians at the point of care has been well studied <TARGET_CITATION/> .,"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>. For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied <TARGET_CITATION/> . The need for information systems to support physicians at the point of care has been well studied <CITATION/>. For a variety of reasons, medicine is an interesting domain of research. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the foursection pattern discussed above <CITATION/>.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d,Generative Content Models for Structural Analysis of Medical Abstracts,2006,Jimmy J. Lin; Damianos G. Karakos; Dina Demner-Fushman; S. Khudanpur,f3a95e6ca7292879062fe368edd82d9bfbbdbe53,Can primary care physicians' questions be answered using the medical journal literature?,1994,P. Gorman; J. Ash; L. Wykoff
1913,D08-1034,External_24680,[0],introduction,"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <TARGET_CITATION/> .","With the efforts of many researchers (Carreras and M rquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as <TARGET_CITATION/> . Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as <CITATION/>. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. With the efforts of many researchers (Carreras and M rquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.",74cc22ca9eeee2997b0ecf2883b57d1a81842299,Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,Weiwei Ding; Baobao Chang,ee4c4fe7fd24125531a0e9eafb6d110cf3c27398,Automatic Semantic Role Labeling for Chinese Verbs,2005,Nianwen Xue; Martha Palmer
1914,J97-4003,External_77175,[1],introduction,"For example , the interaction of lexical rules is explored at runtime , even though the possible interaction can be determined at compiletime given the information available in the lexical rules and the base lexical entries .2 Based on the research results reported in <TARGET_CITATION/> ) , we propose a new computational treatment of lexical rules that overcomes these shortcomings and results in a more efficient processing of lexical rules as used in HPSG .","Treatments of lexical rules as unary phrase structure rules also require their fully explicit specification, which entails the last problem mentioned above. In addition, computationally treating lexical rules on a par with phrase structure rules fails to take computational advantage of their specific properties. For example , the interaction of lexical rules is explored at runtime , even though the possible interaction can be determined at compiletime given the information available in the lexical rules and the base lexical entries .2 Based on the research results reported in <TARGET_CITATION/> ) , we propose a new computational treatment of lexical rules that overcomes these shortcomings and results in a more efficient processing of lexical rules as used in HPSG . For example, the interaction of lexical rules is explored at runtime, even though the possible interaction can be determined at compiletime given the information available in the lexical rules and the base lexical entries.2 Based on the research results reported in Meurers and Minnen (1995, 1996), we propose a new computational treatment of lexical rules that overcomes these shortcomings and results in a more efficient processing of lexical rules as used in HPSG. In addition, computationally treating lexical rules on a par with phrase structure rules fails to take computational advantage of their specific properties. Treatments of lexical rules as unary phrase structure rules also require their fully explicit specification, which entails the last problem mentioned above.",d0dbffcb01f315774230a0450b2f4da8c6a608f7,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,1997,Walt Detmar Meurers; Guido Minnen,ade2a019ebae17d405d397a9b20aaa3ff1c7c851,A Computational Treatment of HPSG Lexical Rules as Covariation in Lexical Entries,1995,Walt Detmar Meurers; Guido Minnen
1916,D08-1036,P07-1094,[4],conclusion,"On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in <TARGET_CITATION/> .","As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear. On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in <TARGET_CITATION/> . On small data sets all of the Bayesian estimators strongly outperform EM (and, to a lesser extent, VB) with respect to all of our evaluation measures, confirming the results reported in <CITATION/>. As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.",bbf3ddb68cc14e886c5cf8be6cc2efc133a89063,A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers,2008,Jianfeng Gao; Mark Johnson,469d720411f8d8d75d2352b170dbe4611b508cff,A fully Bayesian approach to unsupervised part-of-speech tagging,2007,S. Goldwater; T. Griffiths
1917,W06-1705,External_32271,[0],introduction,"In addition , the advantages of using linguistically annotated data over raw data are well documented <TARGET_CITATION/> .","Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem <CITATION/>. This topic generated intense interest at workshops held at the University of Heidelberg <CITATION/>, University of Bologna <CITATION/>, University of Birmingham <CITATION/> and now in Trento in April 2006. In addition , the advantages of using linguistically annotated data over raw data are well documented <TARGET_CITATION/> . In addition, the advantages of using linguistically annotated data over raw data are well documented <CITATION/>. This topic generated intense interest at workshops held at the University of Heidelberg <CITATION/>, University of Bologna <CITATION/>, University of Birmingham <CITATION/> and now in Trento in April 2006. Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem <CITATION/>.",a4a0bc10f9ab6d11196172e33b31c37c76f413a7,Annotated Web as corpus,2006,Paul Rayson; J. Walkerdine; William H. Fletcher; A. Kilgarriff,,the corpusbased study of language change in progress the extra value of tagged corpora,2005,C Mair
1918,J92-1004,External_6789,[2],,"For the A * algorithm <TARGET_CITATION/> as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .","A final alternative is to include a PARTICLE bit among10 Some modification of this scheme is necessary when the input stream is not deterministic. For the A * algorithm <TARGET_CITATION/> as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion . For the A* algorithm <CITATION/> as applied to speech recognition, the actual path score is typically augmented with an estimated score for the unseen portion. 10 Some modification of this scheme is necessary when the input stream is not deterministic. A final alternative is to include a PARTICLE bit among",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,a formal basis for the heuristic determination of minimum cost pathsquot,1968,P Hart; N J Nilsson; B Raphael
1919,A00-1008,P98-1059,[2],introduction,Robust natural language understanding in AtlasAndes is provided by Ros 's CARMEL system <CITATION/> ; it uses the spelling correction algorithm devised by <TARGET_CITATION/> .,The first system we have implemented with APE is a prototype AtlasAndes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues. Figure 4 shows the architecture of AtlasAndes; any other system built with APE would look similar. Robust natural language understanding in AtlasAndes is provided by Ros 's CARMEL system <CITATION/> ; it uses the spelling correction algorithm devised by <TARGET_CITATION/> . Robust natural language understanding in AtlasAndes is provided by Ros's CARMEL system <CITATION/>; it uses the spelling correction algorithm devised by <CITATION/>.Figure 4 shows the architecture of AtlasAndes; any other system built with APE would look similar. The first system we have implemented with APE is a prototype AtlasAndes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.,5a7ffe0cf842cd6a7f12b912bcc82ac4f642fab7,Plan-Based Dialogue Management in a Physics Tutor,2000,Reva Freedman,a2c62e2732d9b8d940a466f5d7ae2c15f1836354,Spelling Correction using Context,1998,Mohammad Ali Elmi; M. Evens
1920,A00-1009,A92-1006,[3],,"The framework represents a generalization of several predecessor NLG systems based on MeaningText Theory : FoG <CITATION/> , LFS <CITATION/> , and JOYCE ( Rambow and <TARGET_CITATION/> ) .","In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.8 History of the Framework and Comparison with Other Systems The framework represents a generalization of several predecessor NLG systems based on MeaningText Theory : FoG <CITATION/> , LFS <CITATION/> , and JOYCE ( Rambow and <TARGET_CITATION/> ) . The framework represents a generalization of several predecessor NLG systems based on MeaningText Theory: FoG <CITATION/>, LFS <CITATION/>, and JOYCE <CITATION/>. 8 History of the Framework and Comparison with Other SystemsIn general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.",6602edbc2f35e085dc4ee0361da214c4f14c5a07,A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing,2000,Benoit Lavoie; R. Kittredge; Tanya Korelsky; Owen Rambow,37100857c059c190fdca074fabe6d6856b480c51,Applied Text Generation,1992,Owen Rambow; Tanya Korelsky
1921,J03-3004,External_61551,[0],introduction,"Nevertheless , <TARGET_CITATION/> ) observes that  a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its casemarking ) , can capture this sort of result quite handily . ''","Juola's (1994, 1998) work on grammar optimization and induction shows that contextfree grammars can be converted to ''markernormal form.'' However, markernormal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a onetoone mapping between a terminal symbol and a word. Nevertheless , <TARGET_CITATION/> ) observes that  a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its casemarking ) , can capture this sort of result quite handily . '' <CITATION/> observes that a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its casemarking), can capture this sort of result quite handily.'' However, markernormal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a onetoone mapping between a terminal symbol and a word. Juola's (1994, 1998) work on grammar optimization and induction shows that contextfree grammars can be converted to ''markernormal form.''",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,55ecd739ed7717067191535a8f7cc318842a10d0,On Psycholinguistic Grammars,1998,P. Juola
1922,J03-3004,C00-1019,[0],introduction,"Other similar approaches include those of Cicekli and G  uvenir ( 1996 ) , <TARGET_CITATION/> , inter alia .","<CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. <CITATION/> combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and G  uvenir ( 1996 ) , <TARGET_CITATION/> , inter alia . Other similar approaches include those of <CITATION/>, inter alia. <CITATION/> combines lexical and dependency mappings to form his generalizations. <CITATION/> identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.",c67e7c60c04f64ea4afa8a3ba1960bebcd47f7b4,wEBMT: Developing and Validating an Example-Based Machine Translation System using the World Wide Web,2003,Andy Way; N. Gough,b5e6bf30c6a232b0b722875e3ba8ed15ceff13f8,Automated Generalization of Translation Examples,2000,Ralf D. Brown
1923,D12-1037,D11-1125,[4],related work,<TARGET_CITATION/> proposed other optimization objectives by introducing a marginbased and rankingbased indirect loss functions .,Several works have proposed discriminative techniques to train loglinear model for SMT. <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. <TARGET_CITATION/> proposed other optimization objectives by introducing a marginbased and rankingbased indirect loss functions . <CITATION/> proposed other optimization objectives by introducing a marginbased and rankingbased indirect loss functions. <CITATION/> used maximum likelihood estimation to learn weights for MT. <CITATION/> employed an evaluation metric as a loss function and directly optimized it. Several works have proposed discriminative techniques to train loglinear model for SMT.,413339a905449a13830d9c43854adb3b02ffc399,Locally Training the Log-Linear Model for SMT,2012,Lemao Liu; Hailong Cao; Taro Watanabe; T. Zhao; Mo Yu; Conghui Zhu,a13d46125ef505d4e687e25ded74b794efc18323,Tuning as Ranking,2011,Mark Hopkins; Jonathan May
1924,Q13-1020,D12-1079,[4],related work,<TARGET_CITATION/> utilized a transformationbased method to learn a sequence of monolingual tree transformations for translation .,<CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. <CITATION/> retrained the linguistic parsers bilingually based on word alignment. <TARGET_CITATION/> utilized a transformationbased method to learn a sequence of monolingual tree transformations for translation . <CITATION/> utilized a transformationbased method to learn a sequence of monolingual tree transformations for translation. <CITATION/> retrained the linguistic parsers bilingually based on word alignment. <CITATION/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.,aaf0174070ddb003bdb4a6452ef9ffd4a05467b8,Unsupervised Tree Induction for Tree-based Translation,2013,Feifei Zhai; Jiajun Zhang; Yu Zhou; Chengqing Zong,14d6b04c5ea78d51bb8cbf46daecc865203e6e9a,Transforming Trees to Improve Syntactic Convergence,2012,David Burkett; D. Klein
1925,D08-1006,External_22097,[2],experiments,"Baseline language model : For P0 we used a trigram with modified kneserney smoothing [ Chen and <TARGET_CITATION/> ] , which is still considered one of the best smoothing methods for ngram language models .","The resulting lexicon contained 603 word types. Our learning framework leaves open a number of design choices: 1. Baseline language model : For P0 we used a trigram with modified kneserney smoothing [ Chen and <TARGET_CITATION/> ] , which is still considered one of the best smoothing methods for ngram language models . Baseline language model: For P0 we used a trigram with modified kneserney smoothing [<CITATION/>], which is still considered one of the best smoothing methods for ngram language models. Our learning framework leaves open a number of design choices: 1. The resulting lexicon contained 603 word types.",00dc508fdf5dcbf78bee0ea779aad408830c20e2,Refining Generative Language Models using Discriminative Learning,2008,Ben Sandbank,d4e8bed3b50a035e1eabad614fe4218a34b3b178,An Empirical Study of Smoothing Techniques for Language Modeling,1996,Stanley F. Chen; Joshua Goodman
1926,K15-1002,External_62,[2],experiments,"Therefore , we preprocess Ontonote5 .0 to derive mention heads using Collins head rules <TARGET_CITATION/> with gold constituency parsing information and gold named entity information .","We report results on the test documents for both datasets.The ACE2004 dataset is annotated with both mention and mention heads, while the OntoNotes5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote5 .0 to derive mention heads using Collins head rules <TARGET_CITATION/> with gold constituency parsing information and gold named entity information . Therefore, we preprocess Ontonote5.0 to derive mention heads using Collins head rules <CITATION/> with gold constituency parsing information and gold named entity information. The ACE2004 dataset is annotated with both mention and mention heads, while the OntoNotes5.0 dataset only has mention annotations. We report results on the test documents for both datasets.",f579ec37be18fc908bdba84a827812e922842b16,A Joint Framework for Coreference Resolution and Mention Head Detection,2015,Haoruo Peng; Kai-Wei Chang; D. Roth,,headdriven statistical models for natural language parsing,1999,M Collins
1927,D10-1002,P05-1022,[4],conclusion,"Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches <TARGET_CITATION/> and products of latent variable grammars <CITATION/> , despite being a single generative PCFG .","First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single selftrained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches <TARGET_CITATION/> and products of latent variable grammars <CITATION/> , despite being a single generative PCFG . Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches <CITATION/> and products of latent variable grammars <CITATION/>, despite being a single generative PCFG. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single selftrained grammars.",e441126a8dd0cb8363272b7b54207ae92e155bc0,Self-Training with Products of Latent Variable Grammars,2010,Zhongqiang Huang; M. Harper; Slav Petrov,0ecb33ced5b0976accdf13817151f80568b6fdcb,Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking,2005,Eugene Charniak; Mark Johnson
1928,N01-1011,A00-2009,[0],conclusion,"We have presented an ensemble approach to word sense disambiguation <TARGET_CITATION/> where multiple Naive Bayesian classifiers , each based on co  occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .","One of our longterm objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation <TARGET_CITATION/> where multiple Naive Bayesian classifiers , each based on co  occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line . We have presented an ensemble approach to word sense disambiguation <CITATION/> where multiple Naive Bayesian classifiers, each based on co occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. One of our longterm objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies.",b15737187e58016731f6c78b7cceb1a09f4d1220,A Decision Tree of Bigrams is an Accurate Predictor of Word Sense,2001,Ted Pedersen,f49231a5bbf89e2566dc04784ac6e99f726b2e72,A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation,2000,Ted Pedersen
1929,P10-4003,External_16891,[0],introduction,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> ."," Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <TARGET_CITATION/> , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/> . Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations <CITATION/>, because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring <CITATION/>.",1b19897e1fabfc87093de93857cdfad7c8c8abe9,Beetle II: A System for Tutoring and Computational Linguistics Experimentation,2010,M. Dzikovska; Johanna D. Moore; Natalie B. Steinhauser; Gwendolyn E. Campbell; Elaine Farrow; Charles B. Callaway,,autotutor a simulation of a human tutor cognitive systems research,1999,A C Graesser; P Wiemer-Hastings; P WiemerHastings; R Kreuz
1931,D09-1056,W07-2069,[0],related work,"Nevertheless , the full document text is present in most systems , sometimes as the only feature <TARGET_CITATION/> and sometimes in combination with others see for instance <CITATION/>  .","The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. Nevertheless , the full document text is present in most systems , sometimes as the only feature <TARGET_CITATION/> and sometimes in combination with others see for instance <CITATION/>  . Nevertheless, the full document text is present in most systems, sometimes as the only feature <CITATION/> and sometimes in combination with others see for instance <CITATION/>. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name <CITATION/>. The most basic is a Bag of Words (BoW) representation of the document text.",a7b930b3297dfbb87813a8f47db1a45acaff76a4,The role of named entities in Web People Search,2009,J. Artiles; Enrique Amigó; Julio Gonzalo,c97cb1afab4490185c2ebb90646c23ac646040f5,TITPI: Web People Search Task Using Semi-Supervised Clustering Approach,2007,Kazunari Sugiyama; M. Okumura
1932,J92-1004,H89-2018,[2],,"The recognizer for these systems is the SUMMIT system <TARGET_CITATION/> , which uses a segmentalbased framework and includes an auditory model in the frontend processing .","In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system <TARGET_CITATION/> , which uses a segmentalbased framework and includes an auditory model in the frontend processing . The recognizer for these systems is the SUMMIT system <CITATION/>, which uses a segmentalbased framework and includes an auditory model in the frontend processing. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,097a9aa2f9bc0feb5e0026b1b354a28d033b6ccb,The Collection and Preliminary Analysis of a Spontaneous Speech Database,1989,V. Zue; Nancy A. Daly-Kelly; James R. Glass; D. Goodine; H. Leung; M. Phillips; J. Polifroni; S. Seneff; M. Soclof
1933,J06-2002,External_33407,[4],,"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead <TARGET_CITATION/> ."," Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead <TARGET_CITATION/> . Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead <CITATION/>.",0cd20231bd846ac75f25446e0a2bf02aa3d30717,Generating Referring Expressions that Involve Gradable Properties,2006,Kees van Deemter,388f9632febf0117bcf9b8907983996fb9418be6,Textual Economy Through Close Coupling of Syntax and Semantics,1998,Matthew Stone; B. Webber
1934,K15-1003,N07-1018,[2],,"To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by <CITATION/> and used by <TARGET_CITATION/> that samples entire parse trees .","The basic idea is that we sample trees according to a simpler proposal distribution Q that approximates the full distribution and for which direct sampling is tractable, and then choose to accept or reject those trees based on the true distribution P. For our model, there is a straightforward and intuitive choice for the proposal distribution: the PCFG model without our context parameters: (ROOT, BIN, UN, TERM, ), which is known to have an efficient sampling method. Our acceptance step is therefore based on the remaining parameters: the context (LCTX, RCTX). To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by <CITATION/> and used by <TARGET_CITATION/> that samples entire parse trees . To sample from our proposal distribution, we use a blocked Gibbs sampler based on the one proposed by <CITATION/> and used by <CITATION/> that samples entire parse trees. Our acceptance step is therefore based on the remaining parameters: the context (LCTX, RCTX). The basic idea is that we sample trees according to a simpler proposal distribution Q that approximates the full distribution and for which direct sampling is tractable, and then choose to accept or reject those trees based on the true distribution P. For our model, there is a straightforward and intuitive choice for the proposal distribution: the PCFG model without our context parameters: (ROOT, BIN, UN, TERM, ), which is known to have an efficient sampling method.",39fbaac080f0f8dce2c41667a35cdefcaa733405,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,2015,Dan Garrette; Chris Dyer; Jason Baldridge; Noah A. Smith,ece4a51f9d1fab08230a527efbb801c57e0249c5,Bayesian Inference for PCFGs via Markov Chain Monte Carlo,2007,Mark Johnson; T. Griffiths; S. Goldwater
1935,W04-0910,E03-1030,[0],,Semantic construction proceeds from the derived tree <TARGET_CITATION/> rather than  as is more common in TAG  from the derivation tree .,"However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form Pn(x1, ... , xn) where Pn is a predicate of arity n and xi is either a constant or a unification variable whose value will be instantiated during processing. Semantic construction proceeds from the derived tree <TARGET_CITATION/> rather than  as is more common in TAG  from the derivation tree . Semantic construction proceeds from the derived tree <CITATION/> rather than  as is more common in TAG  from the derivation tree. Thus the semantic representations we assume are simply set of literals of the form Pn(x1, ... , xn) where Pn is a predicate of arity n and xi is either a constant or a unification variable whose value will be instantiated during processing. However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.",baa4bd0d1d3c6e3c7641177982283fbec39646f7,Paraphrastic grammars,2004,Claire Gardent; Marilisa Amoia; Évelyne Jacquey,88571f16e9f538a913727e0215a54f916d273f58,Semantic construction in F-TAG,2003,Claire Gardent; Laura Kallmeyer
1936,W02-1601,External_4496,[0],,It allows the construction of a nonTAL <TARGET_CITATION/> .,"STAG is a variant of Tree Adjoining Grammar (TAG) introduced by <CITATION/> to characterize correspondences between tree adjoining languages. Considering the original definition of STAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a nonTAL <TARGET_CITATION/> . It allows the construction of a nonTAL <CITATION/>. Considering the original definition of STAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. STAG is a variant of Tree Adjoining Grammar (TAG) introduced by <CITATION/> to characterize correspondences between tree adjoining languages.",582fb43dc66caf69578d572478a08c92b2c50a58,A Synchronization Structure of SSTC and Its Applications in Machine Translation,2002,M. H. Al-Adhaileh; E. Tang; Zaharin Yusoff,e6a125ae8b2f635c050e708ff8e053f26cc3281c,RESTRICTING THE WEAK‐GENERATIVE CAPACITY OF SYNCHRONOUS TREE‐ADJOINING GRAMMARS,1994,Stuart M. Shieber
1937,P07-1068,J01-4004,[2],,"Following previous work ( e.g. , <TARGET_CITATION/> ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .","Our baseline coreference system uses the C4.5 decision tree learner <CITATION/> to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work <TARGET_CITATION/> , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 . Following previous work (e.g., <CITATION/>), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2, ..., NPj_1. Our baseline coreference system uses the C4.5 decision tree learner <CITATION/> to acquire a classifier on the training texts for determining whether two NPs are coreferent.",476c783b7a837392d631a74ebb4b94a5207e5bae,Semantic Class Induction and Coreference Resolution,2007,Vincent Ng,a20bfec3c95aad003dcb45a21a220c19cca8bb66,A Machine Learning Approach to Coreference Resolution of Noun Phrases,2001,Wee Meng Soon; H. Ng; Chung Yong Lim
1938,W04-1610,External_91194,[0],experiments,TFIDF ( term frequencyinverse document frequency ) is one of the widely used feature selection techniques in information retrieval <TARGET_CITATION/> .,"On the other hand, <CITATION/> reports the X2 to produce best performance. In this paper, we use TFIDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in <CITATION/>. TFIDF ( term frequencyinverse document frequency ) is one of the widely used feature selection techniques in information retrieval <TARGET_CITATION/> . TFIDF (term frequencyinverse document frequency) is one of the widely used feature selection techniques in information retrieval <CITATION/>. In this paper, we use TFIDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in <CITATION/>. On the other hand, <CITATION/> reports the X2 to produce best performance.",63774ca26b22750390b83480b96d56f8acf34550,Automatic Arabic Document Categorization Based on the Naïve Bayes Algorithm,2004,Mohamed El Kourdi; A. Bensaid; T. Rachidi,49af3e80343eb80c61e727ae0c27541628c7c5e2,Introduction to Modern Information Retrieval,1983,Gerard Salton; Michael McGill
1939,J92-1004,External_3690,[4],,"The gap mechanism resembles the Hold register idea of ATNs <TARGET_CITATION/> and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . )","Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subjecttagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ''(which article), do you think I should read (t1)?'') <CITATION/>. The gap mechanism resembles the Hold register idea of ATNs <TARGET_CITATION/> and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ) The gap mechanism resembles the Hold register idea of ATNs <CITATION/> and the treatment of bounded domination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ff.)<CITATION/>. Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications subjecttagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ''(which article), do you think I should read (t1)?'')",ac8f1fd58be8a8c9f9599fc4da981ea3040945f6,TINA: A Natural Language System for Spoken Language Applications,1992,S. Seneff,,transition network grammars for natural language analysisquot,1970,W A Woods
1940,D13-1115,External_5501,[2],method,"To solve these scaling issues , we implement Online Variational Bayesian Inference <TARGET_CITATION/> for our models .","Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues , we implement Online Variational Bayesian Inference <TARGET_CITATION/> for our models . To solve these scaling issues, we implement Online Variational Bayesian Inference <CITATION/> for our models. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated.",75c3ba0c7e5b0d4a11e9d2e073ccd02ee688c0c9,"A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",2013,Stephen Roller; Sabine Schulte im Walde,,stochastic variational inference arxiv eprints,2012,Matthew Hoffman; David M Blei; Chong Wang; John Paisley
