token_context,word_context,seg_context,sent_cotext,label
"['start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', 'if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users']","['start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', 'if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users']","['a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'in the first instance, the edited version simply moves ahead the document history.', 'in the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'the lexia will be marked as a derivative work from the original one, and a new document history timeline will start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', 'if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users']","['a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'in the first instance, the edited version simply moves ahead the document history.', 'in the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'the lexia will be marked as a derivative work from the original one, and a new document history timeline will start ( see figure 2 ).', 'authors may choose this right with the no - deriv option of the creative commons licences ( #AUTHOR_TAG ).', 'if nobody claims the document for himself, it will fall in the public domain.', 'the set of lexias in the public domain will form a special document, owned by a special user, called public domain.', ""if the author refuses the permission to create derivative works, i. e. to edit his own lexias, users still have the right to comment the author's work."", 'so as to come to terms with this idea, we need a concept invented by Nelson (1992), i. e. transclusion.', ""rather than copy - and - paste contents from a lexia, a user may recall a quotation of the author's lexia and write a comment in the surroundings."", ""in doing so, the link list of the author's lexia will be updated with a special citation link marker, called quotation link ( see later for details )."", ""usually, the quotation will be'frozen ', as in the moment where it was transcluded ( see figure 3 )."", 'consequently the transclusion resembles a copiedand - pasted text chunk, but the link to the original document will always be consistent, i. e. neither it expires nor it returns an error.', 'otherwise the user who has transcluded the quotation may choose to keep updated the links to the original document.', 'this choice has to be made when the transclusion is done.', 'if so, the transcluded quotation will update automatically, following the history timeline of the original document.', 'for example, if the original document changes topic from stars to pentagons, the quotation transcluded will change topic too ( see figure 4 )']",0
"['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']","['main source of novelle are wikis and blogs.', 'while wikis have spread from a detailed design ( #AUTHOR_TAG ), unfortunately blogs have not been designed under a model.', 'so we have tested and compared the most used tools available for blogging : bloggers, wordpress, movabletype and livejournal']",0
"['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']","['our typology of links, we aim to solve the framing problem as defined in section 1. 2.', 'we want to model views as dynamic objects - the creation of context will be still arbitrary, but changes are very easily.', 'we would also provide a user facility for choosing the right licence for every lexia, following the model of creative commons licences ( #AUTHOR_TAG )']",5
"['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]","['speaking, we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context.', ""the only way to retrieve information is through a search engine or a calendar, i. e. the date of the'post'- a lexia in the jargon of bloggers""]",0
"['takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', 'now they try to collect']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', 'now they try to collect']","['takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', 'now they try to collect']","['emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre ( #AUTHOR_TAG ).', 'furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'so blogs are a literary metagenre which started as authored personal diaries or journals.', ""now they try to collect themselves in so - called'blogspheres '."", 'on the other side, wikis started as collective works where each entry is not owned by a single author - e. g.', 'Wikipedia (2005).', 'now personal wiki tools are arising for brainstorming and mind mapping.', 'see section 4 for further aspects']",0
"['- unlike in the previous times ( #AUTHOR_TAG ).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet,']","['was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'terms as chapter, page or foot - note simply become meaningless in the new texts, or they highly change their meaning.', 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - - i. e. literary criticism - - unlike in the previous times ( #AUTHOR_TAG )."", 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet,']","['- unlike in the previous times ( #AUTHOR_TAG ).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet,']","['was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'terms as chapter, page or foot - note simply become meaningless in the new texts, or they highly change their meaning.', 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - - i. e. literary criticism - - unlike in the previous times ( #AUTHOR_TAG )."", 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', 'for example, a web page is more similar to an infinite canvas than a written page ( mc Cloud, 2001).', 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an opera aperta ( open work ), as eco would define it ( 1962 ).', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc Neill, 2005) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text? which role is suitable for authors? we have to analyse them before presenting the architecture of novelle']",0
"['as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gut']","['. 1 hypertext as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco""]","['. 1 hypertext as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gut']","['. 1 hypertext as a new writing space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( mc Neill, 2005) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']","['##ax is not a technology in itself but a term that refers to the use of a group of technologies together, in particular javascript and xml.', 'in other words ajax is a web development technique for creating interactive web applications using a combination of xhtml and css, document object model ( or dom ), the xmlhttprequest object ( #AUTHOR_TAG )']",0
"[""##t's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']","[""mapping has been used at least in education for over thirty years, in particular at the cornell university, where piaget's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']","[""##t's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']","[""mapping has been used at least in education for over thirty years, in particular at the cornell university, where piaget's ideas gave the roots to the assimilation theory by david ausubel."", 'very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'every arc always has a definite direction, i. e. arcs are arrows ( #AUTHOR_TAG )']",0
['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],['ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes'],5
"['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e.']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e. views with a timestamp.', 'consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'this model will have a strong impact on the role of links and on the underpinning structure of novelle itself']","['wikis every document keeps track of its own history : creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'moreover, a sandbox is a temporary view of a document itself i. e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ).', 'figure 1 shows the model.', 'history snapshots of the timeline may be considered as permanent views, i. e. views with a timestamp.', 'consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'this model will have a strong impact on the role of links and on the underpinning structure of novelle itself']",0
"[').', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up :']","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up :']","[').', 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up :']","['. 1 hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""terms as'chapter ','page'or'footnote'simply become meaningless in the new texts, or they highly change their meaning."", 'when gutenberg invented the printing press and aldo manuzio invented the book as we know it, new forms of writings arose.', ""for example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation - i. e."", 'literary criticism - unlike in the previous times (Eisenstein, 1983).', 'nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web ( berners - Lee, 1999).', ""for example, a'web page'is more similar to an infinite canvas than a written page ( mc Cloud, 2001)."", 'moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""from a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an'opera aperta'( open work ), as eco would define it ( 1962 )."", 'from a more pessimistic one, an author may feel to have lost power in this openness.', 'henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation, comment, and strong editing.', 'they give more power to readers, eventually filling the gap - the so - called active readers become authors as well.', 'this situation could make new problems rise up : who owns the text?', 'which role is suitable for authors?', 'we have to analyse them before presenting the architecture of novelle']",0
"['and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']","['the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'in fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'generally, people avoid commenting, preferring to edit each document.', 'the paradigm is "" write many, read many "" ( #AUTHOR_TAG )']",0
"['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']","['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']","['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']","['used the asyncronous javascript and xml ( or ajax ) paradigm to create the graphical user interface.', 'ajax function lets the communication works asyncronously between a client and a server through a set of messages based on http protocol and xml ( #AUTHOR_TAG )']",0
"['( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german ( Brants et al. , 2002 ) and portuguese ( #AUTHOR_TAG ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ), dutch ( van der Beek et al. , 2002 ) and slovene ( dezeroski et al., 2006 ), where root precision drops more drastically to about 69 %, 71 % and 41 %, respectively, and root recall is also affected negatively.', 'on the other hand, all three languages behave like high - accuracy languages with respect to attachment score.', 'a very similar pattern is found for spanish ( civit torruella and marti antonin, 2002 ), although this cannot be explained by a high proportion of non - projective structures.', 'one possible explanation in this case may be the fact that dependency graphs in the spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features']",1
"['histories to parser actions (Kudo and Matsumoto, 2002).', 'a¢ graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']","['histories to parser actions (Kudo and Matsumoto, 2002).', 'a¢ graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• history - based feature models for predicting the next parser action (Black et al., 1992).', '• support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'a¢ graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• history - based feature models for predicting the next parser action (Black et al., 1992).', '• support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'a¢ graph transformations for recovering nonprojective structures ( #AUTHOR_TAG )']",5
"['( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german ( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ),']","['second observation is that a high proportion of non - projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'this is noticeable for german ( #AUTHOR_TAG ) and portuguese ( Afonso et al. , 2002 ), which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for czech ( b [UNK] ohmov [UNK] a et al., 2003 ), dutch ( van der Beek et al. , 2002 ) and slovene ( dezeroski et al., 2006 ), where root precision drops more drastically to about 69 %, 71 % and 41 %, respectively, and root recall is also affected negatively.', 'on the other hand, all three languages behave like high - accuracy languages with respect to attachment score.', 'a very similar pattern is found for spanish ( civit torruella and marti antonin, 2002 ), although this cannot be explained by a high proportion of non - projective structures.', 'one possible explanation in this case may be the fact that dependency graphs in the spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features']",1
['labeled dependency parsing by #AUTHOR_TAG'],['labeled dependency parsing by #AUTHOR_TAG'],['labeled dependency parsing by #AUTHOR_TAG'],['parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG'],5
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( #AUTHOR_TAG ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ),']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ),']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ),']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ), chinese ( #AUTHOR_TAG ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['(Nivre, 2006).', 'a¢ history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', '• support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• graph transformations for recovering nonprojective structures']","['(Nivre, 2006).', 'a¢ history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', '• support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'a¢ history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', '• support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'a¢ history - based feature models for predicting the next parser action ( #AUTHOR_TAG ).', '• support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• graph transformations for recovering nonprojective structures']",5
"['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']","['experiments have been performed using maltparser ( #AUTHOR_TAG ), version 0. 4, which is made available together with the suite of programs used for preand post - processing.']",5
"[', we use libsvm ( #AUTHOR_TAG )']","[', we use libsvm ( #AUTHOR_TAG )']","[', we use libsvm ( #AUTHOR_TAG )']","['use support vector machines to predict the next parser action from a feature vector representing the history.', 'more specifically, we use libsvm ( #AUTHOR_TAG ) with a quadratic kernel k ( xz, xj ) = ( - yxt xj + r ) 2 and the built - in one - versus - all strategy for multi - class classification.', 'symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the feats field into its atomic components. 4', 'or some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']",5
['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],['##the analysis is reminiscent of the treatment of coordination in the collins parser ( #AUTHOR_TAG )'],1
['to be captured using the pseudoprojective approach of #AUTHOR_TAG'],['to be captured using the pseudoprojective approach of #AUTHOR_TAG'],['are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG'],"['the parser only derives projective graphs, the fact that graphs are labeled allows non - projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG']",0
"['sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']","['some languages, we divide the training data into smaller sets, based on some feature s ( normally the cpos or pos of the next input token ), which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ).', 'to avoid too small training sets, we pool together categories that have a frequency below a certain threshold t']",0
"['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ), chinese ( Chen et al. , 2003 ), danish ( Kromann , 2003 ), and swedish ( Nilsson et al. , 2005 ).', 'japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",0
"['turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']","['overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 detailed specifications of the feature models and learning algorithm parameters can be found on the maltparser web page.', 'before we turn to swedish and turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'if we start by considering languages with a labeled attachment score of 85 % or higher, they are characterized by high precision and recall for root nodes, typically 95 / 90, and by a graceful degradation of attachment score as arcs grow longer, typically 95 - 90 - 85, for arcs of length 1, 2 and 3 - 6.', 'typical examples are bulgarian (Simov et al., 2005;Simov and Osenova, 2003), chinese (Chen et al., 2003), danish (Kromann, 2003), and swedish.', 'japanese ( #AUTHOR_TAG ), despite a very high accuracy, is different in that attachment score drops from 98 % to 85 %, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances']",1
"['', '• history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', '• graph transformations for recovering nonprojective structures']","['', '• history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', '• graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', '• graph transformations for recovering nonprojective structures']","['a deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• history - based feature models for predicting the next parser action (Black et al., 1992).', 'support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ).', '• graph transformations for recovering nonprojective structures']",5
"['##jic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']","['results for arabic ( hajic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']","['##jic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']","['results for arabic ( hajic et al., 2004 ; smrz et al., 2002 ) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length ( from about 93 % for length 1 to 67 % for length 2 ).', 'by contrast, turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ).', 'it is noteworthy that arabic and turkish, being "" typological outliers "", show patterns that are different both from each other and from most of the other languages']",1
"['.', '#AUTHOR_TAG have previously examined the task of categor']","['the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov']","['provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['from them.', 'for example, #AUTHOR_TAG experiment']","['from them.', 'for example, #AUTHOR_TAG experimented with abstracts and full article texts in the task of']","['question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, #AUTHOR_TAG experiment']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence']","['( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected data']","['( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected']","['an attempt to further boost performance, we employed linear discriminant analysis ( lda ) to find a linear projection of the four - dimensional vec - tors that maximizes the separation of the gaussians ( corresponding to the hmm states ).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the lda transform matrix, which entails computing the withinand between - covariance matrices of the classes, and using singular value decomposition ( svd ) to compute the eigenvectors of the new space.', 'each sentence / vector is then mul - tiplied by this matrix, and new hmm models are re - computed from the projected data']",5
"['- valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc']","['content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc Keown et al. , 2003 ).', 'we']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc Keown et al. , 2003 ).', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( mc Keown et al. , 2003 ).', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
"['example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['approaches ( especially svms ) have been shown to be very effective for many supervised classification tasks ; see, for example, ( Joachims , 1998 ; #AUTHOR_TAG ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'in fact, we demonstrate that hmms are competitive with svms, with the added advantage of lower computational complexity.', 'in addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'in the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs']",0
"['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four']","['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four']","['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four states in our hmms correspond to the information that characterizes each section ( "" introduction "", "" methods "", "" results "", and "" conclusions "" ) and state transitions capture the discourse flow from section to section']","['and #AUTHOR_TAG, we employed hidden markov models to model the discourse structure of medline abstracts.', 'the four states in our hmms correspond to the information that characterizes each section ( "" introduction "", "" methods "", "" results "", and "" conclusions "" ) and state transitions capture the discourse flow from section to section']",5
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( Mizuta et al. , 2005 ), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['##s mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a vit']","['connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']","['connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']","['then built a four - state hidden markov model that outputs these four - dimensional vectors.', 'the transition probability matrix of the hmm was initialized with uniform probabilities over a fully connected graph.', 'the output probabilities were modeled as four - dimensional gaussians mixtures with diagonal covariance matrices.', 'using the section labels, the hmm was trained using the htk toolkit ( #AUTHOR_TAG ), which efficiently performs the forward - backward algorithm and baumwelch estimation.', 'for testing, we performed a viterbi ( maximum likelihood ) estimation of the label of each test sentence / vector ( also using the htk toolkit )']",5
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in un - structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; oreasan, 2001 ).', 'the ability to explicitly identify these sections in un - structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'demner - Fushman et al. (2005) found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts']",0
"['entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second,']","['entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second,']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools (']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in ( #AUTHOR_TAG ).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition (Evermann et al., 2004)']",1
"['example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task']","['approaches ( especially svms ) have been shown to be very effective for many supervised classification tasks ; see, for example, ( #AUTHOR_TAG ; Ng and Jordan , 2001 ).', 'however, their high computational complexity ( quadratic in the number of training samples ) renders them prohibitive for massive data processing.', 'under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'since hmms are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'in fact, we demonstrate that hmms are competitive with svms, with the added advantage of lower computational complexity.', 'in addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'in the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs']",0
"['of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction']","['of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction']","['##s ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( #AUTHOR_TAG ) for concept identification and semrep ( Rindflesch and Fiszman , 2003 ) for relation extraction - - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']",1
"['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', 'information that satisfies']","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a""]","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( mc Keown, 1985;Marcu and Echihabi, 2002).', 'our task is closer to the work of #AUTHOR_TAG, who looked at the problem of intellectual attribution in scientific texts']",1
"['is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we']","['is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we chose to focus on randomized controlled trials']","['exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'in truth, our crossvalidation experiments do not correspond to any meaningful naturally - occurring task - structured abstracts are, after all, already appropriately labeled.', 'the true utility of content models is to structure abstracts that have no structure to begin with.', 'thus, our exploratory experiments in applying content models trained with structured rcts on unstructured rcts is a closer approximation of an extrinsically - valid measure of performance.', 'such a component would serve as the first stage of a clinical question answering system ( demner - Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ).', 'we chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured']",3
['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],['( b ) again reproduces the results from #AUTHOR_TAG ( 2003 ) for a comparable task on a different subset of 206 unstructured abstracts'],1
"['( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( Tbahriti et al. , 2005 ), information extraction ( #AUTHOR_TAG ), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', 'information that satisfies']","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a""]","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ),']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ), information retrieval ( #AUTHOR_TAG ), information extraction ( Mizuta et al. , 2005 ), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( hence']","['lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( henceforth, m & s ) created a test collection consisting of 37, 151 rcts from approximately 12 million medline abstracts dated between 1976 and 2001.', 'this collection has significantly more training examples']","['lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( hence']","['results of our second set of experiments ( with rcts only ) are shown in tables 2 ( a ) and 2 ( b ). table 2 ( a ) reports the multi - way classification error rate ; once again, applying the markov assumption to model discourse transitions improves performance, and using lda further reduces error rate.', 'table 2 ( b ) reports accuracy, precision, recall, and f - measure for four separate binary classifiers ( hmm with lda ) specifically trained for each of the sections ( one per row in the table ).', 'the table also presents the closest comparable experimental results reported by #AUTHOR_TAG. 1 mcknight and srinivasan ( henceforth, m & s ) created a test collection consisting of 37, 151 rcts from approximately 12 million medline abstracts dated between 1976 and 2001.', 'this collection has significantly more training examples than our corpus of 27, 075 abstracts, which could be a source of performance differences.', 'furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.', 'nevertheless, our hmm - based approach is at least competitive with svms, perhaps better in some cases']",1
"['rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']","['this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ).', 'our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts']",1
"['( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used']","[') ; cfxxx ( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used']","[') ; cfxxx ( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. ( 2003 ) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cfxxx ( #AUTHOR_TAG ).', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how']","['above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how']","['above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( #AUTHOR_TAG ).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', 'information that satisfies']","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a""]","['above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system (']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) (Lindberg et al., 1993), and the availability of software that leverages this knowledge - metamap (Aronson, 2001) for concept identification and semrep (Rindflesch and Fiszman, 2003) for relation extraction - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction']","['of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction']","['##s ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction']","['types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'as an example, scientific abstracts across many different fields generally follow the pattern of "" introduction "", "" methods "", "" results "", and "" conclusions "" ( salanger - Meyer, 1990;Swales, 1990; orasan, 2001 ).', 'the ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'for example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7. 4 % improvement in f - score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'this paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four - section pattern discussed above ( salanger - Meyer, 1990).', 'for a variety of reasons, medicine is an interesting domain of research.', 'the need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""information that satisfies physicians'needs can be found in the medline database maintained by the u. s. national library of medicine ( nlm ), which also serves as a readily available corpus of abstracts for our experiments."", 'furthermore, the availability of rich ontological resources, in the form of the unified medical language system ( umls ) ( Lindberg et al. , 1993 ), and the availability of software that leverages this knowledge - - metamap ( Aronson , 2001 ) for concept identification and semrep ( #AUTHOR_TAG ) for relation extraction - - provide a foundation for studying the role of semantics in various tasks.', 'mc Knight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of medline abstracts using hidden markov models ( hmms ) ; cf.', '(Barzilay and Lee, 2004).', 'although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well']",0
"['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in']","['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'whereas barzilay and lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'nevertheless, their work bolsters our claims']","['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in']","['not the first to employ a generative approach to directly model content, the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison.', 'however, our study differs in several important respects.', 'barzilay and lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'in contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'whereas barzilay and lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here']",1
"['. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']","['allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']","['. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']","['interesting aspect of our generative approach is that we model hmm outputs as gaussian vectors ( log probabilities of observing entire sentences based on our language models ), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'this technique provides two important advantages.', 'first, gaussian modeling adds an extra degree of freedom during training, by capturing second - order statistics.', 'this is not possible when modeling word sequences, where only the probability of a sentence is actually used in the hmm training.', 'second, using continuous distributions allows us to leverage a variety of tools ( e. g., lda ) that have been shown to be successful in other fields, such as speech recognition ( #AUTHOR_TAG )']",0
"['( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi - technical text on meteorological phenomena ( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']","['work with a semi - technical text on meteorological phenomena ( #AUTHOR_TAG ), meant for primary school students.', 'the text gradually introduces concepts related to precipitation, and explains them.', 'its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'the system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text']",5
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic information is translated into features which, together with information from framenet, wordnet or verbnet, will be used with ml tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005)']","['process a pair p not encountered previously, the system builds a graph centered on the main element ( often the head ) of p.', ""this idea was inspired by #AUTHOR_TAG, who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles ( case relations )."", 'in recent approaches, syntactic information is translated into features which, together with information from framenet, wordnet or verbnet, will be used with ml tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005)']",4
"['arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet ( #AUTHOR_TAG ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 )']",0
"['.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG']","['of its arguments, a noun and each of its modifiers.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG']","['.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG']","['work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.', 'our system incrementally processes a text, and extracts pairs of text units : two clauses, a verb and each of its arguments, a noun and each of its modifiers.', 'for each pair of units, the system builds a syntactic graph surrounding the main element ( main clause, head verb, head noun ).', 'it then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'if such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'we have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'the list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent ( #AUTHOR_TAG a )']",4
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['build complex knowledge bases by combining components : events, entities and modifiers ( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]","['the rapid knowledge formation project ( rkf ) a support system was developed for domain experts.', 'it helps them build complex knowledge bases by combining components : events, entities and modifiers ( #AUTHOR_TAG ).', ""the system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary""]",0
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['##ini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurface']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965;  fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)']","['##ini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965;  fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century b. c. and the work of panini1.', 'he was a grammarian who analysed sanskrit ( #AUTHOR_TAG ).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965;  fill - more, 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005)']",0
"['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 )']",0
"['through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']","['this work we pursue a well - known and often tacitly assumed line of thinking : connections at the syntactic level reflect connections at the semantic level ( in other words, syntax carries meaning ).', 'anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966;Gruber, 1965;Fillmore, 1968).', 'tesniere ( 1959 ), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants - for example, agent or instrument - to such grammatical elements as subject, direct object, indirect object.', 'this idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 )']",0
"['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']","['current work on semantic relation analysis, the focus is on semantic roles - relations between verbs and their arguments.', 'most approaches rely on verbnet ( Kipper et al. , 2000 ) and framenet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG )']",0
"['are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']","['are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']","['syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information, dipett ( #AUTHOR_TAG ).', 'the parser, written in prolog, implements a classic constituency english grammar from Quirk et al. (1985).', 'pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'a dependency parser would produce a similar output, but dipett also provides verb subcategorization information ( such as, for example, subject - verb - object or subject - verb - objectindirect object ), which we use to select the ( best ) matching syntactic structures']",5
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['##ini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']",0
"['forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['##ini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']","['analysing texts, it is essential to see how elements of meaning are interconnected.', 'this is an old idea.', 'the first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century b. c. and the work of panini 1.', 'he was a grammarian who analysed sanskrit (Misra, 1966).', 'the idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ).', 'now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005)']",0
"['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']","['system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'this design idea was adopted from tanka ( #AUTHOR_TAG b ).', 'the only manually encoded knowledge is a dictionary of markers ( subordinators, coordinators, prepositions ).', 'this resource does not affect the syntacticsemantic graph - matching heuristic']",5
"['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ).', 'lists of semantic relations are designed to capture salient domain information']",0
"['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']","['methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ).', 'in other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'such systems extract information from some types of syntactic units ( clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996) ; noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002) ).', 'lists of semantic relations are designed to capture salient domain information']",0
['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG'],['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG'],['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG'],"['list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAG a ).', 'three lists of relations for three syntactic levels - inter - clause, intra - clause ( case ) and nounmodifier relations - were next combined based on syntactic and semantic phenomena.', 'the resulting list is the one used in the experiments we present in this paper.', 'the relations are grouped by general similarity into 6 relation classes ( h denotes the head of a base np, m denotes the modifier ).', 'there is no consensus in the literature on a list of semantic relations that would work in all situations.', 'this is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.', 'all the relations in the list we use were necessary, and sufficient, for the analysis of the input text']",5
"['( #AUTHOR_TAG ), we tried to adapt the same approach']","['by the success of chunk - based verb reordering lattices on arabicenglish ( #AUTHOR_TAG ), we tried to adapt the same approach']","['by the success of chunk - based verb reordering lattices on arabicenglish ( #AUTHOR_TAG ), we tried to adapt the same approach']","['reordering between german and english is a complex problem.', 'encouraged by the success of chunk - based verb reordering lattices on arabicenglish ( #AUTHOR_TAG ), we tried to adapt the same approach to the german - english language pair.', 'it turned out that there is a larger variety of long reordering patterns in this case.', 'nevertheless, some experiments performed after the official evaluation showed promising results.', 'we plan to pursue this work in several directions : defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.', 'applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.', 'finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade - off between decoderdriven short - range and lattice - driven long - range reordering']",4
"['research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over']","['research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ), who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi - nonterminal grammars']",3
"['research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over']","['research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over']","['phrase - based mt suffers from spurious ambiguity : a single translation for a given source sentence can usually be accomplished by many different pscfg derivations.', 'this problem is exacerbated by syntax - augmented mt with its thousands of nonterminals, and made even worse by its joint source - and - target extension.', 'future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG, who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi - nonterminal grammars']",3
"['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be']","['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be']","['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored']","['our prior work ( #AUTHOR_TAG ), we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer - review domain, where the definition of helpfulness is largely influenced by the educational context of peer review.', 'while previously we used the average of two expert - provided ratings as our gold standard of peer - review helpfulness 1, there are other types of helpfulness rating ( e. g.', 'author perceived helpfulness ) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.', 'in fact, we observe that peer - review helpfulness seems to differ not only between students and experts ( example 1 ), but also between types of experts ( example 2 )']",2
"['.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']","['algorithms are provided by weka ( http : / / www. cs. waikato. ac. nz / ml / weka / ).', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']","[').', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']","['both algorithms are provided by weka ( http : / / www. cs. waikato. ac. nz / ml / weka / ).', 'provide complementary perspectives.', 'while the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'to compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( details of how the average - expert model performs can be found in our prior work ( #AUTHOR_TAG ).']",2
"['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see']","['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see']","['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ).', 'each word is broken down']","['selecting features for korean, we have to ac - count for relatively free word order (Chung et al., 2010).', 'we follow our previous work ( #AUTHOR_TAG ) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ).', 'each word is broken down into : stem, affixes, stem pos, and affixes pos.', 'we also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.', 'although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data ( section 3 ) and to provide a basis for a preliminary system ( section 4 )']",2
"['( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters']","['( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']","['. ( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']","['same annotation scheme as in our previous work on anger detection has been applied, see e. g. ( #AUTHOR_TAG ).', '(Schmitt et al., 2009).', 'from all 4, 832 user turns, 68. 5 % were non - angry, 14. 3 % slightly angry, 5. 0 % very angry and 12. 2 % contained garbage, i. e. nonspeech events.', 'in total, the number of interaction parameters servings as input variables for the model amounts to 52']",2
"[') each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']","['the actual word types in the bigram, and ( 3 ) each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']","[') each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']","['simple language model that discovers useful embeddings is known as brown clustering (Brown et al., 1992).', 'a brown clustering is a class - based bigram model in which ( 1 ) the probability of a document is the product of the probabilities of its bigrams, ( 2 ) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and ( 3 ) each word type has non - zero probability only on a single class.', 'given a one - toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'the brown clustering algorithm works by starting with an initial assignment of word types to classes ( which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus ), and then iteratively selecting the pair of classes to merge that would lead to the highest post - merge log - likelihood, doing so until all classes have been merged.', 'this process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ).', 'there are other similar models of distributional clustering of english words which can be similarly effective (Pereira et al., 1993)']",4
"['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as']","['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as']","['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as']","['#AUTHOR_TAG, we also compare the performance of our system with a system using features based on the brown clusters of the word types in a document.', 'since, as seen in section 2. 1, brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type']",5
"['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be']","['this section we describe in detail the baseline ner system we use.', 'it is inspired by the system described in #AUTHOR_TAG.', 'because ner annotations are commonly not nested ( for example, in the text "" the us army "", "" us army "" is treated as a single entity, instead of the location "" us "" and the organization "" us army "" ) it is possible to treat ner as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the bilou encoding, where each token can either begin an entity, be inside an entity, be the last token in an entity, be outside an entity, or be the single unique token in an entity']",4
"['by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be']","['by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be']","['capture syntagmatic patterns, we choose in this work standard n - gram language models as the basis for a concrete model implementing our scheme.', 'this choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly']","['hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'it is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993;Hanks, 2013).', 'this provides grounds to expect that such model has the potential to excel for verbs.', 'to capture syntagmatic patterns, we choose in this work standard n - gram language models as the basis for a concrete model implementing our scheme.', 'this choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ), which successfully utilized such language models to represent word window contexts of target words.', 'however, we note that other richer types of language models, such as class - based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme']",4
"['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is']","['of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is']","['of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is naturally complementary to these efforts, as their methods require a corpus of in - vocab text to serve as seed sentences']","['motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG, which deal with automatic generation of classic fill in the blank questions.', 'our work is naturally complementary to these efforts, as their methods require a corpus of in - vocab text to serve as seed sentences']",4
"['##g work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['##g work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan a et al., 2009 ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['##g work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']","['majority of nlg focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'our work is more similar to nlg work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( ramakrishnan #AUTHOR_TAG ), where specified meter or rhyme schemes are enforced.', 'in these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric']",1
"['. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean']","['. char - n - grams ( g ) : we start with a character n - gram - based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'following the work of King and Abney (2013), we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean']","['. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '']","['. char - n - grams ( g ) : we start with a character n - gram - based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'following the work of King and Abney (2013), we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. length of words ( l ) : instead of using the raw length value as a feature, we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean features to encode capitalization information : whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized']",2
"['available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean']","['available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean']","['( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '']","['. char - n - grams ( g ) : we start with a character n - gram - based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'following the work of King and Abney (2013), we select character n - grams ( n = 1 to 5 ) and the word as the features in our experiments.', '2. presence in dictionaries ( d ) : we use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature, we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( j48 ).', 'we use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. capitalization ( c ) : we use 3 boolean features to encode capitalization information : whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized']",2
"['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts']","['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts']","['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts']","['shape - based metric.', ""the only disambiguation metric that we used in our previous work ( #AUTHOR_TAG b ) was the shape - based metric, according to which the ` ` best'' trees are those that are skewed to the right."", 'the explanation for this metric is that text processing is, essentially, a left - to - right process.', 'in many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'i the more text writers add, the more they elaborate on the text that went before : as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'according to the shape - based metric, we consider that a discourse tree a is "" better "" than another discourse tree b if a is more skewed to the right than b ( see Marcu (1997 c ) for a mathematical formulation of the notion of skewedness )']",2
"['.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel']","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel texts on the web.', ""we will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in clir""]","[', a major obstacle to this approach is the lack of parallel corpora for model training.', 'only a few such corpora exist, including the hansard english - french corpus and the hkust englishchinese corpus ( #AUTHOR_TAG ).', 'in this paper, we will describe a method which automatically searches for parallel texts on the web.', ""we will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in clir""]",0
"['.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"['.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']","['also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']","['.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']","['html markups, other criteria may also be incorporated.', 'for example, it would be helpful to consider strong correspondence between certain english and chinese words, as in ( #AUTHOR_TAG ).', 'we hope to implement such correspondences in our future research']",3
"['.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english']","['parallel web pages we collected from various sites are not all of the same quality.', 'some are highly parallel and easy to align while others can be very noisy.', 'aligning english - chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'a number of alignment techniques have been proposed, varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ).', 'the method we adopted is that of Simard et al. (1992).', 'because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length - based methods.', 'cognates are identical sequences of characters in corresponding words in two languages.', 'they are commonly found in english and french.', 'in the case of english - chinese alignment, where there are no cognates shared by the two languages, only the html markup in both texts are taken as cognates.', 'because the html structures of parallel pages are normally similar, the markup was found to be helpful for alignment']",0
"[""##es is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]","['first system we have implemented with ape is a prototype atlas - andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'figure 4 shows the architecture of atlas - andes ; any other system built with ape would look similar.', ""robust natural language understanding in atlas - andes is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]","[""##es is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]","['first system we have implemented with ape is a prototype atlas - andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'figure 4 shows the architecture of atlas - andes ; any other system built with ape would look similar.', ""robust natural language understanding in atlas - andes is provided by rosa©'s carmel system ( rosa© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG""]",5
"[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]","[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]","[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]","[""see ( #AUTHOR_TAG ) for how mimic's dialoguelevel knowledge is used to override default prosodic assignments for concept - to - speech generation""]",0
"['#AUTHOR_TAG ) ).', 'to instantiate an attribute,']","['#AUTHOR_TAG ) ).', 'to instantiate an attribute,']","['#AUTHOR_TAG ) ).', 'to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing information.', 'in contrast, when mimic has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute ( giveopt']","['strategies employed when mimic has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e. g., ( bennacef et at., 1996 ; #AUTHOR_TAG ) ).', 'to instantiate an attribute, mimic adopts the lnfoseek dialogue act to solicit the missing information.', 'in contrast, when mimic has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute ( giveoptions ).', 'given an invalid query, mimic notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative.', ""when mimic has both initiatives, however, in addition to no - tifyfailure, it suggests an alternative close to the user's original query and provides a limited prompt."", 'finally, when mimic has neither initiative, it simply adopts no - tifyfailure, allowing the user to determine the next discourse goal']",1
"['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative affects the cur - 5an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1']",0
"['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative']","['work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ).', 'thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'since task initiative models contribution to domain / problemsolving goals, while dialogue initiative affects the cur - 5an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in figure 4 based on initiative distribution, as shown in table 1']",0
"['paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user']","['paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user']","['describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme (Walker et al., 1997), were automatically log']","[""conducted two experiments to evaluate mimic's automatic adaptation capabilities."", 'we compared mimic with two control systems : mimic - si, a system - initiative version of mimic in which the system retains both initiatives throughout the dialogue, and mimic - mi, a nonadaptive mixed - initiative version of mimic that resembles the behavior of many existing dialogue systems.', 'in this section we summarize these experiments and their results.', 'a companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']",2
"['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']","['##an alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ), which we leave for future work']",3
"['was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were']","['was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were']","['describes the evaluation process and results in further detail ( chu - Carroll and Nickerson, 2000).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were automatically log']","[""conducted two experiments to evaluate mimic's automatic adaptation capabilities."", 'we compared mimic with two control systems : mimic - si, a system - initiative version of mimic in which the system retains both initiatives throughout the dialogue, and mimic - mi, a nonadaptive mixed - initiative version of mimic that resembles the behavior of many existing dialogue systems.', 'in this section we summarize these experiments and their results.', 'a companion paper describes the evaluation process and results in further detail ( chu - Carroll and Nickerson, 2000).', 'each experiment involved eight users interacting with mimic and mimic - si or mimic - mi to perform a set of tasks, each requiring the user to obtain specific movie information.', 'user satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'furthermore, a number of performance features, largely based on the paradise dialogue evaluation scheme ( #AUTHOR_TAG ), were automatically logged, derived, or manually annotated.', 'in addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response']",5
"['##issa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech -']","['traditional desktop applications - - this is similar to the goals of the melissa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows']","['enable traditional desktop applications - - this is similar to the goals of the melissa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows']","['systems to assist in the development of spoken - langnage systems ( slss ) have focused on building stand - alone, customized applications, such as (Sutton et al., 1996) and (Pargellis et al., 1999).', 'the goal of the javox toolkit is to speech - enable traditional desktop applications - - this is similar to the goals of the melissa project ( #AUTHOR_TAG ).', 'it is intended to both speed the development of slss and to localize the speech - specific code within the application.', 'javox allows developers to add speech interfaces to applications at the end of the development process ; slss no longer need to be built from the ground up']",1
"['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints (Ganssier, 1995;Kupiec, 1993; hua chen and chen, 94 ; Fung, 1995;Evans and Zhai, 1996).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ;  lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part - of - speech tags.', ""an excerpt of the association probabilities of a unit model trained considering only the np - sequences is given in table 3. applying this filter ( referred to as jrnp in the following ) to the 39, 093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35, 939 of them ( 92 % )."", '4 : completion results of several translation models, spared : theoretical proportion of characters saved ; ok : number of target units accepted by the user ; good : number of target units that matched the expected whether they were proposed or not ; nu : number of sentences for which no target unit was found by the translation model ; u : number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed']",0
"['; #AUTHOR_TAG ; Russell , 1998 )']","['; #AUTHOR_TAG ; Russell , 1998 )']","['; #AUTHOR_TAG ; Russell , 1998 )']","['relevant units in a text has been explored in many areas of natural language processing.', 'our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'for sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus.', 'this method allows the efficient retrieval of arbitrary length n - grams ( nagao and mori, 94 ; haruno et al., 96 ; ikehaxa et al., 96 ; #AUTHOR_TAG ; Russell , 1998 )']",0
"['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered']","['way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun - phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua chen and chen, 94 ; Fung , 1995 ; Evans and Zhai , 1996 ).', 'it is also possible to focus on non - compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997; lin, 99 ).', 'another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; furuse and iida, 96 ).', 'in this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part - of - speech tags.', ""an excerpt of the association probabilities of a unit model trained considering only the np - sequences is given in table 3. applying this filter ( referred to as jrnp in the following ) to the 39, 093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35, 939 of them ( 92 % )."", '4 : completion results of several translation models, spared : theoretical proportion of characters saved ; ok : number of target units accepted by the user ; good : number of target units that matched the expected whether they were proposed or not ; nu : number of sentences for which no target unit was found by the translation model ; u : number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed']",5
"['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and']","['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and']","['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and']","['speech and language processing architecture is based on that of the sri commandtalk system ( #AUTHOR_TAG ; stent et a., 1999 ).', 'the system comprises a suite of about 20 agents, connected together using the spd open agent architecture ( oaa ; (Martin et al., 1998) ).', 'speech recognition is performed using a version of the nuance recognizer (Nuance, 2000).', 'initial language processing is carried out using the sri gemini system (Dowding et al., 1993), using a domain ~ independent unification grammar and a domain - specific lexicon.', 'the language processing grammar is compiled into a recognition grarnm ~ kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized.', 'output from the initial language - processing step is represented in a version of quasi logical form ( van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'we refer to these as linguistic level representations']",5
"['interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they']","['to semi - antonomous robots include srrs flakey robot (Konolige et al., 1993) and ncarars interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they']","['basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'as evidence of the importance of this task in the nlp community note that the early, influential system shrdlu (Winograd, 1973) was intended to address just this type of problem.', 'more recent work on spoken language interfaces to semi - antonomous robots include srrs flakey robot (Konolige et al., 1993) and ncarars interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi -']","['basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'as evidence of the importance of this task in the nlp community note that the early, influential system shrdlu (Winograd, 1973) was intended to address just this type of problem.', 'more recent work on spoken language interfaces to semi - antonomous robots include srrs flakey robot (Konolige et al., 1993) and ncarars interbot project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'a number of other systems have addressed part of the task.', 'cornmandtalk ( #AUTHOR_TAG ), circuit fix - it shop ( Smith , 1997 ) and trains - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi - autonomous agents.', ""jack's moose lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi - autonomous."", 'other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995;Pyre et al., 1995).', 'in most of this and other related work the treatment is some variant of the following.', 'if there is a speech interface, the input speech signal is converted into text.', ""text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command ; this formula is then fed into a command interpreter, which executes the command""]",0
"['toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['by an alignment program.', 'this alignment is done on the basis of both length ( gale and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['##e and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['.', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church [ 7 ] ) and a notion of cognateness ( simard [ 161 ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo #AUTHOR_TAG ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']",5
"['##e and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2']","['by an alignment program.', 'this alignment is done on the basis of both length ( gale and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['##e and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']","['.', 'an aligner.', 'after identification of word and sentence boundaries the text is processed into a bi - text by an alignment program.', 'this alignment is done on the basis of both length ( gale and church #AUTHOR_TAG ) and a notion of cognateness ( simard [ 16 ] ).', '2. transducers.', 'in order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'this is done with transducers ( kaplan and kay, [ 10 ] ).', '3. part - of - speech tagger.', 'misleading similarities in graphical form can sometime induce translation mistakes ( deceptive cognates ).', '~ these forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'we do this with a first - order hmm part - ofspeech tagger ( merialdo [ 13 ] ).', 'i in the rest of the paper, we will use deceptive cognate very iosely often to refer to normative usage of word in general']",5
"['##¢ before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the']","['##¢ before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the']","['##¢ before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the text "" for 5 centuries "" matches the durations pattern "" for : cardinal _ timeperiod "", where : car - dinal is the label for cardinal numbers, and _ timeperiod marks a time expression']","['##¢ before indexing the text, we process it with textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ), which performs lemmatization, and discovers proper names and technical terms.', 'we added a new module ( resporator ) which annotates text segments with qa - tokens using pattern matching.', 'thus the text "" for 5 centuries "" matches the durations pattern "" for : cardinal _ timeperiod "", where : car - dinal is the label for cardinal numbers, and _ timeperiod marks a time expression']",5
['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],"['##ana : morphological analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german ( #AUTHOR_TAG )']","['##ana : morphological analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'we are using a lexicon of approx.', '100000 word stems of german ( #AUTHOR_TAG )']",5
"['text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines']","['text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines']","['preprocessing of text documents is carried out by re - using smes, an information extraction core system for real - world german text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines']","['preprocessing of text documents is carried out by re - using smes, an information extraction core system for real - world german text processing ( #AUTHOR_TAG ).', 'the fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient stp components and 4almost all tools we examined build a single multicategorizer except for svm - light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser.', 'the chunk parser itself is subdivided into three components.', 'in the first step, phrasal fragments like general nominal expressions and verb groups are recognized.', 'next, the dependency - based structure of the fragments of each sentence is computed using a set of specific sentence patterns.', 'third, the grammatical functions are determined for each dependency - based structure on the basis of a large subcategorization lexicon.', 'the present application benefits from the high modularity of the usage of the components.', 'thus, it is possible to run only a subset of the components and to tailor their output.', 'the experiments described in section 4 make use of this feature']",5
"['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps']","['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps - - non - recursive noun phrases - - using a very simple algorithm that matches part - of - speech tag sequences based on a learned noun phrase grammar.', 'the approach is able to achieve 94 % precision and recall for base nps derived from the penn treebank wall street journal (Marcus et al., 1993).', 'in the experiments below, the np filter follows the application of the document retrieval and text summarization components.', 'pronoun answer hypotheses are discarded, and the nps are assembled']","['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps']","['np - based qa system.', 'our implementation of the np - based qa system uses the empire noun phrase finder, which is described in detail in #AUTHOR_TAG.', 'empire identifies base nps - - non - recursive noun phrases - - using a very simple algorithm that matches part - of - speech tag sequences based on a learned noun phrase grammar.', 'the approach is able to achieve 94 % precision and recall for base nps derived from the penn treebank wall street journal (Marcus et al., 1993).', 'in the experiments below, the np filter follows the application of the document retrieval and text summarization components.', 'pronoun answer hypotheses are discarded, and the nps are assembled into 50 - byte chunks']",5
"['text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily']","['next hypothesize that query - dependent text summarization algorithms will improve the performance of the qa system by focusing the system on the most relevant portions of the retrieved documents.', 'the goal for query - dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'although a number of methods for query - dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ), we again propose the use of vector space methods from ir, which can be easily extended to the summarization task ( Salton et al. , 1994 )']",1
"['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', '']","['aim of this paper is to give a detailed account of the techniques used in tnt.', 'additionally, we present results of the tagger on the negra corpus (Brants et al., 1999) and the penn treebank (Marcus et al., 1993).', 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ( #AUTHOR_TAG ).', 'for a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999)']",1
"['of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['and the english corpus.', 'they do so for several other corpora as well.', 'the architecture remains applicable to a large variety of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']","['have shown that a tagger based on markov models yields state - of - the - art results, despite contrary claims found in the literature.', 'for example, the markov model tagger used in the comparison of ( van Halteren et al., 1998) yielded worse results than all other taggers.', 'in our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor.', 'the rather large amount of freedom was not handled in detail in previous publications : handling of start - and end - of - sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.', 'note that the decisions we made yield good results for both the german and the english corpus.', 'they do so for several other corpora as well.', 'the architecture remains applicable to a large variety of languages.', 'according to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ), and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ), the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here.', 'it is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both']",1
"['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]","['similarity measures were examined.', 'the cosine coefficient ( r98 ( s, co, ) ) and dot density measure ( r98 ( m, ( lot ) ) yield similar results.', 'our spread activation based semantic measure ( r98 (....., ) ) improved a. ccura ( : y.', ""this confirms that although kozima's approach ( #AUTHOR_TAG ) is computationally expensive, it does produce more precise segmentation""]",1
"[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']","[""##98 (,,,, a ) uses a variant of kozima's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity."", 'word similarity is a function of word co - occurrence statistics in the given document.', 'words that belong to the same sentence are considered to be related.', 'given the co - occurrence frequencies f ( wi, wj ), the transition probability matrix t is computed by equation 10.', 'equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm ( y ) converts a matrix y into a transition matrix, x = 5 was used in the experiment']",2
"['the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']","['the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']","['the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']","['related work, Miyao (1999) describes an ap - proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG, who report large speed - ups from the elimination of disjunction processing during unification.', 'unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison']",1
"['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']","['this paper we have provided an original mathematical argument in favour of this thesis.', 'our results hold for bilexical context - free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism ( see section 1 ).', 'we perceive that these results can be extended to other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ).', 'we leave this for future work']",3
"['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']","['other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']","['this paper we have provided an original mathematical argument in favour of this thesis.', 'our results hold for bilexical context - free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism ( see section 1 ).', 'we perceive that these results can be extended to other language models that properly embed bilexical context - free grammars, as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ).', 'we leave this for future work']",3
"['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not']","['is not the best measure to assess segmentation quality, therefore we also conducted experiments using the windowdiff measure as proposed by #AUTHOR_TAG.', 'windowdiff returns 0 in case of a perfect segmentation ; 1 is the worst possible score.', 'however, it only takes into account segment boundaries and disregards segment types.', 'in section 5. 2, we mentioned that loopy bp is not guaranteed to converge in a finite number of iterations.', 'since we optimize pseudolikelihood for parameter estimation, we are not affected by this limitation in the training phase.', 'however, we use loopy bp with a trp schedule during testing, so we must expect to encounter non - convergence for some examples.', 'theoretical results on this topic are discussed by Heskes (2004).', 'we give here an empirical observation of convergence behaviour of loopy bp in our setting ; the maximum number of iterations of the trp schedule was restricted to 1, 000.', 'table 4 shows the percentage of examples converging within this limit and the average number of iterations required by the converging examples, broken down by the different corpora.', 'from these results, we conclude that there is a connection between the quality of the annotation and the convergence behaviour of loopy bp.', ""in practice, even though loopy bp didn't converge for some examples, the solutions after 1, 000 iterations where satisfactory""]",5
"['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart, • ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart, • ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart, • ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']","['##onyms : for some relations, there is no contradiction when y 1 and y 2 share a meronym, i. e. "" part of "" relation.', 'for example, in the set born in ( mozart, • ) there is no contradiction between the y values "" salzburg "" and "" austria "", but "" salzburg "" conflicts with "" vienna "".', 'although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ), in practice genuine contradictions between y - values sharing a meronym relationship are extremely rare.', 'we therefore simply assigned contradictions between meronyms a probability close to zero.', 'we used the tipster gazetteer 4 and wordnet to identify meronyms, both of which have high precision but low coverage']",4
"['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical φ - features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical φ - features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical φ - features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']","['train our model, we use l - bfgs to locally maximize the log of the objective function ( 1 ) : 15 these are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the o - i tag transition at more than twice the average rate.', 'we do not use any other lexical φ - features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired ( see the end of section 5. 3 ). 14', 'we parse each sentence with the collins parser ( #AUTHOR_TAG ).', 'then the document has one big parse tree, whose root is doc, with each sentence being a child of doc']",5
"[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']","[', we introduced the movie review polarity dataset enriched with annotator rationales. 8', 'it is based on the dataset of #AUTHOR_TAG, 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds ( f0 - - f9 ).', 'all our experiments use f9 as their final blind test set']",2
"['θ.', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principle']","['θ.', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks']","['to a machine learner by highlighting contextual "" rationales "" for each of his or her annotations (Zaidan et al., 2007).', 'how can one exploit this side information to better learn the desired parameters θ?', 'we present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales.', 'thus, observing the rationales helps us infer the true θ.', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principle']","['human annotator can provide hints to a machine learner by highlighting contextual "" rationales "" for each of his or her annotations (Zaidan et al., 2007).', 'how can one exploit this side information to better learn the desired parameters θ?', 'we present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales.', 'thus, observing the rationales helps us infer the true θ.', 'we collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator.', 'our new generative approach exploits the rationales more effectively than our previous "" masking svm "" approach.', 'it is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks']",5
"['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', '']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discourage y = −1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', '']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ).', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discourage y = −1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']",5
"['##sim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using #AUTHOR_TAG']","['et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using #AUTHOR_TAG']","['et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', 'we gather similar words using #AUTHOR_TAG']","['first evaluate d s p on disambiguating positives from pseudo - negatives, comparing to recently - proposed systems that also require no manually - compiled resources like wordnet.', 'we convert da - gan et al. ( 1999 ) s similarity - smoothed probability to mi by replacing the empirical pr n | v in equa - tion ( 2 ) with the smoothed prsim from equation ( 1 ).', 'we also test an mi model inspired by Erk (2007) : misim n ; v = log x simn _ ; n prv ; n _ n _ _ sims n', ""we gather similar words using #AUTHOR_TAG a ), mining similar verbs from a comparable - sized parsed corpus, and collecting similar nouns from a broader 10 gb corpus of english text. 4 we also use Keller and Lapata ( 2003 )'s approach to obtaining web - counts""]",5
['by #AUTHOR_TAG. 2this'],['by #AUTHOR_TAG. 2this'],"['by #AUTHOR_TAG. 2this data provides counts for pairs such as "" edwin moses, hurdler "" and "" william farley, industrialist.', 'we have features for all concepts and therefore learn their association with each verb']","['also made use of the person - name / instance pairs automatically extracted by #AUTHOR_TAG. 2this data provides counts for pairs such as "" edwin moses, hurdler "" and "" william farley, industrialist.', 'we have features for all concepts and therefore learn their association with each verb']",5
"[""that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see foot""]","[', we evaluate dsp on a common application of selectional preferences : choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""we study the cases where a 9recall that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", 'pronoun is the direct object of a verb predicate, v.', ""a pronoun's antecedent must obey v's selectional preferences."", 'if we have a better model of sp, we should be able to better select pronoun antecedents']","[""that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", 'pronoun is the direct object of a verb predicate, v.', ""a pronoun's antecedent must obey v's selectional preferences."", 'if we have a better model of sp, we should be able to better select pronoun antecedents']","[', we evaluate dsp on a common application of selectional preferences : choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""we study the cases where a 9recall that even the #AUTHOR_TAG system, built on the world's largest corpus, achieves only 34 % recall ( table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed, but see footnote 5 )."", 'pronoun is the direct object of a verb predicate, v.', ""a pronoun's antecedent must obey v's selectional preferences."", 'if we have a better model of sp, we should be able to better select pronoun antecedents']",1
"['( n | garnish ).', '#AUTHOR_TAG']","['example, place high weight on features like pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', '#AUTHOR_TAG']","['( n | garnish ).', '#AUTHOR_TAG']","['is interesting to inspect the feature weights returned by our system.', 'in particular, the weights on the verb co - occurrence features ( section 3. 3. 1 )', 'provide a high - quality, argument - specific similarityranking of other verb contexts.', 'the dsp parameters for eat, for example, place high weight on features like pr ( n | braise ), pr ( n | ration ), and pr ( n | garnish ).', ""#AUTHOR_TAG a )'s similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ), because these have similar subjects to eat."", 'discriminative, context - specific training seems to yield a better set of similar predicates, e. g. the highest - ranked contexts for dsp cooc on the verb join, 3 lead 1. 42, rejoin 1. 39, form 1. 34, belong to 1. 31, found 1. 31, quit 1. 29, guide 1. 19, induct 1. 19, launch ( subj ) 1. 18, work at 1. 14 give a better sims ( join ) for equation ( 1 ) than the top similarities returned by (Lin, 1998 a other features are also weighted intuitively.', 'note that case is a strong indicator for some arguments, for example the weight on being lower - case is high for become ( 0. 972 ) and eat ( 0. 505 ), but highly negative for accuse ( - 0. 675 ) and embroil ( - 0. 573 ) which often take names of people and organizations']",0
"['from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']","['available from the ldc as ldc2006t13.', 'this collection was generated from approximately 1 trillion tokens of online text.', 'unfortunately, tokens appearing less than 200 times have been mapped to the unk symbol, and only n - grams appearing more than 40 times are included.', 'unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'the similarity - smoothed examples will be undefined if sims ( w ) is empty.', 'also, the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web.', 'as a reasonable default for these cases, we assign them a negative decision']",5
"['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this data consists of triples ( v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'this data consists of triples ( v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'the models score correctly if they rank observed ( and thus plausible ) arguments above corresponding unobserved ( and thus likely implausible ) ones.', 'we refer to this as pairwise disambiguation.', 'unlike this task, we classify each predicate - argument pair independently as plausible / implausible.', 'we also use mi rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise.']",1
"['the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few meta']","['the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric']","['the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric']","['advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG.', 'they optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric']",1
"['by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']","['by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']","['by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a for a fixed verb, mi is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of ( v, n, n ′ ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by #AUTHOR_TAG']",0
"['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this data consists of triples ( v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been']","['training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for sp models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'this data consists of triples ( v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'the models score correctly if they rank observed ( and thus plausible ) arguments above corresponding unobserved ( and thus likely implausible ) ones.', 'we refer to this as pairwise disambiguation.', 'unlike this task, we classify each predicate - argument pair independently as plausible / implausible.', 'we also use mi rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise.']",1
"['predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']","['predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']","['predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']","['approaches to sps generalize from observed predicate - argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'for example, we might have a class mexican food and learn that the entire class is suitable for eating.', 'usually, the classes are from wordnet ( Miller et al. , 1990 ), although they can also be inferred from clustering ( #AUTHOR_TAG ).', 'Brockmann and Lapata (2003) compare a number of wordnet - based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class - based approaches do not always outperform simple frequency - based models']",0
"['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus,']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'next, we omit verb co - occurrence features for nouns that occur less than 10 times, and instead fire a low - count feature.', 'when we move to a new corpus, previouslyunseen nouns are treated like these low - count training nouns']",1
"['have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ] ⇒ [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in using']","['have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ] ⇒ [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in using']","['##al preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ] ⇒ [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in']","['##al preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ).', 'inferences such as "" [ x wins y ] ⇒ [ x plays y ] "" are only valid for certain argu - ments x and y.', 'we follow Pantel et al. (2007) in using automatically - extracted semantic classes to help characterize plausible arguments']",0
"['##s ( w ).', 'Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG']","['sim ( v ′, v ) returns a real - valued similarity between two verbs v ′ and v ( normalized over all pair similarities in the sum ).', 'in contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross - product of similar pairs.', 'one key issue is how to define the set of similar words, sims ( w ).', 'Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG']","['##s ( w ).', 'Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG']","['sim ( v ′, v ) returns a real - valued similarity between two verbs v ′ and v ( normalized over all pair similarities in the sum ).', 'in contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross - product of similar pairs.', 'one key issue is how to define the set of similar words, sims ( w ).', ""Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the jaccard coefficient and #AUTHOR_TAG a )'s information - theoretic metric work best."", 'similarity - smoothed models are simple to compute, potentially adaptable to new domains, and require no manually - compiled resources such as wordnet']",0
"['the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, τ, to be 0, and the negative - to - positive ratio, k, to be 2']","['parsed the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, τ, to be 0, and the negative - to - positive ratio, k, to be 2']","['parsed the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, τ, to be 0, and the negative - to - positive ratio, k, to be 2']","['parsed the 3 gb aquaint corpus ( Voorhees , 2002 ) using minipar ( #AUTHOR_TAG b ), and collected verb - object and verb - subject frequencies, building an empirical mi model from this data.', 'verbs and nouns were converted to their ( possibly multi - token ) root, and string case was preserved.', 'passive subjects ( the car was bought ) were converted to objects ( bought car ).', 'we set the mi - threshold, τ, to be 0, and the negative - to - positive ratio, k, to be 2']",5
"['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus,']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (']","['previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ).', 'presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'we wish to use our model on arguments of any frequency, including those that never occurred in the training corpus ( and therefore have empty cooccurrence features ( section 3. 3. 1 ) ).', 'we proceed as follows : first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'next, we omit verb co - occurrence features for nouns that occur less than 10 times, and instead fire a low - count feature.', 'when we move to a new corpus, previouslyunseen nouns are treated like these low - count training nouns']",1
"['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the mi between a verb predicate, v, and its object argument, n, is']","['learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'to create the positives, we automatically parse a large corpus, and then extract the predicate - argument pairs that have a statistical association in this data.', 'we measure this association using pointwise mutual information ( mi ) ( #AUTHOR_TAG ).', 'the mi between a verb predicate, v, and its object argument, n, is']",5
"['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", '']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", '']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", '']","['that is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f ( n ).', 'for example, a feature for a verb - object pair might be, "" the verb is eat and the object is lower - case. ""', 'in this representation, features for one predicate will be completely independent from those for every other predicate.', ""thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1for a fixed verb, mi is proportional to #AUTHOR_TAG's conditional probability scores for pseudodisambiguation of ( v, n, n [UNK] ) triples : pr ( v | n ) = pr ( v, n ) / pr ( n ), which was shown to be a better measure of association than co - occurrence frequency f ( v, n )."", 'normalizing by pr ( v ) ( yielding mi ) allows us to use a constant threshold across all verbs.', 'mi was also recently used for inference - rule sps by Pantel et al. (2007)']",4
"['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']","['is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG )']",1
"[', text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for instance, (Braz et al., 2005) represents t,']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for']","['entailment systems are given two textual fragments, text t and hypothesis h, and attempt to decide if the meaning of h can be inferred from the meaning of t (Dagan et al., 2005).', ""while many approaches have addressed this problem, our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ), which convert the inputs into logical forms and then attempt to ` prove'h from t plus a set of axioms."", 'for instance, (Braz et al., 2005) represents t, h, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation']",1
"['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the']","['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the']","['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full penn treebank tag set.', 'we ran all our estimators in both conditions here ( thanks to noah smith for supplying us with his tag set )']","['studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used.', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full penn treebank tag set.', 'we ran all our estimators in both conditions here ( thanks to noah smith for supplying us with his tag set )']",1
"['.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']","['be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']","['.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']","['might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'on small data sets all of the bayesian estimators strongly outperform em ( and, to a lesser extent, vb ) with respect to all of our evaluation measures, confirming the results reported in #AUTHOR_TAG.', 'this is perhaps not too surprising, as the bayesian prior plays a comparatively stronger role with a smaller training corpus ( which makes the likelihood term smaller ) and the approximation used by variational bayes is likely to be less accurate on smaller data sets']",1
"['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']","['resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG )']",1
"['mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the']","['mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the']","['mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', '']","['few kernels based on dependency trees have also been proposed.', 'Zelenko et al. (2003) proposed a tree kernel over shallow parse tree representations of sentences.', 'this tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'in addition to the words, this kernel also incorporates word classes into the kernel.', 'the kernel is based on counting matching subsequences of children of matching nodes.', 'but as was also noted in (Bunescu and Mooney, 2005 a ), this kernel is opaque i. e. it is not obvious what the implicit features are and the authors do not describe it either.', 'in contrast, our dependency - based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths.', 'their kernel is also very time consuming and in their more general sparse setting it requires o ( mn3 ) time and o ( mn2 ) space, where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ).', 'Bunescu and Mooney (2005 a ) give a shortest path dependency kernel for relation extraction.', 'their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'this kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'their kernel also uses word classes in addition to the words themselves']",3
"['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginal']","['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be']","['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'apart from a new decoder, it will be worthwhile adapting the prior probability in our model']","['are various strands of future research.', 'firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'secondly, as ( #AUTHOR_TAG ) show, marginalizing out the different segmentations during decoding leads to improved performance.', 'we plan to build our own decoder ( based on itg ) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'finally, it would be interesting to study properties of the penalized deleted estimation used in this paper']",3
"['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']","['good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'more recently, ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ), ( #AUTHOR_TAG ), and ( Lewis et al. , 2004 )']",0
"[', conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG )']","[', conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG )']","[', conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG )']","['most similar efforts to ours, mainly ( de Nero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'based on this advise ( moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure.', 'in this work we also start out from a generative model with latent segmentation variables.', 'however, we find out that concentrating the learning effort on smoothing is crucial for good performance.', 'for this, we devise itg - based priors over segmentations and employ a penalized version of deleted estimation working with em at its core.', 'the fact that our results ( at least ) match the heuristic estimates on a reasonably sized data set ( 947k parallel sentence pairs ) is rather encouraging']",1
['more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -'],"['more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition - - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'finally, rather than define a fixed set of feature']",['more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -'],"['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007).', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - - see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition - - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'finally, rather than define a fixed set of feature templates as in fig. 2, we would like to refine empirically useful features during training, resulting in language - specific backoff patterns and adaptively sized n - gram windows.', 'many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods']",0
"['transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent']","['training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent']","['transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition - and phonological features and syllable boundaries.', '']","['future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string - transduction tasks.', 'we would like to use features that look at wide context on the input side, which is inexpensive ( #AUTHOR_TAG ).', 'latent variables we wish to consider are an increased number of word classes ; more flexible regions - see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition - and phonological features and syllable boundaries.', 'indeed, our local log - linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'finally, rather than define a fixed set of feature templates as in fig. 2, we would like to refine empirically useful features during training, resulting in language - specific backoff patterns and adaptively sized n - gram windows.', 'many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods']",3
"['methods to improve the similarity estimates.', '#AUTHOR_TAG']","['methods to improve the similarity estimates.', '#AUTHOR_TAG']","['different methods on the propbank dataset.', 'our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'on the full training set the best method performed 2. 33 % better than the fully supervised model, which is a 10. 91 % error reduction.', 'using only 5 % of the training data the best semi - supervised model still achieved 60. 29 %, compared to 40. 49 % by the supervised model, which is an error reduction of 33. 27 %.', 'these results demonstrate that the latent words learned by the lwlm help for this complex information extraction task.', 'furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'we would like to perform experiments on employing this model in other information extraction tasks, such as word sense disambiguation or named entity recognition.', 'the current model uses the context in a very straightforward way, i. e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG']","['have presented the latent words language model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'we then experimented with different methods to incorporate the latent words for semantic role labeling, and tested different methods on the propbank dataset.', 'our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'on the full training set the best method performed 2. 33 % better than the fully supervised model, which is a 10. 91 % error reduction.', 'using only 5 % of the training data the best semi - supervised model still achieved 60. 29 %, compared to 40. 49 % by the supervised model, which is an error reduction of 33. 27 %.', 'these results demonstrate that the latent words learned by the lwlm help for this complex information extraction task.', 'furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'we would like to perform experiments on employing this model in other information extraction tasks, such as word sense disambiguation or named entity recognition.', 'the current model uses the context in a very straightforward way, i. e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples']",0
"['( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search']","['( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search']","['this paper, we extend two classes of model adaptation methods ( i. e., model interpolation and error - driven learning ), which have been well studied in statistical language modeling for speech and natural language applications ( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search applications']","['this paper, we extend two classes of model adaptation methods ( i. e., model interpolation and error - driven learning ), which have been well studied in statistical language modeling for speech and natural language applications ( e. g., #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ), to ranking models for web search applications']",0
"['( #AUTHOR_TAG ), knowledge about implicit']","['( #AUTHOR_TAG ), knowledge about implicit']","['example ( #AUTHOR_TAG ), knowledge about implicit predicates could be potentially useful for a variety of']","['already mentioned in the literature, see for example ( #AUTHOR_TAG ), knowledge about implicit predicates could be potentially useful for a variety of nlp tasks such as language generation, information extraction, question answering or machine translation.', 'many applications of semantic relations in nlp are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).', ""suppose that a search engine or a question answering system receives the query schnelle bombe'quick bomb '."", 'probably, in this case the user is interested in finding information about bombs that explode quickly rather then about bombs in general.', ""knowledge about predicates associated with the noun bombe'bomb'could be used for predicting a set of probable implicit predicates."", 'for generation of the semantically and syntactically correct paraphrases it is sometimes not enough to guess the most probable argument - predicate pairs.', 'information about types of an argument - predicate relation could be helpful, i. e. which semantic and syntactic position does the argument fill in the argument structure of the predicate.', ""for example, compare eine bombe explodiert schnell'a bomb explodes quickly'for schnelle bombe with ein buch schnell lesen / schreiben'to read / write a book quickly'for schnelles buch'quick book '."", 'in the first case the argument bombe fills the subject position, while in the second case buch fills the object position.', 'since framenet contains information about syntactic realization patterns for frame elements, representation of argument - predicate relations in terms of frames directly supports generation of semantically and syntactically correct paraphrases']",0
"['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the']","['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the']","['large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to']","['evaluated methods for self - training high accuracy products of latent variable grammars with large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'a simple but computationally expensive way to do this would be to parse the data with an sm7 product model']",1
"['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to']","['evaluated methods for self - training high accuracy products of latent variable grammars with large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'a simple but computationally expensive way to do this would be to parse the data with an sm7 product model']",1
"['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the']","['large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to']","['evaluated methods for self - training high accuracy products of latent variable grammars with large amounts of genre - matched data.', 'we demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'two primary factors appear to be determining the efficacy of our self - training approach.', 'first, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self - trained grammars.', 'second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'our most accurate single grammar achieves an f score of 91. 6 on the wsj test set, rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ), despite being a single generative pcfg.', 'our most accurate product model achieves an f score of 92. 5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'in future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'one possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'it would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'a simple but computationally expensive way to do this would be to parse the data with an sm7 product model']",1
"['##ntanglement ( #AUTHOR_TAG ).', ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will']","[""possibly critical feature is the'mention of names '."", ""in multi - party discussion people usually mention each other's name for the purpose of disentanglement ( #AUTHOR_TAG )."", ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will']","['##ntanglement ( #AUTHOR_TAG ).', ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will investigate how to exploit these features in our methods.', 'we are also']","[""possibly critical feature is the'mention of names '."", ""in multi - party discussion people usually mention each other's name for the purpose of disentanglement ( #AUTHOR_TAG )."", ""in our corpus we found 175 instances where a participant mentions other participant's name."", ""in addition to these,'subject of the email ','topic - shift cue words'can also be beneficial for a model."", 'as a next step for this research, we will investigate how to exploit these features in our methods.', 'we are also interested in the near future to transfer our approach to other similar domains by hierarchical bayesian multi - task learning and other domain adaptation methods.', 'we plan to work on both synchronous ( e. g., chats, meetings ) and asynchronous ( e. g., blogs ) domains']",0
"[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set']","[', we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'we found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes, and that using these prototypes gave state - of - the - art performance on wsj, as well as improvements on nearly all of the non - english corpora.', 'these promising results suggest a new direction for future research : improving pos induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set']",0
"['.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work,']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work,']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work,']","['run our techniques on a large set of relations to output a first repository of typed functional relations.', 'we release this list for further use by the research community. 2', 'future work : functionality is one of the several properties a relation can possess.', 'others include selectional preferences, transitivity ( #AUTHOR_TAG ), mutual exclusion, symmetry, etc..', 'these properties are very useful in increasing our understanding about these open ie relation strings.', 'we believe that the general principles developed in this work, for example, connecting the open ie knowledge with an existing knowledge resource, will come in very handy in identifying these other properties']",0
"['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using']","['phrase table combination.', 'finally, we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ).', 'the first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the indonesian - english bi - text.', 'the second table is built from the simple concatenation.', 'the two tables are then merged as follows : all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'each phrase pair retains its original scores, which are further augmented with 1 - 3 additional feature scores indicating its origin : the first / second / third feature is 1 if the pair came from the first / second / both table ( s ), and 0 otherwise.', 'we experiment using all three, the first two, or the first feature only ; we also try setting the features to 0. 5 instead of 0. this makes the following six combinations ( 0, 00, 000,. 5,. 5. 5,. 5. 5. 5 ) ; on testing, we use the one that achieves the highest bleu score on the development set']",5
"['or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experiment']","['or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi - text for a resource - poor language ( indonesian or spanish, pretending that spanish is resource - poor ) with a much larger bi - text for a related resource - rich language ( malay or portuguese ) ; the target language of all bi - texts was english.', 'however, our previous work did not attempt language adaptation, except for very simple transliteration for portuguese - spanish that ignored context entirely ; since it could not substitute one word for a completely different word, it did not help much for malay - indonesian, which use unified spelling.', 'still, once we have language - adapted the large bi - text, it makes sense to try to combine it further with the small bi - text ; thus,']","['or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experiment']","['third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'for example, our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi - text for a resource - poor language ( indonesian or spanish, pretending that spanish is resource - poor ) with a much larger bi - text for a related resource - rich language ( malay or portuguese ) ; the target language of all bi - texts was english.', 'however, our previous work did not attempt language adaptation, except for very simple transliteration for portuguese - spanish that ignored context entirely ; since it could not substitute one word for a completely different word, it did not help much for malay - indonesian, which use unified spelling.', 'still, once we have language - adapted the large bi - text, it makes sense to try to combine it further with the small bi - text ; thus, below we will directly compare and combine these two approaches']",1
"['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']","['mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""however, we do include a measure we call productivity to indicate the algorithm's completeness."", 'it is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'tab. 2 shows the evaluation results.', 'we reach our best precision by using the vp - fragment heuristics, which is still more productive than the lcs method.', 'the grammatical filter gives us a higher precision compared to the purely alignment - based approaches.', 'enhancing the system with coreference resolution raises the score even further.', 'we cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'however, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work : one state - of - theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0. 67.', 'we found the same number using our previous approach ( #AUTHOR_TAG ), which is roughly equivalent to our core module.', 'our approach outperforms both by 17 % with similar estimated productivity']",2
"['( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible']","['( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible']","['take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible realizations in natural language.', 'the corpus used in those experiments were short crowd - sourced descriptions of everyday tasks written in bullet point style.', 'we aligned them with a hand - crafted similarity measure that was specifically designed for this text type.', 'in this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'the current approach uses a domainindependent similarity measure instead of a specific hand - crafted similarity score and is thus']","['take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ).', 'in this earlier work, we focused on event structures and their possible realizations in natural language.', 'the corpus used in those experiments were short crowd - sourced descriptions of everyday tasks written in bullet point style.', 'we aligned them with a hand - crafted similarity measure that was specifically designed for this text type.', 'in this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'the current approach uses a domainindependent similarity measure instead of a specific hand - crafted similarity score and is thus applicable to standard texts']",2
"['.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG )']","['like recognizing textual entailment (Dinu and Wang, 2009).', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG )']","['like recognizing textual entailment (Dinu and Wang, 2009).', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG )']","['an applicational point of view, sentential paraphrases are difficult to use in other nlp tasks.', 'at the phrasal level, interchangeable patterns (Shinyama et al., 2002;Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted.', 'in both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e. g., named entities ( ne ) or content words.', 'they are quite successful in ne - centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like recognizing textual entailment (Dinu and Wang, 2009).', 'the research on general paraphrase fragment extraction at the sub - sentential level is mainly based on phrase pair extraction techniques from the mt literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log - likelihood - ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora.', 'our current approach also uses word - word alignment, however, we use syntactic dependency trees to compute grammatical fragments.', 'our use of dependency trees is inspired by the constituent - tree - based experiments of callison - Burch (2008)']",2
"['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (']","['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments ( sec.', '5. 3 ).', 'finally, we filter']","['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (']","['with the candidate fragment elements, we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a ( para - ) phrase.', 'we extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments ( sec.', '5. 3 ).', 'finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs']",2
['to ( #AUTHOR_TAG'],['to ( #AUTHOR_TAG'],['to ( #AUTHOR_TAG'],"['to ( #AUTHOR_TAG a ), our summarization system is, which consists of three key components : an initial sentence pre - selection module to select some important sentence candidates ; the above compression model to generate n - best compressions for each sentence ; and then an ilp summarization method to select the best summary sentences from the multiple compressed sentences']",1
"['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried']","['approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of chinese to english machine translation and found that there is no correlation between sentence length and mt quality.', 'rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple english sentences.', 'this prior work was carried over a dataset containing a single reference translation for each chinese sentence.', 'in the work presented in this paper, we strengthen our findings by examining multiple reference translations for each chinese sentence.', 'we define heavy sentences based on agreement of translator choices and reader preferences']",1
"['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']","['the tectogrammatical parsing of czech, the analytical tree structure is converted into the tectogrammatical one.', 'these automatic transformations are based on linguistic rules ( #AUTHOR_TAG ).', 'subsequently, tectogrammatical functors are assigned by the c4. 5 classifier ( 2abokrtsk9 et al., 2002 )']",5
"['for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']","['analytical parsing of czech runs in two steps : the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'we carried out two parallel experiments with two parsers available for czech, parser i ( Hajie et al. , 1998 ) and parser ii ( #AUTHOR_TAG ).', 'in the second step, we used a module for automatic analytical functor assignment ( 2abokrtskyt et al., 2002 )']",5
"['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']","['make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic czechenglish dictionary by running giza + + training ( translation models 1 - 4, see #AUTHOR_TAG ) on the training part of the english - czech wsj parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary.', 'as a result, the entry / translation pairs seen in the parallel corpus of wsj become more probable.', 'for entry / translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'the translation is "" giza + + se - lected "" if its probability is higher than a threshold, which is in our case set to 0. 10']",5
"['7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite']","['describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite']","['its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( al - Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus']","[', we summarize resources available for the experiments ( section 2 ).', 'section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score ( #AUTHOR_TAG ).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( al - Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus']",5
"['7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']","['describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']","['its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']","[', we summarize resources available for the experiments ( section 2 ).', 'section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of czech input.', 'in section 4 we describe the process of the filtering of dictionaries used in the transfer procedure ( for its characterization, see section 5 ).', 'the generation process consisting mainly of word reordering and lexical insertions is explained in section 6, an example illustrating the generation steps is presented in sec - tion 7.', 'for the evaluation of the results we use the bleu score (Papineni et al., 2001).', 'section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'we also compare the results with the output generated by the statistical translation system giza + + / isi rewrite decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ), trained on the same parallel corpus']",1
"[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp (""]","[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp ( haji 6 et al., 2002 )."", 'we used four reference retranslations of 490 sentences selected from the wsj sections 22, 23, and 24, which were themselves used as the fifth reference.', 'the evaluation method used is to hold']","[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp (""]","[""evaluated our translations with ibm's bleu evaluation metric ( #AUTHOR_TAG ), using the same evaluation method and reference retranslations that were used for evaluation at hlt workshop 2002 at clsp ( haji 6 et al., 2002 )."", 'we used four reference retranslations of 490 sentences selected from the wsj sections 22, 23, and 24, which were themselves used as the fifth reference.', 'the evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five bleu scores']",5
"['from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catal']","['from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catalan.', 'this change is consistent across both error rates, the wer and 100 - bleu.', 'for']","['from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catal']","['compared the two statistical lexica obtained from the baseline system and from the maximum entropy training on the transformed corpus.', 'for the baseline lexicon, we observed an average of 5. 82 catalan translation candidates per english word and 6. 16 spanish translation candidates.', 'these numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy : there, we have an average of 4. 20 for catalan and 4. 46 for spanish.', 'especially for ( nominative ) english pronouns ( which have many verbs as translation candidates in the baseline lexicon ), the number of translation candidates was substantially scaled down by a factor around 4. this shows that our method was successful in producing a more focused lexicon probability distribution.', 'we performed translation experiments with an implementation of the ibm - 4 translation model ( #AUTHOR_TAG ).', 'a description of the system can be found in (Tillmann and Ney, 2002).', 'table 5 presents an assessment of translation quality for both the language pairs english - catalan and english - spanish.', 'we see that there is a significant decrease in error rate for the translation into catalan.', 'this change is consistent across both error rates, the wer and 100 - bleu.', 'for translations from english into spanish, the improvement is less substantial.', 'a reason for this might be that the spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher : 1. 57 for spanish versus 1. 53 for catalan4.', 'this makes it more difficult for the system to choose the correct inflection when generating a spanish sentence.', 'we assume that the extension of our approach to other word classes than verbs will yield a quality gain for translations into spanish']",5
"['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these']","['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these']","['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these constraints are expressed on the basis of feature functions hu, ( s, t )']","['maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources.', 'this principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'the distribution is required to satisfy constraints, which represent facts known from the data.', 'these constraints are expressed on the basis of feature functions hu, ( s, t )']",5
"['example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'if necessary, the inverse of these transformations will be applied to the generated output string.', 'in the work presented here, we restrict ourselves to transforming only one language of the two : the source, which has the less inflected morphology.', 'for descriptions of smt systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']",0
"['instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']","['instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']","['instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']","['a = { am } is the set of model parameters with one weight a, for each feature function hm.', 'for an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 )']",0
"['example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']","['input string can be preprocessed before being passed to the search algorithm.', 'if necessary, the inverse of these transformations will be applied to the generated output string.', 'in the work presented here, we restrict ourselves to transforming only one language of the two : the source, which has the less inflected morphology.', 'for descriptions of smt systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 )']",0
"['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c']","['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c']","['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c']","['to using a global model like crfs, our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAG c ) reported the best results over the evaluated corpora of bakeoff - 2 until now7.', 'though those results are slightly better than the results here, we still see that the results of character - level dependency parsing approach ( scheme e ) are comparable to those state - of - the - art ones on each evaluated corpus']",1
"['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an']","['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an']","['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]","['the representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self - embedding.', 'however, in this paper we use our representation as an intermediate result in approximating an unrestricted context - free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""for this purpose, mohri and pereira's representation offers little advantage""]",1
"['#AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this,']","['#AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this,']","['#AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this,']","['restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context - free language, and therefore a subset approximation results.', 'this idea was proposed by krauwer and des Tombe ( 1981 ), Langendoen and Langsam ( 1987 ), and Pulman ( 1986 ), and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG.', 'since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method']",0
"['##ase the method of #AUTHOR_TAG as follows : first, we construct the']","['rephrase the method of #AUTHOR_TAG as follows : first, we construct the']","['rephrase the method of #AUTHOR_TAG as follows : first, we construct the approximating finite automaton according to the unparameterized rtn method above.', 'then an additional mechanism is introduced that ensures for each rule a - - ~ x1 •.. xm separately that the list of visits to the states qo,.. • •']","['rephrase the method of #AUTHOR_TAG as follows : first, we construct the approximating finite automaton according to the unparameterized rtn method above.', 'then an additional mechanism is introduced that ensures for each rule a - - ~ x1 •.. xm separately that the list of visits to the states qo,.. • • qm satisfies some reasonable criteria : a visit to qi, with 0 < i < m, should be followed by one to qi + l or q0.', 'the latter option amounts to a nested incarnation of the rule.', 'there is a complementary condition for what should precede a visit to qi, with 0 < i < m']",5
"['by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that • for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * w']","['by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that • for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']","['by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that • for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']","['method can be generalized, inspired by #AUTHOR_TAG, who derive n - gram probabilities from stochastic context - free grammars.', 'by ignoring the probabilities, each n = 1, 2, 3.... gives rise to a superset approximation that can be described as follows : the set of strings derivable from a nonterminal a is approximated by the set of strings al... an such that • for each substring v = ai + l... ai + n ( 0 < i < n - - n ) we have a - - + * wvy, for some w and y']",0
['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],['#AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata'],0
"['is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']","['is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']","['is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']","['10 ( b ) is no longer possible, since no nonterminal in the transformed grammar would contain 1 in its superscript. because of the demonstrated increase of the counter f, this transformation is guaranteed to remove self - embedding from the grammar.', 'however, it is not as selective as the transformation we saw before, in the sense that it may also block subderivations that are not of the form a - - * * ~ afl.', 'consider for example the subderivation from figure10, but replacing the lower occurrence of s by any other nonterminal c that is mutually recursive with s, a, and b. such a subderivation s - - - * * b c c d a would also be blocked by choosing d = 0.', 'in general, increasing d allows more of such derivations that are not of the form a ~ "" o ~ afl but also allows more derivations that are of that form. the reason for considering this transformation rather than any other that eliminates self - embedding is purely pragmatic : of the many variants we have tried that yield nontrivial subset approximations, this transformation has the lowest complexity in terms of the sizes of intermediate structures and of the resulting finite automata. in the actual implementation, we have integrated the grammar transformation and the construction of the finite automaton, which avoids reanalysis of the grammar to determine the partition of mutually recursive nonterminals after transformation.', 'this integration makes use, for example, of the fact that for fixed ni and fixed f, the set of nonterminals of the form a, f, with a c ni, is ( potentially ) mutually right - recursive. a set of such nonterminals can therefore be treated as the corresponding case from figure2, assuming the value right. the full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'a very similar formulation, for another grammar transformation, is given in #AUTHOR_TAG']",1
"['an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']","['an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']","['an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']","['states that the letter substring b with left context a and right context c receives the pronunciation ( i. e., phoneme substring ) d. such rules can also be straightforwardly cast in the if... then form commonly featured in high - level programming languages and employed in expert, knowledge - based systems technology.', 'they also constitute a formal model of universal computation ( post 1943 ).', 'conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ), mc Ilroy ( 1973 ), Elovitz et al. ( 1976 ), Hurmicutt ( 1976 ), and #AUTHOR_TAG']",0
"[', #AUTHOR_TAG']","[', #AUTHOR_TAG']","[', #AUTHOR_TAG']","['is also conceivable that data - driven techniques can actually outperform traditional rules.', 'however, this possibility is not usually given much credence.', ""for instance, #AUTHOR_TAG recently wrote : ` ` to our knowledge, learning algorithms, although promising, have not ( yet ) reached the level of rule sets developed by humans'' ( p. 520 )."", '520 ).', 'Dutoit (1997) takes this further, stating "" such training - based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores "" ( p.', '115, note 14 )']",0
"['the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not']","['the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not']","['and Nusbaum (1986, 1991 ).', 'see also the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not']","['##unciation by analogy ( pba ) is a data - driven technique for the automatic phonemization of text, originally proposed as a model of reading, e. g., by Glushko (1979) and Kay and Marcel (1981).', 'it was first proposed for tts applications over a decade ago by dedina and Nusbaum (1986, 1991 ).', 'see also the work of #AUTHOR_TAG, which considers computer - based pronunciation by analogy but does not mention the possible application to text - to - speech synthesis.', 'as detailed by Damper (1995) and Damper and Eastmond (1997), pba shares many similarities with the artificial intelligence paradigms variously called case - based, memory - based, or instance - based reasoning as applied to letter - to - phoneme conversion ( stanfill and waltz 1986 ; lehnert 1987 ; stanfill 1987 Stanfill , 1988 golding 1991 ; golding and rosenbloom 1991 ; van den bosch and daelemans 1993 )']",0
"['., #AUTHOR_TAG ; van']","['including some applications in computational linguistics ( e. g., #AUTHOR_TAG ; van halteren, zavrel, and daelemans 1998 ) and speech technology (']","['., #AUTHOR_TAG ; van halteren, zavrel, and']","[', the above characterization is very wide ranging.', 'consequently, fusion has been applied to a wide variety of pattern recognition and decision theoretic problems - - using a plethora of theories, techniques, and tools - - including some applications in computational linguistics ( e. g., #AUTHOR_TAG ; van halteren, zavrel, and daelemans 1998 ) and speech technology ( e. g., bowles and damper 1989 ; romary and pierre11989 ).', 'according to Abbott (1999, 290 ), "" while the reasons [ that ] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical....', 'a strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates. ""', 'our purpose in this paper is to study and exploit such fusion by model ( or strategy ) combination as a way of achieving performance gains in pba']",0
"['kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']","['gale and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']","['##e and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']","['researchers have proposed greedy algorithms for estimating nonprobabilistic word - to - word translation models, also known as translation lexicons ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 ; fung 1995 ; kumano and hirakawa 1994 ; #AUTHOR_TAG 1995 ; wu and xia 1994 ).', 'most of these algorithms can be summarized as follows']",0
"['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']","['probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAG b ).', 'these models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'i shall review these models using the notation in table 1']",0
"['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability""]","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order']","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order information as in equation 8']","['probability distribution trans (. 1, ~ ) is a word - to - word translation model.', 'unlike the models proposed by #AUTHOR_TAG b ), this model is symmetric, because both word bags are generated together from a joint probability distribution.', ""brown and his colleagues'models, reviewed in section 4. 3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'a sequenceto - sequence translation model can be obtained from a word - to - word translation model by combining equation 11 with order information as in equation 8']",1
"['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this']","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this']","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this property under the hypothesis that "" one sense per collocation "" holds for translational collocations.', 'this hypothesis implies that if u and v are possible mutual translations, and a token u co - occurs with a token v in the bitext, then with very high probability the pair ( u, v ) was generated from the same concept and should be linked.', 'to test this hypothesis, i ran one iteration of method a']","['bitext space is another kind of collocation.', ""if each word's translation is treated as a sense tag ( #AUTHOR_TAG ), then ` ` translational'' collocations have the unique property that the collocate and the word sense are one and the same!"", 'method b exploits this property under the hypothesis that "" one sense per collocation "" holds for translational collocations.', 'this hypothesis implies that if u and v are possible mutual translations, and a token u co - occurs with a token v in the bitext, then with very high probability the pair ( u, v ) was generated from the same concept and should be linked.', 'to test this hypothesis, i ran one iteration of method a on 300, 000 aligned sentence pairs from the canadian hansards bitext.', 'i then plotted the links ( u, v ) ratio ~ for several values of cooc ( u, v ) in figure 2. the curves show that the ratio links ( u, v ) cooc ( u, v ) tends to be either very high or very low.', 'this bimodality is not an artifact of the competitive linking process, because in the first iteration, linking decisions are based only on the initial similarity metric']",5
"['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( #AUTHOR_TAG ), i found that the g2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02.', 'let the cells of the contingency table be named as follows']",2
"['., #AUTHOR_TAG ), a']","['##¢ cross - language information retrieval ( e. g., mccarley 1999 ), a¢ multilingual document filtering ( e. g., oard 1997 ), a¢ computer - assisted language learning ( e. g., #AUTHOR_TAG ), a¢ certain machine - assisted translation tools ( e. g.,']","['., #AUTHOR_TAG ), a']","['##¢ cross - language information retrieval ( e. g., mccarley 1999 ), a¢ multilingual document filtering ( e. g., oard 1997 ), a¢ computer - assisted language learning ( e. g., #AUTHOR_TAG ), a¢ certain machine - assisted translation tools ( e. g., macklovitch 1994 ; melamed 1996a ), a¢ concordancing for bilingual lexicography ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 )']",0
"[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most']","[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most']","[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']","[""this situation, #AUTHOR_TAG b, 293 ) recommend ` ` evaluating the expectations using only a single, probable alignment.''"", 'the single most probable assignment ama ~ is the maximum a posteriori ( map ) assignment']",4
"['##xxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) =']","['in an on - line bilingual dictionary separately from those that do not ( cfxxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) =']","['##xxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) = log b ( links ( u, v ) [ cooc ( u, v ), a z ) "" ( 37 ) section 6. 1. 1 describes the link classes used in the experiments below']","['method b, the estimation of the auxiliary parameters a + and a - depends only on the overall distribution of co - occurrence counts and link frequencies.', 'all word pairs that co - occur the same number of times and are linked the same number of times are assigned the same score.', 'more accurate models can be induced by taking into account various features of the linked tokens.', 'for example, frequent words are translated less consistently than rare words ( catizone, russell, and warwick 1989 ).', 'to account for these differences, we can estimate separate values of a + and a - for different ranges of cooc ( u, v ).', 'similarly, the auxiliary parameters can be conditioned on the linked parts of speech.', 'a kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts.', 'just as easily, we can model link types that coincide with entries in an on - line bilingual dictionary separately from those that do not ( cfxxx #AUTHOR_TAG ).', 'brown et al. 1993 ).', 'when the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class : b ( links ( u, v ) [ cooc ( u, v ), a + ) scorec ( u, vlz = class ( u, v ) ) = log b ( links ( u, v ) [ cooc ( u, v ), a z ) "" ( 37 ) section 6. 1. 1 describes the link classes used in the experiments below']",5
"["". white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly']","[""now, translation models have been evaluated either subjectively ( e. g. white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly 16, 000 corresponding words between on - line versions of the bible in french and english.', 'this bitext was selected to facilitate widespread use and standardization ( see melamed [ 1998c ]']","["". white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly']","[""now, translation models have been evaluated either subjectively ( e. g. white and o'connell 1993 ) or using relative metrics, such as perplexity with respect to other models ( #AUTHOR_TAG b )."", 'objective and more accurate tests can be carried out using a "" gold standard. ""', 'i hired bilingual annotators to link roughly 16, 000 corresponding words between on - line versions of the bible in french and english.', 'this bitext was selected to facilitate widespread use and standardization ( see melamed [ 1998c ] for details ).', 'the entire bible bitext comprised 29, 614 verse pairs, of which 250 verse pairs were hand - linked using a specially developed annotation tool.', 'the annotation style guide ( melamed 1998b ) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'the annotation was replicated five times by seven different annotators']",1
"['##amed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['##amed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']","['informal experiments described elsewhere ( melamed 1995 ), i found that the g2 statistic suggested by #AUTHOR_TAG slightly outperforms 02.', 'let the cells of the contingency table be named as follows']",0
"['##ille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ;']","['above equation holds regardless of how we represent concepts.', 'there are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars ( abeille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ; vossen']","['##ille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ;']","['above equation holds regardless of how we represent concepts.', 'there are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars ( abeille et al. 1990 ; shieber 1994 ; #AUTHOR_TAG ), lexical conceptual structures ( dorr 1992 ) and wordnet synsets ( fellbaum 1998 ; vossen 1998 ).', 'of course, for a representation to be used, a method must exist for estimating its distribution in data']",0
"['., #AUTHOR_TAG ), a']","['cross - language information retrieval ( e. g., #AUTHOR_TAG ), a¢ multilingual document filtering ( e. g., oard 1997 ), a¢ computer - assisted language learning ( e. g., nerbonne et al. 1997 ), a¢ certain machine - assisted translation tools ( e. g.,']","['., #AUTHOR_TAG ), a']","['##¢ cross - language information retrieval ( e. g., #AUTHOR_TAG ), a¢ multilingual document filtering ( e. g., oard 1997 ), a¢ computer - assisted language learning ( e. g., nerbonne et al. 1997 ), a¢ certain machine - assisted translation tools ( e. g., macklovitch 1994 ; melamed 1996a ), a¢ concordancing for bilingual lexicography ( e. g., catizone, russell, and warwick 1989 ; gale and church 1991 )']",0
"[""##los's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic']","[""other such cases are described in danlos's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic']","[""##los's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic optimizations ( such as using pronouns instead of longer referring expressions']","[""other such cases are described in danlos's book ( #AUTHOR_TAG )."", 'the common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints ( such as unambiguous reference ) or performing linguistic optimizations ( such as using pronouns instead of longer referring expressions whenever possible ) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'this is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module']",0
['##g systems ( #AUTHOR_TAG )'],"['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( #AUTHOR_TAG )']",['##los and other pipeline critics do not seem to be a major problem in current applied nlg systems ( #AUTHOR_TAG )'],"['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( #AUTHOR_TAG )']",0
"[', a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by']","[', a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( mittal et al. 1998 )']","[', a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by']","['these arguments, most applied nlg systems use a pipelined architecture ; indeed, a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ).', 'this may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by danlos and other pipeline critics do not seem to be a major problem in current applied nlg systems ( mittal et al. 1998 )']",0
"['##g, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['is used to mediate and map between a language - independent domain model and a language - dependent ontology widely used in nlg, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['##g, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see']","['first i found chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms.', 'however, at a second, more - careful, reading, everything falls into place.', ""the resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993)."", 'the lexicon is used to mediate and map between a language - independent domain model and a language - dependent ontology widely used in nlg, the upper model ( #AUTHOR_TAG ).', 'although the idea of a two - level representation accommodating language - neutral and language - specific requirements is not new ( see for example nirenburg and levin [ 1992 ], dorr and voss [ 1993 ], and di eugenio [ 1998 ] ), stede is among the few who make effective use of those two levels in a complex system']",0
"["", ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system,""]","["", ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system,""]","[""##e 1968 ), kornai points out, ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system,""]","[""after the publication of the sound pattern of english ( chomsky and halle 1968 ), kornai points out, ` ` Johnson ( 1970 ) demonstrated that the context - sensitive machinery of spe... [ could ] be replaced by a much simpler one, based on finite - state transducers ( fsts ) ; the same conclusion was reached independently by kaplan and kay, whose work remained an underground classic until it was finally published in #AUTHOR_TAG.''"", ""these works inspired koskenniemi's two - level system, and the xerox rule compiler ( dalrymple et al. 1987 )."", 'both are now dominant tools in the fields of computational phonology and morphology, as exemplified by tateno et al. ( chapter 6 ), "" the japanese lexical transducer based on stem - suffix style forms "" and kim and jang ( chapter 7 ), "" acquiring rules for reducing morphological ambiguity from pos tagged corpus in korean. ""', 'the latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part - of - speech tagged corpora']",0
"['this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG']","['this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG']","['simple surface - level constraints that partially mask one another. 1 whether this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG b ) is of course an empirical question']","['therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface - level constraints that partially mask one another. 1 whether this is always possible under an appropriate definition of "" simple constraints "" ( e. g., #AUTHOR_TAG b ) is of course an empirical question']",0
"[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', 'the tension between']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', 'the tension between']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', ""the tension between preserving the original author's text ( faithfulness to the underlying form ) and making it readable in various ways ( well - formedness ) is right up ot's alley."", 'the same applies to document layout : i have often wished i could write ot - style tex macros ~ third, even in statistical corpus - based nlp, estimating a full gibbs distribution is not always']","[', weights are an annoyance when writing grammars by hand.', 'in some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part - of - speech tagging results using a handcrafted approach that is close to ot. 3 more speculatively, imagine an ot grammar for stylistic revision of parsed sentences.', ""the tension between preserving the original author's text ( faithfulness to the underlying form ) and making it readable in various ways ( well - formedness ) is right up ot's alley."", 'the same applies to document layout : i have often wished i could write ot - style tex macros ~ third, even in statistical corpus - based nlp, estimating a full gibbs distribution is not always feasible.', 'even if strict ranking is not quite accurate, sparse data or the complexity of parameter estimation may make it easier to learn a good ot grammar than a good arbitrary gibbs model.', ""a well - known example is Yarowsky's (1996) work on word sense disambiguation using decision lists ( a kind of ot grammar )."", 'although decision lists are not very powerful because of their simple output space, they have the characteristic ot property that each generalization partially masks lower - ranked generalizations']",0
"['), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms,']","['example, consider the relevance to hidden markov models ( hmms ), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms,']","['), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms, ot generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'when is this appropriate?', 'it seems to me that there are three possible uses']","['example, consider the relevance to hidden markov models ( hmms ), another restricted class of gibbs distributions used in speech recognition or part - of - speech tagging.', ""just like ot grammars, hmm viterbi decoders are functions that pick the optimal output from ~ ', based on criteria of well - formedness ( transition probabilities ) and faithfulness to the input ( emission probabilities )."", 'but typical ot grammars offer much richer finite - state models of left context ( #AUTHOR_TAG a ) than provided by the traditional hmm finite - state topologies.', 'now, among methods that use a gibbs distribution to choose among linguistic forms, ot generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'when is this appropriate?', 'it seems to me that there are three possible uses']",0
"['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']","['original question concerned the extent to which recall and precision are influenced by the size of the window.', 'it turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest - frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'it is common practice in information retrieval to discard the lowest - frequency words a priori as nonsignificant ( rijsbergen 1979 ).', ""in smadja's collocation algorithm xtract, the lowest - frequency words are effectively discarded as well ( smadja 1993 )."", '#AUTHOR_TAG use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five']",0
"[""reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]","[""reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]","['/ 139 = 46. 0 %', 'and 79 / 170 = 46. 7 %,', ""respectively, using fisher's exact test."", ""note that this technique is optimal for the extraction of the lowest - frequency words, leading to identical performance for g 2 and fisher's exact test for these words."", ""for the higherfrequency words, fisher's exact test leads to a slightly better recall with the same precision scores ( 0. 31 for both tests )."", ""while we have observed reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]","[""that we want to retain dis legomena with a 2 - 0 distribution, we proceed to compute the corresponding significance levels for both g 2 and fisher's exact test by equations 1 and 2. the critical x 2 value for g 2 equals 3. 65, the critical p for fisher's exact test is 0. 161."", 'the extraction results for both tests as measured by f are 0. 31 and 0. 33, respectively.', 'this procedure allows us to extract 64 / 139 = 46. 0 % of the lowfrequency words and 66 / 170 - ~ 38. 8 % of the high - frequency words using g 2, and 64 / 139 = 46. 0 %', 'and 79 / 170 = 46. 7 %,', ""respectively, using fisher's exact test."", ""note that this technique is optimal for the extraction of the lowest - frequency words, leading to identical performance for g 2 and fisher's exact test for these words."", ""for the higherfrequency words, fisher's exact test leads to a slightly better recall with the same precision scores ( 0. 31 for both tests )."", ""while we have observed reasonable results with both g2 and fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( mi ) measure ( #AUTHOR_TAG )""]",0
"['is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical linguistics']","['is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical']","[', the complexity of a grammar class is measured by the worst case : a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long - enough sentences that parse in time x by this grammar.', 'however, what matters in engineering practice is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical linguistics - - have any direct consequences for an engineering linguistics?', 'even if a chinese grammar includes a non - context - flee rule for parsing such numerals, how frequently will it be activated?', 'does it imply impossibility of processing real chinese texts in reasonable time?', 'clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst - case complexity of a grammar class ; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one - - building real - world systems and comparing their efficiency and coverage']","[', the complexity of a grammar class is measured by the worst case : a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long - enough sentences that parse in time x by this grammar.', 'however, what matters in engineering practice is the average case for a specific grammar.', 'specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time - consuming behavior of the algorithm never happens for this grammar.', 'average, since it can happen that the grammar does admit hard - toparse sentences that are not used ( or at least not frequently used ) in the real corpus.', 'for example, #AUTHOR_TAG proves that chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context - free, which implies that chinese is not a context - free language and thus might parse in exponential worst - case time.', 'do such arguments - - no doubt important for mathematical linguistics - - have any direct consequences for an engineering linguistics?', 'even if a chinese grammar includes a non - context - flee rule for parsing such numerals, how frequently will it be activated?', 'does it imply impossibility of processing real chinese texts in reasonable time?', 'clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst - case complexity of a grammar class ; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one - - building real - world systems and comparing their efficiency and coverage']",0
"['in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']","['in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']","['. g., part - of - speech taggers, parsers, etc. ) can provide similar measures.', 'the appropriate analysis - level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'for example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']","['particular analysis - level style markers can be calculated only when this specific computational tool is utilized.', 'however, the scbd is a general - purpose tool and was not designed for providing stylistic information exclusively.', 'thus, any nlp tool ( e. g., part - of - speech taggers, parsers, etc. ) can provide similar measures.', 'the appropriate analysis - level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'for example, some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ).', 'this parser produces trees to represent the structure of the sentences that compose the text.', 'however, it is set to "" skip "" or surrender attempts to parse clauses after reaching a time - out threshold.', 'when the parser skips, it notes that in the parse tree.', 'the measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis - level style markers']",0
['by researchers in automatic text genre detection ( #AUTHOR_TAG b ;'],['by researchers in automatic text genre detection ( #AUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables'],"['cx is the covariance matrix of x.', 'using this classification method we can also derive the probability that a case belongs to a particular group ( i. e., posterior probabilities ), which is roughly proportional to the mahalanobis distance from that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAG b ;']","['cx is the covariance matrix of x.', 'using this classification method we can also derive the probability that a case belongs to a particular group ( i. e., posterior probabilities ), which is roughly proportional to the mahalanobis distance from that group centroid.', 'discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAG b ; karlgren and cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables']",0
"['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']","['algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation.', 'he classified the senses of a word on the basis of other words that the given word co - occurs with.', '']",4
"['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']","['automatic annotation of nouns and verbs in the corpus has been done by matching them with the wordnet database files.', 'before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and wordnet.', 'the changes made were inspired by those described in #AUTHOR_TAG, page 75 ).', 'to lemmatize the words we used morpha, a lemmatizer developed by john a. carroll and freely available at the address : http : / / www. informatics. susx. ac. uk. / research / nlp / carroll / morph. html.', 'upon simple observation, it showed a better performance than the frequently used porter stemmer for this task']",4
"['the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG']","['the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG']","['the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG']","['section 2 we describe our general framework of the generic beam - search algorithm and the generalized perceptron.', 'then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'we give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'for the segmentation task, we also compare our beam - search framework with alternative decoding algorithms including an exact dynamic - programming method, showing that the beam - search method is significantly faster with comparable accuracy.', 'for the joint segmentation and pos - tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAG a ), while being more than an order of magnitude faster']",1
"['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature']","['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature']","['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests.', 'in each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'we summarize the findings of this study in this section']","['( #AUTHOR_TAG ), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests.', 'in each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'we summarize the findings of this study in this section']",2
"['into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']","['into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']","['order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']","['order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'this evaluation set - up is an improvement versus the one we previously reported ( #AUTHOR_TAG ), in which fixed partitions were used for training, development, and testing']",2
"['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other hypotheses that can be constructed from the same bag of words are non - gold hypotheses.', 'from the generation perspective this assumption is too strong, because sentences can have multiple orderings ( with multiple derivations ) that are both gram - matical and fluent.', 'nevertheless, it is the most feasible choice given the training data available']","['now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'ccgbank ( #AUTHOR_TAG ) is used to train the model.', 'for each training sentence, the corresponding ccgbank derivation together with all its sub - derivations are treated as gold - standard hypotheses.', 'all other hypotheses that can be constructed from the same bag of words are non - gold hypotheses.', 'from the generation perspective this assumption is too strong, because sentences can have multiple orderings ( with multiple derivations ) that are both gram - matical and fluent.', 'nevertheless, it is the most feasible choice given the training data available']",5
"['is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also']","['is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also']","[', we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'when a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'the data structure for accepted hypotheses is similar to that used for best - first parsing ( caraballo and charniak 1998 ), and we adopt the term chart for this structure.', 'however, note there are important differences to the parsing problem.', 'first, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also']","['our formulation of the word ordering problem, a hypothesis is a phrase or sentence together with its ccg derivation.', 'hypotheses are constructed bottom - up : starting from single words, smaller phrases are combined into larger ones according to ccg rules.', 'to allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'when a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'the data structure for accepted hypotheses is similar to that used for best - first parsing ( caraballo and charniak 1998 ), and we adopt the term chart for this structure.', 'however, note there are important differences to the parsing problem.', 'first, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'in our previous papers ( #AUTHOR_TAG ; zhang, blackwood, and clark 2012 ), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase - based mt decoding ( koehn 2010 ).', 'however, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'we will also investigate the possibility of applying dynamic - programming - style pruning to the chart']",1
"['.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that α - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that α - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full information to compute w *.', 'rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'the feedback data in this experiment were generated by searching the n - best list']","['experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'we select feedback of varying grade by directly inspecting the optimal w *, thus this feedback is idealized.', 'however, the experiment also has a realistic background since we show that α - informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized ter, and that learning from weak and strong feedback leads to convergence in ter on test data.', 'for this experiment, the post - edit data from the lig corpus were randomly split into 3 subsets : pe - train ( 6, 881 sentences ), pe - dev, and pe - test ( 2, 000 sentences each ).', 'pe - train was used for our online learning experiments.', ""pe - test was held out for testing the algorithms'progress on unseen data."", 'pe - dev was used to obtain w * to define the utility model.', 'this was done by mert optimization ( #AUTHOR_TAG ) towards post - edits under the ter target metric.', 'note that the goal of our experi - ments is not to improve smt performance over any algorithm that has access to full information to compute w *.', 'rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'the feedback data in this experiment were generated by searching the n - best list for translations that are α - informative at α ∈ { 0. 1, 0. 5, 1. 0 } ( with possible non - zero slack ).', 'this is achieved by scanning the n - best list output for every input x t and returning the firsty t = y t that satisfies equation ( 2 ). 5 this setting can be thought of as an idealized scenario where a user picks translations from the n - best list that are considered improvements under the optimal w *']",5
"[', perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the']","[', perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the']","[', perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the difference to the online learning scenario is that we treat the multi - epoch algorithm as an empirical risk minimizer that selects a final weight vector w t, k whose expected loss on unseen data we would like to bound.', 'we assume that the algorithm is fed with a sequence of examples x 1,...', ', x t, and at each epoch k = 1,...', ', k it makes a prediction y t, k.', 'the correct label is y * t.', 'for k = 1,...', ', k and t = 1,...', ', t, let t, k = u ( x t, y * t ) − u ( x t, y t, k ), and denote by ∆ t, k and ξ t, k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'finally, we denote']","['for online - to - batch conversion.', 'in practice, perceptron - type algorithms are often applied in a batch learning scenario, i. e., the algorithm is applied for k epochs to a training sample of size t and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ).', 'the difference to the online learning scenario is that we treat the multi - epoch algorithm as an empirical risk minimizer that selects a final weight vector w t, k whose expected loss on unseen data we would like to bound.', 'we assume that the algorithm is fed with a sequence of examples x 1,...', ', x t, and at each epoch k = 1,...', ', k it makes a prediction y t, k.', 'the correct label is y * t.', 'for k = 1,...', ', k and t = 1,...', ', t, let t, k = u ( x t, y * t ) − u ( x t, y t, k ), and denote by ∆ t, k and ξ t, k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'finally, we denote by d t, k = t t = 1 ∆ 2 t, k, and by w t, k the final weight vector returned after k epochs.', 'we state a condition of convergence']",1
"['phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar mos']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits (Potet et al., 2012).', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system ( Potet et al. , 2010 ), based on the moses phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit (Heafield, 2011) on news. en', '']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits (Potet et al., 2012).', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system ( Potet et al. , 2010 ), based on the moses phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit (Heafield, 2011) on news. en', '']","['used the lig corpus 3 which consists of 10, 881 tuples of french - english post - edits (Potet et al., 2012).', 'the corpus is a subset of the newscommentary dataset provided at wmt 4 and contains input french sentences, mt outputs, postedited outputs and english references.', 'to prepare smt outputs for post - editing, the creators of the corpus used their own wmt10 system ( Potet et al. , 2010 ), based on the moses phrase - based decoder ( #AUTHOR_TAG ) with dense features.', 'we replicated a similar moses system using the same monolingual and parallel data : a 5 - gram language model was estimated with the kenlm toolkit (Heafield, 2011) on news. en', 'data ( 48. 65m', 'sentences, 1. 13b tokens ), pre - processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'perceptron cycling theorem (Block and Levin, 1970;Gelfand et al., 2010) should suffice to show a similar bound.', 'parallel data ( europarl + news - comm, 1. 64m sentences ) were similarly pre - processed and aligned with fast align (Dyer et al., 2013).', 'in all experiments, training is started with the moses default weights.', 'the size of the n - best list, where used, was set to 1, 000.', 'irrespective of the use of re - scaling in perceptron training, a constant learning rate of −5 was used for learning from simulated feedback, and 10 −4 for learning from surrogate translations']",5
"['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']","['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']","['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']","['contrast, a single statistical model allows one to maintain a single table ( #AUTHOR_TAG )']",0
['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],['##n uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates'],0
"[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]","[""that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen, unless we know in advance all of the properties that can be attributed to a given object, as in the case of jordan's work on the coconut domain ( #AUTHOR_TAG )""]",0
"['such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']","['such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']","['has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAG a )']",0
['the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )'],"['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )']","['wordnet, annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2some descriptions of int modifiers can be found in ( #AUTHOR_TAG b )']",0
"[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]","[""addition to a referring function, noun phrases ( np ) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG )""]",0
"['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']","['what follows we explain the properties of the model by applying it to a small number of adjective - noun combinations taken from the lexical semantics literature.', 'table 1 gives the interpretations of eight adjective - noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ).', 'table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections ( v is the most likely interpretation, v 2 is the second most likely interpretation, etc. )']",5
"['., the noun or noun class they modify ( see #AUTHOR_TAG and the references']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i. e., the regular and predictable sense alternations certain classes of words are subject to.', 'adjectives, more than other categories, are a striking example of regular polysemy since they are able to take on different meanings depending on their context, viz., the noun or noun class they modify ( see #AUTHOR_TAG and the references']","['., the noun or noun class they modify ( see #AUTHOR_TAG and the references']","['recent work in lexical semantics has been concerned with accounting for regular polysemy, i. e., the regular and predictable sense alternations certain classes of words are subject to.', 'adjectives, more than other categories, are a striking example of regular polysemy since they are able to take on different meanings depending on their context, viz., the noun or noun class they modify ( see #AUTHOR_TAG and the references therein )']",0
"['a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly']","['a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly']","['a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong ).', 'these adjectives had to be unambiguous with respect to their part - of - speech : each adjective was unambiguously tagged as "" adjective "" 98. 6 % of the time, measured as the number of different part - of - speech tags assigned to the word in the bnc.', 'we identified adjective - noun pairs using gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'gsearch was run on a lemmatized version of the bnc so as to compile a comprehensive corpus count of all nouns occurring in a modifier - head relationship with each of the nine adjectives.', 'from the syntactic analysis provided by we used the model']","['chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the bnc.', 'we chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ).', 'from these we randomly sampled nine adjectives ( difficult, easy, fast, good, hard, right, safe, slow, wrong ).', 'these adjectives had to be unambiguous with respect to their part - of - speech : each adjective was unambiguously tagged as "" adjective "" 98. 6 % of the time, measured as the number of different part - of - speech tags assigned to the word in the bnc.', 'we identified adjective - noun pairs using gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'gsearch was run on a lemmatized version of the bnc so as to compile a comprehensive corpus count of all nouns occurring in a modifier - head relationship with each of the nine adjectives.', 'from the syntactic analysis provided by we used the model outlined in section 2 to derive meanings for the 90 adjective - noun combinations.', 'we employed no threshold on the frequencies f ( a, v ) and f ( rel, v, n ).', 'in order to obtain the frequency f ( a, v ) the adjective was mapped to its corresponding adverb.', 'in particular, good was mapped to good and well, fast to fast, easy to easily, hard to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow and wrong to wrongly and wrong.', 'the adverbial function of the adjective difficult is expressed only periphrastically ( i. e., in a difficult manner, with difficulty ).', 'as a result, the frequency f ( difficult, v ) was estimated only on the basis of infinitival constructions ( see ( 17 ) ).', 'we estimated the probability p ( a, n, v, rel ) for each adjective - noun pair by varying both the terms v and rel']",5
"[': a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enum']","['a difficult language is a language that is difficult to learn, speak, or write.', 'adjectives like good allow either verb - subject or verb - object interpretations : a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the']","['a difficult language is a language that is difficult to learn, speak, or write.', 'adjectives like good allow either verb - subject or verb - object interpretations : a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify.', 'pustejovsky treats nouns as having a qualia structure as part of']","['2 ) a. easy problem b. difficult language c. good cook d. good soup adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'the meaning of adjective - noun combinations like those in ( 1 ) and ( 2 ) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'for example, an easy problem is "" a problem that is easy to solve "" or "" a problem that one can solve easily "".', 'in order to account for the meaning of these combinations Vendler (1968, 92 ) points out that "" in most cases not one verb, but a family of verbs is needed "".', 'vendler further observes that the noun figuring in an adjective - noun combination is usually the subject or object of the paraphrasing verb.', 'although fast usually triggers a verb - subject interpretation ( see ( 1 ) ), easy and difficult trigger verb - object interpretations ( see ( 2a, b ) ).', 'an easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'adjectives like good allow either verb - subject or verb - object interpretations : a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify.', 'pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'for example, the telic ( purpose ) role of the qualia structure for problem has a value equivalent to solve.', 'when the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective - noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'furthermore, in cases where more than one interpretations are provided ( see Vendler (1968) ), no information is given with respect to the likelihood of these interpretations.', 'outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others : fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly']",0
"['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']","['have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple naive bayesian classifiers, each based on co - - occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line']",0
['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],['( #AUTHOR_TAG )'],0
['definitions of predicates may be found in ( #AUTHOR_TAG )'],['definitions of predicates may be found in ( #AUTHOR_TAG )'],['definitions of predicates may be found in ( #AUTHOR_TAG )'],['definitions of predicates may be found in ( #AUTHOR_TAG )'],0
"['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']","['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']","['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']","['appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over - generalisation compared with those given in #AUTHOR_TAG']",1
['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],['task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 )'],1
"['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']","['problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important ( #AUTHOR_TAG )']",4
"[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic (']","[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic ( g2 ) is more appropriate for corpus - based nlp']","[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic (']","[', #AUTHOR_TAG claims that the log - likelihood chisquared statistic ( g2 ) is more appropriate for corpus - based nlp']",4
['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],['task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG'],1
"['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']","['x2 statistic is performing at least as well as g2, throwing doubt on the claim by #AUTHOR_TAG that the g2 statistic is better suited for use in corpus - based nlp']",1
"['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 )""]",1
"['escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of']","['escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of hale and keyser, but in addition to advancing a more refined theory of verbal argument structure, i also describe a computational implementation']","['observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of']","['observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is, in fact, encoded syntactically.', 'they describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'this present framework builds on the work of hale and keyser, but in addition to advancing a more refined theory of verbal argument structure, i also describe a computational implementation']",0
"['is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical']","['is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical']","['is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression']","['light verb v do licenses an atelic non - inchoative event, and is compatible with verbal roots expressing activity.', 'it projects a functional head, voice ( #AUTHOR_TAG ), whose specifier is the external argument.', 'lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression']",0
"['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 )""]",1
"['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']","['this paper, i present a computational implementation of distributed morphology ( #AUTHOR_TAG ), a non - lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation.', 'this framework leads to finer - grained semantics capable of better capturing linguistic generalizations']",5
"['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( Baker et al. , 1998 ) and propbank ( #AUTHOR_TAG )""]",0
"['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']","['( #AUTHOR_TAG ), i present evidence from mandarin chinese that this analysis is on the right track.', 'the rest of this paper, however, will be concerned with the computational implementation of my theoretical framework']",2
"['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAG b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']",0
"['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles']",0
"['hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","['hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","['hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']","[""know run believe walk accomplishments achievements paint a picture recognize make a chair find under vendler's classification, activities and states both depict situations that are inherently temporally unbounded ( atelic ) ; states denote static situations, whereas activities denote on - going dynamic situations."", 'accomplishments and achievements both express a change of state, and hence are temporally bounded ( telic ) ; achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration, which is often a question of granularity']",0
"['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated']","['currently implemented system is still at the "" toy parser "" stage.', 'although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'with a minimal set of features and a small number of lexical entries, Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis.', 'i believe that with a suitable lexicon ( either hand crafted or automatically induced ), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles']",0
"['the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG']","['the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG']","['the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG']","['that the only difference between flat. adj and flatten. v is the suffix - en, it must be the source of inchoativity and contribute the change of state reading that distinguishes the verb from the adjective.', 'here, we have evidence that derivational affixes affect the semantic representation of lexical items, that is, fragments of event structure are directly associated with derivational morphemes.', 'we have the following situation : in this case, the complete event structure of a word can be compositionally derived from its component morphemes.', ""this framework, where the ` ` semantic load'' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content, is essentially the model advocated by #AUTHOR_TAG a ), among many others."", 'note that such an approach is no longer lexicalist : each lexical item does not fully encode its associated syntactic and semantic structures.', 'rather, meanings are composed from component morphemes']",0
"['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]","['theory of verbal argument structure can be imple - mented in a unified morpho - syntactic parsing model that interleaves syntactic and semantic parsing.', ""the system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of chomsky's minimalist program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG )""]",1
"['in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical']","['in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical']","['the non - lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs']","['the non - lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so - called light verbs.', 'here, i adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots.', 'verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to ( assumed ) universal primitives of the human cognitive system.', 'on the other hand, verbal roots represent abstract ( categoryless ) concepts and basically correspond to open - class items drawn from encyclopedic knowledge.', 'i assume an inventory of three verbalizing heads, each corresponding to an aforementioned primitive']",5
"['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]","['common lexical semantic representation in the computational linguistics literature is a frame - based model where syntactic arguments are associated with various semantic roles ( essentially frame slots ).', 'verbs are viewed as simple predicates over their arguments.', ""this approach has its roots in fillmore's Case Grammar ( 1968 ), and serves as the foundation for two current large - scale semantic annotation projects : framenet ( #AUTHOR_TAG ) and propbank ( Kingsbury et al. , 2002 )""]",0
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']",0
"['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently']","['understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences']",0
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991 b ; Rappaport Hovav and Levin , 1998 ).', 'consider the following example']",0
['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to'],['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to'],['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to'],"['has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to his paper for a description of the parsing algorithm.', 'i have adapted it for my needs and developed grammar fragments that reflect my non - lexicalist semantic framework.', 'as an example, a simplified derivation of the sentence "" the tire flattened. "" is shown in figure 1']",2
"['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ),']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ),']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ),']","['typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ), possibly arranged in an inheritance hierarchy.', 'the argument structure and syntax - tosemantics mapping would then only need to be specified once for each verb class.', 'in addition, lexical rules could be formulated to derive certain alternations from more basic forms']",1
"['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']","['consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']","['is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure - - representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991 b ; rappaport #AUTHOR_TAG ).', 'consider the following example']",0
"[""corresponding to vendler's event classes ( #AUTHOR_TAG ) : (""]","[""corresponding to vendler's event classes ( #AUTHOR_TAG ) : (""]","['##s, the activity of sweeping the floor and its result, the state of the floor being clean.', ""a more recent approach, advocated by Rappaport Hovav and Levin ( 1998 ), describes a basic set of event templates corresponding to vendler's event classes ( #AUTHOR_TAG ) : (""]","['##ty breaks the event described by ( 2 ) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""a more recent approach, advocated by Rappaport Hovav and Levin ( 1998 ), describes a basic set of event templates corresponding to vendler's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x act < manner > ] ( activity ) b. [ x < state > ] ( state ) c. [ become [ x < state > ] ] ( achievement ) d. [ x cause [ become [ x < state > ] ] ] ( accomplishment""]",0
"['are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006']","['are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAG b ) and Topkara et al. ( 2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['synonyms.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by #AUTHOR_TAG.', 'later works, such as Atallah et al. (2001 a ), Bolshakov (2004), Taskiran et al. (2006  and Topkara et al. (2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), #AUTHOR_TAG and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as #AUTHOR_TAG a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n - gram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), Bolshakov ( 2004 ), Taskiran et al. ( 2006 ) and #AUTHOR_TAG b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), #AUTHOR_TAG, Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioriti']","['synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']","['simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'the first lexical substitution method was proposed by Chapman and Davida (1997).', 'later works, such as Atallah et al. ( 2001 a ), #AUTHOR_TAG, Taskiran et al. ( 2006 ) and Topkara et al. ( 2006 b ), further made use of part - ofspeech taggers and electronic dictionaries, such as wordnet and verbnet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model ; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006 b ) report an average embedding capacity of 0. 67 bits per sentence for the synonym substitution method']",0
"['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed']","['google n - gram data was collected by google research for statistical language modelling, and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ), and contains english n - grams and their observed frequency counts, for counts of at least 40.', 'the striking feature of the n - gram corpus is the large number of n - grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of english text on publicly accessible web pages collected in january 2006.', 'for example, the 5 - gram phrase the part that you were has a count of 103.', 'the compressed data is around 24 gb on disk']",0
"['( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it,']","['( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it,']","['is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it,']","['##ganography is concerned with hiding information in some cover medium, by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ).', 'the covert communication is such that the very act of communication is to be kept secret from outside observers.', 'a related area is watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright.', 'here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed']",0
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and #AUTHOR_TAG a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['##p technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition,']","['semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state - ofthe - art for nlp technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition,']","['##p technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition,']","['semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state - ofthe - art for nlp technology.', 'it requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation ( tmr ) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition, with the idea that some presuppositional information can be removed without changing the meaning of a sentence']",0
"['sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']","['sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']","['sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']","['1 gives summary statistics of the paraphrase dictionary and its coverage on section 00 of the penn treebank.', 'the length of the extracted n - gram phrases ranges from unigrams to five - grams.', 'the coverage figure gives the percentage of sentences which have at least one phrase in the dictionary.', 'the coverage is important for us because it determines the payload capacity of the embedding method described in section 5. original phrase paraphrases the end of this year later this year the end of the year year end a number of people some of my colleagues differences the european peoples party the ppe group dictionary is a mapping from phrases to sets of possible paraphrases.', 'each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'the examples show that, while some of the paraphrases are of a high quality, some are not.', 'for example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'for example, year end is an unsuitable paraphrase for the end of this year in the sentence the chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context.', 'section 4 describes our method for determining if a paraphrase is suitable in a given context']",0
"['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']","['cover text used for our experiments consists of newspaper sentences from section 00 of the penn treebank (Marcus et al., 1993).', 'hence we require possible paraphrases for phrases that occur in section 00.', 'the paraphrase dictionary that we use was generated for us by chris callison - burch, using the technique described in #AUTHOR_TAG, which exploits a parallel corpus and methods developed for statistical machine translation']",5
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG, Meral et al. ( 2007 ), Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words ( #AUTHOR_TAG ).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in callison - Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text']",1
"['the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the']","['the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to']","['2 describes some of the previous transformations used in linguistic steganography.', 'note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e. g. the amount of white space between words (Por et al., 2008).', 'our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG, in which the application of paraphrases from the dictionary encodes secret bits.', 'one advantage of the dictionary is that it has wide coverage, being automatically extracted ; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text']",5
"[', in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an']","['observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic']","['being perceptable by a human observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic transformations which can be applied reliably and often is a challenging problem for linguistic steganography']","['is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark.', 'image stegosystems exploit the redundancy in an image representation together with limitations of the human visual system.', 'for example, a standard image stegosystem uses the least - significant - bit ( lsb ) substitution technique.', 'since the difference between 11111111 and 11111110 in the value for red / green / blue intensity is likely to be undetectable by the human eye, the lsb can be used to hide information other than colour, without being perceptable by a human observer. 1', 'key question for any steganography system is the choice of cover medium.', 'given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'however, the literature on linguistic steganography, in which linguistic properties of a text are modified to hide information, is small compared with other media ( #AUTHOR_TAG ).', 'the likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'language has the property that even small local changes to a text, e. g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'hence finding linguistic transformations which can be applied reliably and often is a challenging problem for linguistic steganography']",0
"['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']","['order to improve the grammaticality checking, we use a parser as an addition to the basic google ngram method.', 'we use the #AUTHOR_TAG ccg parser to analyse the sentence before and after paraphrasing.', 'combinatory categorial grammar ( ccg ) is a lexicalised grammar formalism, in which ccg lexical categories - typically expressing subcategorisation information - are assigned to each word in a sentence.', 'the grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'if there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'hence the grammar check is at the word, rather than derivation, level ; however, ccg lexical categories contain a large amount of syntactic information which this method is able to exploit']",5
"['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a )']","['to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']","['second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'this method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'the first syntactic transformation method is presented by Atallah et al. (2001 a ).', 'Later, Atallah et al. (2001 b ) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'in other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ), #AUTHOR_TAG, Murphy ( 2001 ), Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006 a ) all belong to the syntactic transformation category.', 'after embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001 b ) and Topkara et al. (2006 a ) attained the embedding capacity of 0. 5 bits per sentence with the syntactic transformation method']",0
"['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to token']","['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to tokenized arabic and our da - msa transfer component used feature transfer rules only.', 'we did not use a language model to pick the best path ; instead we kept the ambiguity in the lattice and passed it to our smt system.', 'in contrast, in this paper, we run elissa on untokenized arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the']","['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to token']","['our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ), we applied our approach to tokenized arabic and our da - msa transfer component used feature transfer rules only.', 'we did not use a language model to pick the best path ; instead we kept the ambiguity in the lattice and passed it to our smt system.', 'in contrast, in this paper, we run elissa on untokenized arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated msa lattice through a language model.', ""certain aspects of our approach are similar to Riesa and Yarowsky (2006)'s, in that we use morphological analysis for da to help da - english mt ; but unlike them, we use a rule - based approach to model da morphology""]",1
"['##m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the']","['use the open - source moses toolkit (Koehn et al., 2007) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the']","['##m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext']","['use the open - source moses toolkit (Koehn et al., 2007) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + ( #AUTHOR_TAG ).', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext augmented with english gigaword (Graff and Cieri, 2003).', 'we use a 5 - gram language model with modified kneser - ney smoothing.', 'feature weights are tuned to maximize bleu on the nist mteval 2006 test set using minimum error rate training (Och, 2003).', 'this is only done on the baseline systems.', 'the english data is tokenized using simple punctuation - based rules.', 'the arabic side is segmented according to the arabic treebank ( atb ) tokenization scheme (Maamouri et al., 2004) using the mada + tokan morphological analyzer and tokenizer v3. 1 (Habash and Rambow, 2005;Roth et al., 2008).', 'the arabic text is also alif / ya normalized.', 'mada - produced arabic lemmas are used for word alignment']",5
"['low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']","['low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']","['low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']","['the last system group, phrase + word - based selection, phrase - based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'phrase - based trans - lation is also added to word - based translation.', 'results show that selecting and translating phrases improve the three best performers of word - based selection.', 'the best performer, shown in the last raw, suggests using phrase - based selection and restricted word - based selection.', 'the restriction is to include oov words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'comparing the best performer to the oov selection mode system shows that translating low frequency in - vocabulary dialectal words and phrases to their msa paraphrases can improve the english translation.', 'this is a similar conclusion to our previous work in #AUTHOR_TAG']",1
['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ('],"['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + (Och and Ney, 2003).', 'phrase translations of up to 10 words are extracted in the']",['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ('],"['use the open - source moses toolkit ( #AUTHOR_TAG ) to build a phrase - based smt system trained on mostly msa data ( 64m words on the arabic side ) obtained from several ldc corpora including some limited da data.', 'our system uses a standard phrase - based architecture.', 'the parallel corpus is word - aligned using giza + + (Och and Ney, 2003).', 'phrase translations of up to 10 words are extracted in the moses phrase table.', 'the language model for our system is trained on the english side of the bitext augmented with english gigaword (Graff and Cieri, 2003).', 'we use a 5 - gram language model with modified kneser - ney smoothing.', 'feature weights are tuned to maximize bleu on the nist mteval 2006 test set using minimum error rate training (Och, 2003).', 'this is only done on the baseline systems.', 'the english data is tokenized using simple punctuation - based rules.', 'the arabic side is segmented according to the arabic treebank ( atb ) tokenization scheme (Maamouri et al., 2004) using the mada + tokan morphological analyzer and tokenizer v3. 1 (Habash and Rambow, 2005;Roth et al., 2008).', 'the arabic text is also alif / ya normalized.', 'mada - produced arabic lemmas are used for word alignment']",5
"['are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; ']","['are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; ']","[', speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing ( see, e. g., tanenhaus et al. 1995 ).', 'this sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well - formed utterance.', 'few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  shriberg, bear, & Dowding , 1992 )']","[""implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren't disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."", 'the first assumption is clearly false ; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses.', 'the rate is lower for speech to machines (Oviatt, 1995;Shriberg, 1996), due in part to utterance length ; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative.', 'the average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully.', 'the good news is that speakers can adapt to machines ; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.', 'as for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky.', 'in humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing ( see, e. g., tanenhaus et al. 1995 ).', 'this sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well - formed utterance.', 'few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ;  shriberg, bear, & Dowding , 1992 )']",0
"['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']","['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']","['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']","['to our previous work ( Chan and Ng , 2005 b ), we used the supervised wsd approach described in ( #AUTHOR_TAG ) for our experiments, using the naive bayes algorithm as our classifier.', 'knowledge sources used include partsof - speech, surrounding words, and local collocations.', 'this approach achieves state - of - the - art accuracy.', 'all accuracies reported in our experiments are micro - averages over all test examples']",5
"['the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences']","['our system is completely data - driven.', 'however, the comparison is indirect because our partitions of the ctb corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences']","['the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences']","['overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.', 'one likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora ( which may explain the higher baseline accuracy ), while our system is completely data - driven.', 'however, the comparison is indirect because our partitions of the ctb corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10 - fold cross validation, but used an uneven split.', 'we chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison']",5
"['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation )']","['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from Collins (2002) and are']","['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from Collins (2002) and are']","['built a two - stage baseline system, using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron pos tagging model from Collins ( 2002 ).', 'we use baseline system to refer to the system which performs segmentation first, followed by pos tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only ; and baseline postagger to refer to the collins tagger which performs pos tagging only ( given segmentation ).', 'the features used by the baseline segmentor are shown in table 1.', 'the features used by the pos tagger, some of which are different to those from Collins (2002) and are specific to chinese, are shown in table 2']",2
"['.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore,']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore,']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore, extending it']","['present an extension of the hierarchical dirichlet process ( hdp ) model which is able to represent each observable object ( i. e., event mention ) by a finite number of feature types l.', 'our hdp extension is also inspired from the bayesian model proposed by #AUTHOR_TAG.', 'however, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008)']",4
"['by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class. we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']",4
"['by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']","['approach has the merit of easily combining different features to predict the probability of each class.', 'we incorporate into the me based model the following informative context - based features to train cbsm and cbtm.', 'these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1']",4
"['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is']","['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is']","['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is no need for a pre - determined threshold as used in (Huang, 2009).', 'also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'our strategy is to divers']","['recently, an alignment selection approach was proposed in ( #AUTHOR_TAG ), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold.', 'the alignments used in that work were generated from different aligners ( hmm, block model, and maximum entropy model ).', 'in this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'there is no need for a pre - determined threshold as used in (Huang, 2009).', 'also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low - resource languages']",1
"[""by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by npˆs.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', '']","[""by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by npˆs.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', 'e. g., npˆs implicitly represents subject']","[""by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by npˆs.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', '']","['introduce two types of chunks.', 'the first is simply the phrase type, such as np, pp, of current chunk.', 'the column chunk 1 illustrates this kind of chunk type definition.', 'the second is more complicated.', ""inspired by ( #AUTHOR_TAG ), we split one phrase type into several subsymbols, which contain category information of current constituent's parent."", 'for example, an np immediately dominated by a s, will be substituted by npˆs.', 'this strategy severely increases the number of chunk types and make it hard to train chunking models.', 'to shrink this number, we linguistically use a cluster of ctb phrasal types, which was introduced in.', 'the column chunk 2 illustrates this definition.', 'e. g., npˆs implicitly represents subject while npˆvp represents object']",4
"['by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context']","['by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']","['feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']","['feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach ( #AUTHOR_TAG ).', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'however, as the size of the corpus increases, the maximum entropy model will become larger.', 'similarly, in (Shen et al., 2009), context language model is proposed for better rule selection.', 'taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information']",4
"['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']","['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']","['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']","['sentiment - analysis work in different domains has considered inter - document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter - document references in the form of hyperlinks ( #AUTHOR_TAG )']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and #AUTHOR_TAG.', 'Zhu (2005) maintains a survey of this area']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']","['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']","['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']","['sophisticated approaches have been proposed ( #AUTHOR_TAG ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ).', 'also relevant is work on the gen - eral problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)']",0
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 )']",0
"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['##xxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for']","['sentiment - polarity classifiers proposed in the recent literature categorize each document in - dependently. a few others incorporate various measures of inter - document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'many interesting opinion - oriented docu - ments, however, can be linked through certain re - lationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement be - tween two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see Mullen and Malouf (2006) but cfxxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categorize a complicated ( or overly terse ) document if we find within it indica - tions of agreement with a clearly positive text']","['##xxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for']","['sentiment - polarity classifiers proposed in the recent literature categorize each document in - dependently. a few others incorporate various measures of inter - document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'many interesting opinion - oriented docu - ments, however, can be linked through certain re - lationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement be - tween two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see Mullen and Malouf (2006) but cfxxx #AUTHOR_TAG.', 'agreement evidence can be a powerful aid in our classification task : for ex - ample, we can easily categorize a complicated ( or overly terse ) document if we find within it indica - tions of agreement with a clearly positive text']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresent']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained svm decision plane : ds 234s d s 234s ds 234s def ds = + 234s']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresent']","['our experiments, we employed the well - known classifier svmlight to obtain individual - document classification scores, treating y as the positive class and using plain unigrams as features. 5', 'fol - lowing standard practice in sentiment analysis ( #AUTHOR_TAG ), the input to svmlight con - sisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors.', 'the ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained svm decision plane : ds 234s d s 234s ds 234s def ds = + 234s 2 ind s ; y where 34s is the standard deviation of d s over all speech segments s in the debate in question, and def ind s ; n = ind s ; y']",5
"['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG )']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 )""]",0
"['has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']","['has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyper - links (Agrawal et al., 2003)']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be inter - esting to investigate the application of such meth - ods to our problem.', 'however, we also believe that our approach has important advantages, in - cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 )']",3
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( #AUTHOR_TAG ), and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 )']",0
"['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', '']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual 4 evidence of a high likelihood of agreement be - 4 because we are most interested in techniques applicable across domains, we restrict consideration to nlp aspects of the problem, ignoring external problem - specific information.', 'for example, although most votes in our corpus were almost completely along party lines ( and despite the fact that sameparty information is easily incorporated via the methods we propose ), we did not use party - affiliation data.', ""indeed, in other settings ( e. g., a movie - discussion listserv ) one may not be able to determine the participants'political leanings, and such information may not lead to significantly improved results even if it were available""]",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed ( Hillard et al. , 2003 ), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ).', 'also relevant is work on the general problems of dialog - act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002)']",0
"['in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['has been previously observed and exploited in the nlp literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"["": electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text,']","["": electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text,']","["": electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text,']","[""the united states, for example, governmental bodies are providing and soliciting political documents via the internet, with lofty goals in mind : electronic rulemaking ( erulemaking ) initiatives involving the ` ` electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process'', may ` ` [ alter ] the citizen - government relationship'' ( #AUTHOR_TAG )."", 'additionally, much media attention has been focused recently on the potential impact that internet sites may have on politics 2, or at least on political journalism 3.', 'regardless of whether one views such claims as clear - sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process']",0
"['distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 )']",0
"['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), kondor and Lafferty (2002), and Joachims (2003).', '#AUTHOR_TAG maintains a survey of this area']",0
"['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']",0
"['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), #AUTHOR_TAG, and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"['in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['has been previously observed and exploited in the nlp literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"['by #AUTHOR_TAG, integrates both']","['by #AUTHOR_TAG, integrates both']","['by #AUTHOR_TAG, integrates both perspectives,']","['support / oppose classification problem can be approached through the use of standard classifiers such as support vector machines ( svms ), which consider each text unit in isolation.', 'as discussed in section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'our classification framework, directly inspired by #AUTHOR_TAG, integrates both perspectives, optimizing its labeling of speech segments based on both individual speech - segment classification scores and preferences for groups of speech segments to receive the same label.', 'in this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships']",5
"['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']","['have applied the nlp technologies of near - duplicate detection and topic - based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 )']",0
"['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), Hearst ( 1992 ), Sack ( 1994 ), and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include #AUTHOR_TAG, Bansal et al. ( 2002 ), Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']",0
"['in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages,']","['in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages,']","['currently do not have an efficient means to encode disagreement information as hard constraints ; we plan to investigate incorporating such information in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages,']","['currently do not have an efficient means to encode disagreement information as hard constraints ; we plan to investigate incorporating such information in future work.', 'relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e. g., between requests and satisfactions thereof ) to classify messages, and thus also explicitly exploit the structure of conversations']",0
"[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']","[""##ative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'people are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that ( u. s. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ).', 'moreover, political opinions are exsional bills and related data was launched in january 1995, when mosaic was not quite two years old and altavista did not yet exist']",0
"['in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination']","['has been previously observed and exploited in the nlp literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.', 'in our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision']",1
"['distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 )""]",0
"['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), #AUTHOR_TAG, Kondor and Lafferty ( 2002 ), and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']",0
"['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994) ; see Esuli (2006) for an active bibliography ).', ""in particular, since we treat each individual speech within a debate as a single ` ` document'', we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG )""]",0
"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes #AUTHOR_TAG, Hearst ( 1992 ), Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']","[', several alternative, often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ).', 'it would be interesting to investigate the application of such methods to our problem.', 'however, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve']",0
"['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']","['early papers on graph - based semisupervised learning include Blum and Chawla ( 2001 ), Bansal et al. ( 2002 ), #AUTHOR_TAG, and Joachims ( 2003 ).', 'Zhu (2005) maintains a survey of this area']",0
"['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']","['used a simple method to learn to identify cross - speaker references indicating agreement.', 'more sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ), citation analysis ( Lehnert et al. , 1990 ), and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG )']",0
"['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['"" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is #AUTHOR_TAG, who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']","['##light is available at svmlight. joachims. org.', 'default parameters were used, although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG )']",3
"['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused']","['##ly - oriented text sentiment analysis has specifically been proposed as a key enabling technology in erulemaking, allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ).', 'there has also been work focused upon determining the political leaning ( e. g., "" liberal "" vs. "" conservative "" ) of a document or author, where most previously - proposed methods make no direct use of relationships between the documents to be classified ( the "" unlabeled "" texts ) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'an exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site']",0
"['! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task :']","['example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task :']","['example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task :']","['sentiment - polarity classifiers proposed in the recent literature categorize each document independently.', 'a few others incorporate various measures of inter - document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'many interesting opinion - oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'for example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions ( i second that! ) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfxxx Agrawal et al. ( 2003 ) ).', 'agreement evidence can be a powerful aid in our classification task : for example, we can easily categorize a complicated ( or overly terse ) document if we find within it indications of agreement with a clearly positive text']",0
"['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']","['sentiment - analysis work in different domains has considered inter - document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter - document references in the form of hyperlinks (Agrawal et al., 2003)']",0
"['determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 )']","['properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ), #AUTHOR_TAG, Sack ( 1994 ), and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ).', 'in particular, since we treat each individual speech within a debate as a single "" document "", we are considering a version of document - level sentiment - polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003)']",0
"['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']","['have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'when the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'for example, when using bleu, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single - sentence objective functions.', 'our plan is to implement a windowed or moving - average version of bleu as in ( #AUTHOR_TAG )']",3
"['. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i.']","['( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e. (Bloehdorn and Moschitti, 2007 a ; Bloehdorn and Moschitti, 2007 b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '( ii ) the latter can be further boosted by studying complex structural kernels,']","['. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e. (Bloehdorn and Moschitti, 2007 a ; Bloehdorn and Moschitti, 2007 b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '(']","['flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness, e. g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ).', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'however, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'therefore, the use of the syntactic / semantic kernels, i. e. (Bloehdorn and Moschitti, 2007 a ; Bloehdorn and Moschitti, 2007 b ), to syntactically contextualize word similarities may improve the reranker accuracy.', '( ii ) the latter can be further boosted by studying complex structural kernels, e. g.', '(Moschitti, 2008;Nguyen et al., 2009;Dinarelli et al., 2009).', '( iii ) more specific predicate argument structures such those proposed in framenet, e. g.', '(Baker et al., 1998;Giuglea and Moschitti, 2004;Giuglea and Moschitti, 2006;Johansson and Nugues, 2008 b ) may be useful to characterize the opinion holder and the sentence semantic context.', 'finally, while the strategy based on reranking resulted in a significant performance boost, it remains to be seen whether a higher accuracy can be achieved by developing a more sophisticated inference algorithm based on dynamic programming.', 'however, while the development of such an algorithm is an interesting problem, it will not necessarily result in a more usable system - when using a reranker, it is easy to trade accuracy for efficiency']",0
"['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']","['right column in table 1 shows the scores if ( using the product - of - ranks algorithm ) four source languages are taken into account in parallel.', 'as can be seen, with an average score of 51. 8 the improvement over the english only variant ( 50. 6 ) is minimal.', 'this contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages.', 'so this casts some doubt on these.', 'however, as english was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'this is not the case here, where we try to improve on a score of around 50 for english.', 'remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'as this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'therefore, perhaps we should take it as a success that the product - of - ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non - english languages was probably mostly detrimental']",1
['#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.'],"['#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i. e. the words occurring in a particular word equation ) within the association vector of a translation candidate, and by multiplying these ranks.', 'so for each candidate we obtain a product of ranks.', 'we then assume that the candidate with the smallest product will be the best translation. 3', '']",['#AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.'],"['far, we always computed translations to single source words.', 'however, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product - of - ranks algorithm.', 'as suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i. e. the words occurring in a particular word equation ) within the association vector of a translation candidate, and by multiplying these ranks.', 'so for each candidate we obtain a product of ranks.', 'we then assume that the candidate with the smallest product will be the best translation. 3', 'et us illustrate this by an example : if the given words are the variants of the word nervous in english, french, german, and spanish, i. e. nervous, nerveux, nervos, and nervioso, and if we want to find out their translation into italian, we would look at the association vectors of each word in our italian target vocabulary.', 'the association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words.', 'then for each vector we compute the product of the four ranks, and finally sort the italian vocabulary according to these products.', 'we would then expect that the correct italian translation, namely nervoso, ends up in the first position, i. e. has the smallest value for its product of ranks']",4
"['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a']","['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a']","['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'for example, it is more']","['#AUTHOR_TAG dealt only with an english corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora.', 'we were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'for example, it is more promising to look at occurrences of english words in a german corpus rather than the other way around.', 'because of the special status of english it is also advisable to use it as a pivot wherever possible']",1
"['we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically']","['we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically']","['from text.', 'in future work, we will focus on mapping text ( in monologue form ) to dialogue.', 'for this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'for automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically generating dialogues from naturally - occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue']","['current work has focussed on high - level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text.', 'in future work, we will focus on mapping text ( in monologue form ) to dialogue.', 'for this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'for automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG.', 'an important component of our future effort will be to evaluate whether automatically generating dialogues from naturally - occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue']",3
"['.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this']","['to have fair coverage for simstring and gazetteer features.', 'while an advantage over internal was observed for super grec, simstring features showed no benefit over gazetteer features.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this']","['##a, genia and id we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'by contrast, for super grec there are several distinct classes for which we expected lexical resources to have fair coverage for simstring and gazetteer features.', 'while an advantage over internal was observed for super grec, simstring features showed no benefit over gazetteer features.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']","['we expected to see clear benefits from both using gazetteers and simstring features, our exper - iments returned negative results for the majority of the corpora.', 'for nlpba, genia and id we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'by contrast, for super grec there are several distinct classes for which we expected lexical resources to have fair coverage for simstring and gazetteer features.', 'while an advantage over internal was observed for super grec, simstring features showed no benefit over gazetteer features.', 'the methods exhibited the expected result on only one of the six corpora, calbc cii, where there is a clear advantage for gazetteer over internal and a further clear advantage for simstring over gazetteer.', 'disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'although we have not been successful in figure 6 : learning curve for super grec figure 7 : learning curve for epi proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'it may be that our notion of distance to lexical resource entries is too naive.', 'a possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG.', 'this would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry']",3
"['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']","['article classifier is a discriminative model that draws on the state - of - the - art approach described in #AUTHOR_TAG.', 'the model makes use of the averaged perceptron ( ap ) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'the article module uses the pos and chunker output to generate some of its features and candidates ( likely contexts for missing articles )']",5
"['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb (']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb ( details in Rozovskaya and Roth (2011) )']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb (']","['choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine - learning methods on error correction tasks ( #AUTHOR_TAG ).', 'thus, the classifiers trained on the learner data make use of a discriminative model.', 'because the google corpus does not contain complete sentences but only n - gram counts of length up to five, training a discriminative model is not desirable, and we thus use nb ( details in Rozovskaya and Roth (2011) )']",4
"['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']",1
"['by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']","['by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']","['by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']","['first row of the table 6 represents the baseline results.', 'though the precision is high for agreement category, the recall is quite low and that results in a poor overall f1 measure.', ""this shows that even though markers like'agree'or'disagree'for the next set of experiments we used a supervised machine learning approach for the two - way classification ( agree / disagree )."", 'we use support vector machines ( svm ) as our machine - learning algorithm for classification as implemented in weka (Hall et al., 2009) and ran 10 - fold cross validation.', 'as a svm baseline, we first use all unigrams in callout and target as features ( table 6, row 2 ).', 'we notice that the recall improves significantly when compared with the rule - based method.', 'to further improve the classification accuracy, we use mutual information ( mi ) to select the words in the callouts and targets that are likely to be associated with the categories agree and disagree, respectively.', 'specifically, we sort each word based on its mi value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'the feature vector includes only words present in the mi list.', 'compared to the all unigrams baseline, the mi - based unigrams improve the f1 by 4 % ( agree ) and 2 % ( disagree ) ( table 6 ).', 'the mi approach discovers the words that are highly associated with agree / disagree categories and these words turn to be useful features for classification.', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG )']",4
"['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']","['line of research that is correlated with ours is recognition of agreement / disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums.', 'for future work, we can utilize textual features ( contextual, dependency, discourse markers ), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the agree / disagree classification accuracy']",1
"['by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']","['first row of the table 6 represents the baseline results.', 'though the precision is high for agreement category, the recall is quite low and that results in a poor overall f1 measure.', ""this shows that even though markers like'agree'or'disagree'for the next set of experiments we used a supervised machine learning approach for the two - way classification ( agree / disagree )."", 'we use support vector machines ( svm ) as our machine - learning algorithm for classification as implemented in weka (Hall et al., 2009) and ran 10 - fold cross validation.', 'as a svm baseline, we first use all unigrams in callout and target as features ( table 6, row 2 ).', 'we notice that the recall improves significantly when compared with the rule - based method.', 'to further improve the classification accuracy, we use mutual information ( mi ) to select the words in the callouts and targets that are likely to be associated with the categories agree and disagree, respectively.', 'specifically, we sort each word based on its mi value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'the feature vector includes only words present in the mi list.', 'compared to the all unigrams baseline, the mi - based unigrams improve the f1 by 4 % ( agree ) and 2 % ( disagree ) ( table 6 ).', 'the mi approach discovers the words that are highly associated with agree / disagree categories and these words turn to be useful features for classification.', 'in addition, we consider several types of lexical features ( lexf ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 )']",4
