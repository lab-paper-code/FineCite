{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finecite'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     17\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m/home/dataconv/deallab/lasse/CCE\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfinecite\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfinecite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_seed, DATA_DIR, get_class_weights, CustomDataset, load_processor\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfinecite\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MODEL_DESCRIPTION\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'finecite'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "FINECITE_PATH = os.getenv('FINECITE_PATH')\n",
    "if FINECITE_PATH not in sys.path:\n",
    "    sys.path.append('/home/dataconv/deallab/lasse/CCE')\n",
    "\n",
    "from finecite import set_seed, DATA_DIR, get_class_weights, CustomDataset, load_processor\n",
    "from finecite.model import MODEL_DESCRIPTION\n",
    "from finecite.model import CCAModule\n",
    "#fix sample extraction (only possible to extract one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Seq_tagger parser')\n",
    "\n",
    "    #input arguments\n",
    "    parser.add_argument('--model_name', required=True, help='scibert llm2vec_mistral llm2vec_llama3')\n",
    "    parser.add_argument('--dataset', required=True, help='acl-arc, act2, scicite, multicite, finecite')\n",
    "    parser.add_argument('--task', required=True, help='ext cls')\n",
    "    parser.add_argument('--ext_type', required=True, help='linear, bilstm, crf, bilstm_crf')\n",
    "    parser.add_argument('--cls_type', default= 'balanced', help='weighted, balanced, linear, auto_wighted')\n",
    "    parser.add_argument('--cls_weights', type=list[int], default=None)\n",
    "    parser.add_argument('--heal_token', default=None)\n",
    "    \n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=4, help='')\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-05, help='')\n",
    "    parser.add_argument('--dropout', type=float, default=0.0, help='')\n",
    "\n",
    "    parser.add_argument('--save_model', type=bool, defualt=True, help='')\n",
    "    parser.add_argument('--debug', action='store_true', help='')\n",
    "    parser.add_argument('--debug_size', type=int, default=5, help='')\n",
    "    parser.add_argument(\"--seed\", type=int, default=4455, help='')\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Seq_tagger parser')\n",
    "args = parser.parse_args([])\n",
    "args.model_name = 'scibert' # scibert llm2vec_mistral llm2vec_llama3 modern_bert# scibert 4, 2e-05, 0.1\n",
    "args.dataset = 'd-act2' # 'acl-arc', 'act2', 'scicite', 'multicite', finecite\n",
    "args.task = 'cls' # 'ext', 'cls'\n",
    "args.ext_type = 'linear' # linear, bilstm, crf, bilstm_crf\n",
    "args.cls_type = 'linear' # weighted, balanced, linear, 'auto_wighted total #best is total and linear, auto weight shows importance of scopes\n",
    "args.cls_weights = [1, 2, 0]\n",
    "args.heal_token = 'word' # phrase, word\n",
    "\n",
    "args.batch_size = 4\n",
    "args.learning_rate = 2e-05\n",
    "args.crf_learning_rate = 0.005\n",
    "args.dropout = 0.1\n",
    "\n",
    "args.save_model = False\n",
    "args.cached_data = True\n",
    "args.debug = True\n",
    "args.debug_size = None\n",
    "args.seed = 4455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static arguments\n",
    "args.max_epochs = 20\n",
    "args.patients = 3\n",
    "args.adam_epsilon = 1e-08\n",
    "args.weight_decay = 0.0\n",
    "args.use_prompt = None#args.model_name != 'scibert'\n",
    "args.max_len = 512 if args.model_name == 'scibert' else 740\n",
    "args.dtype = torch.float32\n",
    "\n",
    "# model description\n",
    "args.model_desc = MODEL_DESCRIPTION[args.model_name]\n",
    "\n",
    "# input directory\n",
    "args.input_dir = f'{DATA_DIR}/model_training/{args.dataset}/'\n",
    "\n",
    "# output directory\n",
    "if args.debug:\n",
    "    args.output_dir = f\"./output/_debug/{args.dataset}/{args.model_name}/{args.ext_type}_{args.batch_size}_{args.learning_rate}_{args.dropout}_{datetime.now().strftime('%m_%d_%H_%M_%S')}/\"\n",
    "else: \n",
    "    args.output_dir = f\"./output/{args.dataset}/{args.model_name}/{args.ext_type}_{args.batch_size}_{args.learning_rate}_{args.dropout}_{datetime.now().strftime('%m_%d_%H_%M_%S')}/\"\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "if args.save_model:\n",
    "    args.model_output_dir = f\"{DATA_DIR}/model_training/output/{args.dataset}/{args.model_name}/{args.batch_size}_{args.learning_rate}_{args.dropout}_{datetime.now().strftime('%m_%d_%H_%M_%S')}/\"\n",
    "    os.makedirs(args.model_output_dir, exist_ok=True)\n",
    "\n",
    "# model cache dir\n",
    "if args.model_name != 'scibert':\n",
    "    args.base_model_dir =  f'{DATA_DIR}/model_training/llm2vec_models/{args.model_name}/'\n",
    "    os.makedirs(args.base_model_dir, exist_ok=True)\n",
    "    \n",
    "#data cache dir\n",
    "if args.task == 'cls':\n",
    "    args.chache_dir =  f'{DATA_DIR}/.cache/{args.dataset}/'\n",
    "    os.makedirs(args.chache_dir, exist_ok=True)\n",
    "\n",
    "# model input dir\n",
    "args.trained_model_dir = f'{DATA_DIR}/model_training/output/finecite/{args.model_name}/{args.ext_type}'  #if args.dataset != 'finecite' else None\n",
    "\n",
    "set_seed(args.seed)\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "loading scibert model from /raid/deallab/CCE_Data/model_training/output/finecite/scibert/linear\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer\n",
    "print('loading model...')\n",
    "model = CCAModel(args)\n",
    "model.load_pretrained()\n",
    "tokenizer = model.tokenizer\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33646112600536193, 1.5987261146496816, 2.9186046511627906, 11.952380952380953, 1.5886075949367089, 0.742603550295858]\n",
      "[0.29733163913595934, 1.3371428571428572, 2.3168316831683167, 7.8, 1.786259541984733, 1.3]\n",
      "adding classifier...\n",
      "configuring optimizer...\n",
      "loading scibert model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding classifier...\n",
      "configuring optimizer...\n",
      "1506 1404\n"
     ]
    }
   ],
   "source": [
    "#load data processor\n",
    "processor = load_processor(args)\n",
    "dataset = CustomDataset(args, tokenizer)\n",
    "\n",
    "# load data\n",
    "train_data = processor.load_train_data()\n",
    "test_data = processor.load_test_data()\n",
    "\n",
    "# create dataset\n",
    "train_ds, weights1, num_labels = dataset.load_data(train_data)\n",
    "test_ds, weights2, num_labels = dataset.load_data(test_data)\n",
    "weights = [(w1 * len(train_ds) + w2 * len(test_ds)) / (len(train_ds) + len(test_ds)) for w1, w2 in zip(weights1, weights2)]\n",
    "num_training_steps = int(len(train_data) / args.batch_size) * args.max_epochs\n",
    "model.configurate(weights, num_labels, num_training_steps)\n",
    "\n",
    "# add context labels if cls\n",
    "if args.task == 'cls':\n",
    "    train_file = f'{args.model_name}_{args.ext_type}_{args.heal_token}_train.pt'\n",
    "    test_file = f'{args.model_name}_{args.ext_type}_{args.heal_token}_test.pt'\n",
    "    # check cached examples\n",
    "    if args.cached_data and train_file in os.listdir(args.chache_dir) and test_file in os.listdir(args.chache_dir):\n",
    "        train_ds = torch.load(os.path.join(args.chache_dir, train_file), weights_only=False)\n",
    "        test_ds = torch.load(os.path.join(args.chache_dir, test_file), weights_only=False)\n",
    "    else:\n",
    "    \n",
    "        # dataloader\n",
    "        train_dl = DataLoader(train_ds, batch_size = args.batch_size, shuffle=False)\n",
    "        test_dl = DataLoader(test_ds, batch_size = args.batch_size, shuffle=False)\n",
    "        \n",
    "        #extract context\n",
    "        train_lbls = model.extract(train_dl)\n",
    "        test_lbls = model.extract(test_dl)\n",
    "        \n",
    "        #add context to dataset\n",
    "        train_ds = dataset.add_context_lbls(train_ds, train_lbls)\n",
    "        test_ds = dataset.add_context_lbls(test_ds, test_lbls)\n",
    "        \n",
    "        #cache data\n",
    "        torch.save(train_ds, os.path.join(args.chache_dir, train_file))\n",
    "        torch.save(test_ds, os.path.join(args.chache_dir, test_file))\n",
    "        \n",
    "    #reload model from_pretrained\n",
    "    model.reload()\n",
    "\n",
    "print(len(train_ds), len(test_ds))\n",
    "\n",
    "#Dataloader\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=args.batch_size, num_workers=0) \n",
    "val_dataloader =  DataLoader(test_ds, shuffle=True, batch_size=args.batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging run_setup\n",
      "Logging input sample\n",
      "First example input text: ['[CLS]', 'an', 'important', 'issue', 'is', 'how', 'the', 'vortex', 'shedding', 'is', 'modified', 'when', 'a', 'magnetic', 'field', 'is', 'present', '.', 'full', '##scale', 'numerical', 'simulations', 'of', 'emerging', 'magnetic', 'flux', 'tubes', 'have', 'demonstrated', 'the', 'importance', 'of', 'this', 'effect', 'and', 'the', 'oscillating', '(', '\"', 'zig', '##za', '##ging', '\"', ')', 'trajectory', 'of', 'the', 'tube', '<', 'citation', '/', '>', '.', '<', 'citation', '/', '>', 'simulated', 'the', 'interaction', 'of', 'a', 'cylinder', 'with', 'a', 'magnet', '##ised', 'flow', '.', 'it', 'was', 'found', 'that', 'for', 'the', 'field', 'parallel', 'to', 'the', 'axis', 'of', 'the', 'cylinder', ',', 'the', 'stro', '##uh', '##al', 'number', 'was', 'about', '0', '.', '2', '.', 'thus', ',', 'it', 'can', 'be', 'taken', 'that', ',', 'at', 'least', 'in', 'the', 'case', 'when', 'the', 'external', 'magnetic', 'field', '(', 'b', '0', 'in', 'fig', '.', '1', ')', 'is', 'parallel', 'to', 'the', 'axis', 'of', 'the', 'loop', 'segment', ',', 'the', 'up', '##flow', 'is', 'efficiently', 'accompanied', 'by', 'the', 'vortex', 'shedding', 'phenomenon', '.', 'on', 'the', 'other', 'hand', ',', 'if', 'the', 'field', 'in', 'the', 'flow', 'is', 'perpendicular', 'to', 'the', 'axis', 'of', 'the', 'cylinder', ',', 'magnetic', 'recon', '##nec', '##tion', 'should', 'be', 'taken', 'into', 'account', '.', 'the', 'range', 'of', 'parameters', 'in', 'the', 'simulations', 'of', '<', 'citation', '/', '>', 'was', 'very', 'different', 'from', 'the', 'coronal', 'plasma', '.', 'however', ',', 'the', 'phenomenon', 'of', 'vortex', 'shedding', 'has', 'been', 'considered', 'in', 'space', 'plasma', '##s', ',', 'too', '.', 'in', 'the', 'ion', '##ospheric', 'context', 'the', 'effect', 'of', 'vortex', 'shedding', 'has', 'been', 'intensively', 'studied', 'in', 'connection', 'with', 'f', 'region', 'bubbles', ',', 'the', 'field', 'aligned', 'plasma', 'density', 'depletion', '##s', 'percol', '##ating', 'through', 'the', 'equatorial', 'ion', '##osphere', '<', 'citation', '/', '>', '.', 'the', 'wake', 'of', 'the', 'bubble', 'tends', 'to', 'be', 'a', 'series', 'of', 'vortices', 'alternatively', 'being', 'shed', 'from', 'either', 'side', 'of', 'the', 'bubble', ',', 'causing', 'a', 'slight', 'rock', '##ing', 'of', 'the', 'bubble', 'in', 'the', 'transverse', 'direction', '.', '<', 'target', '_', 'citation', '/', '>', 'linked', 'long', '##period', 'geo', '##magn', '##etic', 'puls', '##ations', 'pc', '##5', 'with', 'vortex', 'shedding', 'in', 'the', 'solar', 'wind', 'stream', '##lin', '##ing', 'the', 'earth', \"'\", 's', 'magnet', '##osphere', ',', 'and', 'estimated', 'the', 'stro', '##uh', '##al', 'number', 'as', '0', '.', '3', '.', 'clear', 'evidence', 'of', 'roll', '##edu', '##p', 'mh', '##ds', '##cale', 'vortices', 'in', 'the', 'magnet', '##osphere', 'has', 'come', 'from', 'multi', '##space', '##craft', 'observations', 'by', 'cluster', '<', 'citation', '/', '>', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Num pred targets (cls: [CLS], sep: [SEP], pad: [PAD]): 357\n",
      "Special tokens in input: []\n",
      "Labels [-100, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Labels [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# log model setup\n",
    "print(f'Logging run_setup')\n",
    "print_args = {k: str(v) for k,v in vars(args).items()}\n",
    "with open(os.path.join(args.output_dir, f'run_setup.json'), 'w') as f_out:\n",
    "    json.dump(print_args, f_out, indent=4)\n",
    "\n",
    "# log imput sample\n",
    "print(f'Logging input sample')\n",
    "input_sample = [tokenizer.convert_ids_to_tokens(ids=train_ds[i]['input_ids']) for i in range(3)]\n",
    "\n",
    "with open(os.path.join(args.output_dir, f'input_sample.json'), 'w') as f_out:\n",
    "    json.dump(input_sample, f_out, indent=4)\n",
    "    \n",
    "# print sample text\n",
    "first_example = train_ds[0]\n",
    "sample_text = tokenizer.convert_ids_to_tokens(ids=first_example['input_ids'])\n",
    "print(f'First example input text: {sample_text}')\n",
    "#print number of predicting targets\n",
    "num_pred_targets = len([token for token in first_example['input_ids'] if token not in [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]])\n",
    "print(f'Num pred targets (cls: {tokenizer.cls_token}, sep: {tokenizer.sep_token}, pad: {tokenizer.pad_token}): {num_pred_targets}')\n",
    "#print special token in example\n",
    "special_token_ids = [token for token in first_example['input_ids'] if token in tokenizer.additional_special_tokens_ids]\n",
    "print(f'Special tokens in input: {tokenizer.convert_ids_to_tokens(ids=special_token_ids)}')\n",
    "#print labels in example\n",
    "labels = first_example['tok_lbl'].tolist()\n",
    "print(f'Labels {labels}')\n",
    "if 'cls' in args.task:\n",
    "    labels = first_example['int_lbl'].tolist()\n",
    "    print(f'Labels {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting training...\n",
      "{'current_epoch': 1, 'current_step': 100, 'avg_loss': 1.81595, 'max_loss': 8.33996, 'min_loss': 0.55976}\n",
      "{'current_epoch': 1, 'current_step': 200, 'avg_loss': 1.62481, 'max_loss': 5.70148, 'min_loss': 0.57726}\n",
      "{'current_epoch': 1, 'current_step': 300, 'avg_loss': 1.91373, 'max_loss': 7.42978, 'min_loss': 0.58009}\n",
      "{'epoch': 1, 'loss': 1.895, 'macro_f1': 0.087, 'micro_f1': 0.109, 'precision': [0.0, 0.167, 0.082, 0.0, 0.089, 0.23], 'recall': [0.0, 0.017, 0.109, 0.0, 0.71, 0.256], 'f1': [0.0, 0.031, 0.094, 0.0, 0.158, 0.242]}\n",
      "{'current_epoch': 2, 'current_step': 400, 'avg_loss': 1.68692, 'max_loss': 4.64877, 'min_loss': 0.67063}\n",
      "{'current_epoch': 2, 'current_step': 500, 'avg_loss': 1.68206, 'max_loss': 6.68931, 'min_loss': 0.57771}\n",
      "{'current_epoch': 2, 'current_step': 600, 'avg_loss': 1.59016, 'max_loss': 6.74469, 'min_loss': 0.47892}\n",
      "{'current_epoch': 2, 'current_step': 700, 'avg_loss': 1.57176, 'max_loss': 6.42027, 'min_loss': 0.57718}\n",
      "{'epoch': 2, 'loss': 1.89, 'macro_f1': 0.104, 'micro_f1': 0.152, 'precision': [1.0, 0.154, 0.0, 0.056, 0.102, 0.206], 'recall': [0.001, 0.057, 0.0, 0.033, 0.527, 0.739], 'f1': [0.003, 0.083, 0.0, 0.042, 0.171, 0.322]}\n",
      "{'current_epoch': 3, 'current_step': 800, 'avg_loss': 1.26088, 'max_loss': 2.4493, 'min_loss': 0.5182}\n",
      "{'current_epoch': 3, 'current_step': 900, 'avg_loss': 1.3101, 'max_loss': 5.39671, 'min_loss': 0.43909}\n",
      "{'current_epoch': 3, 'current_step': 1000, 'avg_loss': 1.28881, 'max_loss': 5.34263, 'min_loss': 0.40886}\n",
      "{'current_epoch': 3, 'current_step': 1100, 'avg_loss': 1.32504, 'max_loss': 6.89484, 'min_loss': 0.40843}\n",
      "{'epoch': 3, 'loss': 1.867, 'macro_f1': 0.251, 'micro_f1': 0.371, 'precision': [0.639, 0.203, 0.133, 0.158, 0.078, 0.323], 'recall': [0.398, 0.274, 0.04, 0.2, 0.115, 0.75], 'f1': [0.49, 0.234, 0.061, 0.176, 0.093, 0.452]}\n",
      "{'current_epoch': 4, 'current_step': 1200, 'avg_loss': 0.93924, 'max_loss': 2.7876, 'min_loss': 0.2508}\n",
      "{'current_epoch': 4, 'current_step': 1300, 'avg_loss': 1.03192, 'max_loss': 2.75607, 'min_loss': 0.27941}\n",
      "{'current_epoch': 4, 'current_step': 1400, 'avg_loss': 0.96956, 'max_loss': 2.6911, 'min_loss': 0.24605}\n",
      "{'current_epoch': 4, 'current_step': 1500, 'avg_loss': 0.94674, 'max_loss': 2.13129, 'min_loss': 0.31341}\n",
      "{'epoch': 4, 'loss': 2.078, 'macro_f1': 0.217, 'micro_f1': 0.305, 'precision': [0.602, 0.165, 0.059, 0.0, 0.069, 0.516], 'recall': [0.294, 0.463, 0.139, 0.0, 0.053, 0.528], 'f1': [0.395, 0.244, 0.083, 0.0, 0.06, 0.522]}\n",
      "{'current_epoch': 5, 'current_step': 1600, 'avg_loss': 0.78908, 'max_loss': 2.35383, 'min_loss': 0.26073}\n",
      "{'current_epoch': 5, 'current_step': 1700, 'avg_loss': 0.8145, 'max_loss': 2.83159, 'min_loss': 0.23052}\n",
      "{'current_epoch': 5, 'current_step': 1800, 'avg_loss': 0.76988, 'max_loss': 2.24855, 'min_loss': 0.12169}\n",
      "{'epoch': 5, 'loss': 2.204, 'macro_f1': 0.222, 'micro_f1': 0.345, 'precision': [0.62, 0.173, 0.061, 0.0, 0.069, 0.433], 'recall': [0.379, 0.371, 0.139, 0.0, 0.038, 0.572], 'f1': [0.47, 0.236, 0.085, 0.0, 0.049, 0.493]}\n",
      "{'current_epoch': 6, 'current_step': 1900, 'avg_loss': 0.52475, 'max_loss': 0.9722, 'min_loss': 0.22414}\n",
      "{'current_epoch': 6, 'current_step': 2000, 'avg_loss': 0.6241, 'max_loss': 2.10224, 'min_loss': 0.18251}\n",
      "{'current_epoch': 6, 'current_step': 2100, 'avg_loss': 0.66331, 'max_loss': 2.52643, 'min_loss': 0.14623}\n",
      "{'current_epoch': 6, 'current_step': 2200, 'avg_loss': 0.67754, 'max_loss': 1.95755, 'min_loss': 0.14046}\n",
      "{'epoch': 6, 'loss': 2.542, 'macro_f1': 0.228, 'micro_f1': 0.316, 'precision': [0.627, 0.187, 0.098, 0.0, 0.104, 0.47], 'recall': [0.301, 0.411, 0.059, 0.0, 0.313, 0.483], 'f1': [0.407, 0.257, 0.074, 0.0, 0.156, 0.477]}\n",
      "Logging validation scores for best epoch\n",
      "Logging best output samples\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "print('\\nstarting training...')\n",
    "model.to(args.device)\n",
    "args.best_value = 0\n",
    "args.best_value_epoch = 0\n",
    "best_val_res = {}\n",
    "best_output_samples = ''\n",
    "\n",
    "for epoch in range(args.max_epochs):\n",
    "    model.epoch()\n",
    "    model.train_epoch(train_dataloader)\n",
    "    val_metric, val_res, output_samples = model.evaluate(val_dataloader)\n",
    "    if val_metric > args.best_value:\n",
    "        args.best_value = val_metric\n",
    "        args.best_value_epoch = epoch\n",
    "        best_val_res = val_res\n",
    "        best_output_samples = output_samples\n",
    "        if args.save_model:\n",
    "            model.save_pretrained(args.model_output_dir)\n",
    "    else:\n",
    "        if epoch >= args.best_value_epoch + args.patients:\n",
    "            break\n",
    "        \n",
    "print(f'Logging validation scores for best epoch')\n",
    "with open(os.path.join(args.output_dir, f'best_scores.json'), 'w') as f_out:\n",
    "        json.dump(best_val_res, f_out, indent=4)\n",
    "        \n",
    "print(f'Logging best output samples')\n",
    "with open(os.path.join(args.output_dir, f'output_samples.txt'), 'w') as f_out:\n",
    "        f_out.write(best_output_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting training...\n",
    "# {'epoch': 1, 'loss1': [3.3438, 2.8906, 0.4453], 'loss2': [3.5312, 3.1094, 0.4277], 'acc': [0.316, 0.308], 'macro_f1': [0.181, 0.169], 'total_f1': [0.568, 0.545], 'inf_f1': [0.285, 0.239], 'perc_f1': [0.149, 0.158], 'backg_f1': [0.11, 0.111], 'inf_emb': [0.23828125, 0.23828125], 'perc_emb': [0.33984375, 0.33984375], 'back_emb': [0.265625, 0.36328125]}\n",
    "# {'epoch': 2, 'loss1': [2.9688, 2.5781, 0.3809], 'loss2': [3.0312, 2.6719, 0.3633], 'acc': [0.406, 0.416], 'macro_f1': [0.216, 0.218], 'total_f1': [0.577, 0.565], 'inf_f1': [0.327, 0.321], 'perc_f1': [0.148, 0.172], 'backg_f1': [0.173, 0.162], 'inf_emb': [0.23046875, 0.23828125], 'perc_emb': [0.328125, 0.23046875], 'back_emb': [0.1875, 0.22265625]}\n",
    "# {'epoch': 3, 'loss1': [2.9062, 2.5625, 0.3535], 'loss2': [2.875, 2.5469, 0.334], 'acc': [0.456, 0.476], 'macro_f1': [0.231, 0.248], 'total_f1': [0.578, 0.572], 'inf_f1': [0.358, 0.386], 'perc_f1': [0.192, 0.206], 'backg_f1': [0.144, 0.153], 'inf_emb': [0.1796875, 0.1953125], 'perc_emb': [1.0, 1.0], 'back_emb': [0.203125, 0.1875]}\n",
    "\n",
    "\n",
    "#ACT2\n",
    "# scibert - 'macro_f1': 0.2815, 'micro_f1': 0.4059,\n",
    "# finecite scibert w pre_cls - 'macro_f1': 0.3, 'micro_f1': 0.421\n",
    "\n",
    "# scibert + citance {'epoch': 8, 'loss': 1.876, 'macro_f1': 0.228, 'micro_f1': 0.342, 'precision': [0.558, 0.292, 0.03, 0.079, 0.191, 0.319], 'recall': [0.421, 0.067, 0.026, 0.538, 0.22, 0.576], 'f1': [0.48, 0.109, 0.028, 0.137, 0.205, 0.411]}\n",
    "# pretrained scibert + citance {'epoch': 11, 'loss': 2.344, 'macro_f1': 0.257, 'micro_f1': 0.312, 'precision': [0.56, 0.344, 0.137, 0.2, 0.082, 0.282], 'recall': [0.311, 0.212, 0.333, 0.231, 0.119, 0.554], 'f1': [0.4, 0.262, 0.194, 0.214, 0.097, 0.374]}\n",
    "# finecite scibert balanced (bs 8, ls 3e-05) {'epoch': 8, 'loss': 2.715, 'macro_f1': 0.299, 'micro_f1': 0.371, 'precision': [0.586, 0.344, 0.189, 0.444, 0.036, 0.3], 'recall': [0.388, 0.308, 0.179, 0.308, 0.034, 0.696], 'f1': [0.467, 0.325, 0.184, 0.364, 0.035, 0.42]}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
