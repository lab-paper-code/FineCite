{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from uuid import uuid4\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/deallab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use NLTK's TorqueNizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dir name\n",
    "\n",
    "#inport settings\n",
    "sample = True\n",
    "\n",
    "#set input and output dir\n",
    "if sample:\n",
    "    input_dir = '../data/sample/2_clean_json'\n",
    "    output_dir = '../data/sample/3_structured_data'\n",
    "else:\n",
    "    input_dir = '../data/result/2_clean_json'\n",
    "    output_dir = '../data/result/3_structured_data'\n",
    "    \n",
    "#set ouput path if not exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditionize tokenizer\n",
    "tokenizer = MWETokenizer(separator='')\n",
    "tokenizer.add_mwe(('[', 'REF', ']'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper \n",
    "\n",
    "#add doc to doc_df and / or return doc uuid\n",
    "def get_uuid_of_doc(title, pub_year, authors):\n",
    "    if not title:\n",
    "        title = 'unknown'\n",
    "        s = doc_df['title'].str.contains(re.escape('title'))\n",
    "    else:\n",
    "        s = doc_df['title'].str.contains(re.escape(title))\n",
    "        \n",
    "    if any(s) and doc_df[s]['pub_year'].iloc[0] == pub_year:\n",
    "        id = doc_df[s]['id'].iloc[0]\n",
    "        return id\n",
    "    else:\n",
    "        id = str(uuid4())\n",
    "        pub_year = pub_year if pub_year else 'unknown'\n",
    "        authors = authors if authors else 'unknown'\n",
    "        doc_df.loc[len(doc_df)] = [id, title, pub_year, authors]\n",
    "        return id\n",
    "\n",
    "# concat_authors\n",
    "def concat_auth(authors):\n",
    "    return ' ,'.join([' '.join([v if v is not None else 'unk' for v in auth.values()]) for auth in authors])\n",
    "\n",
    "def is_valid_ref(ref, targets):\n",
    "    match = re.search(r'<ref.*?target=\"(.*?)\">', ref) #match all ref tags (group 1 for REF and group 2 for GREF)\n",
    "    if match is None:\n",
    "        print(ref)\n",
    "    ref_ids = re.findall(r'(?:b|n)\\d{1,3}', match.group(1))\n",
    "    if 'n999' in ref_ids:\n",
    "        return False\n",
    "    return any([id in targets.keys() for id in ref_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_columns = ['id', 'ref_loc', 'type', 'cited_doc_id', 'par_id']\n",
    "par_columns = ['id', 'text', 'section_title', 'doc_id',]\n",
    "doc_columns = ['id', 'title', 'pub_year', 'authors']\n",
    "# create dataframes\n",
    "ref_df = pd.DataFrame(columns=ref_columns)\n",
    "par_df = pd.DataFrame(columns=par_columns)\n",
    "doc_df = pd.DataFrame(columns=doc_columns)\n",
    "\n",
    "unk_id = str(uuid4())\n",
    "doc_df.loc[0] = [unk_id, 'unknown', 'unknown', 'unknown' ]\n",
    "\n",
    "#load files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):  \n",
    "        input_file_path = os.path.join(input_dir, filename)\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # add origin document\n",
    "        doc_id = get_uuid_of_doc(data['title'], data['pub_year'], concat_auth(data['authors'])) \n",
    "            \n",
    "        # add references\n",
    "        ref_id_targets = {} # dictionary mapping citation id (b23) to uuid of reference\n",
    "        for ref in data['references']:\n",
    "            ref_id_targets[ref['id']] = get_uuid_of_doc(ref['title'], ref['pub_year'], concat_auth(ref['authors']))\n",
    "            \n",
    "        #paragraphs\n",
    "        for section in data['sections']:\n",
    "            for paragraph_text in section['paragraphs']:\n",
    "                \n",
    "                refs = []\n",
    "                par_id = str(uuid4()) # set par id\n",
    "\n",
    "                #replace target values in text\n",
    "                references = re.findall(r'(<ref.*?<\\/ref>)', paragraph_text)\n",
    "                cleaned_text = re.sub(r'(<ref.*?<\\/ref>)', ' [REF] ', paragraph_text)\n",
    "\n",
    "                #replace all ;\n",
    "                cleaned_text = re.sub(r';', ',', cleaned_text)\n",
    "                 \n",
    "                # tokenized paragraph text\n",
    "                tokenized_text = tokenizer.tokenize(cleaned_text.split())\n",
    "                \n",
    "                #replace [REF] token with <ref> tags\n",
    "                ref_locs = [i for i, token in enumerate(tokenized_text) if '[REF]' in token]\n",
    "                if len(ref_locs) == len(references):\n",
    "                    for i, loc in enumerate(ref_locs):\n",
    "                        tokenized_text[loc] = re.sub(r' target=\".*?\"','',references[i])\n",
    "                else:\n",
    "                    print(f'The references array is of length {len(refs)} while the ref_locs array is of length {len(ref_locs)}')\n",
    "\n",
    "                # add par to par_df\n",
    "                par_df.loc[len(par_df)] = [par_id, ';'.join(tokenized_text), section['section_name'], doc_id]\n",
    "                \n",
    "                if references is None:\n",
    "                    continue\n",
    "                \n",
    "                for index, ref in enumerate(references):\n",
    "                    match = re.search(r'type=\"(.*?)\" target=\"(.*?)\"', ref)\n",
    "                    if match is None:\n",
    "                        print(ref)\n",
    "                    ref_type = match.group(1)\n",
    "                    ref_ids = [m.replace('#', '') for m in match.group(2).split(';')]\n",
    "                    cited_docs = [ref_id_targets[ref_id] if ref_id in ref_id_targets.keys() else unk_id for ref_id in ref_ids]\n",
    "                    ref_df.loc[len(ref_df)] = [uuid4(), ref_locs[index], ref_type, ';'.join(str(id) for id in cited_docs), par_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test structured data \n",
    "par_ids = par_df['id'].array\n",
    "doc_ids = doc_df['id'].array\n",
    "\n",
    "for i, row in ref_df.iterrows():\n",
    "    if row['par_id'] not in par_ids:\n",
    "        id = row['id']\n",
    "        print (f'par id in refs {id} not resolved')\n",
    "    for doc_id in row['cited_doc_id'].split(';'):\n",
    "        if doc_id == 'unknown':\n",
    "            continue\n",
    "        if doc_id not in doc_ids:\n",
    "            id = row['id']\n",
    "            print (f'doc id {doc_id} in refs {id} not resolved')\n",
    "\n",
    "for i, row in par_df.iterrows():\n",
    "    if row['doc_id'] not in doc_ids:\n",
    "        id = row['id']\n",
    "        print(f'doc id in pars {id} not resolved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df to csv\n",
    "ref_df.to_csv(output_dir + '/ref.csv', index=False)\n",
    "par_df.to_csv(output_dir + '/par.csv', index=False)\n",
    "doc_df.to_csv(output_dir + '/doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_df['par_id'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
