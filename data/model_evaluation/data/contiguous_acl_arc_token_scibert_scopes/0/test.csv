token_context,word_context,seg_context,sent_cotext,label
['realization of deep - syntactic structures in nlg  #TAUTHOR_TAG'],['realization of deep - syntactic structures in nlg  #TAUTHOR_TAG'],"['joyce  #AUTHOR_TAG.', 'the framework was originally developed for the realization of deep - syntactic structures in nlg  #TAUTHOR_TAG']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog ( kittredge and polgu ~ re, 1991 ), lfs  #AUTHOR_TAG, and joyce  #AUTHOR_TAG.', 'the framework was originally developed for the realization of deep - syntactic structures in nlg  #TAUTHOR_TAG']",0
['framework  #TAUTHOR_TAG'],['framework  #TAUTHOR_TAG'],"['to language specific ordering ( polgu ~ re, 1991 in the implemented applications, the dsyntss are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic - based mt  #AUTHOR_TAG.', 'figure 2 illustrates a dsynts from a meteorological application, meteocogent  #AUTHOR_TAG, represented using the standard graphical notation and also the realpro ascii notation used internally in the framework  #TAUTHOR_TAG']","['first of these characteristics makes a dependency tree structure a very useful representation for mt and multilingual nlg, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering ( polgu ~ re, 1991 in the implemented applications, the dsyntss are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic - based mt  #AUTHOR_TAG.', 'figure 2 illustrates a dsynts from a meteorological application, meteocogent  #AUTHOR_TAG, represented using the standard graphical notation and also the realpro ascii notation used internally in the framework  #TAUTHOR_TAG']",2
"['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in  #TAUTHOR_TAG can be accounted for using our formalism can be found in  #AUTHOR_TAG']","['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in  #TAUTHOR_TAG can be accounted for using our formalism can be found in  #AUTHOR_TAG']","['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in  #TAUTHOR_TAG can be accounted for using our formalism can be found in  #AUTHOR_TAG']","['- o ii a failli pleuvoir.', 'more details on how the structural divergences described in  #TAUTHOR_TAG can be accounted for using our formalism can be found in  #AUTHOR_TAG']",0
"['( rambow and  #TAUTHOR_TAG,']","['joyce ( rambow and  #TAUTHOR_TAG,']","['( rambow and  #TAUTHOR_TAG,']","['this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'it has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation ( nlg ) and for transfer in machine translation ( mt ).', 'our work extends directions taken in systems such as ariane  #AUTHOR_TAG, fog  #AUTHOR_TAG, joyce ( rambow and  #TAUTHOR_TAG, and lfs  #AUTHOR_TAG.', 'although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics - based approaches to mt']",2
"['( rambow and  #TAUTHOR_TAG.', 'the']","['predecessor nlg systems based on meaning - text theory : fog  #AUTHOR_TAG, lfs  #AUTHOR_TAG, and joyce ( rambow and  #TAUTHOR_TAG.', 'the']","['( rambow and  #TAUTHOR_TAG.', 'the framework was originally developed for the realization of deep - syntactic structures in nlg']","['framework represents a generalization of several predecessor nlg systems based on meaning - text theory : fog  #AUTHOR_TAG, lfs  #AUTHOR_TAG, and joyce ( rambow and  #TAUTHOR_TAG.', 'the framework was originally developed for the realization of deep - syntactic structures in nlg']",2
"[""typically require input in the format of a single sentence per line ( for example brill's tagger  #TAUTHOR_TAG ) and parsers generally aim to""]","[""speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger  #TAUTHOR_TAG ) and parsers generally aim to""]","[""speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger  #TAUTHOR_TAG ) and parsers generally aim to""]","['1 : example text shown in standard and asr format ation which is not available in asr output is sentence boundary information.', 'however, knowledge of sentence boundaries is required by many nlp technologies.', ""part of speech taggers typically require input in the format of a single sentence per line ( for example brill's tagger  #TAUTHOR_TAG ) and parsers generally aim to produce a tree spanning each sentence""]",0
"['is a multilingual enhancement of cocktail  #TAUTHOR_TAG, a coreference']","['', 'swizzle is a multilingual enhancement of cocktail  #TAUTHOR_TAG, a coreference']","['is a multilingual enhancement of cocktail  #TAUTHOR_TAG, a coreference resolution system']","['both languages, we resolved coreference by using swizzle, our implementation of a bilingual coreference resolver.', 'swizzle is a multilingual enhancement of cocktail  #TAUTHOR_TAG, a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information.', 'when cocktail was applied separately on the english and the ro - manian texts, coreferring links were identified for each english and romanian document respectively.', 'when aligned referential expressions corefer with non - aligned anaphors, swizzle derived new heuris - tics for coreference.', 'our experiments show that swizzleoutperformed cocktailon both english and romanian test documents']",2
"['( cfxxx  #TAUTHOR_TAG,  #AUTHOR_TAG  #AUTHOR_TAG ).', 'for example, cogniac  #AUTHOR_TAG, a']","['( cfxxx  #TAUTHOR_TAG,  #AUTHOR_TAG  #AUTHOR_TAG ).', 'for example, cogniac  #AUTHOR_TAG, a']","['##ing accuracy ( cfxxx  #TAUTHOR_TAG,  #AUTHOR_TAG  #AUTHOR_TAG ).', 'for example, cogniac  #AUTHOR_TAG, a system based on seven ordered heuristics, generates high - precision resolution ( over']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, se - mantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cfxxx  #TAUTHOR_TAG,  #AUTHOR_TAG  #AUTHOR_TAG ).', 'for example, cogniac  #AUTHOR_TAG, a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution sys - tem (  #AUTHOR_TAG ) that imple - ments different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints ( e. g. term repetition ) combined with lexical and textual coherence cues ( e. g. subjects of communication verbs are more likely to refer to the last person mentioned in the text ).', 'these constraints are implemented as a set of heuristics ordered by their priority.', 'moreover, the cocktailframework uniformly addresses the prob - lem of interaction between different forms of coref - erence, thus making the extension to multilingual coreference very natural']",0
"['antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in  #TAUTHOR_TAG']","['antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in  #TAUTHOR_TAG']","['must be less specific than the antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in  #TAUTHOR_TAG']","['third class of heuristics resolves coreference by coercing nominals.', 'sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'on other occasions, coercions are obtained as paths of meronyms ( e. g. is - part re - lations ) and hypernyms ( e. g. is - a relations ).', 'consistency checks implemented for this class of coref - erence are conservative : either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'table 1 lists the top performing heuristics of cocktailfor pronominal and nominal coreference.', 'examples of the heuristics operation on the muc data are presented presented in table 2.', 'details of the top performing heuristics of cocktail were reported in  #TAUTHOR_TAG']",0
['(  #TAUTHOR_TAG )'],['(  #TAUTHOR_TAG )'],"['of pronominal reference.', 'for this research, we used a coreference resolution system (  #TAUTHOR_TAG )']","[', some of the best - performing and most robust coreference resolution systems employ knowledge - based techniques.', 'traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'the acquisition of such knowledge is time - consuming, difficult, and error - prone.', 'nevertheless, recent results show that knowledge - poor methods perform with amazing accuracy ( cf.', ' #AUTHOR_TAG,  #AUTHOR_TAG  #AUTHOR_TAG ).', 'for example, cogniac  #AUTHOR_TAG, a system based on seven ordered heuristics, generates high - precision resolution ( over 90 % ) for some cases of pronominal reference.', 'for this research, we used a coreference resolution system (  #TAUTHOR_TAG ) that implements different sets of heuristics corresponding to various forms of coreference.', 'this system, called cocktail, resolves coreference by exploiting several textual cohesion constraints ( e. g. term repetition ) combined with lexical and textual coherence cues ( e. g.', 'subjects of communication verbs are more likely to refer to the last person mentioned in the text ).', 'these constraints are implemented as a set of heuristics ordered by their priority.', 'moreover, the cocktail framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural']",5
['f.  #TAUTHOR_TAG'],"['prediction, c. f.  #TAUTHOR_TAG']","['f.  #TAUTHOR_TAG.', 'the results from each component are evaluated to']","['deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'for example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'each component will return a confidence measure of the reliability of its prediction, c. f.  #TAUTHOR_TAG.', 'the results from each component are evaluated to determine the final category of the word']",4
"['( based on  #TAUTHOR_TAG ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of']","['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on  #TAUTHOR_TAG ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of']","['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on  #TAUTHOR_TAG ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of the oxford advanced learners dictionary (']","['first feature represents the part of speech of the word.', 'we use an in - house statistical tagger ( based on  #TAUTHOR_TAG ) to tag the text in which the unknown word occurs.', 'the tag set used is a simplified version of the tags used in the machinereadable version of the oxford advanced learners dictionary ( oald ).', 'the tag set contains just one tag to identify nouns']",5
"['that is more similar in goal to that outlined in this paper is vosse  #TAUTHOR_TAG.', 'voss']","['that is more similar in goal to that outlined in this paper is vosse  #TAUTHOR_TAG.', 'vosse uses a simple algorithm to identify three classes of unknown words : misspellings, neologisms,']","['that is more similar in goal to that outlined in this paper is vosse  #TAUTHOR_TAG.', 'voss']","['that is more similar in goal to that outlined in this paper is vosse  #TAUTHOR_TAG.', 'vosse uses a simple algorithm to identify three classes of unknown words : misspellings, neologisms, and names.', 'capitalization is his sole means of identifying names.', 'however, capitalization information is not available in closed captions.', 'hence, his system would be ineffective on the closed caption domain with which we are working.', ' #AUTHOR_TAG uses expectations generated by scripts to anmyze unknown words.', 'the drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described ; in this case, naval ship - to - shore messages']",1
['frequency :  #TAUTHOR_TAG differentiates between miss'],['frequency :  #TAUTHOR_TAG differentiates between misspellings and neologisms ( new words ) in'],['frequency :  #TAUTHOR_TAG differentiates between miss'],"['frequency :  #TAUTHOR_TAG differentiates between misspellings and neologisms ( new words ) in terms of their frequency.', 'his algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'our corpus frequency variable specifies the frequency of each unknown word in a 2. 6 million word corpus of business news closed captions']",5
"['performance  #TAUTHOR_TAG.', 'however, that']","['performance  #TAUTHOR_TAG.', 'however, that']","['dialogues in which the user experienced poor speech recognizer performance  #TAUTHOR_TAG.', 'however, that']","['research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance  #TAUTHOR_TAG.', 'however, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.', 'in addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime']",2
['and percentages is based on the assumption that these features are likely to produce generalized predictors  #TAUTHOR_TAG'],"['as running percentages ( percent - reprompts, percentconfirms, percent - subdials ).', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors  #TAUTHOR_TAG']","['##ts, percentconfirms, percent - subdials ).', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors  #TAUTHOR_TAG']","['dm features also include running tallies for the number of reprompts ( num - reprompts ), number of confirmation prompts ( num. confirms ), and number of subdialogue prompts ( num - subdials ), that had been played up to each point in the dimogue, as well as running percentages ( percent - reprompts, percentconfirms, percent - subdials ).', 'the use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors  #TAUTHOR_TAG']",4
"['on this dataset  #TAUTHOR_TAG.', 'specifically,']","['on this dataset  #TAUTHOR_TAG.', 'specifically,']","['on this dataset  #TAUTHOR_TAG.', 'specifically,']","['f ( • ) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and z θ ( x ) def = y u ( x, y ) is a normalizer.', 'we use the same set of binary features as in previous work on this dataset  #TAUTHOR_TAG.', 'specifically, let v = { v 1,..., v 17744 } be the set of word types with count ≥ 4 in the full 2000 - document corpus.', 'define f h ( x, y ) to be y if v h appears at least once in x, and 0 otherwise.', 'thus θ ∈ r 17744, and positive weights in θ favor class label y = + 1 and equally discourage y = −1, while negative weights do the opposite.', 'this standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'future work should consider more complex features and how they are signaled by rationales, as discussed in section 3. 2']",5
"["". g. including long - range n - grams  #AUTHOR_TAG, class n - grams  #TAUTHOR_TAG, grammatical features  #AUTHOR_TAG, etc '""]","['was intended primarily to demonstrate the efficacy of our approach.', ""in future work we plan to experiment with richer representations, e. g. including long - range n - grams  #AUTHOR_TAG, class n - grams  #TAUTHOR_TAG, grammatical features  #AUTHOR_TAG, etc '""]","["". g. including long - range n - grams  #AUTHOR_TAG, class n - grams  #TAUTHOR_TAG, grammatical features  #AUTHOR_TAG, etc '""]","['sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""in future work we plan to experiment with richer representations, e. g. including long - range n - grams  #AUTHOR_TAG, class n - grams  #TAUTHOR_TAG, grammatical features  #AUTHOR_TAG, etc '""]",3
"['generating negative samples.', 'unfortunately, as shown in  #TAUTHOR_TAG, with the represetation of sentences that we use, linear classifiers']","['generating negative samples.', 'unfortunately, as shown in  #TAUTHOR_TAG, with the represetation of sentences that we use, linear classifiers']","['generating negative samples.', 'unfortunately, as shown in  #TAUTHOR_TAG, with the represetation of sentences that we use, linear classifiers']","['- supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'rather, welling at al. demonstrated its effectiveness in modeling hand - written digits and on synthetic data.', '[UNK] both cases essentially linear classifiers were used as features.', 'as these are computationally very efficient, the authors could use a variant of gibbs sampling for generating negative samples.', 'unfortunately, as shown in  #TAUTHOR_TAG, with the represetation of sentences that we use, linear classifiers can not discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non - linear large - margin classifier ( see section 3 for details ).', 'while large - margin classifiers consistently out - perform other learning algorithms in many nlp tasks, their non - linear variations are also notoriously slow when it comes to computing their decision function - taking time that can be linear in the size of their training data.', 'this means that mcmc techniques like gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.', 'for this reason we use a different sampling scheme which we refer to as rejection sampling.', ""this allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline""]",4
"['our features we used large - margin classifiers trained using the online algorithm described in  #AUTHOR_TAG.', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in  #TAUTHOR_TAG, using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd']","['our features we used large - margin classifiers trained using the online algorithm described in  #AUTHOR_TAG.', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in  #TAUTHOR_TAG, using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd']","['our features we used large - margin classifiers trained using the online algorithm described in  #AUTHOR_TAG.', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in  #TAUTHOR_TAG, using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'no special effort was otherwise']","['our features we used large - margin classifiers trained using the online algorithm described in  #AUTHOR_TAG.', 'the code for the classifier was generously provided by daisuke okanohara.', 'this code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'as shown in  #TAUTHOR_TAG, using this representation, a linear classifier can not distinguish sentences sampled from a trigram and real sentences.', 'therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'no special effort was otherwise made in order to optimize the parameters of the classifiers']",4
"['used a trigram with modified kneser - ney smoothing [ chen and  #TAUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2']","['used a trigram with modified kneser - ney smoothing [ chen and  #TAUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation :']","['language model : for p0 we used a trigram with modified kneser - ney smoothing [ chen and  #TAUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation : each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'a coordinate was reserved for']","['language model : for p0 we used a trigram with modified kneser - ney smoothing [ chen and  #TAUTHOR_TAG ], which is still considered one of the best smoothing methods for n - gram language models.', '2. sentence representation : each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'a coordinate was reserved for each such n - gram which appeared in the data, whether real or sampled.', ""the value of then'th coordinate in the vector representation of interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0. 0001."", 'this indicates that satisfying the constraint ( 18 ) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated']",5
"['algorithm described in  #TAUTHOR_TAG.', 'when a tat is']","['algorithm described in  #TAUTHOR_TAG.', 'when a tat is']","['features can be easily obtained by modifying the tat extraction algorithm described in  #TAUTHOR_TAG.', 'when a tat is extracted from a word - aligned, source - parsed parallel sentence, we just record the contextual features and the features of the sub - trees.', 'then we use the toolkit implemented by  #AUTHOR_TAG to']","['features can be easily obtained by modifying the tat extraction algorithm described in  #TAUTHOR_TAG.', 'when a tat is extracted from a word - aligned, source - parsed parallel sentence, we just record the contextual features and the features of the sub - trees.', 'then we use the toolkit implemented by  #AUTHOR_TAG to train mers models for the ambiguous source syntactic trees separately.', 'we set the iteration number to 100 and gaussian prior to 1']",2
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['parse reranking  #TAUTHOR_TAG'],"['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking  #TAUTHOR_TAG and history - based parsing ( nivre and mc  #AUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #AUTHOR_TAG, or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['parse reranking  #TAUTHOR_TAG'],"['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking  #TAUTHOR_TAG and history - based parsing ( nivre and mc  #AUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #AUTHOR_TAG, or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']",3
"[' #AUTHOR_TAG and history - based parsing  #TAUTHOR_TAG.', 'we could also introduce new variables,']","[' #AUTHOR_TAG and history - based parsing  #TAUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #AUTHOR_TAG, or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']","[' #AUTHOR_TAG and history - based parsing  #TAUTHOR_TAG.', 'we could also introduce new variables,']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking  #AUTHOR_TAG and history - based parsing  #TAUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #AUTHOR_TAG, or secondary links m ij ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']",3
"['( nivre and mc  #AUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #TAUTHOR_TAG, or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']","['( nivre and mc  #AUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #TAUTHOR_TAG, or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']","['( nivre and mc  #AUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #TAUTHOR_TAG, or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']","['propagation improves non - projective dependency parsing with features that would make exact inference intractable.', 'for projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, we are interested in extending these ideas to phrase - structure and lattice parsing, and in trying other higher - order features, such as those used in parse reranking  #AUTHOR_TAG and history - based parsing ( nivre and mc  #AUTHOR_TAG.', 'we could also introduce new variables, e. g., nonterminal refinements  #TAUTHOR_TAG, or secondary links mid ( not constrained by tree / ptree ) that augment the parse with representations of control, binding, etc.  #AUTHOR_TAG buch -  #AUTHOR_TAG']",3
"['- string system  #TAUTHOR_TAG,']","['system  #TAUTHOR_TAG,']","['1 point on a state - of - the - art tree - to - string system  #TAUTHOR_TAG,']","['instead propose a novel approach that extracts rules from packed forests ( section 3 ), which compactly encodes many more alternatives than kbest lists.', 'experiments ( section 5 ) show that forestbased extraction improves bleu score by over 1 point on a state - of - the - art tree - to - string system  #TAUTHOR_TAG, which is also 0. 5 points better than ( and twice as fast as ) extracting on 30 - best parses.', 'when combined with our previous orthogonal work on forest - based decoding  #AUTHOR_TAG, the forest - forest approach achieves a 2. 5 bleu points improvement over the baseline, and even outperforms the hierarchical system of hiero, one of the best - performing systems to date']",2
[' #TAUTHOR_TAG which translates a packed forest from a parser ; it is also'],[' #TAUTHOR_TAG which translates a packed forest from a parser ; it is also'],"['forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models  #AUTHOR_TAG.', 'the first direct application of parse forest in translation is our previous work  #TAUTHOR_TAG which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']","['forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models  #AUTHOR_TAG.', 'the first direct application of parse forest in translation is our previous work  #TAUTHOR_TAG which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ).', 'this work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding']",2
"[' #TAUTHOR_TAG, and machine']","[' #TAUTHOR_TAG, and machine']","['), information extraction  #TAUTHOR_TAG, and machine translation ( boas 2002 ).', 'with the efforts']","['', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction  #TAUTHOR_TAG, and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['system and  #TAUTHOR_TAG,  #AUTHOR_TAG.', ' #AUTHOR_TAG is the best srl']","['system and  #TAUTHOR_TAG,  #AUTHOR_TAG.', ' #AUTHOR_TAG is the best srl']","['system and  #TAUTHOR_TAG,  #AUTHOR_TAG.', ' #AUTHOR_TAG is the best srl system']","['prove that our method is effective, we also make a comparison between the performances of our system and  #TAUTHOR_TAG,  #AUTHOR_TAG.', ' #AUTHOR_TAG is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed  #AUTHOR_TAG with a relative error reduction rate of 9. 8 %']",1
"['answering  #TAUTHOR_TAG, information']","['answering  #TAUTHOR_TAG, information']","['answering  #TAUTHOR_TAG, information extraction (']","['', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering  #TAUTHOR_TAG, information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
['- base of contemporary chinese  #TAUTHOR_TAG'],['from the semantic knowledge - base of contemporary chinese  #TAUTHOR_TAG'],"['.', 'the semantic categories of verbs and other words are extracted from the semantic knowledge - base of contemporary chinese  #TAUTHOR_TAG']","['##cat ( semantic category ) of predicate, sem - cat of first word, semcat of head word, semcat of last word, semcat of predicate + semcat of first word, semcat of predicate + semcat of last word, predicate + semcat of head word, semcat of predicate + head word.', 'the semantic categories of verbs and other words are extracted from the semantic knowledge - base of contemporary chinese  #TAUTHOR_TAG']",5
"['systems, although it is more efficient.', ' #TAUTHOR_TAG did very encouraging']","['traditional srl systems, although it is more efficient.', ' #TAUTHOR_TAG did very encouraging']","['a little worse than the traditional srl systems, although it is more efficient.', ' #TAUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', '']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #TAUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['.', 'after the propbank  #TAUTHOR_TAG was']","['produced promising results.', 'after the propbank  #TAUTHOR_TAG was']","['produced promising results.', 'after the propbank  #TAUTHOR_TAG was built,  #AUTHOR_TAG']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank  #TAUTHOR_TAG was built,  #AUTHOR_TAG and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"[', developing features that capture the right kind of information is crucial.', 'experiments on chinese srl  #TAUTHOR_TAG, xue 2008 ) reassured these findings']","['semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl  #TAUTHOR_TAG, xue 2008 ) reassured these findings']","['semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl  #TAUTHOR_TAG, xue 2008 ) reassured these findings']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl  #TAUTHOR_TAG, xue 2008 ) reassured these findings']",4
['has built a semantic role'],['has built a semantic role'],"['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'se - mantic context features indicates the features']","['has built a semantic role classifier exploiting the interdependence of semantic roles.', 'it has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'se - mantic context features indicates the features ex - tracted from the arguments around the current one.', 'we can use window size to represent the scope of the context.', 'window size [ - m, n ] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu - ments will be utilized for the classification of cur - rent semantic role.', 'there are two kinds of argu - ment sequences in  #AUTHOR_TAG, and we only test the linear sequence.', 'take the sentence in fig - ure 1 as an example.', 'the linear sequence of the arguments in this sentence is : _ _ _ _ ( until then ), _ _ _ _ ( the insurance company ), _ ( has ), _ _ _ _ _ ( for the sanxia project ), _ _ _ _ ( in - surance services ).', 'for the argument _ ( has ), if the semantic context window size is [ - 1, 2 ], the seman - tic context features e. g. headword, phrase type and etc. of _ _ _ _ ( the insurance company ), _ _ _ _ _ ( for the sanxia project ) and _ _ _ _ ( insurance services ) will be utilized to serve the classification task of _ ( has )']",5
"['', ' #TAUTHOR_TAG has']","['', ' #TAUTHOR_TAG has']","['', ' #TAUTHOR_TAG has made the first attempt working on the single semantic role level']","['we make discriminations of arguments and adjuncts, the analysis is still coarse - grained.', ' #TAUTHOR_TAG']",1
"[' #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did']","[' #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did']","[' #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"[',  #TAUTHOR_TAG.', ' #AUTHOR_TAG is the best srl']","[',  #TAUTHOR_TAG.', ' #AUTHOR_TAG is the best srl']","[',  #TAUTHOR_TAG.', ' #AUTHOR_TAG is the best srl system']","['prove that our method is effective, we also make a comparison between the performances of our system and  #AUTHOR_TAG,  #TAUTHOR_TAG.', ' #AUTHOR_TAG is the best srl system until now and it has the same data setting with ours.', 'the results are presented in table 6 we have to point out that all the three systems are based on gold standard parsing.', 'from the table 6, we can find that our system is better than both of the related systems.', 'our system has outperformed  #AUTHOR_TAG with a relative error reduction rate of 9. 8 %']",1
['in  #TAUTHOR_TAG'],['in  #TAUTHOR_TAG'],['in  #TAUTHOR_TAG'],"['semantic role classifiers always did the classification problem in one - step.', 'however, in this paper, we did src in two steps.', 'the architectures of hierarchical semantic role classifiers can 2 extra features e. g.', 'predicate may be still useful because that the information, provided by the high - level description of selfdescriptive features, e. g.', 'phrase type, are limited.', 'be found in figure 2, which is similar with that in  #TAUTHOR_TAG']",1
"[', verb class + phrase type, from  #TAUTHOR_TAG']","[', verb class + phrase type, from  #TAUTHOR_TAG']","[', predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from  #TAUTHOR_TAG']","[', subcat frame, phrase type, first word, last word, subcat frame +, predicate, path, head word and its pos, predicate + head word, predicate + phrase type, path to ba and bei, verb class 3, verb class + head word, verb class + phrase type, from  #TAUTHOR_TAG']",5
"['. 2003 ), and machine translation  #TAUTHOR_TAG.', 'with the']","['al. 2003 ), and machine translation  #TAUTHOR_TAG.', 'with the']","['. 2003 ), and machine translation  #TAUTHOR_TAG.', 'with the efforts']","['', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation  #TAUTHOR_TAG.', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"['data setting with  #TAUTHOR_TAG, however a bit different from  #AUTHOR_TAG']","['data setting with  #TAUTHOR_TAG, however a bit different from  #AUTHOR_TAG']","['.', 'we use the same data setting with  #TAUTHOR_TAG, however a bit different from  #AUTHOR_TAG']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with  #TAUTHOR_TAG, however a bit different from  #AUTHOR_TAG']",5
"[' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', ' #AUTHOR_TAG did']","[' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', ' #AUTHOR_TAG did']","[' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', ' #AUTHOR_TAG did']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"[',  #TAUTHOR_TAG']","[',  #TAUTHOR_TAG reassured these']","['semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005,  #TAUTHOR_TAG']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue and palmer 2005,  #TAUTHOR_TAG reassured these findings']",0
"['.', ' #TAUTHOR_TAG has']","['on chinese srl.', ' #TAUTHOR_TAG has']","['systematic research on chinese srl.', ' #TAUTHOR_TAG has made']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG, and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #TAUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"[' #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did']","[' #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did']","[' #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did']","['to the research on english, the research on chinese srl is still in its infancy stage.', 'previous work on chinese srl mainly focused on how to transplant the machine learning methods which has been successful with english, such as  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG did the preliminary work on chinese srl without any large semantically annotated corpus of chinese.', 'they just labeled the predicate - argument structures of ten specified verbs to a small collection of chinese sentences, and used support vector machines to identify and classify the arguments.', 'this paper made the first attempt on chinese srl and produced promising results.', 'after the propbank ( xue and palmer 2003 ) was built, and  #AUTHOR_TAG have produced more complete and systematic research on chinese srl.', ' #AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'however, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'so the hierarchical system in their paper performs a little worse than the traditional srl systems, although it is more efficient.', ' #AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling.', 'they found out that different features suited for different sub tasks of srl, i. e. semantic role identification and classification.', 'for semantic analysis, developing features that capture the right kind of information is crucial.', 'experiments on chinese srl ( xue andpalmer 2005, xue 2008 ) reassured these findings']",0
"['data setting with  #AUTHOR_TAG, however a bit different from  #TAUTHOR_TAG']","['data setting with  #AUTHOR_TAG, however a bit different from  #TAUTHOR_TAG']","['.', 'we use the same data setting with  #AUTHOR_TAG, however a bit different from  #TAUTHOR_TAG']","['use chinese propbank 1. 0 ( ldc number : ldc2005t23 ) in our experiments.', 'propbank 1. 0 includes the annotations for files chtb _ 001. fid', 'to chtb _ 931. fid,', 'or the first 250k words of the chinese treebank 5. 1.', 'for the experiments, the data of propbank is divided into three parts.', '648 files ( from chtb _ 081 to chtb _ 899. fid )', 'are used as the training set.', 'the development set includes 40 files, from chtb _ 041. fid to chtb _ 080. fid.', 'the test set includes 72 files, which are chtb _ 001 to chtb _ 041, and chtb _ 900 to chtb _ 931.', 'we use the same data setting with  #AUTHOR_TAG, however a bit different from  #TAUTHOR_TAG']",1
['candidate feature templates include : voice from  #TAUTHOR_TAG'],['candidate feature templates include : voice from  #TAUTHOR_TAG'],['candidate feature templates include : voice from  #TAUTHOR_TAG'],['candidate feature templates include : voice from  #TAUTHOR_TAG'],5
"['##bank  #TAUTHOR_TAG.', '']","['sentences from the chinese treebank  #TAUTHOR_TAG.', '']","['sentences from the chinese treebank  #TAUTHOR_TAG.', '']","['chinese propbank has labeled the predicateargument structures of sentences from the chinese treebank  #TAUTHOR_TAG.', 'it is constituted of two parts.', 'one is the labeled data, which indicates the positions of the predicates and its arguments in the chinese treebank.', 'the other is a dictionary which lists the frames of all the labeled predicates.', 'figure 1 is an example from the propbank 1.', 'we put the word - by - word translation and the translation of the whole sentence below the example.', 'it is quite a complex sentence, as there are many semantic roles in it.', 'in this sentence, all the semantic roles of the verb [UNK] [UNK] ( provide ) are presented in the syntactic tree.', 'we can separate the semantic roles into two groups']",5
"[') was first defined in  #TAUTHOR_TAG.', 'the purpose of sr']","['role labeling ( srl ) was first defined in  #TAUTHOR_TAG.', 'the purpose of srl task is to identify and']","[') was first defined in  #TAUTHOR_TAG.', 'the purpose of srl task is to identify and']","['role labeling ( srl ) was first defined in  #TAUTHOR_TAG.', 'the purpose of srl task is to identify and classify the semantic roles of each predicate in a sentence.', 'the semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'typical tags include agent, patient, source, etc. and some adjuncts such as temporal, manner, extent, etc.', 'since the arguments can provide useful semantic information, the srl is crucial to many natural language processing tasks, such as question and answering ( narayanan and harabagiu 2004 ), information extraction ( surdeanu et al. 2003 ), and machine translation ( boas 2002 ).', 'with the efforts of many researchers ( carreras and marquez 2004, moschitti 2004, pradhan et al 2005, zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made srl task progress fast']",0
"[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['most used feature for the web people search task, however, are nes.', ' #AUTHOR_TAG introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', ' #AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['most used feature for the web people search task, however, are nes.', ' #AUTHOR_TAG introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', ' #AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['most used feature for the web people search task, however, are nes.', ' #AUTHOR_TAG introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', ' #AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['', 'other representations use the link structure  #TAUTHOR_TAG or generate graph representations of']","[' #AUTHOR_TAG -.', 'other representations use the link structure  #TAUTHOR_TAG or generate graph representations of']","['', 'other representations use the link structure  #TAUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with otherssee for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #TAUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve']","['or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve']","['or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with otherssee for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
['defined as an nlp task web people search on its own  #TAUTHOR_TAG'],['defined as an nlp task web people search on its own  #TAUTHOR_TAG'],['defined as an nlp task web people search on its own  #TAUTHOR_TAG'],"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd )  #AUTHOR_TAG and cross - document coreference ( cdc )  #AUTHOR_TAG.', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own  #TAUTHOR_TAG']",0
"['##the weps - 1 corpus includes data from the web03 testbed  #TAUTHOR_TAG which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed  #TAUTHOR_TAG which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed  #TAUTHOR_TAG which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']","['##the weps - 1 corpus includes data from the web03 testbed  #TAUTHOR_TAG which follows similar annotation guidelines, although the number of document per ambiguous name is more variable']",5
"['of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in']","['of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in']","['of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with otherssee for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
['defined as an nlp task web people search on its own  #TAUTHOR_TAG'],['defined as an nlp task web people search on its own  #TAUTHOR_TAG'],['defined as an nlp task web people search on its own  #TAUTHOR_TAG'],"['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd )  #AUTHOR_TAG and cross - document coreference ( cdc )  #AUTHOR_TAG.', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task web people search on its own  #TAUTHOR_TAG']",0
['instance  #TAUTHOR_TAG -'],['instance  #TAUTHOR_TAG -'],['instance  #TAUTHOR_TAG -'],"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with others see for instance  #TAUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['extracted features  #TAUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve']","['extracted features  #TAUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve']","['', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #TAUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with otherssee for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #TAUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['as person names  #TAUTHOR_TAG.', 'according to the data available']","['as person names  #TAUTHOR_TAG.', 'according to the data available']","['the queries were composed of a person name with additional terms and 4 % were identified as person names  #TAUTHOR_TAG.', 'according to the data available']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names  #TAUTHOR_TAG.', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people  #AUTHOR_TAG.', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']",0
"['process.', ' #TAUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation']","['process.', ' #TAUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation']","['', ' #TAUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation']","['most used feature for the web people search task, however, are nes.', ' #AUTHOR_TAG introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance  #AUTHOR_TAG -.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', ' #TAUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"['of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in']","['of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in']","['of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #TAUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with otherssee for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['##ed ne recognition covering 100 different ne types  #TAUTHOR_TAG.', 'given the sparse']","['( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types  #TAUTHOR_TAG.', 'given the sparseness']","['##ed ne recognition covering 100 different ne types  #TAUTHOR_TAG.', 'given the sparseness']","['7 is a rule based english analyser that includes many functionalities ( pos tagger, stemmer, chunker, named entity ( ne ) tagger, dependency analyser, parser, etc ).', 'it provides a fine grained ne recognition covering 100 different ne types  #TAUTHOR_TAG.', 'given the sparseness of most of these fine - grained ne types, we have merged them in coarser groups : event, facility, location, person, organisation, product, periodx, timex and numex']",5
"['people  #TAUTHOR_TAG.', 'as the']","['people  #TAUTHOR_TAG.', 'as the']","['1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people  #TAUTHOR_TAG.', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']","['study of the query log of the alltheweb and altavista search sites gives an idea of the relevance of the people search task : 11 - 17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names  #AUTHOR_TAG.', 'according to the data available from 1990 u. s. census bureau, only 90, 000 different names are shared by 100 million people  #TAUTHOR_TAG.', 'as the amount of information in the www grows, more of these people are mentioned in different web pages.', 'therefore, a query for a common name in the web will usually produce a list of results where different people are mentioned']",0
"['or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve']","['or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve']","['or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']","['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with otherssee for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #TAUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign  #TAUTHOR_TAG, consists of grouping search results for a given name according to the different people that share']","['information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign  #TAUTHOR_TAG, consists of grouping search results for a given name according to the different people that share']","['the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign  #TAUTHOR_TAG, consists of grouping search results for a given name according to the different people that share it']","['situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'the user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'in some cases, the existence of a predominant person ( such as a celebrity or a historical figure ) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'the web people search task, as defined in the first weps evaluation campaign  #TAUTHOR_TAG, consists of grouping search results for a given name according to the different people that share it']",0
"['1  #TAUTHOR_TAG, 2007 ) 2 and weps - 2  #AUTHOR_TAG evaluation campaigns 3']","['have used the testbeds from weps - 1  #TAUTHOR_TAG, 2007 ) 2 and weps - 2  #AUTHOR_TAG evaluation campaigns 3']","['1  #TAUTHOR_TAG, 2007 ) 2 and weps - 2  #AUTHOR_TAG evaluation campaigns 3']","['have used the testbeds from weps - 1  #TAUTHOR_TAG, 2007 ) 2 and weps - 2  #AUTHOR_TAG evaluation campaigns 3']",5
"[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","[' #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature']","['most used feature for the web people search task, however, are nes.', ' #AUTHOR_TAG introduced a rule - based approach that tackles both variation and ambiguity analysing the structure of names.', 'in most recent research, nes ( person, location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance  #TAUTHOR_TAG.', 'for instance,  #AUTHOR_TAG uses nes coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', ' #AUTHOR_TAG compared the performace of nes versus bow features.', 'in his experiments a only a representation based on organisation nes outperformed the word based approach.', 'furthermore, this result is highly dependent on the choice of metric weighting ( nes achieve high precision at the cost of a low recall and viceversa for bow )']",0
"[')  #TAUTHOR_TAG.', '']","[')  #TAUTHOR_TAG.', '']","[')  #TAUTHOR_TAG.', '']","['disambiguation of person names in web results is usually compared to two other natural language processing tasks : word sense disambiguation ( wsd )  #AUTHOR_TAG and cross - document coreference ( cdc )  #TAUTHOR_TAG.', 'most of early research work on person name ambiguity focuses on the cdc problem or uses methods found in the wsd literature.', 'it is only recently that the web name ambiguity has been approached as a separate problem and defined as an nlp task - web people search - on its own  #AUTHOR_TAG']",0
"['use of ne features  #TAUTHOR_TAG.', 'due to the complexity of systems, the results of the weps evaluation']","['use of ne features  #TAUTHOR_TAG.', 'due to the complexity of systems, the results of the weps evaluation']","['noun phrases  #AUTHOR_TAG, word n - grams  #AUTHOR_TAG, emails and urls ( del valle -  #AUTHOR_TAG, etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features  #TAUTHOR_TAG.', 'due to the complexity of systems, the results of the weps evaluation do not provide']","['the 16 teams that submitted results for the first weps campaign, 10 of them 1 used nes in their document representation.', 'this makes nes the second most common type of feature ; only the bow feature was more popular.', 'other features used by the systems include noun phrases  #AUTHOR_TAG, word n - grams  #AUTHOR_TAG, emails and urls ( del valle -  #AUTHOR_TAG, etc.', 'in 2009, the second weps campaign showed similar trends regarding the use of ne features  #TAUTHOR_TAG.', 'due to the complexity of systems, the results of the weps evaluation do not provide a direct answer regarding the advantages of using nes over other computationally lighter features such as bow or word n - grams.', 'but the weps campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'in the next section we describe this dataset and how it has been adapted for our purposes']",0
['only feature  #TAUTHOR_TAG'],['only feature  #TAUTHOR_TAG'],['as the only feature  #TAUTHOR_TAG'],"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #TAUTHOR_TAG and sometimes in combination with others see for instance  #AUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
['instance  #TAUTHOR_TAG -'],['instance  #TAUTHOR_TAG -'],['instance  #TAUTHOR_TAG -'],"['different features have been used to represent documents where an ambiguous name is mentioned.', 'the most basic is a bag of words ( bow ) representation of the document text.', 'withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name  #AUTHOR_TAG.', 'nevertheless, the full document text is present in most systems, sometimes as the only feature  #AUTHOR_TAG and sometimes in combination with others see for instance  #TAUTHOR_TAG -.', 'other representations use the link structure  #AUTHOR_TAG or generate graph representations of the extracted features  #AUTHOR_TAG.', 'some researchers  #AUTHOR_TAG have explored the use of wikipedia information to improve the disambiguation process.', 'wikipedia provides candidate entities that are linked to specific mentions in a text.', 'the obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'these approaches are yet to be applied to the specific task of grouping search results']",0
"['current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially']","['current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially']","['current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.', 'considerable research has been done on sp acquisition most of which has involved collecting argument headwords from data and generalizing to word - net classes.', ' #AUTHOR_TAG have showed that wordnet - based approaches do not always outperform simple frequency - based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']",3
"['current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially']","['current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially']","['current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially']","['addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring sps for verb classification.', 'considerable research has been done on sp acquisition most of which has involved collecting argument headwords from data and generalizing to word - net classes.', ' #AUTHOR_TAG have showed that wordnet - based approaches do not always outperform simple frequency - based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach  #TAUTHOR_TAG.', 'the number and type ( and combination ) of grs for which sps can be reliably acquired, especially when the data is sparse, requires also further investigation']",3
[' #TAUTHOR_TAG for self training'],[' #TAUTHOR_TAG for self training'],"['combining the pcfg - la parser with discriminative reranking approaches  #TAUTHOR_TAG for self training.', 'self - training']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches  #TAUTHOR_TAG for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations  #AUTHOR_TAG, although training would be much slower compared to using generative models, as in our case']",3
[' #TAUTHOR_TAG for self training'],[' #TAUTHOR_TAG for self training'],"['combining the pcfg - la parser with discriminative reranking approaches  #TAUTHOR_TAG for self training.', 'self - training']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches  #TAUTHOR_TAG for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations  #AUTHOR_TAG, although training would be much slower compared to using generative models, as in our case']",3
"['parsers with latent annotations  #TAUTHOR_TAG, although training would be much slower compared to using generative models, as in our case']","['parsers with latent annotations  #TAUTHOR_TAG, although training would be much slower compared to using generative models, as in our case']","['benefit other discriminatively trained parsers with latent annotations  #TAUTHOR_TAG, although training would be much slower compared to using generative models, as in our case']","['conjecture based on our analysis that the em training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over - fitting.', 'better results would be expected by combining the pcfg - la parser with discriminative reranking approaches  #AUTHOR_TAG for self training.', 'self - training should also benefit other discriminatively trained parsers with latent annotations  #TAUTHOR_TAG, although training would be much slower compared to using generative models, as in our case']",3
"['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']",3
"['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']",3
"['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #TAUTHOR_TAG']",3
"[': i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG']","[': i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG']","[': i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG']","['future work, there are many research line that may be followed : i ) capturing more features by employing external knowledge such as ontological, lexical resource or wordnet - based features  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG or shallow semantic trees,  #AUTHOR_TAG']",3
"[', we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique  #TAUTHOR_TAG or a memory - efficient']","['of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique  #TAUTHOR_TAG or a memory - efficient']","['of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique  #TAUTHOR_TAG or a memory - efficient trie implementation based on a succinct data structure  #AUTHOR_TAG to']","['plan to apply our method to wider range of classifiers used in various nlp tasks.', 'to speed up classifiers used in a real - time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'when we run our classifiers on resource - tight environments such as cell - phones, we can use a random feature mixing technique  #TAUTHOR_TAG or a memory - efficient trie implementation based on a succinct data structure  #AUTHOR_TAG to reduce required memory usage']",3
"['translation  #TAUTHOR_TAG.', 'the core hypothesis in this']","['translation  #TAUTHOR_TAG.', 'the core hypothesis in this']","['translation  #TAUTHOR_TAG.', 'the core hypothesis in this work is that function words']","['reordering models we describe follow our previous work using function word models for translation  #TAUTHOR_TAG.', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']",2
['system  #TAUTHOR_TAG'],['to the ualign system  #TAUTHOR_TAG'],"['to the ualign system  #TAUTHOR_TAG.', 'however, ualign uses deep syntactic analysis and hand - cra']","['reordering model is closely related to the model proposed by  #AUTHOR_TAG ; 2007a ), with respect to conditioning the reordering predictions on lexical items.', 'these related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'with respect to the focus on function words, our reordering model is closely related to the ualign system  #TAUTHOR_TAG.', 'however, ualign uses deep syntactic analysis and hand - crafted heuristics in its model']",1
"[', we employ the orientation model introduced by  #TAUTHOR_TAG']","[', we employ the orientation model introduced by  #TAUTHOR_TAG']","['e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by  #TAUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori (']","['model o ( li, s a t ), o ( ri, s a t ), i. e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by  #TAUTHOR_TAG.', 'formally, this model takes the form of probability distribution p ori ( o ( l i, s→t ), o ( r i, s→t ) | y i, s→t ), which conditions the reordering on the lexical identity of the function word alignment ( but independent of the lexical identity of its neighboring phrases ).', '']",5
"['translation  #TAUTHOR_TAG.', 'the core hypothesis in this']","['translation  #TAUTHOR_TAG.', 'the core hypothesis in this']","['translation  #TAUTHOR_TAG.', 'the core hypothesis in this work is that function words']","['reordering models we describe follow our previous work using function word models for translation  #TAUTHOR_TAG.', 'the core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'to make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word ( which functions as an anchor ) using two kinds of information : 1 ) the relative ordering of the phrases with respect to the function word anchor ; and 2 ) the span of the phrases.', 'this section provides a high level overview of our reordering model, which attempts to leverage this information']",2
"['neighboring function word phrase pairs, we utilize the pairwise dominance model of  #TAUTHOR_TAG']","['neighboring function word phrase pairs, we utilize the pairwise dominance model of  #TAUTHOR_TAG']","['beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of  #TAUTHOR_TAG.', 'taking']","['model d ( fwi a 1, s a t ), d ( fwi + 1, s a t ), i. e. whether li, s a t and ri, s a t extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of  #TAUTHOR_TAG.', 'taking d ( f w i−1, s→t ) as a case in point, this model takes the']",5
"[' #TAUTHOR_TAG, we started an initial investigation on conversation']","[' #TAUTHOR_TAG, we started an initial investigation on conversation']","[' #TAUTHOR_TAG, we started an initial investigation on conversation entailment.', 'we have collected a dataset of 875 instances.', '']","['our previous work  #TAUTHOR_TAG, we started an initial investigation on conversation entailment.', 'we have collected a dataset of 875 instances.', 'each instance consists of a conversation segment and a hypothesis ( as described in section 1 ).', 'the hypotheses are statements about conversation participants and are further categorized into four types : about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'we developed an approach that is motivated by previous work on textual entailment.', 'we use clauses in the logic - based approaches as the underlying representation of our system.', 'based on this representation, we apply a two stage entailment process similar to mac  #AUTHOR_TAG developed for textual entailment : an alignment stage followed by an entailment stage']",2
[' #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in table 1'],[' #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in table 1'],[' #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in table 1'],"['the implicit modeling of argument consistency, we follow the same approach as in our previous work  #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in table 1']",2
"['that in our original work  #TAUTHOR_TAG, only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing']","['that in our original work  #TAUTHOR_TAG, only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work  #TAUTHOR_TAG, only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']","['that in our original work  #TAUTHOR_TAG, only development data were used to show some initial observations.', 'here we trained our mod - els on the development data and results shown are from the testing data']",1
['of rules learned from the development dataset as in  #TAUTHOR_TAG'],['of rules learned from the development dataset as in  #TAUTHOR_TAG'],"['measure the closeness between any two verbs.', 'again this model is trained from our development data described in  #AUTHOR_TAG.', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in  #TAUTHOR_TAG']","['string representation of paths is used to capture both the subject consistency and the object consistency.', 'since they are non - numerical features, and the variability of their values can be extremely large, so we applied an instance - based classification model ( e. g., k - nearest neighbor ) to determine alignments between verb terms.', 'we measure the distance between two path features by their minimal string edit distance, and then simply use the euclidean distance to measure the closeness between any two verbs.', 'again this model is trained from our development data described in  #AUTHOR_TAG.', 'figure 3 shows an example of alignment between the conversation terms and hypothesis terms in example 2. note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'this alignment is obtained by following the same set of rules learned from the development dataset as in  #TAUTHOR_TAG']",5
[' #TAUTHOR_TAG has initiated an investigation on the'],[' #TAUTHOR_TAG has initiated an investigation on the'],[' #TAUTHOR_TAG has initiated an investigation on the problem'],"['address this limitation, our previous work  #TAUTHOR_TAG has initiated an investigation on the problem of conversation entailment.', 'the problem was formulated as follows : given a conversation discourse d and a hypothesis h concerning its participant, the goal was to identify whether d entails h.', 'for instance, as in example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'while our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'it is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome']",2
"['our previous work  #TAUTHOR_TAG, conversation']","['our previous work  #TAUTHOR_TAG, conversation']","['our previous work  #TAUTHOR_TAG,']","['our previous work  #TAUTHOR_TAG, conversation entailment is formulated as the following : given a conversation segment d which is represented by a set of clauses d = d1 a §... a § dm, and a hypothesis h represented by another set of clauses h = h1 a §... a § hn, the prediction on whether d entails h is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1... dm as follows.', 'this is based on a simple as - sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses']",2
"[' #TAUTHOR_TAG, and propose one new structure.', 'although we experimented with all of their structures, 3 here we only present']","[' #TAUTHOR_TAG, and propose one new structure.', 'although we experimented with all of their structures, 3 here we only present']","[' #TAUTHOR_TAG, and propose one new structure.', 'although we experimented with all of their structures, 3 here we only present']","['learning machines are one of the most popular machines used for classification problems.', 'the objective of a typical classification problem is to learn a function that separates the data into different classes.', 'the data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'a drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over - fit.', 'the research community therefore prefers linear classifiers over other complex classifiers.', 'but more often than not, the data is not linearly separable.', 'it can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.', 'this is where kernels come to the rescue.', 'the well - known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.', 'the essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a function of the dot product of feature vectors in the lower dimensional space.', 'moreover, convolution kernels ( first introduced by  #AUTHOR_TAG ) can be used to compare abstract objects instead of feature vectors.', 'this is because these kernels involve a recursive calculation over the "" parts "" of a discrete structure.', 'this calculation is usually made computationally efficient using dynamic programming techniques.', 'therefore, convolution kernels alleviate the need of feature extraction ( which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data ).', 'therefore, we use convolution kernels with a linear learning machine ( support vector machines ) for our classification task.', 'now we present the "" discrete "" structures followed by the kernel we used.', 'we use the structures previously used by  #TAUTHOR_TAG, and propose one new structure.', 'although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'all the structures and their combinations are derived from a variation of the underlying structures, phrase structure trees ( pst ) and dependency trees ( dt ).', 'for all trees we first extract their path enclosed tree, which is the smallest common subtree that contains the two target entities  #AUTHOR_TAG.', 'we use the stanford parser  #AUTHOR_TAG to get the basic psts and dts.', 'following are the structures that we refer to in our experiments and results section : pet : this refers to the smallest common phrase structure tree that contains the two target entities.', 'dependency words ( dw ) tree : this is the smallest common dependency tree that contains the two target entities.', '']",5
"['suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of  #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a']","['suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of  #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a']","['suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of  #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']","['this paper, we have introduced the novel tasks of social event detection and classification.', 'we show that data sampling techniques play a crucial role for the task of relation detection.', 'through oversampling we achieve an increase in f1 - measure of 22. 2 % absolute over a baseline system.', 'our experiments show that as a result of how language expresses the relevant information, dependency - based structures are best suited for encoding this information.', 'furthermore, because of the complexity of the task, a combination of phrase based structures and dependency - based structures perform the best.', 'this revalidates the observation of  #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task.', 'we also introduced a new sequence structure ( sqgrw ) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks']",1
['of  #TAUTHOR_TAG where gr performed much'],['of  #TAUTHOR_TAG where gr performed much'],['gr kernel perform similar : this is different from the results of  #TAUTHOR_TAG where gr performed much'],"['', 'we found that worst performing kernel with under - sampling is sk1 with an f1 - measure of 39. 2 % which is better than the best performance without undersampling.', 'these results make it clear that doing under - sampling greatly improves the performance of the classifier, despite the fact that we are using less training data ( fewer negative examples ).', 'this is as expected because we are evaluating on f1 - measure and the classifier is optimizing for accuracy.', 'absolute.', 'as in the baseline system, a combination of structures performs best.', 'as in the undersampled system, when the data is balanced, sqgrw ( sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes ) achieves the best recall.', 'here, the pet and gr kernel perform similar : this is different from the results of  #TAUTHOR_TAG where gr performed much worse than pet for ace data.', 'this exemplifies the difference in the nature of our event annotations from that of ace relations.', 'since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger']",1
"['gained by  #TAUTHOR_TAG, who observed that in crossdomain polarity analysis adding more training data is not always beneficial.', 'apparently even the smallest training']","['gained by  #TAUTHOR_TAG, who observed that in crossdomain polarity analysis adding more training data is not always beneficial.', 'apparently even the smallest training']","['results also confirm the insights gained by  #TAUTHOR_TAG, who observed that in crossdomain polarity analysis adding more training data is not always beneficial.', 'apparently even the smallest training dataset ( cameras ) contain enough feature instances to']","['results also confirm the insights gained by  #TAUTHOR_TAG, who observed that in crossdomain polarity analysis adding more training data is not always beneficial.', 'apparently even the smallest training dataset ( cameras ) contain enough feature instances to learn a model which performs well on the testing data']",1
"['adaptation  #TAUTHOR_TAG,']","['adaptation  #TAUTHOR_TAG,']","['of domain adaptation  #TAUTHOR_TAG,']","['', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation  #TAUTHOR_TAG, perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as  #AUTHOR_TAG regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'it is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user - generated discourse']",3
"['adaptation  #TAUTHOR_TAG,']","['adaptation  #TAUTHOR_TAG,']","['of domain adaptation  #TAUTHOR_TAG,']","['', 'for future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation  #TAUTHOR_TAG, perform in comparison to our approach.', 'since three of the features we employed in our crf - based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'we observe similar challenges as  #AUTHOR_TAG regarding the analysis of complex sentences.', 'although our data is user - generated from web 2. 0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'it is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user - generated discourse']",3
['implementation of the transition - based dependency parsing frame - work  #TAUTHOR_TAG using an'],"['implementation of the transition - based dependency parsing frame - work  #TAUTHOR_TAG using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in  #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first']",['implementation of the transition - based dependency parsing frame - work  #TAUTHOR_TAG using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in  #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce'],"['implementation of the transition - based dependency parsing frame - work  #TAUTHOR_TAG using an arc - eager transi - tion strategy and are trained using the percep - tron algorithm as in  #AUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word iden - tity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label iden - tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunc - tions are included']",5
"['and constraint driven learning  #TAUTHOR_TAG.', 'the']","['and constraint driven learning  #TAUTHOR_TAG.', 'the']","['and constraint driven learning  #TAUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc  #AUTHOR_TAG, posterior regularization  #AUTHOR_TAG and constraint driven learning  #TAUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",0
"['- best lists  #TAUTHOR_TAG, where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined over a']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc  #AUTHOR_TAG.', 'we use the non - projective k - best mst algorithm to generate k - best lists  #TAUTHOR_TAG, where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined over a word,']","['- best lists  #TAUTHOR_TAG, where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined over a word, w i at position i ; the head of this word w ρ ( i ) where ρ ( i ) provides the index of the head word ; and partof - speech tags of these words t i.', 'we use the following set of features similar to mc  #AUTHOR_TAG']","['graph - based : an implementation of graphbased parsing algorithms with an arc - factored parameterization ( mc  #AUTHOR_TAG.', 'we use the non - projective k - best mst algorithm to generate k - best lists  #TAUTHOR_TAG, where k = 8 for the experiments in this paper.', 'the graphbased parser features used in the experiments in this paper are defined over a word, w i at position i ; the head of this word w ρ ( i ) where ρ ( i ) provides the index of the head word ; and partof - speech tags of these words t i.', 'we use the following set of features similar to mc  #AUTHOR_TAG']",5
"['of questions.', ' #TAUTHOR_TAG observed that dependency parsers']","['of questions.', ' #TAUTHOR_TAG observed that dependency parsers']","[', the case of questions.', ' #TAUTHOR_TAG observed that dependency parsers']","['application of the augmented - loss framework is to improve parser domain portability in the presence of partially labeled data.', 'consider, for example, the case of questions.', ' #TAUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the penntreebank.', 'table 2 we consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'the question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'root - f1 scores from table 2 suggest that one simple question is "" what is the main verb of this sentence? ""', 'for sentences that are questions.', 'in most cases this task is straight - forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'we feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data']",1
['##our rules are similar to those from  #TAUTHOR_TAG'],['##our rules are similar to those from  #TAUTHOR_TAG'],['##our rules are similar to those from  #TAUTHOR_TAG'],['##our rules are similar to those from  #TAUTHOR_TAG'],1
['sentiment analysis  #TAUTHOR_TAG'],['sentiment analysis  #TAUTHOR_TAG'],['sentiment analysis  #TAUTHOR_TAG'],"['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #AUTHOR_TAG, sentiment analysis  #TAUTHOR_TAG, mt reordering  #AUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"[' #TAUTHOR_TAG.', 'we use the non - projective']","['implementation of graph - based parsing algorithms with an arc - factored parameterization  #TAUTHOR_TAG.', 'we use the non - projective k - best mst algorithm to generate k - best lists  #AUTHOR_TAG, where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined over a word, wi at po - sition i ;']","['based parsing algorithms with an arc - factored parameterization  #TAUTHOR_TAG.', 'we use the non - projective k - best mst algorithm to generate']","['implementation of graph - based parsing algorithms with an arc - factored parameterization  #TAUTHOR_TAG.', 'we use the non - projective k - best mst algorithm to generate k - best lists  #AUTHOR_TAG, where k = 8 for the experiments in this paper.', 'the graph - based parser features used in the experiments in this paper are defined over a word, wi at po - sition i ; the head of this word w _ ( i ) where _ ( i ) provides the index of the head word ; and part - of - speech tags of these words ti.', 'we use the following set of features similar to mc  #AUTHOR_TAG']",5
"['it is similar in vein to self - training  #TAUTHOR_TAG, with the exception that the new parse']","['it is similar in vein to self - training  #TAUTHOR_TAG, with the exception that the new parse']","['it is similar in vein to self - training  #TAUTHOR_TAG, with the exception that the new parse data is targeted to']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training  #TAUTHOR_TAG, with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
"[' #TAUTHOR_TAG and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0']","[' #TAUTHOR_TAG and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where']","[', meteor has also been shown to directly correlate with translation quality  #TAUTHOR_TAG and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0']","['our experiments we work with a set of english - japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'we use a reordering score based on the reordering penalty from the meteor scoring metric.', 'though we could have used a further downstream measure like bleu, meteor has also been shown to directly correlate with translation quality  #TAUTHOR_TAG and is simpler to measure.', 'results for three augmented - loss schedules are shown : 0. 5 where for every two treebank updates we make one augmented - loss update, 1 is a 1 - to - 1 mix, and 2 is where we make twice as many augmented - loss updates as treebank updates']",4
"['mt reordering  #TAUTHOR_TAG, and many other']","['mt reordering  #TAUTHOR_TAG, and many other']","['mt reordering  #TAUTHOR_TAG, and many other tasks.', 'in']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, mt reordering  #TAUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
['algorithm as in  #TAUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to'],['perceptron algorithm as in  #TAUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to'],[' #AUTHOR_TAG using an arc - eager transition strategy and are trained using the perceptron algorithm as in  #TAUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to'],"['transition - based : an implementation of the transition - based dependency parsing framework  #AUTHOR_TAG using an arc - eager transition strategy and are trained using the perceptron algorithm as in  #TAUTHOR_TAG with a beam size of 8. beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - ofspeech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']",5
"[' #TAUTHOR_TAG.', 'but these']","[' #TAUTHOR_TAG.', 'but these']","['run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, mt reordering  #AUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"[')  #TAUTHOR_TAG.', 'we also']","['( ptb )  #TAUTHOR_TAG.', 'we also']","[')  #TAUTHOR_TAG.', 'we also']","['terms of treebank data, the primary training corpus is the penn wall street journal treebank ( ptb )  #TAUTHOR_TAG.', 'we also make use of the brown corpus, and the question treebank ( qtb )']",5
"[' #TAUTHOR_TAG.', 'but these']","[' #TAUTHOR_TAG.', 'but these']","['run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, mt reordering  #AUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['of  #TAUTHOR_TAG, who introduced the constraint driven']","['of  #TAUTHOR_TAG, who introduced the constraint driven']","['of  #TAUTHOR_TAG, who introduced the constraint driven learning algorithm (']","['work that is most similar to ours is that of  #TAUTHOR_TAG, who introduced the constraint driven learning algorithm ( codl ).', 'their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data ( what we call extrinsic datasets ).', 'for each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'these induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'the augmented - loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented - loss functions directly ( rather than adding a set of examples to the training set ).', 'unlike the codl approach, we do not perform complete optimization on each iteration over the unlabeled dataset ; rather, we incorporate the updates in our online learning algorithm.', 'as mentioned earlier, codl is one example of learning algorithms that use weak supervision, others include mann and mc -  #AUTHOR_TAG and  #AUTHOR_TAG.', 'again, these works are typically interested in using the extrinsic metric - or, in general, extrinsic information - to optimize the intrinsic metric in the absence of any labeled intrinsic data.', 'our goal is to optimize both simultaneously']",1
"['.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron - based algorithm']","['of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron - based algorithm']","['of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron - based algorithm']","['recent study by also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc  #AUTHOR_TAG, with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
['recent study by  #TAUTHOR_TAG also investigates the task of training parsers'],['recent study by  #TAUTHOR_TAG also investigates the task of training parsers'],['recent study by  #TAUTHOR_TAG also investigates the task of training parsers'],"['recent study by  #TAUTHOR_TAG also investigates the task of training parsers to improve mt reordering.', 'in that work, a parser is used to first parse a set of manually reordered sentences to produce k - best lists.', 'the parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'the method is called targeted self - training as it is similar in vein to self - training ( mc  #AUTHOR_TAG, with the exception that the new parse data is targeted to produce accurate word reorderings.', 'our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'this allows us to give guarantees of convergence.', 'furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #AUTHOR_TAG presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system.', 'similar to the inline - ranker loss function presented here, they use a k - best lists of hypotheses in order to identify parameters which can improve a global objective function : bleu score.', 'in their work, they are interested in learning a parameterization over translation phrases ( including the underlying wordalignment ) which optimizes the bleu score.', 'their goal is considerably different ; they want to incorporate additional features into their model and define an objective function which allows them to do so ; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic ( parsing ) objectives']",1
[' #TAUTHOR_TAG and textual entail'],[' #TAUTHOR_TAG and textual entailment  #AUTHOR_TAG'],['like information extraction  #TAUTHOR_TAG and textual entailment  #AUTHOR_TAG'],"['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc  #AUTHOR_TAG and these dependencies are typically the most meaningful for down - stream tasks, e. g., main verb dependencies for tasks like information extraction  #TAUTHOR_TAG and textual entailment  #AUTHOR_TAG']",0
"['. g.,  #TAUTHOR_TAG, by inserting in loss - separability for normal separability']","['standard perceptron proof, e. g.,  #TAUTHOR_TAG, by inserting in loss - separability for normal separability']","['. g.,  #TAUTHOR_TAG, by inserting in loss - separability for normal separability']","['training set d is loss - separable with margin γ > 0 if there exists a vector u with u = 1 such that for all y, y ∈ y x and ( x, y ) ∈ d, if l ( y, y ) < l ( y, y ), then u • φ ( y ) −u • φ ( y ) ≥ γ.', 'furthermore, let r ≥ | | φ ( y ) − φ ( y ) | |, for all y, y.', 'assumption 1. assume training set d is lossseparable with margin γ.', 'theorem 1.', 'given assumption 1.', 'let m be the number of mistakes made when training the perceptron ( algorithm 2 ) with inline ranker loss ( algorithm 3 ) on d, where a mistake occurs for ( x, y ) ∈ d with parameter vector θ when ∃y j ∈ f k - best θ ( x ) where y j = y 1 and l ( y j, y ) < l ( y 1, y ).', 'if training is run indefinitely, then m ≤ r 2 γ 2.', 'proof.', 'identical to the standard perceptron proof, e. g.,  #TAUTHOR_TAG, by inserting in loss - separability for normal separability']",0
"['', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies  #TAUTHOR_TAG and these dependencies are']","['arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies  #TAUTHOR_TAG and these dependencies are']","['', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies  #TAUTHOR_TAG and these dependencies are typically the']","['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies  #TAUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction  #AUTHOR_TAG and textual entailment  #AUTHOR_TAG']",4
"[' #TAUTHOR_TAG.', 'but these']","[' #TAUTHOR_TAG.', 'but these']","['run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, mt reordering  #AUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
"['##ing  #TAUTHOR_TAG.', 'in such a setting,']","['parser reranking  #TAUTHOR_TAG.', 'in such a setting,']","['parser reranking  #TAUTHOR_TAG.', 'in such a setting,']","['obvious approach to this problem is to employ parser reranking  #TAUTHOR_TAG.', 'in such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'the standard setting involves training the base parser and applying it to a development set ( this is often done in a cross - validated jack - knife training framework ).', 'the reranker can then be trained to optimize for the downstream or extrinsic objective.', 'while this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser']",0
[' #TAUTHOR_TAG'],[' #AUTHOR_TAG and textual entailment  #TAUTHOR_TAG'],['like information extraction  #AUTHOR_TAG and textual entailment  #TAUTHOR_TAG'],"['augmented - loss framework can be used to incorporate multiple treebank - based loss functions as well.', 'labeled attachment score is used as our base model loss function.', 'in this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average ( labeled ) arc - length score : _ _ _ ( _, _ ) ( i _ _ ) for each word of the sentence we compute the dis - tance between the words position i and the posi - tion of the words head _ i.', 'the arc - length score is the summed length of all those with correct head assignments ( _ ( _ i, _ i ) is 1 if the predicted head and the correct head match, 0 otherwise ).', 'the score is normalized by the summed arc lengths for the sentence.', 'the labeled version of this score requires that the labels of the arc are also correct.', 'optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( mc  #AUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks, e. g., main verb dependencies for tasks like information extraction  #AUTHOR_TAG and textual entailment  #TAUTHOR_TAG']",0
"['and constraint driven learning  #TAUTHOR_TAG.', 'the']","['and constraint driven learning  #TAUTHOR_TAG.', 'the']","['and constraint driven learning  #TAUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation ( mann and mc  #AUTHOR_TAG, posterior regularization  #AUTHOR_TAG and constraint driven learning  #TAUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",1
[' #TAUTHOR_TAG using an'],[' #TAUTHOR_TAG using an arc - eager transition strategy and are trained using'],"['implementation of the transition - based dependency parsing framework  #TAUTHOR_TAG using an arc - eager transition strategy and are trained using the perceptron algorithm as in  #AUTHOR_TAG with a beam size of 8.', 'beams with varying sizes can be used to']","['implementation of the transition - based dependency parsing framework  #TAUTHOR_TAG using an arc - eager transition strategy and are trained using the perceptron algorithm as in  #AUTHOR_TAG with a beam size of 8.', 'beams with varying sizes can be used to produce k - best lists.', 'the features used by all models are : the part - of - speech tags of the first four words on the buffer and of the top two words on the stack ; the word identities of the first two words on the buffer and of the top word on the stack ; the word identity of the syntactic head of the top word on the stack ( if available ) ; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer ( if available ).', 'all feature conjunctions are included']",5
"['on generalized expectation  #TAUTHOR_TAG, posterior regularization  #AUTHOR_TAG and constraint driven learning  #AUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven']","['on generalized expectation  #TAUTHOR_TAG, posterior regularization  #AUTHOR_TAG and constraint driven learning  #AUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven']","['on generalized expectation  #TAUTHOR_TAG, posterior regularization  #AUTHOR_TAG and constraint driven learning  #AUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven']","['have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'this includes work on generalized expectation  #TAUTHOR_TAG, posterior regularization  #AUTHOR_TAG and constraint driven learning  #AUTHOR_TAG.', 'the work of  #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in section 5.', 'in these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'for our setting this would mean using weak application specific signals to improve dependency parsing.', 'though we explore such ideas in our experiments, in particular for semi - supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training']",0
[') and labeled attachment score ( las )  #TAUTHOR_TAG'],[') and labeled attachment score ( las )  #TAUTHOR_TAG'],[') and labeled attachment score ( las )  #TAUTHOR_TAG'],"['the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented - loss for each one.', 'augmented - loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'we show empirical results for two extrinsic loss - functions ( optimizing for the downstream task ) : machine translation and domain adaptation ; and for one intrinsic loss - function : an arclength parsing score.', 'for some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( uas ) and labeled attachment score ( las )  #TAUTHOR_TAG']",5
"[' #TAUTHOR_TAG.', 'but these']","[' #TAUTHOR_TAG.', 'but these']","['run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to']","['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, mt reordering  #AUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #TAUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
['answering  #TAUTHOR_TAG'],['answering  #TAUTHOR_TAG'],['on question answering  #TAUTHOR_TAG'],"['accuracy and speed of state - of - the - art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'this includes work on question answering  #TAUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, mt reordering  #AUTHOR_TAG, and many other tasks.', 'in most cases, the accuracy of parsers degrades when run on out - of - domain data  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'but these accuracies are measured with respect to gold - standard out - of - domain parse trees.', 'there are few tasks that actually depend on the complete parse tree.', 'furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'while this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain - specific data sets which could help direct our search for optimal parameters during parser training.', 'the goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure']",0
['and data used in our experiments are based on the work of  #TAUTHOR_TAG'],['and data used in our experiments are based on the work of  #TAUTHOR_TAG'],['and data used in our experiments are based on the work of  #TAUTHOR_TAG'],['and data used in our experiments are based on the work of  #TAUTHOR_TAG'],5
"['- svm  #TAUTHOR_TAG, we propose a local training method, which trains sentence - wise weights instead of a single weight,']","['knn - svm  #TAUTHOR_TAG, we propose a local training method, which trains sentence - wise weights instead of a single weight,']","['- svm  #TAUTHOR_TAG, we propose a local training method, which trains sentence - wise weights instead of a single weight,']","['this paper, inspired by knn - svm  #TAUTHOR_TAG, we propose a local training method, which trains sentence - wise weights instead of a single weight, to address the above two problems.', 'compared with global training methods, such as mert, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'this online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'to put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set']",4
"['this question since the exact line search routine does not hold here.', 'motivated by  #TAUTHOR_TAG, 2003 ;  #AUTHOR_TAG, we']","['this question since the exact line search routine does not hold here.', 'motivated by  #TAUTHOR_TAG, 2003 ;  #AUTHOR_TAG, we']","['this question since the exact line search routine does not hold here.', 'motivated by  #TAUTHOR_TAG, 2003 ;  #AUTHOR_TAG, we approximate the error in ( 5 ) by']","['to the existence of l2 norm in objective function ( 5 ), the optimization algorithm mert can not be applied for this question since the exact line search routine does not hold here.', ""motivated by  #TAUTHOR_TAG, 2003 ;  #AUTHOR_TAG, we approximate the error in ( 5 ) by the expected loss, and then derive the following function : x 2iiw−wbii2 + a j = 1 systems nist02 nist05 nist06 nist08 moses 30. 39 26. 31 25. 34 19. 07 moses hier 33. 68 26. 94 26. 28 18. 65 in - hiero 31. 24 27. 07 26. 32 19. 03 table 1 : the performance comparison of the baseline inhiero vs moses and moses hier. with exp [ αw · h ( fj, e ) ] pα ( e | fj ; w ) = ( 7 ) ee'ec ; exp [ αw · h ( fj, e') ], where α > 0 is a real number valued smoother."", 'one can see that, in the extreme case, for α — * oc, ( 6 ) converges to ( 5 )']",4
['introduced the log - linear model for statistical machine translation ( sm'],"['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']",['introduced the log - linear model for statistical machine translation ( sm'],"['introduced the log - linear model for statistical machine translation ( smt ), in which translation is considered as the following optimization problem']",0
"['translation memory  #TAUTHOR_TAG.', 'instead of using translation examples to']","['translation memory  #TAUTHOR_TAG.', 'instead of using translation examples to']","['translation memory  #TAUTHOR_TAG.', 'instead of using translation examples to']","['method resorts to some translation examples, which is similar as example - based translation or translation memory  #TAUTHOR_TAG.', 'instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights']",1
"['local training is not the bottleneck.', 'actually, if we use lsh technique  #TAUTHOR_TAG in retrieval process, the local method can be easily']","['local training is not the bottleneck.', 'actually, if we use lsh technique  #TAUTHOR_TAG in retrieval process, the local method can be easily']","['the local training is not the bottleneck.', 'actually, if we use lsh technique  #TAUTHOR_TAG in retrieval process, the local method can be easily']","['2 depicts that testing each sentence with local training method takes 2. 9 seconds, which is comparable to the testing time 2. 0 seconds with global training method 4.', 'this shows that the local method is efficient.', 'further, compared to the retrieval, the local training is not the bottleneck.', 'actually, if we use lsh technique  #TAUTHOR_TAG in retrieval process, the local method can be easily scaled to a larger training data']",3
"['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']","['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['works have proposed discriminative tech - niques to train log - linear model for smt.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimiza - tion objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['local training method  #TAUTHOR_TAG is widely employed in computer vision  #AUTHOR_TAG.', 'compared with the global training method which tries to fit a single weight on the training']","['local training method  #TAUTHOR_TAG is widely employed in computer vision  #AUTHOR_TAG.', 'compared with the global training method which tries to fit a single weight on the training data,']","['local training method  #TAUTHOR_TAG is widely employed in computer vision  #AUTHOR_TAG.', 'compared with the global training method which tries to fit a single weight on the training']","['local training method  #TAUTHOR_TAG is widely employed in computer vision  #AUTHOR_TAG.', 'compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'it is superior to the global one when the data sets are not evenly distributed  #AUTHOR_TAG']",0
['rate  #TAUTHOR_TAG'],['rate  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood  #AUTHOR_TAG, error rate  #TAUTHOR_TAG, margin  #AUTHOR_TAG and ranking  #AUTHOR_TAG, and among which minimum error rate training ( mert )  #AUTHOR_TAG is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences  #AUTHOR_TAG, there are some shortcomings in this pipeline']",0
"['portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #TAUTHOR_TAG.', 'in our experiments the translation']","['sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #TAUTHOR_TAG.', 'in our experiments the translation']","['the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #TAUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #AUTHOR_TAG']","['run giza + +  #AUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #TAUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #AUTHOR_TAG']",5
"['portion of the english gigaword corpus using the srilm toolkits  #TAUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation']","['sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #TAUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation']","['the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #TAUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #AUTHOR_TAG']","['run giza + +  #AUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #TAUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #AUTHOR_TAG']",5
['rate  #TAUTHOR_TAG'],['rate  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood  #AUTHOR_TAG, error rate  #TAUTHOR_TAG, margin  #AUTHOR_TAG and ranking  #AUTHOR_TAG, and among which minimum error rate training ( mert )  #AUTHOR_TAG is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences  #AUTHOR_TAG, there are some shortcomings in this pipeline']",0
"['of machine learning research, incremental training has been employed in the work  #TAUTHOR_TAG, but there is little']","['of machine learning research, incremental training has been employed in the work  #TAUTHOR_TAG, but there is little']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work  #TAUTHOR_TAG, but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of']","['with retraining mode, incremental training can improve the training efficiency.', 'in the field of machine learning research, incremental training has been employed in the work  #TAUTHOR_TAG, but there is little work for tuning parameters of statistical machine translation.', 'the biggest difficulty lies in that the fea - ture vector of a given training example, i. e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'in this section, we will investigate the incremental trainingmethodsinsmtscenario']",0
"['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative tech - niques to train log - linear model for smt.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
['by paired bootstrap re - sampling  #TAUTHOR_TAG'],['by paired bootstrap re - sampling  #TAUTHOR_TAG'],"['the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #TAUTHOR_TAG']","['run giza + +  #AUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #TAUTHOR_TAG']",5
"[') via tuning as ranking  #TAUTHOR_TAG, 2011 ).', 'since score']","['b ). the nonlinearly separable classification problem transformed from ( a ) via tuning as ranking  #TAUTHOR_TAG, 2011 ).', 'since score']","[') via tuning as ranking  #TAUTHOR_TAG, 2011 ).', 'since score']","['b ). the nonlinearly separable classification problem transformed from ( a ) via tuning as ranking  #TAUTHOR_TAG, 2011 ).', 'since score of e11 is greater than that of e12, ( 1, 0 ) corresponds to a possitive example denoted as ” • ”, and ( −1, 0 ) corresponds to a negative example denoted as ” * ”.', 'since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'however, one can obtain e11 and e21 with weights : ( 1, 1 ) and ( −1, 1 ), respectively']",0
"['works have proposed discriminative techniques to train log - linear model for smt.', ' #TAUTHOR_TAG used maximum likelihood estimation to']","['works have proposed discriminative techniques to train log - linear model for smt.', ' #TAUTHOR_TAG used maximum likelihood estimation to']","['works have proposed discriminative techniques to train log - linear model for smt.', ' #TAUTHOR_TAG used maximum likelihood estimation to']","['works have proposed discriminative techniques to train log - linear model for smt.', ' #TAUTHOR_TAG used maximum likelihood estimation to learn weights for mt.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['test sentence,  #TAUTHOR_TAG defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['test sentence,  #TAUTHOR_TAG defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['a test sentence,  #TAUTHOR_TAG defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']","['metric we consider here is derived from an example - based machine translation.', 'to retrieve translation examples for a test sentence,  #TAUTHOR_TAG defined a metric based on the combination of edit distance and tf - idf ( manning and sch [UNK] utze, 1999 ) as follows']",5
['rate  #TAUTHOR_TAG'],['rate  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood  #AUTHOR_TAG, error rate  #TAUTHOR_TAG, margin  #AUTHOR_TAG and ranking  #AUTHOR_TAG, and among which minimum error rate training ( mert )  #AUTHOR_TAG is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences  #AUTHOR_TAG, there are some shortcomings in this pipeline']",0
"['i based on w b.', 'we employ the idea of ultraconservative update  #TAUTHOR_TAG to propose two incremental methods']","['i based on w b.', 'we employ the idea of ultraconservative update  #TAUTHOR_TAG to propose two incremental methods']","['d i based on w b.', 'we employ the idea of ultraconservative update  #TAUTHOR_TAG to propose two incremental methods']","['the notations in algorithm 2, w b is the baseline weight, d i = { f i j, c i j, r i j } k j = 1 denotes training examples for t i.', 'for the sake of brevity, we will drop the index i, d i = { f j, c j, r j } k j = 1, in the rest of this paper.', 'our goal is to find an optimal weight, denoted by w i, which is a local weight and used for decoding the sentence t i.', 'unlike the global method which performs tuning on the whole development set dev + d i as in algorithm 1, w i can be incrementally learned by optimizing on d i based on w b.', 'we employ the idea of ultraconservative update  #TAUTHOR_TAG to propose two incremental methods for local training in algorithm 2 as follows']",5
"['', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized']","['', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for mt.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #AUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
['translation  #TAUTHOR_TAG as'],['translation  #TAUTHOR_TAG as'],['use an in - house developed hierarchical phrase - based translation  #TAUTHOR_TAG as'],"['use an in - house developed hierarchical phrase - based translation  #TAUTHOR_TAG as our baseline system, and we denote it as in - hiero.', 'to obtain satisfactory baseline performance, we tune in - hiero system for 5 times using mert, and then se -  #AUTHOR_TAG.', 'both of these systems are with default setting.', 'all three systems are trained by mert with 100 best candidates']",5
['run giza + +  #TAUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to'],['run giza + +  #TAUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to'],['run giza + +  #TAUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to'],"['run giza + +  #TAUTHOR_TAG on the training corpus in both directions  #AUTHOR_TAG to obtain the word alignment for each sentence pair.', 'we train a 4 - gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits  #AUTHOR_TAG with modified kneser - ney smoothing  #AUTHOR_TAG.', 'in our experiments the translation performances are measured by case - insensitive bleu4 metric  #AUTHOR_TAG and we use mteval - v13a. pl', 'as the evaluation tool.', 'the significance testing is performed by paired bootstrap re - sampling  #AUTHOR_TAG']",5
"['.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['mt.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']","['works have proposed discriminative techniques to train log - linear model for smt.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for mt.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions']",1
"['ranking  #TAUTHOR_TAG, and among which minimum']","['ranking  #TAUTHOR_TAG, and among which minimum']","['ranking  #TAUTHOR_TAG, and among which minimum error rate training ( mert )  #AUTHOR_TAG is the']","['f and e ( e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight w. parameter estimation is one of the most important components in smt, and various training methods have been proposed to tune w.', 'some methods are based on likelihood  #AUTHOR_TAG, error rate  #AUTHOR_TAG, margin  #AUTHOR_TAG and ranking  #TAUTHOR_TAG, and among which minimum error rate training ( mert )  #AUTHOR_TAG is the most popular one.', 'all these training methods follow the same pipeline : they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'we call them a global training method.', 'one of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'however, due to the diversity and uneven distribution of source sentences  #AUTHOR_TAG, there are some shortcomings in this pipeline']",0
"['is consistent with results from previous work  #TAUTHOR_TAG.', 'however, what is more interesting here is that while graph - based approaches']","['is consistent with results from previous work  #TAUTHOR_TAG.', 'however, what is more interesting here is that while graph - based approaches']","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work  #TAUTHOR_TAG.', 'however, what is more interesting here is that while graph - based approaches']","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'although evaluated on different data sets, this result is consistent with results from previous work  #TAUTHOR_TAG.', ""however, what is more interesting here is that while graph - based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40 % performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes""]",1
"['., identifying visual objects in the environment given language descriptions  #TAUTHOR_TAG.', '']","['reference resolution task, i. e., identifying visual objects in the environment given language descriptions  #TAUTHOR_TAG.', '']","['., identifying visual objects in the environment given language descriptions  #TAUTHOR_TAG.', '']","['semantics provides a bridge to connect symbolic labels or words with lower level visual features  #AUTHOR_TAG.', 'previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i. e., identifying visual objects in the environment given language descriptions  #TAUTHOR_TAG.', 'for the referring expression generation task here, we also need a lexicon with grounded semantics']",2
"[' #TAUTHOR_TAG.', 'in that']","[' #TAUTHOR_TAG.', 'in that work, the main focus is on reference resolution : given referential descriptions']","['referential communication in situated dialogue was investigated in our previous work  #TAUTHOR_TAG.', 'in that']","['this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work  #TAUTHOR_TAG.', 'in that work, the main focus is on reference resolution : given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation ( reg ) becomes an equally important problem in situated dialogue.', 'robots have much lower perceptual capabilities of the environment than humans.', 'how can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to']",2
"[',  #TAUTHOR_TAG showed that a different feature - topic model improved']","[',  #TAUTHOR_TAG showed that a different feature - topic model improved']","['semantic interference tasks.', 'in a similar vein,  #TAUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #AUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #TAUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter cat - egory, the']","['works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter cat - egory, the']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter cat - egory, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG )']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter cat - egory, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG )']",0
"['has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #TAUTHOR_TAG to create a bimodal']","['has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #TAUTHOR_TAG to create a bimodal']","['other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #TAUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #TAUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG, act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']","[' #TAUTHOR_TAG, act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']","['in object recognition  #TAUTHOR_TAG, act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #TAUTHOR_TAG, act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']",0
['automatic location identification of twitter users  #TAUTHOR_TAG'],['automatic location identification of twitter users  #TAUTHOR_TAG'],['automatic location identification of twitter users  #TAUTHOR_TAG'],"['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identification of twitter users  #TAUTHOR_TAG']",0
"['meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
['original mlda model of  #TAUTHOR_TAG ( 2009 ) and generalize'],['original mlda model of  #TAUTHOR_TAG ( 2009 ) and generalize'],"['is, we simply take the original mlda model of  #TAUTHOR_TAG ( 2009 ) and generalize']","['is, we simply take the original mlda model of  #TAUTHOR_TAG ( 2009 ) and generalize it in the same way they generalize lda.', 'at first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi']",2
"['mapped to logical structure meaning  #TAUTHOR_TAG.', '']","['mapped to logical structure meaning  #TAUTHOR_TAG.', '']","['semantic parsing, where words and sentences are mapped to logical structure meaning  #TAUTHOR_TAG.', '']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #TAUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identification of twitter users  #AUTHOR_TAG']",0
['extend lda to allow'],['extend lda to allow'],['extend lda to allow'],"['extend lda to allow for the inference of document and topic distributions in a multimodal corpus.', 'in their model, a document consists of a set of ( word, feature ) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'topics consist of multinomial distributions over words, β k, but are extended to also include multinomial distributions over features, ψ k.', 'the generative process is amended to include these feature distributions']",0
"['clustered into 5, 000 visual codewords ( centroids ) using k - means clustering  #TAUTHOR_TAG, and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we']","['clustered into 5, 000 visual codewords ( centroids ) using k - means clustering  #TAUTHOR_TAG, and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we']","['commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering  #TAUTHOR_TAG, and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']","[', we compute a simple bag of visual words ( bovw ) model for our images using surf keypoints  #AUTHOR_TAG.', 'surf is a method for selecting points - of - interest within an image.', 'it is faster and more forgiving than the commonly known sift algorithm.', 'we compute surf keypoints for every image in our data set using sim - plecv 3 and randomly sample 1 % of the keypoints.', 'the keypoints are clustered into 5, 000 visual codewords ( centroids ) using k - means clustering  #TAUTHOR_TAG, and images are then quantized over the 5, 000 codewords.', 'all images for a given word are summed together to provide an average representation for the word.', 'we refer to this representation as the surf modality']",5
"['- test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']","['text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']","['of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']",1
"['in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']","['in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']","['in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']",0
"['several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g.,  #TAUTHOR_TAG']","['several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g.,  #TAUTHOR_TAG']","['several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g.,  #TAUTHOR_TAG']","['approaches to multimodal research have succeeded by abstracting away raw perceptual in - formation and using high - level representations in - stead.', 'some works abstract perception via the us - age of symbolic logic representations  #AUTHOR_TAG, while others choose to employ concepts elicited from psycholin - guistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g.,  #TAUTHOR_TAG']",0
['interpreting navigation directions  #TAUTHOR_TAG'],['interpreting navigation directions  #TAUTHOR_TAG'],['interpreting navigation directions  #TAUTHOR_TAG'],"['language grounding problem has come in many different flavors with just as many different ap - proaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #TAUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation ( feng and la - pata, 2010a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identifica - tion of twitter users  #AUTHOR_TAG']",0
"['generation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, text illustration  #AUTHOR_TAG,']","['automatic image caption generation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, text illustration  #AUTHOR_TAG,']","['automatic image caption generation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, text illustration  #AUTHOR_TAG,']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #AUTHOR_TAG a ;  #TAUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identification of twitter users  #AUTHOR_TAG']",0
"['meaning with perceptual information  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
['method as  #TAUTHOR_TAG for generating our multimodal corpora : for'],['method as  #TAUTHOR_TAG for generating our multimodal corpora : for'],"[""as  #TAUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features']","['order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non - textual modalities.', ""we use the same method as  #TAUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus, a feature is selected stochastically from the word's feature distribution, creating a word - feature pair."", 'words without grounded features are all given the same placeholder feature, also resulting in a word - feature pair. 5', 'that is, for the feature norm modal - ity, we generate ( word, feature norm ) pairs ; for the surf modality, we generate ( word, codeword ) pairs, etc.', 'the resulting stochastically generated corpus is used in its corresponding experiments']",5
"['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al.,']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #AUTHOR_TAG aziz -  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', 'previously lda has been successfully used']","[' #TAUTHOR_TAG.', 'previously lda has been successfully used']","[' #TAUTHOR_TAG.', 'previously lda has been successfully used']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by  #TAUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together  #AUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers  #AUTHOR_TAG.', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks  #AUTHOR_TAG.', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",5
"['likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of  #TAUTHOR_TAG b )'s suggestion that""]","['likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of  #TAUTHOR_TAG b )'s suggestion that""]","['likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of  #TAUTHOR_TAG b )'s suggestion that""]","['see that the image modalities are much more useful than they are in compositionality prediction.', 'the surf modality does extremely well in particular, but the gist features also provide statistically significant improvements over the text - only model.', 'since the surf and gist image features tend to capture object - likeness and scene - likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""this seems to provide additional evidence of  #TAUTHOR_TAG b )'s suggestion that something like a distributional hypothesis of images is plausible""]",1
"['.', 'it is frequently used in tasks like scene identification, and  #TAUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors,']","['of the whole image.', 'it is frequently used in tasks like scene identification, and  #TAUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors,']","['of the whole image.', 'it is frequently used in tasks like scene identification, and  #TAUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']","['also compute gist vectors  #AUTHOR_TAG for every image using leargist  #AUTHOR_TAG.', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and  #TAUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']",4
"['word similarity prediction.', ' #TAUTHOR_TAG a ) show how a bovw model']","['word similarity prediction.', ' #TAUTHOR_TAG a ) show how a bovw model']","['word similarity prediction.', ' #TAUTHOR_TAG a ) show how a bovw model']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #TAUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']",0
"['generation  #TAUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG,']","['automatic image caption generation  #TAUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG,']","['automatic image caption generation  #TAUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG,']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #TAUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identification of twitter users  #AUTHOR_TAG']",0
"['##wac, a large german web corpus created by the wacky group  #TAUTHOR_TAG containing approximately']","['our text modality, we use dewac, a large german web corpus created by the wacky group  #TAUTHOR_TAG containing approximately']","['##wac, a large german web corpus created by the wacky group  #TAUTHOR_TAG containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less']","['our text modality, we use dewac, a large german web corpus created by the wacky group  #TAUTHOR_TAG containing approximately 1. 7 b word tokens.', 'we filtered the corpus by : removing words with unprintable characters or encoding troubles ; removing all stopwords ; removing word types with a total frequency of less than 500 ; and removing documents with a length shorter than 100.', 'the resulting corpus has 1, 038, 883 documents consisting of 75, 678 word types and 466m word tokens']",5
"['- before - seen objects ), and  #TAUTHOR_TAG show that verb clusters']","['zero - shot classification ( i. e., classifying never - before - seen objects ), and  #TAUTHOR_TAG show that verb clusters']","['. e., classifying never - before - seen objects ), and  #TAUTHOR_TAG show that verb clusters']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples,  #AUTHOR_TAG and  #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and  #TAUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']",0
"['on a fill - in - the - blank task.', ' #TAUTHOR_TAG take an entirely different']","['on a fill - in - the - blank task.', ' #TAUTHOR_TAG take an entirely different']","['semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #TAUTHOR_TAG take an entirely different approach by showing that one can']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #AUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #TAUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['3. since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following  #TAUTHOR_TAG, we measure association norm prediction as an']","['3. since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following  #TAUTHOR_TAG, we measure association norm prediction as an']","[', we also evaluate using the association norms data set described in section 3. since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following  #TAUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our']","[', we also evaluate using the association norms data set described in section 3. since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'following  #TAUTHOR_TAG, we measure association norm prediction as an average of percentile ranks.', 'for all possible pairs of words in our vocabulary, we compute the negative symmetric kl divergence between the two words.', 'we then compute the percentile ranks of similarity for each word pair, e. g., "" cat "" is more similar to "" dog "" than 97. 3 % of the rest of the vocabulary.', 'we report the weighted mean percentile ranks for all cue - association pairs, i. e., if a cue - association is given more than once, it is counted more than once']",5
"['multimodal latent dirichlet allocation model of  #TAUTHOR_TAG.', 'we']","['multimodal latent dirichlet allocation model of  #TAUTHOR_TAG.', 'we']","['compatibility with the multimodal latent dirichlet allocation model of  #TAUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda']","['this paper, we evaluated the role of low - level image features, surf and gist, for their compatibility with the multimodal latent dirichlet allocation model of  #TAUTHOR_TAG.', 'we found both fea - ture sets were directly compatible with multimodal lda and provided significant gains in their ability to predict association norms over traditional text - only lda.', 'surf features also provided significant gains over text - only lda in predicting the compositionality of noun compounds']",5
['vectors  #TAUTHOR_TAG for'],['gist vectors  #TAUTHOR_TAG for'],"['gist vectors  #TAUTHOR_TAG for every image using leargist  #AUTHOR_TAG.', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation']","['also compute gist vectors  #TAUTHOR_TAG for every image using leargist  #AUTHOR_TAG.', 'unlike surf descriptors, gist produces a single vector representation for an image.', 'the vector does not find points of interest in the image, but rather attempts to provide a representation for the overall "" gist "" of the whole image.', 'it is frequently used in tasks like scene identification, and  #AUTHOR_TAG shows that distance in gist space correlates well with semantic distance in wordnet.', 'after computing the gist vectors, each textual word is represented as the centroid gist vector of all its images, forming the gist modality']",5
['automatic location identification of twitter users  #TAUTHOR_TAG'],['automatic location identification of twitter users  #TAUTHOR_TAG'],['automatic location identification of twitter users  #TAUTHOR_TAG'],"['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identification of twitter users  #TAUTHOR_TAG']",0
"['cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the']","['cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the']","['feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the path']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #AUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['the two modalities.', 'to name a few examples,  #AUTHOR_TAG and  #TAUTHOR_TAG show how semantic information from text']","['the two modalities.', 'to name a few examples,  #AUTHOR_TAG and  #TAUTHOR_TAG show how semantic information from text']","['the two modalities.', 'to name a few examples,  #AUTHOR_TAG and  #TAUTHOR_TAG show how semantic information from text can be used']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples,  #AUTHOR_TAG and  #TAUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and  #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']",0
"['into groups of images, called synsets  #TAUTHOR_TAG.', 'multiple synsets exist for']","['into groups of images, called synsets  #TAUTHOR_TAG.', 'multiple synsets exist for']","['into groups of images, called synsets  #TAUTHOR_TAG.', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']","['##ernetle ( "" little imagenet "" in swabian german ) is our new data set of german noun - to - imagenet synset mappings.', 'imagenet is a large - scale and widely used image database, built on top of wordnet, which maps words into groups of images, called synsets  #TAUTHOR_TAG.', 'multiple synsets exist for each meaning of a word.', 'for example, im - agenet contains two different synsets for the word mouse : one contains images of the animal, while the other contains images of the computer peripheral.', 'this bildernetle data set provides mappings from german noun types to images of the nouns via imagenet']",5
"['out feature norms from weighted mixtures based on textual similarity.', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on canonical']","['out feature norms from weighted mixtures based on textual similarity.', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on canonical']","['infer held out feature norms from weighted mixtures based on textual similarity.', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and']","['helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['sort of distributional hypothesis for images.', 'more recently,  #TAUTHOR_TAG show that visual attribute classifiers, which have been immensely']","['sort of distributional hypothesis for images.', 'more recently,  #TAUTHOR_TAG show that visual attribute classifiers, which have been immensely']","['a sort of distributional hypothesis for images.', 'more recently,  #TAUTHOR_TAG show that visual attribute classifiers, which have been immensely']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #TAUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']",0
['selected as the best from  #TAUTHOR_TAG'],['selected as the best from  #TAUTHOR_TAG'],"['time.', 'the high dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from  #TAUTHOR_TAG']","['all settings, we fix all dirichlet priors at 0. 1, use a learning rate 0. 7, and use minibatch sizes of 1024 documents.', 'we do not optimize these hyperparameters or vary them over time.', 'the high dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from  #TAUTHOR_TAG']",5
"['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
"['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al.,']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #AUTHOR_TAG aziz -  #AUTHOR_TAG']",0
"['infer unsupervised joint topic distributions over words and feature norms together  #TAUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes']","['infer unsupervised joint topic distributions over words and feature norms together  #TAUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes']","['infer unsupervised joint topic distributions over words and feature norms together  #TAUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers  #AUTHOR_TAG.', 'these multimodal lda models ( hereafter, ml']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by  #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together  #TAUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers  #AUTHOR_TAG.', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks  #AUTHOR_TAG.', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",0
"['meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #TAUTHOR_TAG b ;  #AUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
"['in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']","['in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']","['in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #AUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #TAUTHOR_TAG']",0
"['norms ( an ) is a collection of association norms collected by schulte im  #TAUTHOR_TAG.', 'in association norm experiments, subjects are presented with a']","['norms ( an ) is a collection of association norms collected by schulte im  #TAUTHOR_TAG.', 'in association norm experiments, subjects are presented with a']","['norms ( an ) is a collection of association norms collected by schulte im  #TAUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1,']","['norms ( an ) is a collection of association norms collected by schulte im  #TAUTHOR_TAG.', 'in association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'with enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'after removing responses given only once in the entire study, the data set contains a total of 95, 214 cue - response pairs for 1, 012 nouns and 5, 716 response types']",5
"['dirichlet allocation  #TAUTHOR_TAG, or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that']","['dirichlet allocation  #TAUTHOR_TAG, or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that']","['dirichlet allocation  #TAUTHOR_TAG, or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that']","['dirichlet allocation  #TAUTHOR_TAG, or lda, is an unsupervised bayesian probabilistic model of text documents.', 'it assumes that all documents are probabilistically generated from a shared set of k common topics, where each topic is a multinomial distribution over the vocabulary ( notated as β ), and documents are modeled as mixtures of these shared topics ( notated as θ ).', 'lda assumes every document in the corpus is generated using the fol - lowing generative process']",0
"['works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose']","['works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose']","['##ing away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['the two modalities.', 'to name a few examples,  #TAUTHOR_TAG and  #AUTHOR_TAG show how semantic information from text']","['the two modalities.', 'to name a few examples,  #TAUTHOR_TAG and  #AUTHOR_TAG show how semantic information from text']","['the two modalities.', 'to name a few examples,  #TAUTHOR_TAG and  #AUTHOR_TAG show how semantic information from text can be used']","['computer vision community has also benefited greatly from efforts to unify the two modalities.', 'to name a few examples,  #TAUTHOR_TAG and  #AUTHOR_TAG show how semantic information from text can be used to improve zero - shot classification ( i. e., classifying never - before - seen objects ), and  #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos']",0
"['works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose']","['works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose']","['##ing away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #TAUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['association norms.', ' #TAUTHOR_TAG furthered this']","['association norms.', ' #TAUTHOR_TAG furthered this']","['association norms.', ' #TAUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms,']","['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #AUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #AUTHOR_TAG in the prediction of association norms.', ' #TAUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
"['using visual classifiers  #TAUTHOR_TAG.', 'these multimodal lda models ( hereafter, ml']","['using visual classifiers  #TAUTHOR_TAG.', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible']","['infer unsupervised joint topic distributions over words and feature norms together  #AUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers  #TAUTHOR_TAG.', 'these multimodal lda models ( hereafter, ml']","['experiments are based on the multimodal extension of latent dirichlet allocation developed by  #AUTHOR_TAG.', 'previously lda has been successfully used to infer unsupervised joint topic distributions over words and feature norms together  #AUTHOR_TAG.', 'it has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers  #TAUTHOR_TAG.', 'these multimodal lda models ( hereafter, mlda ) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks  #AUTHOR_TAG.', 'however, prior work using mlda is limited to two modalities at a time.', 'in this section, we describe bimodal mlda and define two methods for extending it to three or more modalities']",0
"['generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #TAUTHOR_TAG,']","['automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #TAUTHOR_TAG,']","['automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #TAUTHOR_TAG,']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #AUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #TAUTHOR_TAG, or automatic location identification of twitter users  #AUTHOR_TAG']",0
"['robot commands  #TAUTHOR_TAG.', 'some']","['robot commands  #TAUTHOR_TAG.', 'some']","['robot commands  #TAUTHOR_TAG.', 'some efforts have tackled tasks']","['language grounding problem has come in many different flavors with just as many different approaches.', 'some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning  #AUTHOR_TAG.', 'others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions  #AUTHOR_TAG or robot commands  #TAUTHOR_TAG.', 'some efforts have tackled tasks such as automatic image caption generation  #AUTHOR_TAG a ;  #AUTHOR_TAG, text illustration  #AUTHOR_TAG, or automatic location identification of twitter users  #AUTHOR_TAG']",0
"[', 2005 ;  #TAUTHOR_TAG']","['language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #TAUTHOR_TAG']","['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #AUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #TAUTHOR_TAG']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #AUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #TAUTHOR_TAG']",0
"['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian']","['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian']","['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true']","['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']",5
"['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
"['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line']","['line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information  #AUTHOR_TAG b ;  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics : that the "" meaning of words is entirely given by other words ""  #AUTHOR_TAG b )']",0
"['- test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']","['text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']","['of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']","['1 shows our results for each of our selected models with our compositionality evaluation.', 'the 2d models employing feature norms and association norms do significantly better than the text - only model ( two - tailed t - test ).', 'this result is consistent with other works using this model with these features  #TAUTHOR_TAG']",1
"['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al.,']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #AUTHOR_TAG aziz -  #AUTHOR_TAG']",0
"['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between']","['g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al.,']","['language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets ( e. g. flickr,  #AUTHOR_TAG ), computing power, improved computer vision models  #TAUTHOR_TAG and neurological evidence of ties between the language, perceptual and motor systems in the brain ( pulverm [UNK] uller et al., 2005 ;  #AUTHOR_TAG aziz -  #AUTHOR_TAG']",0
[' #TAUTHOR_TAG in'],[' #TAUTHOR_TAG in'],['latent semantic analysis  #TAUTHOR_TAG in the prediction'],"['approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high - level representations instead.', 'some works abstract perception via the usage of symbolic logic representations  #AUTHOR_TAG, while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'within the latter category, the two most common representations have been association norms, where subjects are given a 1 http : / / stephenroller. com / research / emnlp13 cue word and name the first ( or several ) associated words that come to mind ( e. g.,  #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept ( e. g., mc  #AUTHOR_TAG ).', ' #AUTHOR_TAG helped pave the path for cognitive - linguistic multimodal research, showing that latent dirichlet allocation outperformed latent semantic analysis  #TAUTHOR_TAG in the prediction of association norms.', ' #AUTHOR_TAG furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'in a similar vein,  #AUTHOR_TAG showed that a different feature - topic model improved predictions on a fill - in - the - blank task.', ' #AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', ' #AUTHOR_TAG introduce a new method of multimodal integration based on canonical correlation analysis, and performs a systematic comparison between their cca - based model and others on association norm prediction, held out feature prediction, and word similarity']",0
"['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian']","['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian']","['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true']","['solve these scaling issues, we implement online variational bayesian inference  #TAUTHOR_TAG for our models.', 'in variational bayesian inference ( vbi ), one approximates the true posterior using simpler distributions with free variables.', 'the free variables are then optimized in an em - like algorithm to minimize difference between the true and approximate posteriors.', 'online vbi differs from normal vbi by using randomly sampled minibatches in each em step rather than the entire data set.', 'online vbi easily scales and quickly converges in all of our experiments.', 'a listing of the inference algorithm may be found in the supplementary materials and the source code is available as open source']",5
"['has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #TAUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal']","['has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #TAUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal']","['other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #TAUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'the first work to do this with topic models is  #TAUTHOR_TAG b ).', 'they use a bag of visual words ( bovw ) model  #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'the topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #AUTHOR_TAG a ) show how a bovw model may be easily combined with a distributional vector space model of language using only vector concatenation.', ' #AUTHOR_TAG b ) show that the contextual visual words ( i. e. the visual features around an object, rather than of the object itself ) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'more recently,  #AUTHOR_TAG show that visual attribute classifiers, which have been immensely successful in object recognition  #AUTHOR_TAG, act as excellent substitutes for feature norms.', 'other work on modeling the meanings of verbs using video recognition has also begun showing great promise  #AUTHOR_TAG']",0
"['word similarity  #TAUTHOR_TAG.', 'while prior']","['word similarity  #TAUTHOR_TAG.', 'while prior']","['word similarity  #TAUTHOR_TAG.', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model']","['this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'the model we rely on was originally developed by  #AUTHOR_TAG and is based on a generalization of latent dirichlet allocation.', 'this model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity  #TAUTHOR_TAG.', 'while prior work has used the model only with feature norms and visual attributes, we show that low - level image features are directly compatible with the model and provide improved representations of word meaning.', 'we also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'finally, we describe two ways to extend the model by incorporating three or more modalities.', 'we find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'we release both our code and data to the community for future research.']",2
[' #TAUTHOR_TAG a ) : since'],[' #TAUTHOR_TAG a ) : since'],"[' #TAUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining']","['p2, on the other hand, we recast sc as a se - quence labeling task.', 'in other words, we train a sc model that assumes as input a post sequence and outputs a stance sequence, with one stance la - bel for each post in the input post sequence.', 'this choice is motivated by an observation we made previously  #TAUTHOR_TAG a ) : since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.']",2
"['on stance classification  #TAUTHOR_TAG c ), we employ three types of features computed based on the']","['on stance classification  #TAUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of']","['on stance classification  #TAUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post']","['- semantic features.', 'while dependencybased features capture the syntactic dependencies, frame - semantic features encode the semantic representation of the concepts in a sentence.', 'following our previous work on stance classification  #TAUTHOR_TAG c ), we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from semafor  #AUTHOR_TAG.', 'frame - word interaction features encode whether two words appear in different elements of the same frame.', 'hence, each frame - word interaction feature consists of ( 1 ) the name of the frame f from which it is created,']",2
"[' #TAUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype  #AUTHOR_TAG, caitra  #AUTHOR_TAG b )']","[' #TAUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype  #AUTHOR_TAG, caitra  #AUTHOR_TAG b ), thot ( ortiz - martinez and  #AUTHOR_TAG, transcenter  #AUTHOR_TAG b ), and casmacat  #AUTHOR_TAG.', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the hci literature']","[' #TAUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype  #AUTHOR_TAG, caitra  #AUTHOR_TAG b ), thot ( ort']","['process study most similar to ours is that of  #AUTHOR_TAG a ), who compared scratch, post - edit, and simple interactive modes.', 'however, he used undergraduate, non - professional subjects, and did not consider re - tuning.', 'our experimental design with professional bilingual translators follows our previous work  #TAUTHOR_TAG a ) comparing scratch translation to post - edit.', 'many research translation uis have been proposed including transtype  #AUTHOR_TAG, caitra  #AUTHOR_TAG b ), thot ( ortiz - martinez and  #AUTHOR_TAG, transcenter  #AUTHOR_TAG b ), and casmacat  #AUTHOR_TAG.', 'however, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the hci literature']",2
['with our previous findings from  #TAUTHOR_TAG that candidates with higher'],['with our previous findings from  #TAUTHOR_TAG that candidates with higher'],['with our previous findings from  #TAUTHOR_TAG that candidates with higher power attempt to shift topics'],"[""1 shows the pearson's product correlation between each topical feature and candidate's power."", 'we obtain a highly significant ( p = 0. 002 ) negative correlation between topic shift tendency of a candidate ( pi ) and his / her power.', ""in other words, the variation in the topic shifting tendencies is significantly correlated with the candidates'recent poll standings."", 'candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'this is in line with our previous findings from  #TAUTHOR_TAG that candidates with higher power attempt to shift topics less often than others when responding to moderators.', '']",1
"[' #TAUTHOR_TAG b ) and restrict bridging to non - coreferential cases.', 'we also exclude comparative anaph']","[' #TAUTHOR_TAG b ) and restrict bridging to non - coreferential cases.', 'we also exclude comparative anaphora  #AUTHOR_TAG']","['- head coreference ).', 'we follow our previous work  #TAUTHOR_TAG b ) and restrict bridging to non - coreferential cases.', 'we also exclude comparative anaphora  #AUTHOR_TAG']","['or associative anaphora has been widely discussed in the linguistic literature  #AUTHOR_TAG lobner, 1998 ).', ' #AUTHOR_TAG and  #AUTHOR_TAG include cases where antecedent and anaphor are coreferent but do not share the same head noun ( different - head coreference ).', 'we follow our previous work  #TAUTHOR_TAG b ) and restrict bridging to non - coreferential cases.', 'we also exclude comparative anaphora  #AUTHOR_TAG']",2
['##feats with more features from our previous work  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection'],['+ atomfeats we augment mlsystem rulefeats with more features from our previous work  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection'],"['##system rulefeats with more features from our previous work  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']","['in isnotes, 71 % of np antecedents occur in the same or up to two sentences prior to the anaphor.', 'initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'to deal with data imbalance, the svm light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'to compare the learning - based approach to the rulebased system described in section 3 directly, we report the mlsystem rulefeats we provide mlsystem rulefeats with the same knowledge resources as the rule - based system.', 'all rules from the rule - based system are incorporated into mlsystem rulefeats as the features.', 'mlsystem rulefeats + atomfeats we augment mlsystem rulefeats with more features from our previous work  #TAUTHOR_TAG a ;  #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection.', 'some of these features overlap with the atomic features used in the rule - based system']",2
"['', 'in history - based models  #TAUTHOR_TAG, the probability estimate for']","['', 'in history - based models  #TAUTHOR_TAG, the probability estimate for']","['', 'in history - based models  #TAUTHOR_TAG, the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..']","['', ""because there is a one - to - one mapping from phrase structure trees to our derivations, the probability of a derivation p ( di,..., dm ) is equal to the joint probability of the derivation's tree and the input sentence."", 'the probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history - based models  #TAUTHOR_TAG, the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d, _ 1, which is called the derivation history at step i.', 'this allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions']",5
"['the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']","['the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']","['the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']",1
['##a11 our results are computed with the evalb program following the now - standard criteria in  #TAUTHOR_TAG'],['##a11 our results are computed with the evalb program following the now - standard criteria in  #TAUTHOR_TAG'],['##a11 our results are computed with the evalb program following the now - standard criteria in  #TAUTHOR_TAG'],['##a11 our results are computed with the evalb program following the now - standard criteria in  #TAUTHOR_TAG'],5
"['- based probability model  #TAUTHOR_TAG, where the probability of']","['statistical parsers  #AUTHOR_TAG are based on a history - based probability model  #TAUTHOR_TAG, where the probability of']","['statistical parsers  #AUTHOR_TAG are based on a history - based probability model  #TAUTHOR_TAG, where the probability of']","['statistical parsers  #AUTHOR_TAG are based on a history - based probability model  #TAUTHOR_TAG, where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history  #AUTHOR_TAG.', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns )  #AUTHOR_TAG.', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing  #AUTHOR_TAG, and only marginally below the current state - of - the - art for parsing the penn treebank']",0
"['the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']","['the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']","['the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']","['most important step in designing a statistical parser with a history - based probability model is choosing a method for estimating the parameters d, _ 1 ).', 'the main difficulty with this estimation is that the history d 1,..., di _ 1 is of unbounded length.', 'most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'the standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history  #TAUTHOR_TAG']",1
['on re - ranking using a finite set of features  #TAUTHOR_TAG'],['on re - ranking using a finite set of features  #TAUTHOR_TAG'],"[""a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features  #TAUTHOR_TAG""]","['', 'structures of the form [ x [ x... ] [ y... ] ] ) with a special "" modifier "" link in the tree ( becoming [ x... [ mod y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'we also compiled some frequent chains of non - branching nodes ( such as [ s [ vp... 1 ] ) into a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', "" #AUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features  #TAUTHOR_TAG""]",0
['beneficial  #TAUTHOR_TAG'],['beneficial  #TAUTHOR_TAG'],['beneficial  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['a problem.', ' #TAUTHOR_TAG define a kernel over parse']","['a problem.', ' #TAUTHOR_TAG define a kernel over parse']","['a problem.', ' #TAUTHOR_TAG define a kernel over parse trees']","['', 'structures of the form [ x [ x... ] [ y... ] ] ) with a special "" modifier "" link in the tree ( becoming [ x... [ mod y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'we also compiled some frequent chains of non - branching nodes ( such as [ s [ vp... 1 ] ) into a single node with a new label ( becoming [ s - vp... ] ).', 'these transforms are undone before any evaluation is performed on the output trees.', 'we do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', "" #TAUTHOR_TAG define a kernel over parse trees and apply it to re - ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as collins'previous work on re - ranking using a finite set of features  #AUTHOR_TAG""]",0
"['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger  #TAUTHOR_TAG to tag the words and then used these in the input to the system']","['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger  #TAUTHOR_TAG to tag the words and then used these in the input to the system']","['compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger  #TAUTHOR_TAG to tag the words and then used these in the input to the system']","['these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand - corrected tags which come with the corpus.', 'we used a publicly available tagger  #TAUTHOR_TAG to tag the words and then used these in the input to the system']",5
"['recent statistical parsers  #TAUTHOR_TAG.', 'the']","['recent statistical parsers  #TAUTHOR_TAG.', 'the']","['1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers  #TAUTHOR_TAG.', 'the performance of the lexicalized model falls in the middle of this range, only being beaten']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers  #TAUTHOR_TAG.', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', '']",1
"['statistical parsers  #TAUTHOR_TAG are based on a history - based probability model  #AUTHOR_TAG, where the probability of']","['statistical parsers  #TAUTHOR_TAG are based on a history - based probability model  #AUTHOR_TAG, where the probability of']","['statistical parsers  #TAUTHOR_TAG are based on a history - based probability model  #AUTHOR_TAG, where the probability of']","['statistical parsers  #TAUTHOR_TAG are based on a history - based probability model  #AUTHOR_TAG, where the probability of each decision in a parse is conditioned on the previous decisions in the parse.', 'a major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'previous approaches have used a hand - crafted finite set of features to represent the unbounded parse history  #AUTHOR_TAG.', 'in the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'we perform this induction using an artificial neural network architecture, called simple synchrony networks ( ssns )  #AUTHOR_TAG.', 'because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'the resulting parser achieves performance far greater than previous approaches to neural network parsing  #AUTHOR_TAG, and only marginally below the current state - of - the - art for parsing the penn treebank']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['recent statistical parsers  #TAUTHOR_TAG.', 'the']","['recent statistical parsers  #TAUTHOR_TAG.', 'the']","['1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers  #TAUTHOR_TAG.', 'the performance of the lexicalized model falls in the middle of this range, only being beaten']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers  #TAUTHOR_TAG.', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', '']",1
"['recent statistical parsers  #TAUTHOR_TAG.', 'the']","['recent statistical parsers  #TAUTHOR_TAG.', 'the']","['1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers  #TAUTHOR_TAG.', 'the performance of the lexicalized model falls in the middle of this range, only being beaten']","['bottom panel of table 1 lists the results for the chosen lexicalized model ( ssn - freq > 200 ) and five recent statistical parsers  #TAUTHOR_TAG.', 'the performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', '']",1
"['best current statistical parsers  #TAUTHOR_TAG.', 'the difference from previous approaches is in the nature of the input to']","['best current statistical parsers  #TAUTHOR_TAG.', 'the difference from previous approaches is in the nature of the input to']","['are the inspiration behind one of the best current statistical parsers  #TAUTHOR_TAG.', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']","['this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'the method is a form of multi - layered artificial neural network called simple synchrony networks  #AUTHOR_TAG.', 'the outputs of this network are probability estimates computed with a log - linear model ( also known as a maximum entropy model ), as is done in  #AUTHOR_TAG.', 'log - linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers  #TAUTHOR_TAG.', 'the difference from previous approaches is in the nature of the input to the log - linear model.', 'we do not use handcrafted features, but instead we use a finite vector of real - valued features which are induced as part of the neural network training process.', 'these induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'in neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features']",1
['over  #TAUTHOR_TAG'],['over  #TAUTHOR_TAG'],['over  #TAUTHOR_TAG'],"['', 'this is roughly an 11 % relative reduction in error rate over  #TAUTHOR_TAG and bods pcfg - reduction reported in table 1.', 'compared to the reranking technique in  #AUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']",1
"['s method obtains very competitive results on the wall', 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ).  #TAUTHOR_TAG showed how the percept']","['. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ).  #TAUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', '']","['s method obtains very competitive results on the wall', 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ).  #TAUTHOR_TAG showed how the perceptron algorithm']","['', '- terminal it contains.  #AUTHOR_TAG used an alternative technique which samples a', 'fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. although bod', ""' s method obtains very competitive results on the wall"", 'street journal ( wsj ) task, the parsing time was reported to be over 200 seconds per sentence ( bod 2003 ).  #TAUTHOR_TAG showed how the perceptron algorithm can be used to efficiently', ""compute the best parse with dop1's subtrees, reporting a 5. 1 % relative reduction in error rate over the model in  #AUTHOR_TAG on the wsj.  #AUTHOR_TAG furthermore showed"", ""how bonnema et al.'s ( 1999 ) and  #AUTHOR_TAG estimators can be incorporated in his pcfgreduction, but did not report any experiments with these"", 'reductions']",0
"["".'s estimator performs worse and is comparable to  #TAUTHOR_TAG""]","["" #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to  #TAUTHOR_TAG""]","["".'s estimator performs worse and is comparable to  #TAUTHOR_TAG""]","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and  #AUTHOR_TAG estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', ' #AUTHOR_TAG bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to  #AUTHOR_TAG and  #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to  #TAUTHOR_TAG""]",0
"['in  #TAUTHOR_TAG, who obtained an lp of 89']","['in  #TAUTHOR_TAG, who obtained an lp of 89. 9']","['in  #TAUTHOR_TAG, who obtained an lp of 89']","['', ""this is roughly an 11 % relative reduction in error rate over  #AUTHOR_TAG and bod's pcfg - reduction reported in table 1."", 'compared to the reranking technique in  #TAUTHOR_TAG, who obtained an lp of 89. 9 % and an lr of 89. 6 %, our results show a 9 % relative error rate reduction.', 'while sl - dop and ls - dop have been compared before']",1
"[' #TAUTHOR_TAG, bonnema et']","["" #TAUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to  #AUTHOR_TAG""]","[' #TAUTHOR_TAG, bonnema et']","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and  #AUTHOR_TAG estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', ' #AUTHOR_TAG bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to  #AUTHOR_TAG and  #TAUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to  #AUTHOR_TAG""]",0
"['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse']","['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse']","['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely (']","['dop models, such as in  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e.', 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of  #AUTHOR_TAG given in section 2. 2']",0
"['statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and  #TAUTHOR_TAG.', 'as to']","['statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and  #TAUTHOR_TAG.', 'as to']","['with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and  #TAUTHOR_TAG.', 'as to the processing time, the pcfg reduction']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', 'collins 1996, charniak 1997, collins 1999 and  #TAUTHOR_TAG.', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in  #AUTHOR_TAG bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in  #AUTHOR_TAG are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in  #AUTHOR_TAG were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in  #AUTHOR_TAG']",1
"['our experiments we used the standard division of the wsj  #TAUTHOR_TAG, with sections 2 through 21 for training ( approx.', '']","['our experiments we used the standard division of the wsj  #TAUTHOR_TAG, with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual,']","['our experiments we used the standard division of the wsj  #TAUTHOR_TAG, with sections 2 through 21 for training ( approx.', '']","['our experiments we used the standard division of the wsj  #TAUTHOR_TAG, with sections 2 through 21 for training ( approx.', '40, 000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set.', 'as usual, all trees were stripped off their semantic tags, co - reference information and quotation marks.', 'without loss of generality, all trees were converted to binary branching ( and were reconverted to n - ary trees after parsing ).', 'we employed the same unknown ( category ) word model as in  #AUTHOR_TAG, based on statistics on word - endings, hyphenation and capitalization in combination with good - turing ( bod 1998 : 85 - 87 ).', 'we used "" evalb "" 4 to compute the standard parseval scores for our results ( manning & schiitze 1999 ).', 'we focused on the labeled precision ( lp ) and labeled recall ( lr ) scores, as these are commonly used to rank parsing systems']",5
"['( resp.', ' #TAUTHOR_TAG, charnia']","['( resp.', ' #TAUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction']","['( resp.', ' #TAUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction']","['http : / / www. cs. nyu. edu / cs / projects / proteus / evalb /', 'our first experimental goal was to compare the two pcfg - reductions in section 2. 2, which we will refer to resp.', 'as bod01 and bon99.', 'table 1 gives the results of these experiments and compares them with some other statistical parsers ( resp.', ' #TAUTHOR_TAG, charniak 1997, collins 1999 and charniak 2000 ).', '( 1996 ).', 'as to the processing time, the pcfg reduction parses each sentence 100 words ) in 3. 6 seconds average, while the parser in  #AUTHOR_TAG bod (, 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'this corresponds to a speedup of over 60 times.', 'it should be mentioned that the best precision and recall scores reported in  #AUTHOR_TAG are slightly better than the ones reported here ( the difference is only 0. 2 % for sentences 100 words ).', 'this may be explained by the fact our best results in  #AUTHOR_TAG were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the pcfg - reduction.', 'in the following section we will see that our new definition of best parse tree also outperforms the best results obtained in  #AUTHOR_TAG']",1
"['results on the wsj, comparable to  #TAUTHOR_TAG']","['results on the wsj, comparable to  #TAUTHOR_TAG']","['- art results on the wsj, comparable to  #TAUTHOR_TAG']","[""paper presents the first published results with goodman's pcfg - reductions of both bonnema et al.'s ( 1999 ) and  #AUTHOR_TAG estimators on the wsj."", 'we show that these pcfg - reductions result in a 60 times speedup in processing time w. r. t.', ' #AUTHOR_TAG bod (, 2003.', ""but while bod's estimator obtains state - of - the - art results on the wsj, comparable to  #TAUTHOR_TAG and  #AUTHOR_TAG, bonnema et al.'s estimator performs worse and is comparable to  #AUTHOR_TAG""]",0
"['2000 ; goodman 1998 ).', 'and  #TAUTHOR_TAG argues for']","['( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', 'and  #TAUTHOR_TAG argues for']","['1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', 'and  #TAUTHOR_TAG argues for']","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include nonlexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of  #AUTHOR_TAG and  #AUTHOR_TAG restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', ""and  #TAUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in  #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in  #AUTHOR_TAG""]",0
"['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely (']","['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely (']","['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely (']","['dop models, such as in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of  #AUTHOR_TAG given in section 2. 2']",0
"['in  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse']","['in  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse']","['in  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely (']","['dop models, such as in  #AUTHOR_TAG,  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e.', 'most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of  #AUTHOR_TAG given in section 2. 2']",0
"['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse']","['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse']","['in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely (']","['dop models, such as in  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG, use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i. e. most probable ) tree as a candidate for the best tree of a sentence.', 'we will refer to these models as likelihood - dop models, but in this paper we will specifically mean by "" likelihood - dop "" the pcfg - reduction of  #AUTHOR_TAG given in section 2. 2']",0
"['large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by  #TAUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on  #AUTHOR_TAG']","['large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by  #TAUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on  #AUTHOR_TAG']","['dop1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by  #TAUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on  #AUTHOR_TAG']","['dop1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree : everything from counts of single - level rules to counts of entire trees.', 'a disadvantage of this model is that an extremely large number of subtrees ( and derivations ) must be taken into account.', 'fortunately, there exists a compact pcfg - reduction of dop1 that generates the same trees with the same probabilities, as shown by  #TAUTHOR_TAG, 2002 ).', 'here we will only sketch this pcfg - reduction, which is heavily based on  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG, 1999 ),  #AUTHOR_TAG, 1997 ),  #AUTHOR_TAG,  #AUTHOR_TAG, and many others']","[' #TAUTHOR_TAG, 1999 ),  #AUTHOR_TAG, 1997 ),  #AUTHOR_TAG,  #AUTHOR_TAG, and many others']","[' #TAUTHOR_TAG, 1999 ),  #AUTHOR_TAG, 1997 ),  #AUTHOR_TAG,  #AUTHOR_TAG, and many others']","['##egner 1992 ; pereira and schabes 1992 ).', 'the dop model, on the other hand, was the first model ( to the best of our knowledge ) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'this approach has now gained wide usage, as exemplified by the work of  #TAUTHOR_TAG, 1999 ),  #AUTHOR_TAG, 1997 ),  #AUTHOR_TAG,  #AUTHOR_TAG, and many others']",4
"['##roversial ( e. g. collins 1999 ;  #TAUTHOR_TAG ; goodman 1998 ).', ' #AUTHOR_TAG argues for']","['( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g. collins 1999 ;  #TAUTHOR_TAG ; goodman 1998 ).', ' #AUTHOR_TAG argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has']","['##roversial ( e. g. collins 1999 ;  #TAUTHOR_TAG ; goodman 1998 ).', ' #AUTHOR_TAG argues for']","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include non - lexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of  #AUTHOR_TAG and  #AUTHOR_TAG restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g. collins 1999 ;  #TAUTHOR_TAG ; goodman 1998 ).', ' #AUTHOR_TAG argues for "" keeping track of counts of arbitrary fragments within parse trees "", which has indeed been carried out in  #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in  #AUTHOR_TAG']",0
['been carried out in  #TAUTHOR_TAG who use exactly'],['been carried out in  #TAUTHOR_TAG who use exactly'],"[""keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in  #TAUTHOR_TAG who use exactly""]","['other innovation of dop was to take ( in principle ) all corpus fragments, of any size, rather than a small subset.', 'this innovation has not become generally adopted yet : many approaches still work either with local trees, i. e. single level rules with limited means of information percolation, or with restricted fragments, as in stochastic tree - adjoining grammar ( schabes 1992 ; chiang 2000 ) that do not include nonlexicalized fragments.', 'however, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'while the models of  #AUTHOR_TAG and  #AUTHOR_TAG restricted the fragments to the locality of head - words, later models showed the importance of including context from higher nodes in the tree ( charniak 1997 ; johnson 1998a ).', 'the importance of including nonheadwords has become uncontroversial ( e. g.', 'collins 1999 ; charniak 2000 ; goodman 1998 ).', "" #AUTHOR_TAG argues for ` ` keeping track of counts of arbitrary fragments within parse trees'', which has indeed been carried out in  #TAUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in  #AUTHOR_TAG""]",4
"['a1  #TAUTHOR_TAG.', ""goodman's main theorem is that this construction""]","[', have probability 1 / a1  #TAUTHOR_TAG.', ""goodman's main theorem is that this construction""]","[', have probability 1 / a1  #TAUTHOR_TAG.', ""goodman's main theorem is that this construction""]","['##man then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1 / a.', 'and subderivations headed by a1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1 / a1  #TAUTHOR_TAG.', ""goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability."", 'this means that summing up over derivations of a tree in dop yields the same probability as summing over all the isomorphic derivations in the pcfg.', ""note that goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence : there may still be exponentially many derivations generating the same tree."", 'but goodman shows that with his pcfg - reduction he can efficiently compute the aforementioned maximum constituents parse.', ""moreover, goodman's pcfg reduction may also be used to estimate the most probable parse by viterbi n - best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", "" #AUTHOR_TAG needed to use a very large sample from the wsj subtrees to do this, goodman's method can do the same job with a more compact grammar""]",0
"['.', ' #TAUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of']","['probable parse.', ' #TAUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of']","['probable parse.', ' #TAUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of']","['', ' #TAUTHOR_TAG, 1998 ) developed a polynomial time pcfg - reduction of dop1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', '']",0
"['as a five - gram9 using the srilm - toolkit  #TAUTHOR_TAG.', 'we run mert separately for']","['as a five - gram9 using the srilm - toolkit  #TAUTHOR_TAG.', 'we run mert separately for']","['alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit  #TAUTHOR_TAG.', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with mos']","['evaluate our end - to - end system, we perform the well - studied task of news translation, using the moses smt package.', 'we use the english / german data released for the 2009 acl workshop on machine translation shared task on translation. 7', 'there are 82, 740 parallel sentences from news - commentary09. de - en and 1, 418, 115 parallel sentences from europarl - v4. de - en.', 'the monolingual data contains 9. 8 m sentences. 8', 'o build the baseline, the data was tokenized using the moses tokenizer and lowercased.', 'we use giza + + to generate alignments, by running 5 iterations of model 1, 5 iterations of the hmm model, and 4 iterations of model 4. we symmetrize using the "" grow - diag - final - and "" heuristic.', 'our moses systems use default settings.', 'the lm uses the monolingual data and is trained as a five - gram9 using the srilm - toolkit  #TAUTHOR_TAG.', 'we run mert separately for each system.', 'the recaser used is the same for all systems.', 'it is the standard recaser supplied with moses, trained on all german training data.', 'the dev set is wmt - 2009 - a and the test set is wmt - 2009 - b, and we report end - to - end case sensitive bleu scores against the unmodified reference sgml file.', 'the blind test set used is wmt - 2009 - blind ( all lines )']",5
"['of  #TAUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed']","['of  #TAUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed']","['to merge and ii ) how to merge.', 'following the work of  #TAUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed']","['translation, compound parts have to be resynthesized into compounds before inflection.', 'two decisions have to be taken : i ) where to merge and ii ) how to merge.', 'following the work of  #TAUTHOR_TAG, we implement a linear - chain crf merging system using the following features : stemmed ( separated ) surface form, part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word + 1, word as true prefix, word + 1 as true suffix, plus frequency comparisons of these.', 'the crf is trained on the split monolingual data.', 'it only proposes merging decisions, merging itself uses a list extracted from the monolingual data  #AUTHOR_TAG']",5
"['compound splitting, we follow  #TAUTHOR_TAG, using linguistic']","['compound splitting, we follow  #TAUTHOR_TAG, using linguistic']","['compound splitting, we follow  #TAUTHOR_TAG, using linguistic knowledge encoded in']","['compound splitting, we follow  #TAUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ).', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",5
"['stem, applying a 7 - gram pos model.', ' #TAUTHOR_TAG showed that the']","['stem, applying a 7 - gram pos model.', ' #TAUTHOR_TAG showed that the']","['each stem, applying a 7 - gram pos model.', ' #TAUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']","['use an additional target factor to obtain the coarse pos for each stem, applying a 7 - gram pos model.', ' #TAUTHOR_TAG showed that the use of a pos factor only results in negligible bleu improvements, but we need access to the pos in our inflection prediction models']",0
"['resulted in too many compounds.', 'we follow  #TAUTHOR_TAG,']","['resulted in too many compounds.', 'we follow  #TAUTHOR_TAG,']","['and then selecting the best analysis based on the ge - ometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ). compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #TAUTHOR_TAG,']","['compound splitting, we follow  #AUTHOR_TAG, using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the ge - ometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ). compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging ap - proach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #TAUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merg - ing ) on one of our two test sets']",5
"['inflected forms.', ' #TAUTHOR_TAG improved on this by marking prep']","['inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the']","['inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the']","['a stem such as brother, toutanova et.', 'al\'s system might generate the "" stem and inflection "" corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', ' #AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', ' #AUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #TAUTHOR_TAG.', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list -']","['based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #TAUTHOR_TAG.', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging']","['and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #TAUTHOR_TAG.', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #AUTHOR_TAG,']","['compound splitting, we follow  #AUTHOR_TAG, using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., po  #AUTHOR_TAG ) or are ( almost ) knowledge - free ( e. g.,  #TAUTHOR_TAG.', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",1
"['', ' #TAUTHOR_TAG used unification in an smt']","['important markups in our system ).', 'both efforts were ineffective on large data sets.', ' #TAUTHOR_TAG used unification in an smt']","[').', 'both efforts were ineffective on large data sets.', ' #TAUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex con - text features']","['a stem such as brother, toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'viewing and and his as inflection is problematic since a map - ping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., ad - jectives ) separating his and brother.', 'this required mapping is a significant problem for generaliza - tion.', 'we view this issue as a different sort of prob - lem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', ' #AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to in - flected forms.', ' #AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', ' #TAUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex con - text features']",1
['of german  #TAUTHOR_TAG'],['of german  #TAUTHOR_TAG'],['of german  #TAUTHOR_TAG'],"['key linguistic knowledge sources that we use are morphological analysis and generation of german based on smor, a morphological analyzer / generator of german  #AUTHOR_TAG and the bitpar parser, which is a state - of - the - art parser of german  #TAUTHOR_TAG']",5
"['of  #TAUTHOR_TAG.', 'first, possible split points are']","['of  #TAUTHOR_TAG.', 'first, possible split points are']","['prepare the training data by splitting compounds in two steps, following the technique of  #TAUTHOR_TAG.', 'first, possible split points are extracted using smor,']","['prepare the training data by splitting compounds in two steps, following the technique of  #TAUTHOR_TAG.', 'first, possible split points are extracted using smor, and second, the best split points are selected using the geometric mean of word part frequencies.', 'training data is then stemmed as described in section 2. 3.', 'the formerly modifying words of the compound ( in our example the words to the left of the rightmost word ) do not have a stem markup assigned, except for two cases : i ) they are nouns themselves or ii ) they are particles separated from a verb.', 'in these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization']",5
"['of linguistic - feature - based approaches will increase.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful']","['of linguistic - feature - based approaches will increase.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful']","['english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach']","['have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step.', 'the direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.', 'however, it is reasonable to expect that the use of features ( and morphological generation ) could also be problematic as this requires the use of morphologically - aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.', 'despite this, our research clearly shows that the feature - based approach is superior for english - to - german smt.', 'this is a striking result considering state - of - theart performance of german parsing is poor compared with the best performance on english parsing.', 'as parsing performance improves, the performance of linguistic - feature - based approaches will increase.', ' #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #TAUTHOR_TAG, and others are primarily concerned with using morpheme segmentation in smt, which is a useful approach for dealing with issues of word - formation.', 'however, this does not deal directly with linguistic features marked by inflection.', 'in german these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'so it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing']",1
"['on solving inflection.', ' #AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #TAUTHOR_TAG tried to']","['on solving inflection.', ' #AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #TAUTHOR_TAG tried to']","['with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', ' #AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #TAUTHOR_TAG tried to']","['a stem such as brother, toutanova et.', 'al\'s system might generate the "" stem and inflection "" corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a "" split in preprocessing and resynthesize in postprocessing "" approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et.', 'al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marino ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', ' #AUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #TAUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', ' #AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', ' #AUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['on solving inflection.', ' #TAUTHOR_TAG introduced factored sm']","['on solving inflection.', ' #TAUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system']","['with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', ' #TAUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system']","['a stem such as brother, toutanova et. als system might generate the stem and inflection corresponding to and his brother.', 'viewing and and his as inflection is problematic since a mapping from the english phrase and his brother to the arabic stem for brother is required.', 'the situation is worse if there are english words ( e. g., adjectives ) separating his and brother.', 'this required mapping is a significant problem for generalization.', 'we view this issue as a different sort of problem entirely, one of word - formation ( rather than inflection ).', 'we apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of toutanova et. al.', 'the only work that we are aware of which deals with both issues is the work of de gispert and marin _ _ o ( 2008 ), which deals with verbal morphology and attached pronouns.', 'there has been other work on solving inflection.', ' #TAUTHOR_TAG introduced factored smt.', 'we use more complex context features.', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an smt system for translating from stems to inflected forms.', ' #AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ).', 'both efforts were ineffective on large data sets.', ' #AUTHOR_TAG used unification in an smt system to model some of the agreement phenomena that we model.', 'our crf framework allows us to use more complex context features']",1
"['those of  #AUTHOR_TAG,  #TAUTHOR_TAG and others.', 'to']","['those of  #AUTHOR_TAG,  #TAUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most""]","['those of  #AUTHOR_TAG,  #TAUTHOR_TAG and others.', 'to']","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of  #AUTHOR_TAG,  #TAUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']",1
"['those of  #TAUTHOR_TAG,  #AUTHOR_TAG and others.', 'to']","['those of  #TAUTHOR_TAG,  #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is""]","['those of  #TAUTHOR_TAG,  #AUTHOR_TAG and others.', 'to']","['previous work looks at the impact of using source side information ( i. e., feature functions on the aligned english ), such as those of  #TAUTHOR_TAG,  #AUTHOR_TAG and others.', 'toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'using additional source side information beyond the markup did not produce a gain in performance']",1
"['., pos - tags  #TAUTHOR_TAG or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ).', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging']","[', pos - tags  #TAUTHOR_TAG or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ).', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging']","['., pos - tags  #TAUTHOR_TAG or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ).', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #AUTHOR_TAG,']","['compound splitting, we follow  #AUTHOR_TAG, using linguistic knowledge en - coded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'other approaches use less deep linguistic resources ( e. g., pos - tags  #TAUTHOR_TAG or are ( almost ) knowledge - free ( e. g.,  #AUTHOR_TAG ).', 'compound merging is less well studied.', ' #AUTHOR_TAG used a simple, list - based merging approach, merging all consecutive words included in a merging list.', 'this approach resulted in too many compounds.', 'we follow  #AUTHOR_TAG, for compound merging.', 'we trained a crf using ( nearly all ) of the features they used and found their approach to be effective ( when combined with inflection and portmanteau merging ) on one of our two test sets']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['approach to extract and classify social events builds on our previous work  #TAUTHOR_TAG, which in turn builds on work from the relation extraction community  #AUTHOR_TAG.', 'therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', 'researchers have used other notions of semantics in the literature such as latent semantic analysis  #AUTHOR_TAG and relation - specific semantics  #AUTHOR_TAG.', 'to the best of our knowledge, there is only one work that uses frame semantics for relation extraction  #AUTHOR_TAG.', ' #AUTHOR_TAG propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.', 'they, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.', 'we believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by  #AUTHOR_TAG']",2
"['of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in  #TAUTHOR_TAG.', 'typed feature structures as normal form ir']","['of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in  #TAUTHOR_TAG.', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in  #TAUTHOR_TAG.', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]","['##this view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in  #TAUTHOR_TAG.', ""typed feature structures as normal form ir ~'~ eterms are merely syntactic objects""]",0
['a more detailed discussion of various aspects of the proposed parser can be found in  #TAUTHOR_TAG'],['a more detailed discussion of various aspects of the proposed parser can be found in  #TAUTHOR_TAG'],['a more detailed discussion of various aspects of the proposed parser can be found in  #TAUTHOR_TAG'],['a more detailed discussion of various aspects of the proposed parser can be found in  #TAUTHOR_TAG'],0
['ale parser  #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to'],['ale parser  #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to'],"['99 example, the ale parser  #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to']","['control strategies depends on a way to differentiate between types of constraints.', ""proceedings of eacl'99 example, the ale parser  #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom - up or topdown."", 'in the case of selective magic parsing we use so - called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'a literal ( goal ) is considered a parse lype literal ( goal ) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° all types in the type hierarchy can be used as parse types.', 'this way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'however, in the remainder we will concentrate on a specific class of parse types : we assume the specification of type sign and its subtypes as parse types.', '11 this choice is based on the observation that the constraints on type sign and its sub - types play an important guiding role in the parsing process and are best interpreted bottom - up given the lexical orientation of i - ipsg.', ""the parsing process corresponding to such a parse type specification is represented schematically in figure 8. starting from the lexical entries, i. e., word word word figure 8 : schematic representation of the selective magic parsing process the : r ~'l definite clauses that specify the word objects in the grammar, phrases are built bottomup by matching the parse type literals of the definite clauses in the grammar against the edges in the table."", 'the non - parse type literals are processed according to the top - down control strategy 1°the notion of a parse type literal is closely related to that of a memo literal as in ( johnson and d  #AUTHOR_TAG']",0
"['', 'as shown in  #TAUTHOR_TAG a¢']","['1992 ).', 'as shown in  #TAUTHOR_TAG a¢']","['', 'as shown in  #TAUTHOR_TAG a¢ the presented research was carried out at the university of']","['is a compilation technique originally developed for goal - directed bottom - up processing of logic programs.', 'see, among others, ( ramakrishnan et al. 1992 ).', 'as shown in  #TAUTHOR_TAG a¢ the presented research was carried out at the university of tubingen, germany, as part of the sonderforschungsbereich 340']",0
"['implementations of head - driven phrase structure grammar ( hpsg ;  #TAUTHOR_TAG', '3  #AUTHOR_TAG propose a compilation of lexical rules']","['implementations of head - driven phrase structure grammar ( hpsg ;  #TAUTHOR_TAG', '3  #AUTHOR_TAG propose a compilation of lexical rules']","['implementations of head - driven phrase structure grammar ( hpsg ;  #TAUTHOR_TAG', '3  #AUTHOR_TAG propose a compilation of lexical rules']","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ;  #TAUTHOR_TAG', '3  #AUTHOR_TAG propose a compilation of lexical rules into t ~ r / : definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in  #AUTHOR_TAG.', ""typed feature structures as normal form ir ~'~ e terms are merely syntactic objects""]",0
"['implementations of head - driven phrase structure grammar  #TAUTHOR_TAG.', '3  #AUTHOR_TAG propose a']","['implementations of head - driven phrase structure grammar  #TAUTHOR_TAG.', '3  #AUTHOR_TAG propose a']","['implementations of head - driven phrase structure grammar  #TAUTHOR_TAG.', '3  #AUTHOR_TAG propose a compilation of lexical rules']","['feature grammars can be used as the basis for implementations of head - driven phrase structure grammar  #TAUTHOR_TAG.', '3  #AUTHOR_TAG propose a compilation of lexical rules into tit definite clauses which are used to restrict lexical entries.', '( g  #AUTHOR_TAG b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4', 'because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( g  #AUTHOR_TAG b ) implements the above men - tioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['others,  #TAUTHOR_TAG.', 'as shown in  #AUTHOR_TAG magic is']","['others,  #TAUTHOR_TAG.', 'as shown in  #AUTHOR_TAG magic is']","['others,  #TAUTHOR_TAG.', 'as shown in  #AUTHOR_TAG magic is an interesting technique with']","['is a compilation technique originally developed for goal - directed bottom - up processing of logic programs.', 'see, among others,  #TAUTHOR_TAG.', 'as shown in  #AUTHOR_TAG magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv : ; g  #AUTHOR_TAG.', 'typed feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ;  #AUTHOR_TAG as discussed in ( g  #AUTHOR_TAG a ) and  #AUTHOR_TAG.', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system ( g  #AUTHOR_TAG b ) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation']",0
"['of space limitations we have to refrain from an example.', 'the controll grammar development system as described in  #TAUTHOR_TAG b ) implements the']","['of space limitations we have to refrain from an example.', 'the controll grammar development system as described in  #TAUTHOR_TAG b ) implements the']","['of space limitations we have to refrain from an example.', 'the controll grammar development system as described in  #TAUTHOR_TAG b ) implements the above mentioned techniques']","['##e  #AUTHOR_TAG for a discussion of the appropriateness of t ~ - £ : for hpsg and a comparison with other feature logic approaches designed for hpsg.', 'append ( [ ~, [ ~, [ ~ ).', ' #AUTHOR_TAG b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in  #TAUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']",0
"['example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also  #TAUTHOR_TAG']","['example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also  #TAUTHOR_TAG']","['example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also  #TAUTHOR_TAG']","['##outining appears under many different guises, like for example, suspension, residuation, ( goal ) freezing, and blocking.', 'see also  #TAUTHOR_TAG']",0
[') of  #TAUTHOR_TAG'],[') of  #TAUTHOR_TAG'],"[') of  #TAUTHOR_TAG.', 'unlike']",[' #TAUTHOR_TAG'],1
"['implementations of head - driven phrase structure grammar ( hpsg ;  #AUTHOR_TAG as discussed in  #TAUTHOR_TAG a ) and  #AUTHOR_TAG.', 'typed feature grammar constraints that are inexpensive to resolve']","['implementations of head - driven phrase structure grammar ( hpsg ;  #AUTHOR_TAG as discussed in  #TAUTHOR_TAG a ) and  #AUTHOR_TAG.', 'typed feature grammar constraints that are inexpensive to resolve']","['implementations of head - driven phrase structure grammar ( hpsg ;  #AUTHOR_TAG as discussed in  #TAUTHOR_TAG a ) and  #AUTHOR_TAG.', 'typed feature grammar constraints that are inexpensive to']","['is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'in this paper we investigate the selective application of magic to typed feature grammars a type of constraint - logic grammar based on typed feature logic ( tgv£ : ; g  #AUTHOR_TAG.', 'typed feature grammars can be used as the basis for implementations of head - driven phrase structure grammar ( hpsg ;  #AUTHOR_TAG as discussed in  #TAUTHOR_TAG a ) and  #AUTHOR_TAG.', 'typed feature grammar constraints that are inexpensive to resolve are dealt with using the top - down interpreter of the controll grammar development system ( g  #AUTHOR_TAG b ) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation']",2
['see  #TAUTHOR_TAG'],['see  #TAUTHOR_TAG'],['see  #TAUTHOR_TAG'],"['see  #TAUTHOR_TAG for a discussion of the appropriateness of tig for hpsg and a comparison with other feature logic approaches designed for hpsg.', 'append ( [ ~, [ ~, [ ~ ).', ' #AUTHOR_TAG b ) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 because of space limitations we have to refrain from an example.', 'the controll grammar development system as described in ( g  #AUTHOR_TAG b ) implements the above mentioned techniques for compiling an hpsg theory into typed feature grammars']",0
"[',  #TAUTHOR_TAG ; watanabe']","[',  #TAUTHOR_TAG ; watanabe']","[',  #TAUTHOR_TAG ; watanabe 1995 ) to']","['is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.', 'instead, the aim is to produce bilingual ( i. e., synchronized, see below ) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.', 'for example, headwords in both languages are chosen to force a synchronized alignment ( for better or worse ) in order to simplify cases involving so - called head - switching.', 'this contrasts with one of the traditional approaches ( e. g.,  #TAUTHOR_TAG ; watanabe 1995 ) to posing the translation problem, i. e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language']",1
"['- lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers  #TAUTHOR_TAG and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['- lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers  #TAUTHOR_TAG and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers  #TAUTHOR_TAG and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']","['the transducers produced by the training method described in this paper, the source and target positions are in the set - lcb - - 1, 0, 1 - rcb -, though we have also used handcoded transducers  #TAUTHOR_TAG and automatically trained transducers ( alshawi and douglas 2000 ) with a larger range of positions']",5
[' #TAUTHOR_TAG ; brown'],[' #TAUTHOR_TAG ; brown'],['ibm  #TAUTHOR_TAG ; brown'],"['the same time, we believe our method has advantages over the approach developed initially at ibm  #TAUTHOR_TAG ; brown et al. 1993 ) for training translation systems automatically.', 'one advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'another is that the compilation of this decomposition into lexically anchored finite - state head transducers produces implementations that are much more efficient than those for the ibm model.', 'in particular, our search algorithm finds optimal transductions of test sentences in less than "" real time "" on a 300mhz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application']",1
"[""` a generation system must maintain the kinds of information outlined by grosz and sidner''  #TAUTHOR_TAG, 203 )."", 'their planner uses plan structures similar to']","[""or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner''  #TAUTHOR_TAG, 203 )."", 'their planner uses plan structures similar to']","[""` a generation system must maintain the kinds of information outlined by grosz and sidner''  #TAUTHOR_TAG, 203 )."", 'their planner uses plan structures similar to']","['##n constructs its plans using a hierarchical planning algorithm ( nilsson 1980 ).', 'the planner first checks all of its top - level plans to see which have effects that match the goal.', ""each matching plan's preconditions are checked ; if they are currently ( believed to be ) true, the planner then attempts to find all instantiations of the plan's body."", ""1 a° the body of a plan can be an action or sequence of actions, a goal or sequence 9 moore and paris also note that ` ` a generation system must maintain the kinds of information outlined by grosz and sidner''  #TAUTHOR_TAG, 203 )."", ""their planner uses plan structures similar to igen's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from rhetorical structure theory ( mann and thompson 1987 )."", 'in igen, the plans can involve any goals or actions that could be achieved via communication']",0
"[';  #TAUTHOR_TAG.', 'reiter describes a pipelined modular']","[';  #TAUTHOR_TAG.', 'reiter describes a pipelined modular']","['( meteer 1994 ; panaget 1994 ;  #TAUTHOR_TAG.', 'reiter describes a pipelined modular approach as a consensus architecture underlying']","['', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ;  #TAUTHOR_TAG.', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"[')  #TAUTHOR_TAG.', 'while this']","[')  #TAUTHOR_TAG.', 'while this']","['on an ad hoc basis )  #TAUTHOR_TAG.', 'while this']","['opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'whatever problems result will be handled as best they can, on a case - by - case basis.', 'this approach is the one taken ( implicitly or explicitly ) in the majority of generators.', 'in fact, reiter has even argued in favor of this approach, claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis )  #TAUTHOR_TAG.', 'while this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity""]",0
[' #TAUTHOR_TAG ; panaget'],[' #TAUTHOR_TAG ; panaget'],"['', 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component  #TAUTHOR_TAG ; panaget 1994 ; wanner 1994 )']","['', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component  #TAUTHOR_TAG ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( reiter 1994 )']",0
"['. g., elhadad and robin 1992 ; penman 1989 ;  #TAUTHOR_TAG a )']","['is in fact used in some systems ( e. g., elhadad and robin 1992 ; penman 1989 ;  #TAUTHOR_TAG a )']","['. g., elhadad and robin 1992 ; penman 1989 ;  #TAUTHOR_TAG a )']","['point here is not just that igen can produce different lexical realizations for a particular concept.', 'if that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network ( or similar device ) to test various features of the information being expressed.', 'the planner could supply whatever information is needed to drive the network.', 'something like this approach is in fact used in some systems ( e. g., elhadad and robin 1992 ; penman 1989 ;  #TAUTHOR_TAG a )']",0
['the planner  #TAUTHOR_TAG ; sondheimer and nebel'],['the planner  #TAUTHOR_TAG ; sondheimer and nebel'],"[""the planner  #TAUTHOR_TAG ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e""]","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner  #TAUTHOR_TAG ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']",0
"[';  #TAUTHOR_TAG,']","[';  #TAUTHOR_TAG, backtracking on failure ( appelt']","['interleaving the components ( mcdonald 1983 ;  #TAUTHOR_TAG,']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ;  #TAUTHOR_TAG, backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning ( hovy 1988a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.']",0
"[' #AUTHOR_TAG and  #AUTHOR_TAG and, at least implicitly, in  #TAUTHOR_TAG']","[' #AUTHOR_TAG and  #AUTHOR_TAG and, at least implicitly, in  #TAUTHOR_TAG']","[' #AUTHOR_TAG and  #AUTHOR_TAG and, at least implicitly, in  #TAUTHOR_TAG']","['possible response would be to abandon the separation ; the generator could be a single component that handles all of the work.', 'this approach has occasionally been taken, as in  #AUTHOR_TAG and  #AUTHOR_TAG and, at least implicitly, in  #TAUTHOR_TAG and  #AUTHOR_TAG ; however, under this approach, all of the flexibility and simplicity of modular design is lost']",0
"[', bottom - up ) planning  #TAUTHOR_TAG a, 1988c ).', '']","[""), and hovy's notion of restrictive ( i. e., bottom - up ) planning  #TAUTHOR_TAG a, 1988c )."", '']","[', bottom - up ) planning  #TAUTHOR_TAG a, 1988c ).', '']","['have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""these include devices such as interleaving the components ( mcdonald 1983 ; appelt 1983 ), backtracking on failure ( appelt 1985 ; nogier 1989 ), allowing the linguistic component to interrogate the planner ( mann 1983 ; sondheimer and nebel 1986 ), and hovy's notion of restrictive ( i. e., bottom - up ) planning  #TAUTHOR_TAG a, 1988c )."", 'all of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'the text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '']",0
"['##vy has described another text planner that builds similar plans  #TAUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely']","['##vy has described another text planner that builds similar plans  #TAUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arranges']","['##vy has described another text planner that builds similar plans  #TAUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely']","['##vy has described another text planner that builds similar plans  #TAUTHOR_TAG b ).', 'this system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern ; it is thus not a planner in the sense used here ( as hovy makes clear ).', '10 since text planning was not the primary focus of this work, igen is designed to simply assume that any false preconditions are unattainable.', ""igen's planner divides the requirements of a plan into two parts : the preconditions, which are not planned for, and those in the plan body, which are."", 'this has no']",0
['in generation  #TAUTHOR_TAG'],['in generation  #TAUTHOR_TAG'],['recent work in generation  #TAUTHOR_TAG'],"['', 'the precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting ( as opposed to merely organizing ) the information to be expressed.', 'much ( if not most ) work in generation, though, continues to rely on this modular approach for its basic design.', ""for example, diogenes ( nirenburg et al. 1988 ), epicure ( dale 1989 ), spokesman ( meteer 1989 ), sibun's work on local organization of text ( sibun 1991 ), and comet ( fisher and mckeown 1990 ) all are organized this way."", 'mcdonald has even argued for extending the model to a large number of components ( mcdonald 1988 ), and several systems have indeed added an additional component between the planner and the linguistic component ( meteer 1994 ; panaget 1994 ; wanner 1994 ).', 'reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation  #TAUTHOR_TAG']",0
"[';  #TAUTHOR_TAG a ), or simply']","[';  #TAUTHOR_TAG a ), or simply']","[';  #TAUTHOR_TAG a ), or simply']",[' #TAUTHOR_TAG'],0
"['de  #AUTHOR_TAG, and  #TAUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to']","['de  #AUTHOR_TAG, and  #TAUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to']","['de  #AUTHOR_TAG, and  #TAUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to']","['and articles on the topic include  #AUTHOR_TAG, de  #AUTHOR_TAG, and  #TAUTHOR_TAG.', 'still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self - contained']",0
"['associated problem of spurious ambiguity is given in  #TAUTHOR_TAG but again, to our knowledge, there is no predictive']","['associated problem of spurious ambiguity is given in  #TAUTHOR_TAG but again, to our knowledge, there is no predictive']","['associated problem of spurious ambiguity is given in  #TAUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']","['17 ) by a result of  #AUTHOR_TAG, the lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of lambek calculus.', 'combinatory categorial grammar does not concern itself with the capture of all ( or only ) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'an approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in  #TAUTHOR_TAG but again, to our knowledge, there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction']",0
"['application is regulated  #TAUTHOR_TAG, hepple 1990, hend']","['application is regulated  #TAUTHOR_TAG, hepple 1990, hendriks']","['the succession of rule application is regulated  #TAUTHOR_TAG, hepple 1990, hend']","['approach to this problem consists in defining, within the cut - free atomic - id space, normal form derivations in which the succession of rule application is regulated  #TAUTHOR_TAG, hepple 1990, hendriks 1993 ).', '']",0
['cue words and phrases  #TAUTHOR_TAG'],['cue words and phrases  #TAUTHOR_TAG'],"['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases  #TAUTHOR_TAG can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases']","['classification using words is based on the observation that different das use distinctive word strings.', 'it is known that certain cue words and phrases  #TAUTHOR_TAG can serve as explicit indicators of discourse structure.', 'similarly, we find distinctive correlations between certain phrases and da types.', 'for example, 92. 4 % of the uh - huh\'s occur in backchannels, and 88. 4 % of the trigrams "" < start > do you "" occur in yes - no - questions.', 'to leverage this information source, without hand - coding knowledge about which words are indicative of which das, we will use statistical language models that model the full word sequences associated with each da type.', '']",4
['-  #AUTHOR_TAG is transformation - based learning  #TAUTHOR_TAG'],"['for da labeling proposed by samuel, carberry, and vijay -  #AUTHOR_TAG is transformation - based learning  #TAUTHOR_TAG']","['-  #AUTHOR_TAG is transformation - based learning  #TAUTHOR_TAG.', 'finally it should be noted that there are other tasks with a mathematical structure similar to']","['the work mentioned so far uses statistical models of various kinds.', 'as we have shown here, such models offer some fundamental advantages, such as modularity and composability ( e. g., of discourse grammars with da models ) and the ability to deal with noisy input ( e. g., from a speech recognizer ) in a principled way.', 'however, many other classifier architectures are applicable to the tasks discussed, in particular to da classification.', 'a nonprobabilistic approach for da labeling proposed by samuel, carberry, and vijay -  #AUTHOR_TAG is transformation - based learning  #TAUTHOR_TAG.', 'finally it should be noted that there are other tasks with a mathematical structure similar to that of da tagging, such as shallow parsing for natural language processing ( munk 1999 ) and dna classification tasks ( ohler, harbeck, and niemann 1999 ), from which further techniques could be borrowed']",1
"[') and tagging  #TAUTHOR_TAG.', 'it maximizes the probability of getting the']","[') and tagging  #TAUTHOR_TAG.', 'it maximizes the probability of getting the']","['as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging  #TAUTHOR_TAG.', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging  #TAUTHOR_TAG.', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct ( dermatas and kokkinakis 1995 ).', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']",1
"['most da labels correct  #TAUTHOR_TAG.', 'to minimize']","['most da labels correct  #TAUTHOR_TAG.', 'to minimize']","['most da labels correct  #TAUTHOR_TAG.', 'to minimize']","['combination of likelihood and prior modeling, hmms, and viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( bahl, jelinek, and mercer 1983 ) and tagging ( church 1988 ).', 'it maximizes the probability of getting the entire da sequence correct, but it does not necessarily find the da sequence that has the most da labels correct  #TAUTHOR_TAG.', 'to minimize the total number of utterance labeling errors, we need to maximize the probability of getting each da label correct individually, i. e., we need to maximize p ( uile ) for each i = 1..... n.', 'we can compute the per - utterance posterior da probabilities by summing']",0
"[""job as pereira's pronoun abstraction schema in  #TAUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""job as pereira's pronoun abstraction schema in  #TAUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in  #TAUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']","[""equivalence is doing essentially the same job as pereira's pronoun abstraction schema in  #TAUTHOR_TAG."", 'it will identify a pronoun with any term of type e elsewhere in the qlf, relying on the binding conditions to prevent impossible associations']",1
"[' #TAUTHOR_TAG ],']","[' #TAUTHOR_TAG ],']","['standard qlfs [  #TAUTHOR_TAG ],']","['is required is that qlfs are, as here, expressed in a typed higher - order logic, augmented with constructs representing the interpretation of context - dependent elements ( pronouns, ellipsis, focus, etc. ).', 'these constructs correspond as directly as possible to properties of the linguistic structure that express them and are, to as small an extent as possible, dependent on the requirements of contextual resolution ( unlike, say, the metavariables of standard qlfs [  #TAUTHOR_TAG ], or the labels of udrs [ reyle 1996 ], which are motivated entirely by the mechanisms that operate on them after grammatical processing ).', 'syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at qlf ( again unlike standard qlfs ) but are assumed to be available as components of the linguistic context.', '']",0
"['by  #TAUTHOR_TAG and  #AUTHOR_TAG,']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG,']","['by  #TAUTHOR_TAG and  #AUTHOR_TAG, the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in  #AUTHOR_TAG, 1992 ), and implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as rationally reconstructed by  #TAUTHOR_TAG and  #AUTHOR_TAG, the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs repre - sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"['by  #TAUTHOR_TAG.', 'this describes the semantics of ql']","['by  #TAUTHOR_TAG.', 'this describes the semantics of qlfs via a']","['by  #TAUTHOR_TAG.', 'this describes the semantics of ql']","['third problem arises with the approach to the semantics of qlfs that this notion of the relationship between qlf and rqlf encourages one to adopt : it is that taken by  #TAUTHOR_TAG.', '']",1
"['theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by  #TAUTHOR_TAG and  #AUTHOR_TAG ; underspecified discourse representation']","['theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by  #TAUTHOR_TAG and  #AUTHOR_TAG ; underspecified discourse representation']","['with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by  #TAUTHOR_TAG and  #AUTHOR_TAG ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of  #AUTHOR_TAG']","['then go on to compare the current approach with that of some other theories with similar aims : the ` ` standard version of quasi - logical form implemented in the core language engine, as rationally reconstructed by  #TAUTHOR_TAG and  #AUTHOR_TAG ; underspecified discourse representation theory ( reyle 1993 ) ; and the ` ` glue language approach of  #AUTHOR_TAG']",1
"[""theory of quasi - logical form as described in  #TAUTHOR_TAG, 1992 ), and implemented in sri's""]","[""theory of quasi - logical form as described in  #TAUTHOR_TAG, 1992 ), and implemented in sri's""]","[""the theory of quasi - logical form as described in  #TAUTHOR_TAG, 1992 ), and implemented in sri's""]","[""starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi - logical form as described in  #TAUTHOR_TAG, 1992 ), and implemented in sri's core language engine ( cle )."", 'in the cle - qlf approach, as ra - tionally reconstructed by  #AUTHOR_TAG and  #AUTHOR_TAG, the context - independent meaning of a sentence is given by one or more qlfs that are built directly from syntactic and semantic rules.', 'just as here, these qlfs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context']",1
"[' #TAUTHOR_TAG, which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","[' #TAUTHOR_TAG, which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['as a quantifier : sometimes this can be quite a complicated matter, as with any  #TAUTHOR_TAG, which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']","['assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any  #TAUTHOR_TAG, which will resolve in different ways depending on its linguistic context, but here we avoid this complexity']",0
"[""' structure  #TAUTHOR_TAG, with""]","[""in a ` ` packed'' structure  #TAUTHOR_TAG, with""]","[""' structure  #TAUTHOR_TAG, with the resolution process as described here."", 'alternatively, i believe it is worth exploring the approach to disambig']","['are several stategies that might be pursued.', 'one is to adopt pinkal\'s "" radical underspecification "" approach ( pinkal 1995 ) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""the more conservative approach is to try to integrate existing statistical disambiguation schemes for qlfs, either individually or in a ` ` packed'' structure  #TAUTHOR_TAG, with the resolution process as described here."", 'alternatively, i believe it is worth exploring the approach to disambiguation described in  #AUTHOR_TAG, which would mesh nicely with the theory presented here']",3
"['in  #TAUTHOR_TAG, 1991 ), with some differences']","['in  #TAUTHOR_TAG, 1991 ), with some differences']","['in  #TAUTHOR_TAG, 1991 ), with some differences']","['can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'the version proposed here combines a basic insight from  #AUTHOR_TAG with higher - order unification to give an analysis that has a strong resemblance to that proposed in  #TAUTHOR_TAG, 1991 ), with some differences that are commented on below.', ""like pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by lewin."", 'we analyze quantified nps at the qlf level as illustrated in the qlf for : we assume that every determiner has its own equivalence, which resolves it as a quantifier : sometimes this can be quite a complicated matter, as with any ( alshawi 1990 ), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '']",1
"['described in dalrymple, shieber, and  #AUTHOR_TAG and  #TAUTHOR_TAG, 1991 ).', 'recall']","['described in dalrymple, shieber, and  #AUTHOR_TAG and  #TAUTHOR_TAG, 1991 ).', 'recall']","['described in dalrymple, shieber, and  #AUTHOR_TAG and  #TAUTHOR_TAG, 1991 ).', 'recall']","['is interesting to compare this analysis with that described in dalrymple, shieber, and  #AUTHOR_TAG and  #TAUTHOR_TAG, 1991 ).', 'recall that in their treatment, quantified noun phrases are treated in two stages : firstly, what they call a "" free variable "" of type e is introduced in the np position, with an associated "" quantifier assumption, "" which is added as a kind of premise.', 'at a later stage the quantifier assumption is "" discharged, "" capturing all occurrences of the free variable.', 'thus their analysis of something like every manager disappeared would proceed as follows']",1
"['quantifiers are produced  #TAUTHOR_TAG, 47 ), but without']","['quantifiers are produced  #TAUTHOR_TAG, 47 ), but without']","['the quantifiers are produced  #TAUTHOR_TAG, 47 ), but without the need']","['the available five relative scopings of the quantifiers are produced  #TAUTHOR_TAG, 47 ), but without the need for a free variable constraint - - the hou algorithm will not produce any solutions in which a previously bound variable becomes free ; a¢ the equivalences are reversible, and thus the above sentences cart be generated from scoped logical forms ; a¢ partial scopings are permitted ( see reyle [ 19961 ) a¢ scoping can be freely interleaved with other types of reference resolution ; a¢ unscoped or partially scoped forms are available for inference or for generation at every stage']",0
"['', 'but the general outlines are reasonably clear, and we can adapt some of the udrs  #TAUTHOR_TAG work to our own framework']","['to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs  #TAUTHOR_TAG work to our own framework']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs  #TAUTHOR_TAG work to our own framework.', 'reyle points out that']","['a calculus for reasoning with qlfs is too large a task to be undertaken here.', 'but the general outlines are reasonably clear, and we can adapt some of the udrs  #TAUTHOR_TAG work to our own framework.', 'reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'his example is']",5
['present an illustrative first - order fragment along these lines and are able to supply'],['present an illustrative first - order fragment along these lines and are able to supply'],['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics'],"['present an illustrative first - order fragment along these lines and are able to supply a coherent formal semantics for the clf - qlfs themselves, using a technique essentially equivalent to supervaluations : a qlf is true iff all its possible rqlfs are, false iff they are all false, and undefined otherwise']",0
"[', and preston 1996 ;  #TAUTHOR_TAG ; mitkov 1996, 1998b )']","[', and preston 1996 ;  #TAUTHOR_TAG ; mitkov 1996, 1998b )']","[', and preston 1996 ;  #TAUTHOR_TAG ; mitkov 1996, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ;  #TAUTHOR_TAG ; mitkov 1996, 1998b )']",0
['; strube and hahn 1996 ;  #TAUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaph'],['; strube and hahn 1996 ;  #TAUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora'],['; strube and hahn 1996 ;  #TAUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaph'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ;  #TAUTHOR_TAG ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
['##list approach  #TAUTHOR_TAG'],"["", and pollard 1987 ), and strube's 5list approach  #TAUTHOR_TAG""]","['##list approach  #TAUTHOR_TAG.', '']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm ( hobbs 1978 ), bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach  #TAUTHOR_TAG."", '']",0
"[' #TAUTHOR_TAG a, 2001b ).', 'for a more detailed survey of the state of the art in anaph']","[' #TAUTHOR_TAG a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']","[' #TAUTHOR_TAG a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution  #TAUTHOR_TAG a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[';  #TAUTHOR_TAG ; williams,']","[';  #TAUTHOR_TAG ; williams,']","['leass 1994 ; nasukawa 1994 ;  #TAUTHOR_TAG ; williams,']","['of the earlier work in anaphora resolution heavily exploited domain and lin - guistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ;  #TAUTHOR_TAG ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",0
[';  #TAUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaph'],[';  #TAUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora'],[';  #TAUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaph'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ;  #TAUTHOR_TAG ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b )']",0
"['bennett 1995 ;  #TAUTHOR_TAG ; ge, hal']","['bennett 1995 ;  #TAUTHOR_TAG ; ge, hale, and']","['bennett 1995 ;  #TAUTHOR_TAG ; ge, hal']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ;  #TAUTHOR_TAG ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['maiorano 2000 ; mitkov and barbu 2000 ;  #TAUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other mileston']","['maiorano 2000 ; mitkov and barbu 2000 ;  #TAUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent']","['maiorano 2000 ; mitkov and barbu 2000 ;  #TAUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ;  #TAUTHOR_TAG ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a  #AUTHOR_TAG b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[';  #TAUTHOR_TAG ; kennedy and boguraev 1996 ; williams,']","[';  #TAUTHOR_TAG ; kennedy and boguraev 1996 ; williams,']","['and leass 1994 ;  #TAUTHOR_TAG ; kennedy and boguraev 1996 ; williams,']","['of the earlier work in anaphora resolution heavily exploited domain and lin - guistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ;  #TAUTHOR_TAG ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ; mitkov 1996, 1998b )']",0
[';  #TAUTHOR_TAG ; carbonell and brown'],[';  #TAUTHOR_TAG ; carbonell and brown'],"['exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ;  #TAUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process,']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ;  #TAUTHOR_TAG ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
"['maiorano 2000 ;  #TAUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other mileston']","['maiorano 2000 ;  #TAUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent']","['maiorano 2000 ;  #TAUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ;  #TAUTHOR_TAG ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a  #AUTHOR_TAG b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
[' #TAUTHOR_TAG a )'],[' #TAUTHOR_TAG a )'],"[', as a consequence, represent major challenges to the further development of the field  #TAUTHOR_TAG a )']","['last years have seen considerable advances in the field of anaphora resolution, but a number of outstanding issues either remain unsolved or need more attention and, as a consequence, represent major challenges to the further development of the field  #TAUTHOR_TAG a ).', '']",3
['; strube and hahn 1996 ; hahn and strube 1997 ;  #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaph'],['; strube and hahn 1996 ; hahn and strube 1997 ;  #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaphora'],['; strube and hahn 1996 ; hahn and strube 1997 ;  #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaph'],"['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abracos and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ;  #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a, 2001b ).', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[';  #TAUTHOR_TAG, which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing']","[';  #TAUTHOR_TAG, which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing']","[';  #TAUTHOR_TAG, which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ;  #TAUTHOR_TAG, which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
"['described in  #TAUTHOR_TAG,  #AUTHOR_TAG,']","['described in  #TAUTHOR_TAG,  #AUTHOR_TAG,']","['described in  #TAUTHOR_TAG,  #AUTHOR_TAG,']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #TAUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a  #AUTHOR_TAG b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"['1998 ;  #TAUTHOR_TAG ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other mileston']","['recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ;  #TAUTHOR_TAG ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent']","['##uskas 1998 ;  #TAUTHOR_TAG ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ;  #TAUTHOR_TAG ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a  #AUTHOR_TAG b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[', and preston 1996 ; baldwin 1997 ;  #TAUTHOR_TAG, 1998b )']","[', and preston 1996 ; baldwin 1997 ;  #TAUTHOR_TAG, 1998b )']","[', and preston 1996 ; baldwin 1997 ;  #TAUTHOR_TAG, 1998b )']","['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( sidner 1979 ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments ( dagan and itai 1990, 1991 ; lappin and leass 1994 ; nasukawa 1994 ; kennedy and boguraev 1996 ; williams, harvey, and preston 1996 ; baldwin 1997 ;  #TAUTHOR_TAG, 1998b )']",0
"['s naive algorithm  #TAUTHOR_TAG, bf']","[""other pronoun resolution methods : hobbs's naive algorithm  #TAUTHOR_TAG, bfp ( brennan,""]","['s naive algorithm  #TAUTHOR_TAG, bf']","[""##eault's contribution features comparative evaluation involving the author's own centering - based pronoun resolution algorithm called the left - right centering algorithm ( lrc ) as well as three other pronoun resolution methods : hobbs's naive algorithm  #TAUTHOR_TAG, bfp ( brennan, friedman, and pollard 1987 ), and strube's 5list approach ( strube 1998 )."", 'the lrc is an alternative to the original bfp algorithm in that it processes utterances incrementally.', 'it works by first searching for an antecedent in the current sentence ; if none can be found, it continues the search on the cf - list of the previous and the other preceding utterances in a left - to - right fashion']",0
"['barbu 2000 ; mitkov 1999 ;  #TAUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other mileston']","['barbu 2000 ; mitkov 1999 ;  #TAUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent']","['barbu 2000 ; mitkov 1999 ;  #TAUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ;  #TAUTHOR_TAG ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a  #AUTHOR_TAG b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
"[' #TAUTHOR_TAG.', 'the last']","[' #TAUTHOR_TAG.', 'the last']","[' #TAUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects']","['the shift toward knowledge - poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'the inclusion of the coreference task in the sixth and seventh message understanding conferences ( muc - 6 and muc - 7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in  #AUTHOR_TAG,  #AUTHOR_TAG, and  #TAUTHOR_TAG.', 'the last decade of the 20th century saw a number of anaphora resolution projects for languages other than english such as french, german, japanese, spanish, portuguese, and turkish.', 'against the background of a growing interest in multilingual nlp, multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( aone and mckee 1993 ; azzam, humphreys, and gaizauskas 1998 ; harabagiu and maiorano 2000 ; mitkov and barbu 2000 ; mitkov 1999 ; mitkov and stys 1997 ; mitkov, belguith, and stys 1998 ).', 'other milestones of recent research include the deployment of probabilistic and machine learning techniques ( aone and bennett 1995 ; kehler 1997 ; ge, hale, and charniak 1998 ; cardie and wagstaff 1999 ; the continuing interest in centering, used either in original or in revised form ( abra ~ os and lopes 1994 ; strube and hahn 1996 ; hahn and strube 1997 ; tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( mitkov 1998a  #AUTHOR_TAG b.', 'for a more detailed survey of the state of the art in anaphora resolution, see mitkov ( forthcoming )']",0
[' #TAUTHOR_TAG ; carter'],[' #TAUTHOR_TAG ; carter'],['exploited domain and linguistic knowledge  #TAUTHOR_TAG ; carter 1987 ; rich and'],"['of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge  #TAUTHOR_TAG ; carter 1987 ; rich and luperfoy 1988 ; carbonell and brown 1988 ), which was difficult both to represent and to process, and which required considerable human input.', 'however, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical nlp systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge - poor anaphora resolution strategies.', 'a number of proposals in the 1990s deliberately limited the extent to which they relied on domain and / or linguistic knowledge and reported promising results in knowledge - poor operational environments']",0
[' #TAUTHOR_TAG 1998 ) of information'],"['subtask  #TAUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also']","[' #TAUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also']","['names are the main concern of the named - entity recognition subtask  #TAUTHOR_TAG 1998 ) of information extraction.', 'the main objective of this subtask is the identification of proper names and also their classification into semantic categories ( person, organization, location, etc. ). 1', 'there the disambiguation of the first word in a sentence ( and in other ambiguous positions ) is one of the central problems : about 20 % of named entities occur in ambiguous positions.', 'for instance, the word black in the sentenceinitial position can stand for a persons surname but can also refer to the color.', 'even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.', 'in the sentence daily, mason and partners lost their court case, it is clear that daily, mason and partners is the name of a company.', 'in the sentence unfortunately, mason and partners lost their court case, the name of the company does not include the word unfortunately, but the word daily is just as common a word as unfortunately']",0
"[' #TAUTHOR_TAG ], bri']","["" #TAUTHOR_TAG ], brill's""]","[', hidden markov model [ hmm ] [  #TAUTHOR_TAG ], bri']","['', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [  #TAUTHOR_TAG ], brill's [ brill 1995a ], and maxent [ ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
"['that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in  #TAUTHOR_TAG']","['that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in  #TAUTHOR_TAG']","['the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in  #TAUTHOR_TAG']","['second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the sbd task, as we showed in section 3. we tackle capitalized words in a similar fashion as we tackled the abbreviations : through a document - centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'this is implemented as a cascade of simple strategies, which were briefly described in  #TAUTHOR_TAG']",5
['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus  #TAUTHOR_TAG and'],['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus  #TAUTHOR_TAG and'],"['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus  #TAUTHOR_TAG and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and']","['are two corpora normally used for evaluation in a number of text - processing tasks : the brown corpus  #TAUTHOR_TAG and the wall street journal ( wsj ) corpus, both part of the penn treebank ( marcus, marcinkiewicz, and santorini 1993 ).', 'the brown corpus represents general english.', 'it contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'the brown corpus is rich in out - of - vocabulary ( unknown ) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'altogether there are about 500 documents in the brown corpus, with an average length of 2, 300 word tokens']",5
"[' #TAUTHOR_TAG.', 'gale, church, and yarows']","[' #TAUTHOR_TAG.', ""gale, church, and yarowsky's observation is also used in our""]",[' #TAUTHOR_TAG'],"['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de  #AUTHOR_TAG proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', "" #AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac  #AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition  #TAUTHOR_TAG.', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', 'this is similar to "" one']",0
"['classifiers  #TAUTHOR_TAG, neural']","['classifiers  #TAUTHOR_TAG, neural']","[' #TAUTHOR_TAG, neural networks']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers  #TAUTHOR_TAG, neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"['), neural networks  #TAUTHOR_TAG, and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine']","['), neural networks  #TAUTHOR_TAG, and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine']","['( riley 1989 ), neural networks  #TAUTHOR_TAG, and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features']","['trainable software is generally seen as a way of producing sys - tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks  #TAUTHOR_TAG, and maximum - entropy modeling ( reynar and ratnaparkhi 1997 ).', 'ma - chine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con - text of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"[' #TAUTHOR_TAG a ],']","[' #TAUTHOR_TAG a ],']","[' #TAUTHOR_TAG a ],']","['', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [  #TAUTHOR_TAG a ], and maxent [ ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", 'the only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which pos information does not depend on the previous and following history.', 'this issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'for instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'at the same time since this token is unambiguous, it is not affected by the history.', 'a trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other']",1
[' #TAUTHOR_TAG ] )'],[' #TAUTHOR_TAG ] )'],['maxent [  #TAUTHOR_TAG ] ) do not'],"['', 'this requires resolving sentence boundaries before tagging.', ""we see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms ( e. g., hidden markov model [ hmm ] [ kupiec 1992 ], brill's [ brill 1995a ], and maxent [  #TAUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens."", '']",1
['satz system  #TAUTHOR_TAG with'],['satz system  #TAUTHOR_TAG with'],['a combination of the satz system  #TAUTHOR_TAG with'],"['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system  #TAUTHOR_TAG with the alembic system ( aberdeen et al. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by  #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in  #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['alembic workbench  #TAUTHOR_TAG contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains']","['alembic workbench  #TAUTHOR_TAG contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains']","['are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench  #TAUTHOR_TAG contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains']","['exist two large classes of sbd systems : rule based and machine learning.', 'the rule - based systems use manually built rules that are usually encoded in terms of regular - expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'to put together a few rules is fast and easy, but to develop a rule - based system with good performance is quite a labor - consuming enterprise.', 'for instance, the alembic workbench  #TAUTHOR_TAG contains a sentence - splitting module that employs over 100 regular - expression rules written in flex.', 'another well - acknowledged shortcoming of rule - based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains']",0
['satz systems described in  #TAUTHOR_TAG ('],['satz systems described in  #TAUTHOR_TAG ('],['satz systems described in  #TAUTHOR_TAG ('],"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before ( riley 1989 : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in  #TAUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']",1
[' #TAUTHOR_TAG b ] ) enable regular'],[' #TAUTHOR_TAG b ] ) enable regularities to be induced for word'],"["". g., baum - welch [ baum and petrie 1966 ] or brill's [  #TAUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually']","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [ baum and petrie 1966 ] or brill's [  #TAUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'periods as many other closed - class words cannot be successfully covered by such technique']",1
"['wsj corpus, as reported in  #TAUTHOR_TAG.', 'we are not aware of any studies']","['wsj corpus, as reported in  #TAUTHOR_TAG.', 'we are not aware of any studies']","['on the wsj corpus, as reported in  #TAUTHOR_TAG.', 'we are not aware of any studies']","['', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by  #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in  #TAUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"[' #TAUTHOR_TAG rightly pointed out, however, ` ` proper']","[' #TAUTHOR_TAG rightly pointed out, however, ` ` proper']","[' #TAUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized']","['##uation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'as  #TAUTHOR_TAG rightly pointed out, however, ` ` proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not.', 'estimates from the brown corpus can be misleading.', ""for example, the capitalized word'acts'is found twice in the brown corpus, both times as a proper noun ( in a title )."", ""it would be misleading to infer from this evidence that the word'acts'is always a proper noun.""]",1
"["" #TAUTHOR_TAG ] or brill's [ brill""]","["" #TAUTHOR_TAG ] or brill's [ brill 1995b ] ) enable regularities to be induced for""]","["". g., baum - welch [  #TAUTHOR_TAG ] or brill's [ brill 1995b ] ) enable regularities to be induced for""]","['decided to train the tagger with the minimum of preannotated resources.', 'first, we used 20, 000 tagged words to "" bootstrap "" the training process, because purely unsupervised techniques, at least for the hmm class of taggers, yield lower precision.', 'we also used our dca system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99. 8 %.', ""this was done because purely unsupervised techniques ( e. g., baum - welch [  #TAUTHOR_TAG ] or brill's [ brill 1995b ] ) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a pos class occur in different ambiguity patterns."", 'counting all possible pos combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'periods as many other closed - class words cannot be successfully covered by such technique']",1
"['normalization issue.', 'before using the dca method, we applied a russian morphological processor  #TAUTHOR_TAG to']","['normalization issue.', 'before using the dca method, we applied a russian morphological processor  #TAUTHOR_TAG to']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor  #TAUTHOR_TAG to']","[', unlike english, russian is a highly inflected language, we had to deal with the case normalization issue.', 'before using the dca method, we applied a russian morphological processor  #TAUTHOR_TAG to convert each word in the text to its main form : nominative case singular for nouns and adjectives, infinitive for verbs, etc..', 'for words that could be normalized to several main forms ( polysemy ), when secondary forms of different words coincided, we retained all the main forms.', 'since the documents in the bbc news corpus were rather short, we applied the cache module, as described in section 11. 1.', 'this allowed us to reuse information across the documents']",5
"['one sense per collocation "" idea of  #TAUTHOR_TAG']","['one sense per collocation "" idea of  #TAUTHOR_TAG']","['one sense per collocation "" idea of  #TAUTHOR_TAG']","['kuhn and de  #AUTHOR_TAG proposed a cache model that works as a kind of short - term', 'memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model. within certain limits, such a model can adapt itself to changes in word frequencies, depending on', 'the topic of the text passage. the dca system is similar in spirit to such dynamic adaptation : it applies word n -', 'grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. but unlike the cache model, it uses a multipass strategy.  #AUTHOR_TAG developed a way of', 'incorporating standard n - grams into the cache model, using mixtures of', ""language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tag"", '##ger. instead of decaying nonlocal information, we opted for not propagating it from one document for', 'processing of another. for handling very long documents with our method, however, the information decay strategy seems to be', 'the right way to proceed. mani and mac  #AUTHOR_TAG pointed out that little attention had been paid in the named - entity', 'recognition field to the discourse properties of proper names. they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context', ', advocating text - driven processing rather than reliance on pre - existing lists. the dca outlined in this article also uses nonlocal discourse context and does not heavily rely', 'on pre - existing word lists. it has been applied not only to the identification of proper names, as described in this article', ', but also to their classification ( mikheev, grover, and moens 1998 ).  #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse ( ""', 'one sense per discourse "" ). since then this idea has been applied to several tasks, including word sense disambiguation ( ya', ""##rowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ). gale, church, and yarowsky's observation is also used in our dc"", '##a, especially for the identification of abbreviations. in capitalized - word disambiguation, however, we use this assumption with caution and first', 'apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ). this is similar to "" one sense per collocation "" idea of  #TAUTHOR_TAG']",1
"['. g.,  #TAUTHOR_TAG.', 'words in such lists']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g.,  #TAUTHOR_TAG.', 'words in such lists']","['. g.,  #TAUTHOR_TAG.', 'words in such lists are usually supplemented with morphological and pos information (']","['first list on which our method relies is a list of common words.', 'this list includes common words for a given language, but no supplementary information such as pos or morphological information is required to be present in this list.', 'a variety of such lists for many languages are already available ( e. g.,  #TAUTHOR_TAG.', 'words in such lists are usually supplemented with morphological and pos information ( which is not required by our method ).', 'we do not have to rely on pre - existing resources, however.', 'a list of common words can be easily obtained automatically from a raw ( unannotated in any way ) text collection by simply collecting and counting lowercased words in it.', 'we generated such list from the nyt collection.', 'to account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower - cased at least three times in the nyt texts.', 'the list of common words that we developed from the nyt collection contained about 15, 000 english words']",0
['before  #TAUTHOR_TAG :'],['before  #TAUTHOR_TAG : 0. 28'],['before  #TAUTHOR_TAG :'],"['its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'the error rate on sentence boundaries in the brown corpus was not significantly worse than the lowest quoted before  #TAUTHOR_TAG : 0. 28 % vs. 0. 20 % error rate ).', 'on the wsj corpus our system performed slightly better than the combination of the alembic and satz systems described in  #AUTHOR_TAG ( 0. 44 % vs. 0. 5 % error rate ).', 'although these error rates seem to be very small, they are quite significant.', 'unlike general pos tagging, in which it is unfair to expect an error rate of less than 2 % because even human annotators have a disagreement rate of about 3 %, sentence boundaries are much less ambiguous ( with a disagreement of about 1 in 5, 000 ).', 'this shows that an error rate of 1 in 200 ( 0. 5 % ) is still far from reaching the disagreement level.', 'on the other hand, one error in 200 periods means that there is one error in every two documents in the brown corpus and one error in every four documents in the wsj corpus']",1
"['from labeled examples  #TAUTHOR_TAG.', 'the advantage of the named']","['from labeled examples  #TAUTHOR_TAG.', 'the advantage of the namedentity']","['from labeled examples  #TAUTHOR_TAG.', 'the advantage of the namedentity approach is that the system not only identifies proper names but also']","['the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'named - entity recognition systems usually use sets of complex hand - crafted rules that employ a gazetteer and a local context ( krupa and hausman 1998 ).', 'in some systems such dependencies are learned from labeled examples  #TAUTHOR_TAG.', 'the advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'the disadvantage is in the cost of building a wide - coverage set of contextual clues manually or producing annotated training data.', 'also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port']",0
"[')  #TAUTHOR_TAG.', 'we used these texts']","[')  #TAUTHOR_TAG.', 'we used these texts']","[')  #TAUTHOR_TAG.', 'we used these texts']","['abbreviation ( as opposed to regular word ) these four lists can be acquired completely automatically from raw ( unlabeled ) texts.', 'for the development of these lists we used a collection of texts of about 300, 000 words derived from the new york times ( nyt ) corpus that was supplied as training data for the 7th message understanding conference ( muc - 7 )  #TAUTHOR_TAG.', 'we used these texts because the approach described in this article was initially designed to be part of a named - entity recognition system ( mikheev, grover, and moens 1998 ) developed for muc - 7.', 'although the corpus size of 300, 000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on - line sources ( including the internet ) makes this resource cheap to obtain']",5
"['by  #TAUTHOR_TAG, who trained a']","['by  #TAUTHOR_TAG, who trained a']","['. 1995 ) : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by  #TAUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in']","['', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by  #TAUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in  #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
"['identification.', 'one of the better - known approaches is described in  #TAUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbrevi']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in  #TAUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in  #TAUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbrevi']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in  #TAUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then reused in further processing.', 'this is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'the main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', ' #AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', '']",0
['##uation  #TAUTHOR_TAG and named -'],"['or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation  #TAUTHOR_TAG and named - entity']","[').', 'since then this idea has been applied to several tasks, including word sense disambiguation  #TAUTHOR_TAG and named -']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de  #AUTHOR_TAG proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', "" #AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac  #AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation  #TAUTHOR_TAG and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', 'this']",0
['problem can be found in  #TAUTHOR_TAG'],"['the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in  #TAUTHOR_TAG']","['the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in  #TAUTHOR_TAG']","['important task of text normalization is sentence boundary disambiguation ( sbd ) or sentence splitting.', 'segmenting text into sentences is an important aspect in developing many applications : syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. sentence splitting in most cases is a simple matter : a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'in certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end - of - sentence indicator ( fullstop ).', 'a detailed introduction to the sbd problem can be found in  #TAUTHOR_TAG']",0
"['reported in  #TAUTHOR_TAG, which do not heavily rely on word capitalization and are not']","['pos tagger reported in  #TAUTHOR_TAG, which do not heavily rely on word capitalization and are not']","['reported in  #TAUTHOR_TAG, which do not heavily rely on word capitalization and are not']","['all its strong points, there are a number of restrictions to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz ( palmer and hearst 1997 ) or the pos tagger reported in  #TAUTHOR_TAG, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']",1
"['right way to proceed.', ' #TAUTHOR_TAG pointed out that little']","['right way to proceed.', ' #TAUTHOR_TAG pointed out that little']","['method, however, the information decay strategy seems to be the right way to proceed.', ' #TAUTHOR_TAG pointed out that little attention had been paid in']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de  #AUTHOR_TAG proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', "" #AUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', ' #TAUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams ).', '']",5
"['.', 'unlike other pos taggers, this pos tagger  #TAUTHOR_TAG was also trained to']","['4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger  #TAUTHOR_TAG was also trained to']","['evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger  #TAUTHOR_TAG was also trained to']","['test our hypothesis that dca can be used as a complement to a local - context approach, we combined our main configuration ( evaluated in row d of table 4 ) with a pos tagger.', 'unlike other pos taggers, this pos tagger  #TAUTHOR_TAG was also trained to disambiguate sentence boundaries']",5
['##ic system  #TAUTHOR_TAG : a 0'],['system  #TAUTHOR_TAG : a 0. 5'],"[' #TAUTHOR_TAG : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by  #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in']","['c of table 4 summarizes the highest results known to us ( for all three tasks ) produced by automatic systems on the brown corpus and the wsj corpus.', 'state - of - theart machine learning and rule - based sbd systems achieve an error rate of 0. 8 - 1. 5 % measured on the brown corpus and the wsj corpus.', 'the best performance on the wsj corpus was achieved by a combination of the satz system ( palmer and hearst 1997 ) with the alembic system  #TAUTHOR_TAG : a 0. 5 % error rate.', 'the best performance on the brown corpus, a 0. 2 % error rate, was reported by  #AUTHOR_TAG, who trained a decision tree classifier on a 25 - million - word corpus.', 'in the disambiguation of capitalized words, the most widespread method is pos tagging, which achieves about a 3 % error rate on the brown corpus and a 5 % error rate on the wsj corpus, as reported in  #AUTHOR_TAG.', 'we are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the brown corpus or the wsj corpus']",1
['satz  #TAUTHOR_TAG or'],['satz  #TAUTHOR_TAG or'],"['satz  #TAUTHOR_TAG or the pos tagger reported in  #AUTHOR_TAG, which do not heavily rely on word capitalization and are not']","['all its strong points, there are a number of restrictions to the proposed approach.', 'first, in its present form it is suitable only for processing of reasonably "" wellbehaved "" texts that consistently use capitalization ( mixed case ) and do not contain much noisy data.', 'thus, for instance, we do not expect our system to perform well on single - cased texts ( e. g., texts written in all capital or all lower - cased letters ) or on optical character reader - generated texts.', 'we noted in section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'this is where robust syntactic systems like satz  #TAUTHOR_TAG or the pos tagger reported in  #AUTHOR_TAG, which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage']",1
"['uses a multipass strategy.', ' #TAUTHOR_TAG developed a way of incorporating standard n - grams into']","['uses a multipass strategy.', ' #TAUTHOR_TAG developed a way of incorporating standard n - grams into']","['it uses a multipass strategy.', ' #TAUTHOR_TAG developed a way of incorporating standard n - grams into']","['use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'kuhn and de  #AUTHOR_TAG proposed a cache model that works as a kind of short - term memory by which the probability of the most recent n words is increased over the probability of a general - purpose bigram or trigram model.', 'within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'the dca system is similar in spirit to such dynamic adaptation : it applies word n - grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'but unlike the cache model, it uses a multipass strategy.', "" #TAUTHOR_TAG developed a way of incorporating standard n - grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last"", 'in our experiments we applied simple linear interpolation to incorporate the dca system into a pos tagger.', 'instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'for handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'mani and mac  #AUTHOR_TAG pointed out that little attention had been paid in the named - entity recognition field to the discourse properties of proper names.', 'they proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text - driven processing rather than reliance on pre - existing lists.', 'the dca outlined in this article also uses nonlocal discourse context and does not heavily rely on pre - existing word lists.', 'it has been applied not only to the identification of proper names, as described in this article, but also to their classification ( mikheev, grover, and moens 1998 ).', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse ( "" one sense per discourse "" ).', 'since then this idea has been applied to several tasks, including word sense disambiguation ( yarowsky 1995 ) and named - entity recognition ( cucerzan and yarowsky 1999 ).', ""gale, church, and yarowsky's observation is also used in our dca, especially for the identification of abbreviations."", 'in capitalized - word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts ( n - grams']",2
['workbench for linguistic engineering  #TAUTHOR_TAG mentions a'],['workbench for linguistic engineering  #TAUTHOR_TAG mentions a'],['description of the eagle workbench for linguistic engineering  #TAUTHOR_TAG mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position'],"['description of the eagle workbench for linguistic engineering  #TAUTHOR_TAG mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document.', 'this heuristic also employs a database of bigrams and unigrams of lower - cased and capitalized words found in unambiguous positions.', 'it is quite similar to our method for capitalized - word disambiguation.', 'the description of the eagle case normalization module provided by baldwin et al. is, however, very brief and provides no performance evaluation or other details']",1
"['( palmer and hearst 1994 ), and maximum - entropy modeling  #TAUTHOR_TAG.', 'machine']","['( palmer and hearst 1994 ), and maximum - entropy modeling  #TAUTHOR_TAG.', 'machine']","['decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling  #TAUTHOR_TAG.', 'machine learning systems treat the sbd task as a classification problem, using features']","['trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'thus, the second class of sbd systems employs machine learning techniques such as decision tree classifiers ( riley 1989 ), neural networks ( palmer and hearst 1994 ), and maximum - entropy modeling  #TAUTHOR_TAG.', 'machine learning systems treat the sbd task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence - terminating punctuation sign.', 'although training of such systems is completely automatic, the majority of machine learning approaches to the sbd task require labeled examples for training.', 'this implies an investment in the data annotation phase']",0
"['we will use these two terms and the term case normalization interchangeably.', ' #TAUTHOR_TAG, p. 294 ) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that `']","['we will use these two terms and the term case normalization interchangeably.', ' #TAUTHOR_TAG, p. 294 ) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that ` ` sometimes']","['derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', ' #TAUTHOR_TAG, p. 294 ) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that `']","['##uation of capitalized words in mixed - case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'in mixed - case texts capitalized words usually denote proper names ( names of organizations, locations, people, artifacts, etc. ), but there are special positions in the text where capitalization is expected.', 'such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'capitalized words in these and some other positions present a case of ambiguity : they can stand for proper names, as in white later said...', ', or they can be just capitalized common words, as in white elephants are....', 'the disambiguation of capitalized words in ambiguous positions leads to the identification of proper names ( or their derivatives ), and in this article we will use these two terms and the term case normalization interchangeably.', "" #TAUTHOR_TAG, p. 294 ) studied, among other simple text normalization techniques, the effect of case normalization for different words and showed that ` ` sometimes case variants refer to the same thing ( hurricane and hurricane ), sometimes they refer to different things ( continental and continental ) and sometimes they don't refer to much of anything ( e. g., anytime and anytime ).''"", 'obviously these differences arise because some capitalized words stand for proper names ( such as continental, the name of an airline ) and some do not']",0
"['.', ' #TAUTHOR_TAG recently described a hybrid method']","['is similar to other documents.', ' #TAUTHOR_TAG recently described a hybrid method']","['is similar to other documents.', ' #TAUTHOR_TAG recently described a hybrid method']","['much information has been published on abbreviation identification.', 'one of the better - known approaches is described in  #AUTHOR_TAG, which suggested that abbreviations first be extracted from a corpus using abbreviation - guessing heuristics akin to those described in section 6 and then reused in further processing.', 'this is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'the main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', ' #TAUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions.', '']",0
"['.', 'for instance,  #TAUTHOR_TAG report that']","['optimal performance.', 'for instance,  #TAUTHOR_TAG report that']","['optimal performance.', 'for instance,  #TAUTHOR_TAG report that the satz system (']","['significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'the four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'although some sbd systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'for instance,  #TAUTHOR_TAG report that the satz system ( decision tree variant ) was trained on a set of about 800 labeled periods, which corresponds to a corpus of about 16, 000 words.', ""this is a relatively small training set that can be manually marked in a few hours'time."", 'but the error rate ( 1. 5 % ) of the decision tree classifier trained on this small sample was about 50 % higher than that when trained on 6, 000 labeled examples ( 1. 0 % )']",1
"['; veale and way 1997 ;  #TAUTHOR_TAG. 7 as an example, consider the translation into french of the house collapsed']","['; veale and way 1997 ;  #TAUTHOR_TAG. 7 as an example, consider the translation into french of the house collapsed']","['longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ; veale and way 1997 ;  #TAUTHOR_TAG. 7 as an example, consider the translation into french of the house collapsed']","['translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'in order to calculate a ranking for each tl sentence produced, we multiply the weights of each chunk used in its construction.', 'note that this ensures that greater importance is attributed to longer chunks, as is usual in most ebmt systems ( cfxxx sato and nagao 1990 ; veale and way 1997 ;  #TAUTHOR_TAG. 7 as an example, consider the translation into french of the house collapsed']",0
